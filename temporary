::::::::::::::
CEMAC/carbon_isotopes/plot_d13c.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_d13c.py
#PURPOSE
#    This program will plot d13c from ocean tracer 15 in one of jennies pg files
#
# search for 'main program' to find end of functions
# Julia 28/7/2016



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
from mpl_toolkits.basemap import Basemap, shiftgrid


#functions are:
#  def plotdata
#  def surf_lat_lon

# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,minval,maxval,valinc,V,uselog,cbarname):
    lons, lats = np.meshgrid(lon,lat)
    if fileno != 99:
        plt.subplot(2,2,fileno+1)

   # this is good for a tropical region
   # map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')
   # this is good for the globe

    map=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',resolution='c')
    #map.drawmapboundary(fill_color='aqua')
    map.drawmapboundary

    x, y = map(lons, lats)

    map.drawcoastlines()
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal")
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu_r')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='max')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                if uselog =='i': #increasing
                    cs = map.contourf(x,y,plotdata,V,norm=mp.colors.LogNorm(vmin=0,vmax=32),cmap='Reds')
                    cbar = plt.colorbar(cs,orientation="horizontal")
                else:
                    print(np.shape(plotdata))
                    cs = map.contourf(x,y,plotdata,V,extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")


    if fileno != 99:
        plt.title(titlename)
        cbar.set_label(cbarname,labelpad=-40)
    else:
        cbar.set_label(cbarname,labelpad=-70,size=20)
        cbar.ax.tick_params(labelsize=20)
        plt.title(titlename,loc='left',fontsize=20)
   


#end def plotdata

def surf_lat_lon(filein):
    # switch is a dummy variable to allow the program to be called
    #==============
    # preindustrial


    # read in data from multiple files
    print(filein)
    f=Dataset(filein)
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]
    atemp=f.variables['otracer14_2'][:]
    atemp=np.squeeze(atemp)
    nz,ny,nx=np.shape(atemp)
    print(nz,ny,nx)
    
#average across the surface layer
    c13_modelunits=atemp[0,:,:]
    c13_modelunits_10=atemp[15,:,:]

    d13C=(c13_modelunits-100.)*10. 
    d13C_10=(c13_modelunits_10-100.)*10. 

    
    plt.figure(0)
    lontemp=lon
    d13C,lon = shiftgrid(180.,d13C,lon,start=False)
    lon=lontemp
    d13C_10,lon = shiftgrid(180.,d13C_10,lon,start=False)
    
    print('about to plot')

    plotdata(d13C,0,lon,lat,'d13C',0,3,0.2,0.0,'n','permille')
    plotdata(d13C_10,1,lon,lat,'d13C_10',0,3,0.2,0.0,'n','permille')
    plt.show()
    print('plotted first')
    f.close()

#    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_surftemp/MAT_anom_only_'+pliop2_expt+'.eps' 
#    plt.savefig(fileout, bbox_inches='tight')  

#    plt.close()

 

#end def annmean





#end def annmean

################################
# main program

# annual mean
figureno=0
# biology only
#filein='/nfs/see-fs-02_users/earjcti/temporary/jennie_data/xnbqpo#pg000002073c1+.nc'
# biology and air sea
#filein='/nfs/see-fs-02_users/earjcti/temporary/jennie_data/xnbqoo#pg000002038c1+.nc'
# biology and air sea after one year
filein='/nfs/see-fs-02_users/earjcti/temporary/jennie_data/xnbqoo#pg000000001c1+.nc'
surf_lat_lon(filein)


sys.exit(0)

####

::::::::::::::
CEMAC/database_average_HadGEM/Database_temperature_annual_pre_HadCM3.py
::::::::::::::
#NAME
#    Database_temperature_annual.py
#PURPOSE 
#
#  This program will create database like files for annual averages.
#  it is actually used for Dan Hills energy balance calculation
#
#  The program will also get 
#         ?????_Annual_Average_a@pd_TotalPrecipitationRate.nc
#         ?????_Annaul_Average_a@pd_TotalCloud.nc
#
#
# Julia 8.2.2017


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid




#=====================================================
def get_annual_data(expt,startyear,nyears,field):

# this function will extract the data and write out to a file

    outdir='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/database_averages/'+expt+'_Annual_Average'
    infile='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/netcdf/'+expt+'a@pd'

    # loop over all years
    for year in range(startyear,startyear+nyears):
        century=np.floor(year/100.)
        choices = {10 : 'a', 11: 'b',  12: 'c',  13: 'd', 14: 'e', 
                   15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j',  
                   20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o',
                   25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't',
                   30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35:'z'} 
        
        extra=choices.get(century,'unknown') 
        yearuse=np.int(year-(century * 100))
    
        filename=infile+extra+np.str(yearuse)+'*.nc'
        print(filename)
          
 
        f=MFDataset(filename)
        if year == startyear:
            latin = f.variables['latitude'][:]
            latsize=len(latin)
            lonin = f.variables['longitude'][:]
            lonsize=len(lonin)
        
            print(nyears,latsize,lonsize)
            allvar=np.zeros((nyears,latsize,lonsize))

            
        varreq=f.variables[field][:] 
        
        allvar[year-startyear,:,:]=np.mean(varreq,axis=0)
        f.close()

    print('averaging now')
    avgvar=np.mean(allvar,axis=0)

    # write average variable out to a netcdf file

    # set up filename

    fout=outdir+'_a@pd_'+field+'.nc'
    if field == 'temp_1':
        fout=outdir+'_a@pd_Temperature.nc'
    if field == 'precip_1':
        fout=outdir+'_a@pd_TotalPrecipitationRate.nc'
    if field == 'field30':
        fout=outdir+'_a@pd_TotalCloud.nc'
    print(fout)
 
    f2=Dataset(fout,mode='w',format='NETCDF3_CLASSIC')
    # create dimensions
    lon=f2.createDimension('longitude',lonsize)
    lat=f2.createDimension('latitude',latsize)
    level=f2.createDimension('ht',1)
    time=f2.createDimension('time',1)
    # create variables
    lons=f2.createVariable('longitude',np.float32,('longitude',))
    lats=f2.createVariable('latitude',np.float32,('latitude',))
    levels=f2.createVariable('ht',np.float32,('ht',))
    times=f2.createVariable('time',np.float32,('time',))
    if field == 'temp_1':
        varfield=f2.createVariable('temp',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TEMPERATURE AT 1.5M"
        unitsname=u"K"
    if field == 'precip_1':
        varfield=f2.createVariable('precip',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TOTAL PRECIPITATION RATE KG/M2/S"
        unitsname=u"kg m-2"
    if field == 'field30':
        varfield=f2.createVariable('field30',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TOTAL CLOUD AMOUNT - RANDOM OVERLAP"
        unitsname=u"0-1"
    if field == 'field201':
        varfield=f2.createVariable('field201',np.float32,
                              ('time','ht','latitude','longitude'))
        longname=u"OUTGOING SW RAD FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'field208':
        varfield=f2.createVariable('field208',np.float32,
                              ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) DOWN SURFACE SW FLUX"
        unitsname=u"W m-2"
    if field == 'field207_1':
        varfield=f2.createVariable('field207_1',np.float32,
                              ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) UP SURFACE SW FLUX"
        unitsname=u"W m-2"
    if field == 'field200':
        varfield=f2.createVariable('field200',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"INCOMING SW RAD FLUX (TOA): ALL TSS"
        unitsname=u"W m-2"
    if field == 'field207':
        varfield=f2.createVariable('field207',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) UPWARD SW FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'ilr':
        varfield=f2.createVariable('ilr',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"DOWNWARD LW RAD FLUX: SURFACE"
        unitsname=u"W m-2"
    if field == 'olr':
        varfield=f2.createVariable('olr',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"OUTGOING LW RAD FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'csolr':
        varfield=f2.createVariable('field207',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) UPWARD LW FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'longwave':
        varfield=f2.createVariable('longwave',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"NET DOWN SURFACE LW RAD FLUX"
        unitsname=u"W m-2"
    if field == 'solar':
        varfield=f2.createVariable('solar',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"NET DOWN SURFACE SW FLUX: SW TS ONLY"
        unitsname=u"W m-2"
    if field == 'field203':
        varfield=f2.createVariable('field203',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TOTAL DOWNWARD SURFACE SW FLUX"
        unitsname=u"W m-2"
   
    # create variable attributes
    lons.setncatts({'units':u"degrees_east"}) 
    lats.setncatts({'units':u"degrees_north"})
    levels.setncatts({'units':u"m"})
    times.setncatts({'units':u"days since 0000-01-01 00.00",\
                         'calendar':"360_day"})
    varfield.setncatts({'long_name': longname,\
                            'units':unitsname})
    # assign data to variables
    lons[:]=lonin
    lats[:]=latin
    levels[:]=-1.0
    times[:]=0.0
    varfield[0,0,:,:]=avgvar
        
    f2.close()
        



    return 


#=================================================================
# MAIN PROGRAM STARTS HERE

expt='xkvje'
startyear=2350
nyears=50
#field='temp_1'
#get_annual_data(expt,startyear,nyears,field)

#field='field200'
#get_annual_data(expt,startyear,nyears,field)

#field='field207'
#get_annual_data(expt,startyear,nyears,field)

#field='ilr'
#get_annual_data(expt,startyear,nyears,field)
#field='olr'
#get_annual_data(expt,startyear,nyears,field)
#field='csolr'
#get_annual_data(expt,startyear,nyears,field)

#field='longwave'
#get_annual_data(expt,startyear,nyears,field)
#field='field201'
#get_annual_data(expt,startyear,nyears,field)


#field='field208'
#get_annual_data(expt,startyear,nyears,field)
#field='field207_1'
#get_annual_data(expt,startyear,nyears,field)

#field='field203'
#get_annual_data(expt,startyear,nyears,field)
#field='solar'
#get_annual_data(expt,startyear,nyears,field)

field='field30'
get_annual_data(expt,startyear,nyears,field)

    

sys.exit()
::::::::::::::
CEMAC/database_average_HadGEM/Database_temperature_annual.py
::::::::::::::
#NAME
#    Database_temperature_annual.py
#PURPOSE 
#
#  This program will create database like files for annual averages.
#  it is actually used for Dan Hills energy balance calculation
#
#  The program will also get 
#         ?????_Annual_Average_a@pd_TotalPrecipitationRate.nc
#         ?????_Annaul_Average_a@pd_TotalCloud.nc
#
#
# Julia 8.2.2017
# Julia 20.10.2018 ; included the ability to create database HadCM3 files


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
#from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid


def get_glob_avg(varreq, latin, field):
    """
    gets the global average of the fields
    """
    zonalmean = np.mean(varreq, axis=(0,1,3))
    latrad = latin * 2. * np.pi / 360.
    coslatrad = np.cos(latrad)
    
    globavg = np.sum(zonalmean * coslatrad) / np.sum(coslatrad)
    if field == 'temp' or field =='temp_1':
        globavg = globavg-273.15
    if field == 'precip_1':
        globavg = globavg * 60. * 60. *24.
   
    return globavg


#=====================================================
def get_annual_data(expt,startyear,nyears,field):

# this function will extract the data and write out to a file

    if HadCM3 == 'y':
        outdir='/nfs/hera1/earjcti/um/'+expt+'/database_averages/'+expt+'_Annual_Average'
       # infile='/nfs/hera1/earjcti/um/'+expt+'/netcdf/'+expt+'a@pd'
        infile='/nfs/hera1/earxh/'+expt+'/'+expt+'-netcdf/'+expt+'a@pd'
        txtfile='/nfs/hera1/earjcti/um/'+expt+'/database_averages/'+expt+'_'+ field + '_global_avg.txt'
    else:
        outdir='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/database_averages/'+expt+'_Annual_Average'
        infile='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/netcdf/pdfiles/'+expt+'a@pd'
        txtfile='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/database_averages/'+expt+'_'+ field + '_global_avg.txt'
    ftxt=open(txtfile,'w')

    # loop over all years
    for year in range(startyear,startyear+nyears):
        century=np.floor(year/100.)
        choices = {10 : 'a', 11: 'b',  12: 'c',  13: 'd', 14: 'e', 
                   15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j',  
                   20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o',
                   25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't',
                   30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35:'z'} 
        
        extra=choices.get(century,'unknown') 
        yearuse=np.int(year-(century * 100))
    
        if yearuse < 10:
            filename=infile+extra+'0'+np.str(yearuse)+'*.nc'
        else:
            filename=infile+extra+np.str(yearuse)+'*.nc'
        
        print('j1',filename,yearuse)
        f=MFDataset(filename)
       
        if year == startyear:
            latin = f.variables['latitude'][:]
            latsize=len(latin)
            lonin = f.variables['longitude'][:]
            lonsize=len(lonin)
        
            print(nyears,latsize,lonsize)
            allvar=np.zeros((nyears,latsize,lonsize))
            timeseries=np.zeros(nyears)

            
        varreq=f.variables[field][:]
        globavg = get_glob_avg(varreq, latin,field)
        
        allvar[year-startyear,:,:]=np.mean(varreq,axis=0)
        timeseries[year-startyear]=globavg
        ftxt.write(np.str(year) + ',' + np.str(globavg) + '\n')
   
        f.close()
    ftxt.close()

    print('averaging now', np.shape(allvar))
    avgvar=np.mean(allvar,axis=0)

    # write average variable out to a netcdf file

    # set up filename

    fout=outdir+'_a@pd_'+field+'.nc'
    if ((field == 'temp_1' and HadCM3 !='y')
         or (field == 'temp' and HadCM3 =='y')):
        fout=outdir+'_a@pd_Temperature.nc'
    if field == 'precip_1':
        fout=outdir+'_a@pd_TotalPrecipitationRate.nc'
    if field == 'field30':
        fout=outdir+'_a@pd_TotalCloud.nc'
    print(fout)
 
    f2=Dataset(fout,mode='w',format='NETCDF3_CLASSIC')
    # create dimensions
    lon=f2.createDimension('longitude',lonsize)
    lat=f2.createDimension('latitude',latsize)
    level=f2.createDimension('ht',1)
    time=f2.createDimension('time',1)
    # create variables
    lons=f2.createVariable('longitude',np.float32,('longitude',))
    lats=f2.createVariable('latitude',np.float32,('latitude',))
    levels=f2.createVariable('ht',np.float32,('ht',))
    times=f2.createVariable('time',np.float32,('time',))
    if ((field == 'temp_1' and HadCM3 !='y')
         or (field == 'temp' and HadCM3 =='y')):
        varfield=f2.createVariable('temp',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TEMPERATURE AT 1.5M"
        unitsname=u"K"
    if field == 'precip_1':
        varfield=f2.createVariable('precip',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TOTAL PRECIPITATION RATE KG/M2/S"
        unitsname=u"kg m-2"
    if field == 'field30':
        varfield=f2.createVariable('field30',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TOTAL CLOUD AMOUNT - RANDOM OVERLAP"
        unitsname=u"0-1"
    if field == 'field201':
        varfield=f2.createVariable('field201',np.float32,
                              ('time','ht','latitude','longitude'))
        longname=u"OUTGOING SW RAD FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'field207_1':
        varfield=f2.createVariable('field207_1',np.float32,
                              ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) UP SURFACE SW FLUX"
        unitsname=u"W m-2"
    if field == 'field200':
        varfield=f2.createVariable('field200',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"INCOMING SW RAD FLUX (TOA): ALL TSS"
        unitsname=u"W m-2"
    if field == 'field207':
        varfield=f2.createVariable('field207',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) UPWARD SW FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'ilr':
        varfield=f2.createVariable('ilr',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"DOWNWARD LW RAD FLUX: SURFACE"
        unitsname=u"W m-2"
    if field == 'olr':
        varfield=f2.createVariable('olr',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"OUTGOING LW RAD FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'p_1':
        varfield=f2.createVariable('p_1',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"PSTAR AFTER TIMESTEP"
        unitsname=u"Pa"
    if field == 'csolr':
        varfield=f2.createVariable('field207',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) UPWARD LW FLUX (TOA)"
        unitsname=u"W m-2"
    if field == 'longwave':
        varfield=f2.createVariable('longwave',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"NET DOWN SURFACE LW RAD FLUX"
        unitsname=u"W m-2"
    if field == 'solar':
        varfield=f2.createVariable('solar',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"NET DOWN SURFACE SW FLUX: SW TS ONLY"
        unitsname=u"W m-2"
    if field == 'field203':
        varfield=f2.createVariable('field203',np.float32,
                                   ('time','ht','latitude','longitude'))
        longname=u"TOTAL DOWNWARD SURFACE SW FLUX"
        unitsname=u"W m-2"
    if field == 'field208':
        varfield=f2.createVariable('field208',np.float32,
                              ('time','ht','latitude','longitude'))
        longname=u"CLEAR-SKY (II) DOWN SURFACE SW FLUX"
        unitsname=u"W m-2"

   
    # create variable attributes
    lons.setncatts({'units':u"degrees_east"}) 
    lats.setncatts({'units':u"degrees_north"})
    levels.setncatts({'units':u"m"})
    times.setncatts({'units':u"days since 0000-01-01 00.00",\
                         'calendar':"360_day"})
    varfield.setncatts({'long_name': longname,\
                            'units':unitsname})
    # assign data to variables
    lons[:]=lonin
    lats[:]=latin
    levels[:]=-1.0
    times[:]=0.0
    varfield[0,0,:,:]=avgvar
        
    f2.close()
        



    return 


#=================================================================
# MAIN PROGRAM STARTS HERE

#expt='xkvje'
#startyear=2301
#nyears=100
#HadCM3='n'

#expt='xogzl'
#startyear=2980
#nyears=50
#HadCM3='y'

expt='xoori'
startyear=2949
nyears=50
HadCM3='y'

#HadCM3='y'
#expt='xibos'
#startyear=3500
#nyears=50

#field='p_1'
field = 'temp'
get_annual_data(expt,startyear,nyears,field)
#sys.exit(0)
field='olr'
get_annual_data(expt,startyear,nyears,field)

field='field200'
get_annual_data(expt,startyear,nyears,field)

field='field201'
get_annual_data(expt,startyear,nyears,field)

#field='precip_1'
#get_annual_data(expt,startyear,nyears,field)


#temperature at 1.5m
if HadCM3=='y':
    field='temp'   # temperature at 1.5m in HadCM3
else:
    field='temp_1' # temperature at 1.5m in HadGEM
get_annual_data(expt,startyear,nyears,field)



field='field207'
get_annual_data(expt,startyear,nyears,field)

field='ilr'
get_annual_data(expt,startyear,nyears,field)
field='csolr'
get_annual_data(expt,startyear,nyears,field)

field='longwave'
get_annual_data(expt,startyear,nyears,field)



field='field203'
get_annual_data(expt,startyear,nyears,field)
field='solar'
get_annual_data(expt,startyear,nyears,field)

field='field30'
get_annual_data(expt,startyear,nyears,field)

field='field208'
get_annual_data(expt,startyear,nyears,field)

field='field207_1'
get_annual_data(expt,startyear,nyears,field)
    

sys.exit()
::::::::::::::
CEMAC/database_average_HadGEM/Database_temperature.py
::::::::::::::
#NAME
#    Database_temperature.py
#PURPOSE 
#    Alan wants us to be able to run Biome 4 for the HadGEM jobs.  
#    The biome4 script currently seems to work okay but needs the files in the
#    database format.
#    Unfortunately the HadGEM files don't go on the database properly because 
#    of the variable naming. 
#
#    This program will create some files in Database format that can be run 
#    run with the BIOME4 scripts
#
#  The program will also get 
#         ?????_Monthly_Average_January_a@pd_TotalPrecipitationRate.nc
#         ?????_Monthly_Average_January_a@pd_TotalCloud.nc
#
#
# Julia 8.2.2017


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid




#=====================================================
def get_monthly_data(expt,startyear,nyears,field):

# this function will extract the data and write out to a file

    if HadCM3 == 'y':
        outdir='/nfs/hera1/earjcti/um/'+expt+'/database_averages/'+expt+'_Monthly_Average_'
#        infile='/nfs/hera1/earjcti/um/'+expt+'/netcdf/'+expt+'a@pd'
        infile='/nfs/hera1/earxh/'+expt+'/' + expt + '-netcdf/'+expt+'a@pd'
       
    else:
        outdir='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/database_averages/'+expt+'_Monthly_Average_'
        infile='/nfs/hera1/earjcti/um/HadGEM_data/'+expt+'/netcdf/'+expt+'a@pd'

    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']

    for month in range(0,len(monthnames)):
        
        # get long monthname for filenames
        choices = {'ja': 'January', 'fb': 'February', 'mr': 'March', 
                   'ar': 'April', 'my': 'May', 'jn': 'June', 'jl': 'July', 
                   'ag': 'August', 'sp': 'September', 'ot': 'October', 
                   'nv': 'November', 'dc': 'December'}

        longmonthname=choices.get(monthnames[month],month)

        # loop over all years
        for year in range(startyear,startyear+nyears):
            century=np.floor(year/100.)
            choices = {10 : 'a', 11: 'b',  12: 'c',  13: 'd', 14: 'e', 
                       15: 'f', 16: 'g', 17: 'h', 18: 'i', 19: 'j',  
                       20: 'k', 21: 'l', 22: 'm', 23: 'n', 24: 'o',
                       25: 'p', 26: 'q', 27: 'r', 28: 's', 29: 't',
                       30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35:'z'} 

            extra=choices.get(century, np.str(np.int(century))) 
            yearuse=np.int(year-(century * 100))
    
            if yearuse < 10:
                filename=infile+extra+'0' + np.str(yearuse)+monthnames[month]+'.nc'
            else:
               filename=infile+extra+np.str(yearuse)+monthnames[month]+'.nc'
            print(filename)
          
 
            f=Dataset(filename,mode='r')
            if year == startyear:
                latin = f.variables['latitude'][:]
                latsize=len(latin)
                lonin = f.variables['longitude'][:]
                lonsize=len(lonin)
        
                print(nyears,latsize,lonsize)
                allvar=np.zeros((nyears,latsize,lonsize))

            
            varreq=f.variables[field][:] 
        
            allvar[year-startyear,:,:]=varreq
            avgvar=np.mean(allvar,axis=0)
            f.close()

        # write average variable out to a netcdf file

        # set up filename

        fout=outdir+longmonthname+'_a@pd_'+field+'.nc'
        if field == 'temp_1' or field == 'temp':
            fout=outdir+longmonthname+'_a@pd_Temperature.nc'
        if field == 'precip_1' or field == 'precip':
            fout=outdir+longmonthname+'_a@pd_TotalPrecipitationRate.nc'
        if field == 'field30':
            fout=outdir+longmonthname+'_a@pd_TotalCloud.nc'
        print(fout)
 
        f2=Dataset(fout,mode='w',format='NETCDF3_CLASSIC')
        # create dimensions
        lon=f2.createDimension('longitude',lonsize)
        lat=f2.createDimension('latitude',latsize)
        level=f2.createDimension('ht',1)
        time=f2.createDimension('time',1)
        # create variables
        lons=f2.createVariable('longitude',np.float32,('longitude',))
        lats=f2.createVariable('latitude',np.float32,('latitude',))
        levels=f2.createVariable('ht',np.float32,('ht',))
        times=f2.createVariable('time',np.float32,('time',))
        if field == 'temp_1' or field == 'temp':
            varfield=f2.createVariable('temp',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"TEMPERATURE AT 1.5M"
            unitsname=u"K"
        if field == 'precip_1' or field =='precip':
            varfield=f2.createVariable('precip',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"TOTAL PRECIPITATION RATE KG/M2/S"
            unitsname=u"kg m-2"
        if field == 'field30':
            varfield=f2.createVariable('field30',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"TOTAL CLOUD AMOUNT - RANDOM OVERLAP"
            unitsname=u"0-1"
        if field == 'field201':
            varfield=f2.createVariable('field201',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"OUTGOING SW RAD FLUX (TOA)"
            unitsname=u"W m-2"
        if field == 'field200':
            varfield=f2.createVariable('field200',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"INCOMING SW RAD FLUX (TOA): ALL TSS"
            unitsname=u"W m-2"
        if field == 'field207':
            varfield=f2.createVariable('field207',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"CLEAR-SKY (II) UPWARD SW FLUX (TOA)"
            unitsname=u"W m-2"
        if field == 'ilr':
            varfield=f2.createVariable('ilr',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"DOWNWARD LW RAD FLUX: SURFACE"
            unitsname=u"W m-2"
        if field == 'olr':
            varfield=f2.createVariable('olr',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"OUTGOING LW RAD FLUX (TOA)"
            unitsname=u"W m-2"
        if field == 'csolr':
            varfield=f2.createVariable('field207',np.float32,
                              ('time','ht','latitude','longitude'))
            longname=u"CLEAR-SKY (II) UPWARD LW FLUX (TOA)"
            unitsname=u"W m-2"


        # create variable attributes
        lons.setncatts({'units':u"degrees_east"}) 
        lats.setncatts({'units':u"degrees_north"})
        levels.setncatts({'units':u"m"})
        times.setncatts({'units':u"days since 0000-01-01 00.00",\
                         'calendar':"360_day"})
        varfield.setncatts({'long_name': longname,\
                            'units':unitsname})
        # assign data to variables
        lons[:]=lonin
        lats[:]=latin
        levels[:]=-1.0
        times[:]=0.0
        varfield[0,0,:,:]=avgvar
        
        f2.close()
        



    return 


#=================================================================
# MAIN PROGRAM STARTS HERE

expt='xoorb'
startyear=2949
nyears=50
HadCM3 = 'y'

if HadCM3 == 'y':
    field = 'temp'
else:
    field='temp_1'
get_monthly_data(expt,startyear,nyears,field)

#if HadCM3 == 'y':
#    field = 'precip'
#else:
#    field='precip_1'

#get_monthly_data(expt,startyear,nyears,field)

field='field30'
get_monthly_data(expt,startyear,nyears,field)

field='ilr'
get_monthly_data(expt,startyear,nyears,field)
field='olr'
get_monthly_data(expt,startyear,nyears,field)
field='csolr'
get_monthly_data(expt,startyear,nyears,field)

    

sys.exit()
::::::::::::::
CEMAC/database_average_HadGEM/Database_Tmin.py
::::::::::::::
#NAME
#    Database_Tmin.py
#PURPOSE 
#   Biome4 needs the minimum monthly averaged temperature for each month
#   Steve P's script did this but it didn't seem to work very well so 
#   I am doing it here
#  (Need to run Database_temperature before we do this
# Julia 8.2.2017


# Import necessary libraries

import os
import iris
import iris.quickplot as qplt
import numpy as np
import matplotlib.pyplot as plt



expt='xoorb'
Monthnames = ['January','February','March','April','May','June',
              'July','August','September','October','November','December']

FILESTART = ('/nfs/hera1/earjcti/um/' + expt + '/database_averages/' + 
             expt + '_Monthly_Average_')
FILEEND = '_a@pd_Temperature.nc'
FILEOUT = ('/nfs/hera1/earjcti/um/' + expt + '/database_averages/' + 
             expt + '_Tmin.nc')
vals = np.arange(-90, 30, 5)

allcubes = iris.cube.CubeList([])
for i, month in enumerate(Monthnames):
    print(month)
    cube = iris.load_cube(FILESTART + month + FILEEND)
    if i == 0 or i == 7:
        if i == 0: plt.subplot(1,3,1)
        if i == 7: plt.subplot(1,3,2)
        qplt.contourf(iris.util.squeeze(cube) - 273.15, levels=vals)
        plt.title(month)
        plt.gca().coastlines()

        
        
    cube.coord('time').points = i
    allcubes.append(cube)

minval = np.zeros(allcubes[0].shape)
allmonth_cube = allcubes.concatenate_cube()

mincube = allmonth_cube.collapsed('time', iris.analysis.MIN)
iris.save(mincube, FILEOUT, netcdf_format='NETCDF3_CLASSIC')
plt.subplot(1,3,3)
qplt.contourf(iris.util.squeeze(mincube) - 273.15, levels=vals)
plt.title('min')
plt.gca().coastlines()
#plt.show()
::::::::::::::
CEMAC/Jochen/get_average_temp_old.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    get_average_temp.py
#PURPOSE
#    This program will get the long term annual average temperature from
#    both individual and database files.
#
#    It was originally written for Jochen who wanted to verify some values
#    in a spreadsheet of Fergus
#
# search for 'main program' to find end of functions
# Julia 3/8/2017



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
from mpl_toolkits.basemap import Basemap, shiftgrid


#functions are:
#  def plotdata  (not currently used)
#  def annmean_ind
#  def annmean_database

# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,minval,maxval,valinc,V,uselog,cbarname):
    lons, lats = np.meshgrid(lon,lat)
    if fileno != 99:
        plt.subplot(2,2,fileno+1)

   # this is good for a tropical region
   # map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')
   # this is good for the globe

    map=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',resolution='c')
    #map.drawmapboundary(fill_color='aqua')
    map.drawmapboundary

    x, y = map(lons, lats)

    map.drawcoastlines()
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal")
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu_r')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='max')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                if uselog =='i': #increasing
                    cs = map.contourf(x,y,plotdata,V,norm=mp.colors.LogNorm(vmin=0,vmax=32),cmap='Reds')
                    cbar = plt.colorbar(cs,orientation="horizontal")
                else:
                    cs = map.contourf(x,y,plotdata,V,extend='both',cmap='rainbow')
                    cbar = plt.colorbar(cs,orientation="horizontal")


    if fileno != 99:
        plt.title(titlename)
        cbar.set_label(cbarname,labelpad=-40)
    else:
        cbar.set_label(cbarname,labelpad=-70,size=20)
        cbar.ax.tick_params(labelsize=20)
        plt.title(titlename,loc='left',fontsize=20)
   


#end def plotdata

def annmean_ind(exptname,extra,startyear,nyears):

    # if nyears is a negative number read in all the files in the directory
    if nyears < 0:
        f=MFDataset('/nfs/hera2/apps/metadata/experiments/'+exptname+'/netcdf/'+exptname+'a@pd*.nc')
        lat = f.variables['latitude'][:]
        lon = f.variables['longitude'][:]

        atemp=f.variables['temp'][:] # this is temp at 1.5m
        atemp=np.squeeze(atemp)
        ntimes,ny,nx=np.shape(atemp)
    
        btemp=f.variables['temp_1'][:] # surface temperature after timestep
        btemp=np.squeeze(btemp)
        f.close()

    else:
        print('you need to set up for a subset of the directory')
        sys.exit()
        
    
    #average across the time dimension
    temp_1point5=np.mean(atemp,axis=0)
    temp_surf=np.mean(btemp,axis=0)

    # create weighting array
    weightarr=np.zeros(np.shape(temp_surf))
    for i in range(0,len(lon)):
        weightarr[:,i]=np.cos(np.deg2rad(lat))

    # find weighted mean

    avg_temp_surf=np.average(temp_surf,weights=weightarr)
    avg_temp_1point5=np.average(temp_1point5,weights=weightarr)

    print( )
    print('number of files is',ntimes,'for expt',exptname)
    print('Raw files average surface temperature=',avg_temp_surf-273.15)
    print('Raw files average 1.5m temperature=',avg_temp_1point5-273.15)

#end def annmean


def annmean_database(exptname,extra,startyear,nyears):

    f=Dataset('/nfs/hera2/apps/metadata/experiments/'+exptname+'/timeseries/'+exptname+'a@pd_SurfaceTemperature.nc')
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]

    atemp=f.variables['temp'][:] # this is temp at 1.5m
    atemp=np.squeeze(atemp)
    ntimes,ny,nx=np.shape(atemp)

    starttime=ntimes-(nyears*12)
    surface_temperature=atemp[starttime:ntimes,:,:]
       
    #average across the time dimension
    temp_surf=np.mean(surface_temperature,axis=0)

    # create weighting array
    weightarr=np.zeros(np.shape(temp_surf))
    for i in range(0,len(lon)):
        weightarr[:,i]=np.cos(np.deg2rad(lat))

    # find weighted mean

    avg_temp_surf=np.average(temp_surf,weights=weightarr)
   
    print('Database files average surface temperature ',exptname,'=',avg_temp_surf-273.15)

    return(avg_temp_surf-273.15)


#end def annmean



################################
# main program

# annual mean from individual files

exptname='xhckb'
extra='z'
startyear=70
nyears_ind=-10  # if negative number use all of them
nyears_db=30  # if negative number use all of them

annmean_ind(exptname,extra,startyear,nyears_ind)
database_tsurf=annmean_database(exptname,extra,startyear,nyears_db)


exptname='xiomv'
annmean_ind(exptname,extra,startyear,nyears_ind)
database_tsurf=annmean_database(exptname,extra,startyear,nyears_db)


exptname='xhckf'
annmean_ind(exptname,extra,startyear,nyears_ind)
database_tsurf=annmean_database(exptname,extra,startyear,nyears_db)


# experiments needed
# tdcza
# tdhzt
# tdlqa
# tdlqb
# tdlqc
# tdlqd
# tdlqe
# tdlqf
# tdlqg
# tdlqh
# tdlqi
# tdlqj
# tdlxf
# tdlxi
# xgraf
# xgrag
# xgrah
# xgrai
# xgygi
# xgygj
# xgygk
# xgygl
# xgygm
# xgygn
# xgygo
# xgygp
# xgygq
# xgygz
# xhckb
# xhckd
# xhckf
# xhckh
# xhcki
# xhckj
# xhckk
# xhckl
# xhckm
# xhckn
# xhcko
# xhckp
# xhckq
# xhckz
# xhgfk4
# xhsop4
# xhsor4
# xilug
# xiomv
# xjpli
# xhgfa
# tdipf
# tdkgg
# tdkgi
# tdkgf





sys.exit(0)

####

::::::::::::::
CEMAC/Jochen/get_average_temp.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    get_average_temp.py
#PURPOSE
#    This program will get the long term annual average temperature from
#    both individual and database files.
#
#    It was originally written for Jochen who wanted to verify some values
#    in a spreadsheet of Fergus
#
# search for 'main program' to find end of functions
# Julia 3/8/2017



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
from mpl_toolkits.basemap import Basemap, shiftgrid


#functions are:
#  def plotdata  (not currently used)
#  def annmean_ind
#  def annmean_database

# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,minval,maxval,valinc,V,uselog,cbarname):
    lons, lats = np.meshgrid(lon,lat)
    if fileno != 99:
        plt.subplot(2,2,fileno+1)

   # this is good for a tropical region
   # map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')
   # this is good for the globe

    map=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',resolution='c')
    #map.drawmapboundary(fill_color='aqua')
    map.drawmapboundary

    x, y = map(lons, lats)

    map.drawcoastlines()
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal")
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu_r')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='max')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                if uselog =='i': #increasing
                    cs = map.contourf(x,y,plotdata,V,norm=mp.colors.LogNorm(vmin=0,vmax=32),cmap='Reds')
                    cbar = plt.colorbar(cs,orientation="horizontal")
                else:
                    cs = map.contourf(x,y,plotdata,V,extend='both',cmap='rainbow')
                    cbar = plt.colorbar(cs,orientation="horizontal")


    if fileno != 99:
        plt.title(titlename)
        cbar.set_label(cbarname,labelpad=-40)
    else:
        cbar.set_label(cbarname,labelpad=-70,size=20)
        cbar.ax.tick_params(labelsize=20)
        plt.title(titlename,loc='left',fontsize=20)
   


#end def plotdata

def annmean_ind(exptname,extra,startyear,nyears):

    # if nyears is a negative number read in all the files in the directory
    if nyears < 0:

        # check if directory exists
        dirname='/nfs/hera2/apps/metadata/experiments/'+exptname+'/netcdf/'
        if os.path.isdir(dirname):
            pass
        else:
            allraw=[-999.99,-999.99,0]
            return(allraw)


        f=MFDataset(dirname+exptname[0:5]+'a@pd*.nc')
        lat = f.variables['latitude'][:]
        lon = f.variables['longitude'][:]

        atemp=f.variables['temp'][:] # this is temp at 1.5m
        atemp=np.squeeze(atemp)
        ntimes,ny,nx=np.shape(atemp)
    
        
        # check temp_1 is surface temperature after timestep
        varSAT=' '
        for name,variable in f.variables.items():
            if name == 'temp_1':
                lname=getattr(variable,'long_name')
                #print('t1',lname)
                if lname=='TEMP AT TROP LEVEL- NEED HT,PRESS':
                    # try temp 2
                    pass
                if lname=='SURFACE TEMPERATURE AFTER TIMESTEP':
                    varSAT='temp_1'
                if lname=='Temperature T': # we need to assume this is correctT
                                           # as we may not have a temp_2
                    if varSAT==' ':
                        varSAT='temp_1'

            if name == 'temp_2':
                lname=getattr(variable,'long_name')
                #print('t2',lname)
                if lname=='TEMP AT TROP LEVEL- NEED HT,PRESS':
                    # try temp 2
                    pass
                if lname=='SURFACE TEMPERATURE AFTER TIMESTEP':
                    varSAT='temp_2'

                    
                    

        btemp=f.variables[varSAT][:] # surface temperature after timestep
        btemp=np.squeeze(btemp)
        f.close()

        #sys.exit()

    else:
        print('you need to set up for a subset of the directory')
        sys.exit()
        
    
    #average across the time dimension
    temp_1point5=np.mean(atemp,axis=0)
    temp_surf=np.mean(btemp,axis=0)

    # create weighting array
    weightarr=np.zeros(np.shape(temp_surf))
    for i in range(0,len(lon)):
        weightarr[:,i]=np.cos(np.deg2rad(lat))

    # find weighted mean

    avg_temp_surf=np.average(temp_surf,weights=weightarr)-273.15
    avg_temp_1point5=np.average(temp_1point5,weights=weightarr)-273.15

    #print( )
    #print('number of files is',ntimes,'for expt',exptname)
    #print('Raw files average surface temperature=',avg_temp_surf-273.15)
    #print('Raw files average 1.5m temperature=',avg_temp_1point5-273.15)


    allraw=[avg_temp_surf,avg_temp_1point5,ntimes]
    return(allraw)

#end def annmean

#####################################################
def annmean_database(exptname,extra,startyear,nyears,fieldname):

     
    filename='/nfs/hera2/apps/metadata/experiments/'+exptname+'/timeseries/'+exptname[0:5]+'a@pd_'+fieldname+'.nc'

    if os.path.isfile(filename):
        pass
    else:
        return(-999.99)


    f=Dataset(filename)
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]

    atemp=f.variables['temp'][:] # this is temp at 1.5m
    atemp=np.squeeze(atemp)
    ntimes,ny,nx=np.shape(atemp)

    starttime=ntimes-(nyears*12)
    surface_temperature=atemp[starttime:ntimes,:,:]
       
    #average across the time dimension
    temp_surf=np.mean(surface_temperature,axis=0)

    # create weighting array
    weightarr=np.zeros(np.shape(temp_surf))
    for i in range(0,len(lon)):
        weightarr[:,i]=np.cos(np.deg2rad(lat))

    # find weighted mean

    avg_temp_surf=np.average(temp_surf,weights=weightarr)
   
    #print('Database files average surface temperature ',exptname,'=',avg_temp_surf-273.15)

    return(avg_temp_surf-273.15)


#end def annmean_database

##################################
def globmeans_database(exptname):

    filename_SAT='/nfs/hera2/apps/metadata/experiments/'+exptname+'/globalmean/'+exptname+'_Annual_Average_a@pd_SurfaceTemperature.dat'
    
    if os.path.isfile(filename_SAT):
        f=open(filename_SAT,'r')
        SAT_file=f.readline()
        # remove newline and convert to float
        SAT_t=SAT_file.split()
        SAT=float(SAT_t[0])-273.15
        f.close()
    else:
        SAT=-999.99


    filename_T='/nfs/hera2/apps/metadata/experiments/'+exptname+'/globalmean/'+exptname+'_Annual_Average_a@pd_Temperature.dat'

    if os.path.isfile(filename_T):
        f2=open(filename_T,'r')
        T_file=f2.readline()
        # remove newline and convert to float
        T_t=T_file.split()
        T=float(T_t[0])-273.15
        #print(T)
        f2.close()
    else:
        T=-999.99

    glob_means=[SAT,T]

    return(glob_means)




#end def globmeans_database

############################################
def month_avg_ind(exptname):

    monthnames=['January','February','March','April','May','June','July','August','September','October','November','December']

    # SAT

    ntime=0
    temp_surf=0
    for month in range(0,len(monthnames)):
        f=Dataset('/nfs/hera2/apps/metadata/experiments/'+exptname+'/averages/'+exptname+'_Monthly_Average_'+monthnames[month]+'_a@pd_SurfaceTemperature.nc')
        if month == 0:
            dims=(f.dimensions.keys())
            vars=(f.variables.keys())
            if len(dims) == 2:  # lon and lat only
                xname=dims[0]
                yname=dims[1]
            else:
                xname='longitude'
                yname='latitude'
    
            varname=vars[len(dims)]
    

        lat = f.variables[yname][:]
        lon = f.variables[xname][:]


        atemp=f.variables[varname][:] # this is temp at surface
        temp_surf=temp_surf+np.squeeze(atemp)
        ntime=ntime+1

 
    temp_surf=temp_surf/12.


    # check this is a 2 dimensional array
    ndims=len(np.shape(temp_surf))
    if ndims != 2:
        avg_temp_surf=-999.99
    else:
        # create weighting array
        weightarr=np.zeros(np.shape(temp_surf))
        for i in range(0,len(lon)):
            weightarr[:,i]=np.cos(np.deg2rad(lat))

        # find weighted mean

        avg_temp_surf=np.average(temp_surf,weights=weightarr)-273.15

    if ntime != 12:
        avg_temp_surf=-999.99

   

    # temperature
    ntime=0
    temp_1=0
    for month in range(0,len(monthnames)):
        f=Dataset('/nfs/hera2/apps/metadata/experiments/'+exptname+'/averages/'+exptname+'_Monthly_Average_'+monthnames[month]+'_a@pd_Temperature.nc')

        if month == 0:
            dims=(f.dimensions.keys())
            vars=(f.variables.keys())
            if len(dims) == 2:  # lon and lat only
                xname=dims[0]
                yname=dims[1]
            else:
                xname='longitude'
                yname='latitude'
    
            varname=vars[len(dims)]

        lat = f.variables[yname][:]
        lon = f.variables[xname][:]


        atemp=f.variables[varname][:] # this is temp at surface
        temp=np.squeeze(atemp)

        temp_1=temp_1+temp
        ntime=ntime+1

   
    # check this is a 2 dimensional array
    temp_1=temp_1/ntime
    ndims=len(np.shape(temp_1))
    if ndims != 2:
        avg_temp=-999.99
    else:
        # create weighting array
        weightarr=np.zeros(np.shape(temp_1))
        for i in range(0,len(lon)):
            weightarr[:,i]=np.cos(np.deg2rad(lat))
            
        # find weighted mean

        avg_temp=np.average(temp_1,weights=weightarr)-273.15
   
        #print('Average temp from average ',exptname,'=',avg_temp-273.15)


    if ntime != 12:
        avg_temp=-999.99


    mean_from_avg=[avg_temp_surf,avg_temp]
    return(mean_from_avg)


#end def annmean




############################################
def annmean_averages(exptname):

    notes=' '

    # SAT
    f=Dataset('/nfs/hera2/apps/metadata/experiments/'+exptname+'/averages/'+exptname+'_Annual_Average_a@pd_SurfaceTemperature.nc')
    dims=(f.dimensions.keys())
    vars=(f.variables.keys())
    if len(dims) == 2:  # lon and lat only
        xname=dims[0]
        yname=dims[1]
    else:
        xname='longitude'
        yname='latitude'
    
    varname=vars[len(dims)]
    

    lat = f.variables[yname][:]
    lon = f.variables[xname][:]

    if len(lat)!=73:
        if len(lat)==37:
            notes='FAMOUS'
        else:
            print('check dimensions')
            sys.exit()




    atemp=f.variables[varname][:] # this is temp at surface
    temp_surf=np.squeeze(atemp)

    # check this is a 2 dimensional array
    ndims=len(np.shape(temp_surf))
    if ndims != 2:
        avg_temp_surf=-999.99
    else:
        # create weighting array
        weightarr=np.zeros(np.shape(temp_surf))
        for i in range(0,len(lon)):
            weightarr[:,i]=np.cos(np.deg2rad(lat))

        # find weighted mean

        avg_temp_surf=np.average(temp_surf,weights=weightarr)-273.15
   

    # temperature
    f=Dataset('/nfs/hera2/apps/metadata/experiments/'+exptname+'/averages/'+exptname+'_Annual_Average_a@pd_Temperature.nc')
    dims=(f.dimensions.keys())
    vars=(f.variables.keys())
    if len(dims) == 2:  # lon and lat only
        xname=dims[0]
        yname=dims[1]
    else:
        xname='longitude'
        yname='latitude'
    
    varname=vars[len(dims)]

    lat = f.variables[yname][:]
    lon = f.variables[xname][:]

    atemp=f.variables[varname][:] # this is temp at surface
    temp=np.squeeze(atemp)
   
    # check this is a 2 dimensional array
    ndims=len(np.shape(temp))
    if ndims != 2:
        avg_temp=-999.99
    else:
        # create weighting array
        weightarr=np.zeros(np.shape(temp))
        for i in range(0,len(lon)):
            weightarr[:,i]=np.cos(np.deg2rad(lat))

        # find weighted mean

        avg_temp=np.average(temp,weights=weightarr)-273.15
   
        #print('Average temp from average ',exptname,'=',avg_temp-273.15)

    mean_from_avg=[avg_temp_surf,avg_temp,notes]
    return(mean_from_avg)


#end def annmean




################################
# main program

# annual mean from individual files

extra='z'
startyear=70
nyears_ind=-10  # if negative number use all of them
nyears_db=30  # if negative number use all of them

#allexpts=['xhckb','xiomv','xhckf']
allexpts=['tdcza','tdhzt','tdlqa','tdlqb','tdlqc','tdlqd','tdlqe','tdlqf','tdlqg','tdlqh','tdlqi','tdlqj','tdlxf','tdlxi','xgraf','xgrag','xgrah','xgrai','xgygi','xgygj','xgygk','xgygl','xgygm','xgygn','xgygo','xgygp','xgygq','xgygz','xhckb','xhckd','xhckf','xhckh','xhcki','xhckj','xhckk','xhckl','xhckm','xhckn','xhcko','xhckp','xhckq','xhckz','xhgfk4','xhsop4','xhsor4','xilug','xiomv','xjpli','xhgfa','tdipf','tdkgg','tdkgi','tdkgf']

allexpts=['xgrah']

#print('expt ','R_SAT','R_T1.5','Tseries','GM_SAT','GM_T1.5','avg_SAT','avg_1,5')
fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/CEMAC/Jochen/all_expts_mean.txt'
fo=open(fileout,'w')
fo.write('expt, nraw, R_SAT, R_T1.5, TS_S, TS_1.5, GM_SAT, GM_T1.5, Mo_SAT, Mo_1.5\n')


for exptname in allexpts:


    # get data from individual files
    allraw=annmean_ind(exptname,extra,startyear,nyears_ind)
    raw_temp_surf=allraw[0]
    raw_temp_1point5=allraw[1]
    ntimes=allraw[2]

    # get data from timeseries
#    database_tsurf=annmean_database(exptname,extra,startyear,nyears_db,'SurfaceTemperature')
    database_tsurf=annmean_database(exptname,extra,startyear,nyears_db,'SurfaceTemperature_old')
    database_t15=annmean_database(exptname,extra,startyear,nyears_db,'Temperature')

    # get data from global means
    globmeans_data=globmeans_database(exptname)
    globmeans_SAT=globmeans_data[0]
    globmeans_T=globmeans_data[1]

    # get data from annual averages
    mean_from_avg=annmean_averages(exptname)
    avg_SAT=mean_from_avg[0]
    avg_T=mean_from_avg[1]
    notes=mean_from_avg[2]


    # get data from monthly average individual months
    mean_monthavg=month_avg_ind(exptname)
    avg_SAT_month=mean_monthavg[0]
    avg_T_month=mean_monthavg[1]



    print(exptname,ntimes,round(raw_temp_surf,2),round(raw_temp_1point5,2),round(database_tsurf,2),round(globmeans_SAT,2),round(globmeans_T,2),round(avg_SAT,2),round(avg_T,2)),notes


    # if global means missing replace with annual average
    if globmeans_SAT == -999.99:
        globmeans_SAT=avg_SAT
    if globmeans_T == -999.99:
        globmeans_T=avg_T

    fo.write(exptname+', '+str(ntimes)+', '+str(round(raw_temp_surf,2))+', '+str(round(raw_temp_1point5,2))+', '+str(round(database_tsurf,2))+', '+str(round(database_t15,2))+', '+str(round(globmeans_SAT,2))+', '+str(round(globmeans_T,2))+', '+str(round(avg_SAT_month,2))+', '+str(round(avg_T_month,2))+', '+notes+'\n')
    
    

fo.close()
# experiments needed
# 'tdcza',
# 'tdhzt',
# 'tdlqa',
# 'tdlqb',
# 'tdlqc',
# 'tdlqd',
# 'tdlqe',
# 'tdlqf',
# 'tdlqg',
# 'tdlqh',
# 'tdlqi',
# 'tdlqj',
# 'tdlxf',
# 'tdlxi',

# 'xgraf',
# 'xgrag',
# 'xgrah',
# 'xgrai',
# 'xgygi',
# 'xgygj',
# 'xgygk',
# 'xgygl',
# 'xgygm',
# 'xgygn',
# 'xgygo',
# 'xgygp',
# 'xgygq',
# 'xgygz',
# 'xhckb',
# 'xhckd',
# 'xhckf',
# 'xhckh',
# 'xhcki',
# 'xhckj',
# 'xhckk',
# 'xhckl',
# 'xhckm',
# 'xhckn',
# 'xhcko',
# 'xhckp',
# 'xhckq',
# 'xhckz',
# 'xhgfk4',
# 'xhsop4',
# 'xhsor4',
# 'xilug',
# 'xiomv',
# 'xjpli',
# 'xhgfa',
# 'tdipf',
# 'tdkgg',
# 'tdkgi',
# 'tdkgf',





sys.exit(0)

####

::::::::::::::
CEMAC/omniglobe/create_series_of_png.py
::::::::::::::
#!/usr/bin/env python
# Mark Richardson, CEMAC
# adapted from work by Robin Steven at SEE Leeds

from pylab import *
from matplotlib.colors import ListedColormap
import netCDF4 as nc4

# --- PARAMETERS ---
# Should check if the basemap exists
try:
    from mpl_toolkits.basemap import Basemap
    dobasemap = True                     
except ImportError:                         
    dobasemap = False                    

# set a destination for the plots
Home="/nfs/see-fs-02_users/earmgr/tmpOmniGlobeContent"
# Need to choose a file
file_a='base2000_cesm111_v2_hourly_avg.nc'
# this is the reference topology JPG
etopo_jpg = '/nfs/see-fs-02_users/earmgr/Images/etopo3000.jpg'

# parse cmd line for choice of variable to inspect
# O3_SRF, ISOP_SRF (but T is 3D so need some filtering to get surface T)
VarChoice = 1

# Target visualisation system
OmniGlobe=True
FullHD =  False
Mon1680 = False
# The number of pixels in the output is related to the "inches" and dots per inch
# this will be set by user in production code
# 100 dpi seems to be default save image setting
# So values are inches to get a set number of pixels 
# when doing FullHD need to think about 80dpi (how to set, why to set?)
if OmniGlobe:
  h_inches = 30.0  # 3000 by 1500 i.e. OmniGlobe at 100dpi
  v_inches = 15.0

elif FullHD:
  h_inches = 19.2   # 1920 by 1080 i.e. FullHD@100dpi
  v_inches = 10.8
  #h_inches = 24.0 v_inches = 13.5 # 1920 by 1080 i.e. FullHD@80dpi
elif Mon1680:
  h_inches = 16.8   # 1680x1050@100dpi
  v_inches = 10.5

# --- INITIALIZE ---
lightlevel = zeros([96,145])  # CAUTION size has been hardwired
#
# sunlight level 0.0 = no sunlight, 1.0 is maximum sunlight (tbd)
night = 0.0
light = 1.0

# At midnight 0/01/2001, adjust these per hour (shift by 6 cells to left - subtracting)
# AGAIN CAUTION DUE TO HARDWIRED SIZE
refdusk = 108  
refdawn = 36
CellsPerDay = 144
hr_shift = CellsPerDay/24   # for different resolution will auto adjust to one hour shift

# this is a mechanism for generating light and dark data
lux = np.linspace(0,144,145,endpoint=True)   # make sure lux is a list
# turn on the light (there is a more efficient method for this I am sure
for i in np.arange(CellsPerDay+1):
  lux[i] = light         # cells 0 to CellsPerDay (+1 for faces )

# Should we allow for choosing different variables within the code or preset it?
if VarChoice == 1:
  ChoiceOfVar = 'O3_SRF'
  VarName = "/o3_surf_"
elif VarChoice == 2 :
  ChoiceOfVar = 'ISOP_SRF'
  VarName = "/isop_s_"
elif VarChoice == 3 :
  ChoiceOfVar = 'T'
  VarName = "/degK_"

if dobasemap:
   map = Basemap(resolution='c', llcrnrlon=0.0,llcrnrlat=-90, urcrnrlon=360.0,urcrnrlat=90)

##### OUR DATA for displaying ###############
ncfile_a = nc4.Dataset(file_a, 'r')

# Extract latitude, longitude
lons = ncfile_a.variables['lon'][:]
lats = ncfile_a.variables['lat'][:]

# Cater for lack of 360.0 information (periodic)
nlats = len(lats)
nlons = len(lons)
lons_p = zeros(nlons+1)
lons_p[:-1] = lons
lons_p[-1] = 360.

# Make a copy of colormap and make it scale from transparent to solid
c_map = cm.Reds
m_map = c_map(np.arange(c_map.N) )
m_map[:,-1] = np.linspace(0,1,c_map.N)
m_map = ListedColormap(m_map)

if dobasemap:
   x,y =  map(*meshgrid(lons_p,lats))

# Assume lats lons persist - avoid reading them repeatedly
# For each timeslice of interest (established in the loop limits and stride)
#   Here I expect to see 30 images at 24 hour interval at midday GMT
start = 702
finish = 727
stride = 1
for simhour in arange(start,finish,stride):
  dlight = mod(simhour,24)  # number of hours from midnight, requires first dataset to midnight
  
  # Extract chosen var surface dat (2D geom)
  chosen_a = ncfile_a.variables[ChoiceOfVar][simhour,:,:]
  
  # Min and Max of this field
  dat_lb = amin(chosen_a)
  dat_ub = amax(chosen_a)
  print "Min, Max values this timeslice are",simhour,dat_lb,dat_ub

  # Cater for lack of 360.0 information (periodic)
  chosen_p = zeros( [nlats, nlons+1] ) # P the plottable version has extra column
  chosen_p[:,:-1] = chosen_a          # copy the content of A into P
  chosen_p[:,-1] = chosen_a[:,0]      # insert first column of A into final column of P
  
  # Need an artificial sunlight representation to sync with "simhour"
  # reference where the terminators exist (naive north-south line)
  # Dusk
  dusk=fmod((refdusk-dlight*hr_shift),CellsPerDay)
  if dusk < 0 : 
    dusk = CellsPerDay + dusk
  # Dawn
  dawn=fmod((refdawn-dlight*hr_shift),CellsPerDay)
  if dawn < 0 : 
    dawn = CellsPerDay + dawn
  
  ###print 'Dlight=',dlight,'Dusk=',dusk,' Dawn=',dawn
  # between dusk and dawn it is dark
  if (dusk < dawn):
    lux[0:dusk] = light
    lux[dusk:dawn] = night
    lux[dawn:CellsPerDay] = light
  else:
    lux[0:dawn] = night
    lux[dawn:dusk] = light
    lux[dusk:CellsPerDay] = night
  
  # now propogate over all latitudes (96 in this specific sim)
  for j in np.arange(nlats):
    for i in np.arange(CellsPerDay+1):
      lightlevel[j][i] = lux[i]
  # END OF setting up data now do the figure planning
  
  # the inches were determined near start of script
  figure(figsize=(h_inches,v_inches))
  
  # make canvas as big as figure area (for OmniGlobe PNGs)
  subplots_adjust(0,0,1,1)
  
  if dobasemap:
    # try put the geography under data
    map.warpimage( image=etopo_jpg )

    # draw coastlines, country boundaries, fill continents.
    map.drawcoastlines(linewidth=0.25)
    # draw the edge of the map projection region (the projection limb)
    map.drawmapboundary()
    # draw lat/lon grid lines every 30 degrees. But omit labels
    ###map.drawmeridians(np.arange(0.,360.,60.),labels=[0,0,0,0],fontsize=10,linewidth=0.25)
    ###map.drawparallels(np.arange(-90.,120.,30.),labels=[0,0,0,0],fontsize=10,linewidth=0.25)

    # now plot the quantity on tthe map
###    map.pcolormesh(lons_p, lats, chosen_a, cmap=m_map,alpha=0.6)
    map.pcolormesh(lons_p, lats, chosen_a, cmap=m_map, edgecolors=face)
    # indicate the night and day regions
    map.pcolormesh(lons_p, lats, lightlevel , cmap=cm.gray,alpha=0.2)

  else:
    print "Basemap not found plotting anyway"
    # Now to plot the actual data
    pcolormesh(lons_p, lats, chosen_p , cmap=cm.Purples)
    # indicate the night and day regions
    pcolormesh(lons_p, lats, lightlevel , cmap=cm.gray,alpha=0.2)
  
  plot()    # Render the image (internally without drawing it
  
  # should do better to change this and introduce checking
  TimeStamp = "2001-01-01"   # this should be in the nc file
  Frame = str("%04d"%simhour)
  MetaData = VarName+TimeStamp+"_"+Frame
  fname_out = Home+MetaData+".png"
  
  savefig(fname_out,bbox_inches="tight",pad_inches=0.0)

  # Iteration end one figure per timestep
  print "Saved figure ",fname_out
  if simhour > finish-stride-2:
    show()                        # THIS IS NOT SENSIBLE in production
    #draw()    # not sure if this is right invocation
  clf()    # clear the figure but leave the canvas
  # still trying to decide how to use next as I would like to see at least one figure
  close()  # could be explicit with figure name or number

###show() # Show final figure (I hope)
# END OF SCRIPT
::::::::::::::
CEMAC/omniglobe/omni_deglac_temp_old.py
::::::::::::::
#NAME
#    omni_deglac_temp.py
#PURPOSE 
#    Lauren wants to put the temperature from the BBC runs on the omniglobe
#    This program will produce suitable png files for this on Annie
#
#
# Julia 8.2.2017


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,lon,lat,minval,maxval,diffval):


    # set image size for omniglobe
#    h_inches = 30.0  # 3000 by 1500 i.e. OmniGlobe at 100dpi
#    v_inches = 15.0
#    plt.figure(figsize=(h_inches,v_inches))
  
    # make canvas as big as figure area (for OmniGlobe PNGs)
#    plt.subplots_adjust(0,0,1,1)



    # Cater for lack of 360.0 information (periodic)
    nlats = len(lat)
    nlons = len(lon)
    lon_p = np.zeros(nlons+1)
    plotdata_p=np.zeros((nlats,nlons+1))
    lon_p[:-1] = lon
    lon_p[nlons] = 360.
    plotdata_p[:,:-1]=plotdata
    plotdata_p[:,nlons]=plotdata[:,0]


    lons, lats = np.meshgrid(lon_p,lat)
    map=Basemap(resolution='c', llcrnrlon=0.0,llcrnrlat=-90, urcrnrlon=360.0,urcrnrlat=90)

    x, y = map(lons, lats)
    #map.drawcoaslines()
    map.drawcoastlines(linewidth=0.25)
    V=np.arange(minval,maxval,diffval)
    cs=map.contourf(x,y,plotdata_p,V,extend='both')
    
  



#=====================================================
def extract_SAT(expt):
# this function will extract the SAT from the file


    filename='/nfs/annie/earljg/data/BBC_runs/'+expt+'a.pdcljja.nc'

    print(filename)
    f=Dataset(filename)
    f.dimensions
    f.variables

    lon = f.variables['longitude'][:]
    lat = f.variables['latitude'][:]
    xsize=len(lon)
    ysize=len(lat)
    fieldval=f.variables['temp_mm_1_5m'][:]
    fieldval=np.squeeze(fieldval)


    retdata=[lon,lat,fieldval]

    return retdata


#=================================================================
# MAIN PROGRAM STARTS HERE

minyear=-120
maxyear=0.5
exptinfo=[]
exptinfo.append(["tdwz9",-120.,"120k"])
exptinfo.append(["tdwz8",-116.,"116k"])
exptinfo.append(["tdwz7",-112.,"112k"])
exptinfo.append(["tdwz6",-108.,"108k"])
exptinfo.append(["tdwz5",-104.,"104k"])
exptinfo.append(["tdwz4",-100.,"100k"])
exptinfo.append(["tdwz3",-96.,"96k"])
exptinfo.append(["tdwz2",-92.,"92k"])
exptinfo.append(["tdwz1",-88.,"88k"])
exptinfo.append(["tdwz0",-84.,"84k"])
exptinfo.append(["tdwzZ",-80.,"80k"])
exptinfo.append(["tdwzY",-78.,"78k"])
exptinfo.append(["tdwzX",-76.,"76"])
exptinfo.append(["tdwzW",-74.,"74"])
exptinfo.append(["tdwzV",-72.,"72k"])
exptinfo.append(["tdwzU",-70.,"70k"])
exptinfo.append(["tdwzT",-68.,"68k"])
exptinfo.append(["tdwzS",-66.,"66k"])
exptinfo.append(["tdwzR",-64.,"64k"])
exptinfo.append(["tdwzQ",-62.,"62k"])
exptinfo.append(["tdwzP",-60.,"60k"])
exptinfo.append(["tdwzO",-58,"58k"])
exptinfo.append(["tdwzN",-56.,"56k"])
exptinfo.append(["tdwzM",-54.,"54k"])
exptinfo.append(["tdwzL",-52.,"52k"])
exptinfo.append(["tdwzK",-50.,"50k"])
exptinfo.append(["tdwzJ",-48.,"48k"])
exptinfo.append(["tdwzI",-46.,"46k"])
exptinfo.append(["tdwzH",-44.,"44k"])
exptinfo.append(["tdwzG",-42.,"42k"])
exptinfo.append(["tdwzF",-40.,"40k"])
exptinfo.append(["tdwzE",-38.,"38k"])
exptinfo.append(["tdwzD",-36.,"36k"])
exptinfo.append(["tdwzC",-34.,"34k"])
exptinfo.append(["tdwzB",-32.,"32k"])
exptinfo.append(["tdwzA",-30.,"30k"])
exptinfo.append(["tdwzz",-28.,"28k"])
exptinfo.append(["tdwzy",-26.,"26k"])
exptinfo.append(["tdwzx",-24.,"24k"])
exptinfo.append(["tdwzw",-22.,"22k"])
exptinfo.append(["tdwzv",-21.,"21k"])
exptinfo.append(["tdwzu",-20.,"20k"])
exptinfo.append(["tdwzt",-19.,"19k"])
exptinfo.append(["tdwzs",-18.,"18k"])
exptinfo.append(["tdwzr",-17.,"17k"])
exptinfo.append(["tdwzq",-16.,"16k"])
exptinfo.append(["tdwzp",-15.,"15k"])
exptinfo.append(["tdwzo",-14.,"14k"])
exptinfo.append(["tdwzn",-13.,"13k"])
exptinfo.append(["tdwzm",-12.,"12k"])
exptinfo.append(["tdwzl",-11.,"11k"])
exptinfo.append(["tdwzk",-10.,"10k"])
exptinfo.append(["tdwzj",-9.,"9k"])
exptinfo.append(["tdwzi",-8.,"8k"])
exptinfo.append(["tdwzh",-7.,"7k"])
exptinfo.append(["tdwzg",-6.,"6k"])
exptinfo.append(["tdwzf",-5.,"5k"])
exptinfo.append(["tdwze",-4.,"4k"])
exptinfo.append(["tdwzd",-3.,"3k"])
exptinfo.append(["tdwzc",-2.,"2k"])
exptinfo.append(["tdwzb",-1.,"1k"])
exptinfo.append(["tdwza",-0.,"0k"])


nexpts=len(exptinfo)
print(nexpts)

# get the data from the files
for n in range(0,nexpts):
    indinfo=exptinfo[n]
    exptname=indinfo[0]
    timeperiod=indinfo[1]
    timetitle=indinfo[2]
    retdata=extract_SAT(exptname)
    lon=retdata[0]
    lat=retdata[1]
    SAT=retdata[2]
    
    
    if n==0: 
        alltime=np.zeros(nexpts)
        allSAT_nonint=np.zeros((nexpts,len(lat),len(lon)))
        print('shape',np.shape(allSAT_nonint))
        
    allSAT_nonint[n,:,:]=SAT
    alltime[n]=timeperiod


# interpolate onto a 500 year array

years=np.arange(minyear,maxyear,0.5)
print(years)
nreq=len(years)
print(years,nreq)

SAT_int=np.zeros((nreq,len(lat),len(lon))) # to store 500 years of data
for treq in range(0,nreq):
    for tfile in range(0,len(alltime)):

        # if there is a file for the required time use that
        if years[treq] == alltime[tfile]:
            SAT_int[treq,:,:]=allSAT_nonint[tfile,:,:]
            
        else:
        # if not interpolate
            if years[treq] > alltime[tfile] and  \
                    years[treq] < alltime[tfile+1]:
                diffupp=alltime[tfile+1]-years[treq]
                difflow=years[treq]-alltime[tfile]
                difffull=alltime[tfile+1]-alltime[tfile]
                SAT_int[treq,:,:]=(((difffull-difflow) * allSAT_nonint[tfile,:,:]) +  ((difffull-diffupp) * allSAT_nonint[tfile+1,:,:]))/difffull
                
                
              
# plot interpolated SAT to a file

SAT_int=SAT_int-273.15 # convert to celcius
for treq in range(0,nreq):

    plotdata(SAT_int[treq,:,:],lon,lat,-40,40,1.0)
    fileyear=120+years[treq]
    if fileyear < 10:
        fileyear='00'+str(fileyear)
    else:
        if fileyear < 100:
            fileyear='0'+str(fileyear)
        else:
            fileyear=str(fileyear)
    
    fname_out = '/nfs/annie/earjcti/CEMAC/omniplots/deglac_SAT/SAT_120k-'+fileyear+'K.eps'
    print(fname_out)  
    plt.savefig(fname_out,bbox_inches="tight",pad_inches=0.0)
    plt.close()

    # check by plotting difference in consecutive files
    
#    plotdata(SAT_int[treq+1,:,:]-SAT_int[treq,:,:],lon,lat,-2,2,0.1)
#    fname_out = '/nfs/annie/earjcti/CEMAC/omniplots/deglac_SAT/SAT_diff'+np.str#(years[treq+1])+'_'+np.str(years[treq])+'.eps'
#    print(fname_out)  
#    plt.savefig(fname_out,bbox_inches="tight",pad_inches=0.0)
#    plt.close()

    

sys.exit()
::::::::::::::
CEMAC/omniglobe/omni_deglac_temp.py
::::::::::::::
#NAME
#    omni_deglac_temp.py
#PURPOSE 
#    Lauren wants to put the temperature from the BBC runs on the omniglobe
#    This program will produce suitable png files for this on Annie
#
#
# Julia 8.2.2017


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,lon,lat,minval,maxval,diffval,timeperiod,anomalyplot,fileyear):

#   shiftgrid so we can fit dates on
    plotdata,lon = shiftgrid(30.,plotdata,lon,start=True)


    # set image size for omniglobe
    h_inches = 30.0  # 3000 by 1500 i.e. OmniGlobe at 100dpi
    v_inches = 15.0
    plt.figure(figsize=(h_inches,v_inches))
  
    # make canvas as big as figure area (for OmniGlobe PNGs)
    plt.subplots_adjust(0,0,1,1)



    # Cater for lack of 360.0 information (periodic)
    nlats = len(lat)
    nlons = len(lon)
    lon_p = np.zeros(nlons+1)
    plotdata_p=np.zeros((nlats,nlons+1))
    lon_p[:-1] = lon
    lon_p[nlons] = lon[0]+360.
    plotdata_p[:,:-1]=plotdata
    plotdata_p[:,nlons]=plotdata[:,0]


    lons, lats = np.meshgrid(lon_p,lat)
    map=Basemap(resolution='c', llcrnrlon=30.0,llcrnrlat=-90, urcrnrlon=390.0,urcrnrlat=90)
    map.drawcoastlines(linewidth=3)

    x, y = map(lons, lats)
    #map.drawcoaslines()
    map.drawcoastlines(linewidth=0.25)
    V=np.arange(minval,maxval,diffval)
    if anomalyplot == 'y':
# cmap tried RdBu_r and seismic
        #cs=map.contourf(x,y,plotdata_p,V,extend='both',cmap='RdBu_r')
        # version 1 as ruza asked for
        #V3=[-24.0,-12.0,-6.0,-4.0,-2.0,-1.0,0.0,1.0,2.0,4.0,6.0,12.0,24.0]
        # version 2 on a logirithmic scale
        V=np.logspace(0.0,3.218875825,num=50,base=np.exp(1))
        V=V-1
        V2=V[::-1]*(-1.)
        lenv2=len(V2)
        V3=np.concatenate(((V2[0:lenv2-1]),V),axis=0)
        print(V3)
       # print(mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-24,vmax=24))
        cs = map.contourf(x,y,plotdata_p,V3,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-24,vmax=24),cmap='RdBu_r',extend='both')
         

    else:
        cs=map.contourf(x,y,plotdata_p,V,extend='both',cmap='RdYlBu_r')
   

        



    #plt.text(330,40,timeperiod,fontsize=15,bbox={'facecolor':'white'},ha='center')
    plt.text(75,0,timeperiod,fontsize=48,ha='center',va='center')
    plt.text(345,0,timeperiod,fontsize=48,ha='center',va='center')
    plt.text(210,0,timeperiod,fontsize=48,ha='center',va='center')
  
    #cbar = plt.colorbar(cs,orientation="horizontal")
    #cbar = plt.colorbar(cs,orientation="horizontal",ticks=[-24,-12,-6,-4,-2,0,2,4,6,12,24])
    #cbar.set_label('degC',labelpad=-40)



    if fileyear < 10:
        fileyear='00'+str(fileyear)
    else:
        if fileyear < 100:
            fileyear='0'+str(fileyear)
        else:
            fileyear=str(fileyear)

    
    #fname_out = '/nfs/annie/earjcti/CEMAC/omniplots/deglac_SAT/'+season+'mean_eps/SAT_120k-'+fileyear+'K.eps'

   # plt.savefig(fname_out,bbox_inches="tight",pad_inches=0.0)


    #fname_out = '/nfs/annie/earjcti/CEMAC/omniplots/deglac_SAT/'+season+'mean_png/SAT_120k-'+fileyear+'K.png'

    #plt.savefig(fname_out,bbox_inches="tight",pad_inches=0.0)
    plt.close()


    # plot colorbar to a seperate file and stop

    h_inches = 9.0  # 3000 by 1500 i.e. OmniGlobe at 100dpi
    v_inches = 1.5
    plt.figure(figsize=(h_inches,v_inches))

    if season == 'ann':
        cs = map.contourf(x,y,plotdata_p,V3,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-24,vmax=24),cmap='RdBu_r',extend='both')

        plt.gca().set_visible(False)
        cax = plt.axes([0.1, 0.5, 0.8, 0.3])
        cbar = plt.colorbar(orientation="horizontal",ticks=[-24,-12,-6,-4,-2,0,2,4,6,12,24],cax=cax)
        cbar.ax.tick_params(labelsize=18)

    else:
        cs=map.contourf(x,y,plotdata_p,V,extend='both',cmap='RdYlBu_r')
        plt.gca().set_visible(False)
        cax = plt.axes([0.1, 0.5, 0.8, 0.3])
        cbar = plt.colorbar(orientation="horizontal",cax=cax)
        cbar.ax.tick_params(labelsize=18)


    #cbar.set_label('degC',labelpad=-40)
    degC=u'\N{DEGREE SIGN}'+'C'
    cbar.set_label(degC,fontsize=18)

    fname_out = '/nfs/annie/earjcti/CEMAC/omniplots/deglac_SAT/'+season+'cbar.png'
    plt.savefig(fname_out,bbox_inches="tight",pad_inches=0.0)



    plt.close
    sys.exit()


#=====================================================
def extract_SAT(expt,season):
# this function will extract the SAT from the file


    filename='/nfs/annie/earljg/data/BBC_runs/'+expt+'a.pdcl'+season+'.nc'

    print(filename)
    f=Dataset(filename)
    f.dimensions
    f.variables

    lon = f.variables['longitude'][:]
    lat = f.variables['latitude'][:]
    xsize=len(lon)
    ysize=len(lat)
    fieldval=f.variables['temp_mm_1_5m'][:]
    fieldval=np.squeeze(fieldval)


    retdata=[lon,lat,fieldval]

    return retdata


#=================================================================
# MAIN PROGRAM STARTS HERE

season='jja'

minyear=-120
maxyear=0.5
exptinfo=[]
exptinfo.append(["tdwz9",-120.,"120k"])
exptinfo.append(["tdwz8",-116.,"116k"])
exptinfo.append(["tdwz7",-112.,"112k"])
exptinfo.append(["tdwz6",-108.,"108k"])
exptinfo.append(["tdwz5",-104.,"104k"])
exptinfo.append(["tdwz4",-100.,"100k"])
exptinfo.append(["tdwz3",-96.,"96k"])
exptinfo.append(["tdwz2",-92.,"92k"])
exptinfo.append(["tdwz1",-88.,"88k"])
exptinfo.append(["tdwz0",-84.,"84k"])
exptinfo.append(["tdwzZ",-80.,"80k"])
exptinfo.append(["tdwzY",-78.,"78k"])
exptinfo.append(["tdwzX",-76.,"76"])
exptinfo.append(["tdwzW",-74.,"74"])
exptinfo.append(["tdwzV",-72.,"72k"])
exptinfo.append(["tdwzU",-70.,"70k"])
exptinfo.append(["tdwzT",-68.,"68k"])
exptinfo.append(["tdwzS",-66.,"66k"])
exptinfo.append(["tdwzR",-64.,"64k"])
exptinfo.append(["tdwzQ",-62.,"62k"])
exptinfo.append(["tdwzP",-60.,"60k"])
exptinfo.append(["tdwzO",-58,"58k"])
exptinfo.append(["tdwzN",-56.,"56k"])
exptinfo.append(["tdwzM",-54.,"54k"])
exptinfo.append(["tdwzL",-52.,"52k"])
exptinfo.append(["tdwzK",-50.,"50k"])
exptinfo.append(["tdwzJ",-48.,"48k"])
exptinfo.append(["tdwzI",-46.,"46k"])
exptinfo.append(["tdwzH",-44.,"44k"])
exptinfo.append(["tdwzG",-42.,"42k"])
exptinfo.append(["tdwzF",-40.,"40k"])
exptinfo.append(["tdwzE",-38.,"38k"])
exptinfo.append(["tdwzD",-36.,"36k"])
exptinfo.append(["tdwzC",-34.,"34k"])
exptinfo.append(["tdwzB",-32.,"32k"])
exptinfo.append(["tdwzA",-30.,"30k"])
exptinfo.append(["tdwzz",-28.,"28k"])
exptinfo.append(["tdwzy",-26.,"26k"])
exptinfo.append(["tdwzx",-24.,"24k"])
exptinfo.append(["tdwzw",-22.,"22k"])
exptinfo.append(["tdwzv",-21.,"21k"])
exptinfo.append(["tdwzu",-20.,"20k"])
exptinfo.append(["tdwzt",-19.,"19k"])
exptinfo.append(["tdwzs",-18.,"18k"])
exptinfo.append(["tdwzr",-17.,"17k"])
exptinfo.append(["tdwzq",-16.,"16k"])
exptinfo.append(["tdwzp",-15.,"15k"])
exptinfo.append(["tdwzo",-14.,"14k"])
exptinfo.append(["tdwzn",-13.,"13k"])
exptinfo.append(["tdwzm",-12.,"12k"])
exptinfo.append(["tdwzl",-11.,"11k"])
exptinfo.append(["tdwzk",-10.,"10k"])
exptinfo.append(["tdwzj",-9.,"9k"])
exptinfo.append(["tdwzi",-8.,"8k"])
exptinfo.append(["tdwzh",-7.,"7k"])
exptinfo.append(["tdwzg",-6.,"6k"])
exptinfo.append(["tdwzf",-5.,"5k"])
exptinfo.append(["tdwze",-4.,"4k"])
exptinfo.append(["tdwzd",-3.,"3k"])
exptinfo.append(["tdwzc",-2.,"2k"])
exptinfo.append(["tdwzb",-1.,"1k"])
exptinfo.append(["tdwza",-0.,"0k"])


nexpts=len(exptinfo)
print(nexpts)

# get the data from the files
for n in range(0,nexpts):
    indinfo=exptinfo[n]
    exptname=indinfo[0]
    timeperiod=indinfo[1]
    timetitle=indinfo[2]
    retdata=extract_SAT(exptname,season)
    lon=retdata[0]
    lat=retdata[1]
    SAT=retdata[2]
    
    
    if n==0: 
        alltime=np.zeros(nexpts)
        allSAT_nonint=np.zeros((nexpts,len(lat),len(lon)))
        print('shape',np.shape(allSAT_nonint))
        
    allSAT_nonint[n,:,:]=SAT
    alltime[n]=timeperiod


# interpolate onto a 500 year array

years=np.arange(minyear,maxyear,0.5)
#years=np.arange(minyear,maxyear,5.0)
print(years)
nreq=len(years)
print(years,nreq)

SAT_int=np.zeros((nreq,len(lat),len(lon))) # to store 500 years of data
for treq in range(0,nreq):
    for tfile in range(0,len(alltime)):

        # if there is a file for the required time use that
        if years[treq] == alltime[tfile]:
            SAT_int[treq,:,:]=allSAT_nonint[tfile,:,:]
            
        else:
        # if not interpolate
            if years[treq] > alltime[tfile] and  \
                    years[treq] < alltime[tfile+1]:
                diffupp=alltime[tfile+1]-years[treq]
                difflow=years[treq]-alltime[tfile]
                difffull=alltime[tfile+1]-alltime[tfile]
                SAT_int[treq,:,:]=(((difffull-difflow) * allSAT_nonint[tfile,:,:]) +  ((difffull-diffupp) * allSAT_nonint[tfile+1,:,:]))/difffull
                
                
              
# plot interpolated SAT to a file

SAT_int=SAT_int-273.15 # convert to celcius


# if season is annual then we want an anomaly from 0ka

if season == 'ann':
    SAT_0k=SAT_int[nreq-1,:,:]
    SAT_0k=np.squeeze(SAT_0k)
    SAT_int=SAT_int-SAT_0k

for treq in range(0,nreq):

    fileyear=120+years[treq]
    if season == 'ann':
        plotdata(SAT_int[treq,:,:],lon,lat,-9.75,9.80,0.05,np.str(years[treq]*(-1.0))+' ka','y',fileyear)
    else:
        plotdata(SAT_int[treq,:,:],lon,lat,-40,41,1.0,np.str(years[treq]*(-1.0))+'ka','n',fileyear)


    

sys.exit()
::::::::::::::
CEMAC/PLIOMIP2/Arctic_amp_plots.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
#Created on 05/03/2020


#@author: earjcti

This program will plot the polar amplification diagrams
requested by IPCC in particular

Arctic amplification: I suggest reporting the
average mean annual land + sea temperature for six,
30 deg latitude bands,
 or at least the temperatures for 60-90N vs 0-90N.

So we will do:

For land/sea/total the temperature anomaly for the 6 bands
"""
import sys
import warnings
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import re

warnings.filterwarnings("ignore")

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match.
    """

    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube

def get_lsm(filein):
    """
    returns a  numpy array of land points and one of sea points
    """
    tempcube = iris.load_cube(filein)
    cubegrid = iris.load_cube('one_lev_one_deg.nc')
    lsmcube = tempcube.regrid(cubegrid, iris.analysis.Linear())
    landpoints = lsmcube.data
    seapoints = (lsmcube.data - 1.0) * (-1.0)

    return landpoints, seapoints

def get_mean_data(model, expt):
    """
    gets the cube of mean data for a single model

    Parameters
    ----------
    model : the name of the model we are interested in
    expt : whether it is the experiment or the control

    Returns
    -------
    a cube with the mean data from this file
    grid_areas = the size of the grid for averaging
    """

    filename = (FILESTART + 'regridded/' + model
                + '/' + expt + '.' + FIELD + '.allmean.nc')

    cube = iris.load_cube(filename)
    cube2 = resort_coords(cube)

    cube2.coord('latitude').guess_bounds()
    cube2.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube2)

    return cube2, grid_areas


def get_region(band_upper, band_lower, cube, mask, grid_areas):
    """
    Gets the average temeprature over land within the
    bounded range

    Parameters
    ----------
    band_upper : scalar value denoting the upper latitude of
                 the band
    band_lower : scalar value denoting the lower latitude of
                 the band
    cube : the cube containing average temperatures that
           we want to get the land temperature over
    mask : numpy array containing the mask we want.  Could be a land mask
           a sea mask or ones everywhere (ie all points)
    grid_areas : the size of each gridpoint for weighting

    Returns
    -------
    bound_land_avg : scalar containing the average land
                    temperature over the bounded region

    """

    grid_areas_mask = grid_areas * mask
    grid_areas_band = np.zeros(grid_areas.shape)
    lats = cube.coord('latitude').points

    for j, lat in enumerate(lats):
        if band_lower <= lat <= band_upper:
            grid_areas_band[j, :] = grid_areas_mask[j, :]

    bound_mask_avg = cube.collapsed(['longitude', 'latitude'],
                                    iris.analysis.MEAN,
                                    weights=grid_areas_band)

    return bound_mask_avg.data

def get_pliomip1_data():
    """
    Gets the pliomip1 data for each of the bands

    Returns
    -------
    pliomip1 data

    """
    if LINUX_WIN == 'l':
        PLIOMIP1_FILE = (FILESTART + '/PLIOMIP/means_for_' + FIELD + '.txt')
    else:
        PLIOMIP1_FILE = FILESTART + 'PLIOMIP1/means_for_' + FIELD + '.txt'
    f1 = open(PLIOMIP1_FILE)
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    # find line index which has the title 'modelname, latband mean '
    string = 'modelname, latband mean'
    for i, line in enumerate(lines):
        if line[0:23] == string:
            index = i
        
    # get bands by splitting the line
    bands_line = lines[index + 1]
    bands_str_array = bands_line[10:]
    print(bands_str_array)
    res = bands_str_array.split("], [")
    res = [x.strip('[') for x in res]
    res = [x.strip(']') for x in res]
    nbands = len(res)
    bands_array = np.zeros((nbands, 2))
    for i, x in enumerate(res):
        x1, x2 = x.split(',')
        bands_array[i, 0] = x1
        bands_array[i, 1] = x2
     
    # get the data from the next lines find the mean, min and max for each band
    # for the anomaly only
    minval = np.zeros(nbands)
    minval[:] = 100.
    maxval = np.zeros(nbands)
    meanval = np.zeros(nbands)
    
    for i in range(index + 2, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break      
        modname, eoi400, e280, anom = line.split(',')

        eoi400_val = np.array(eoi400.strip('[]').split(), dtype=float)
        e280_val = np.array(e280.strip('[]').split(), dtype=float)
        
        anom_val = np.array(anom.strip('[]').split(), dtype=float)
        
        for i, anom in enumerate(anom_val):
            if anom < minval[i]:
                minval[i] = anom
            if anom > maxval[i]:
                maxval[i] = anom
        if modname == 'MEAN':
            meanval = anom_val
        
    return meanval, minval, maxval
  
   
   
   
def plot_temp_by_lat(anomaly, uppervals, lowervals, plottype, fileoutstart,
                     mean_p1, min_p1, max_p1):
    """
    plot the temperature anomaly vs the region on one plot

    Parameters
    ----------
    anomaly : temperature anomaly to plot np.shape= nmodels, nbounds
    uppervals : the upper limit of the boundary range (np.shape = nbounds)
    lowervals : the lower limit of the boundary range (np.shape = nbounds)
    plottype : 'Land' 'Ocean' or '' if empty it is all surface types

    Returns
    -------
    None.

    """

    titlename = 'MPWP - PI ' + plottype + ' SAT anomaly'
    latns = {-90.0 : '90S', -60.0: '60S', -30.0 : '30S', 0.0 : '0N',
             30.0 : '30N', 60.0 : '60N', 90.0 : '90N'}


    labels = []
    for i, upval in enumerate(uppervals):
        labels.append(latns.get(upval) + '-' + latns.get(lowervals[i]))

    ax = plt.subplot(1, 1, 1)
    for i, model in enumerate(MODELNAMES):
        if i < len(MODELNAMES) / 2.0:
            ax.plot(anomaly[i, :], labels, label=model)
        else:
            ax.plot(anomaly[i, :], labels, label=model, linestyle='dashed')


    ax.plot(np.mean(anomaly, axis=0), labels, color='black',
            linestyle='dashed',
            linewidth=2, label='avg')
    plt.title(titlename)
    plt.xlabel(UNITS)

    if PLIOMIP1 == 'y' and plottype == '':
        ax.plot(mean_p1, labels, label = 'PlioMIP1', color='black',
                linestyle = 'dotted', linewidth=2)
        ax.fill_betweenx(labels, min_p1, max_p1, alpha=0.2, 
                        color="grey")
       
    FILETXT = (FILESTART + '/regridded/alldata/data_for_supp_fig2_' 
               + plottype + '.txt')

    txtout = open(FILETXT, "w+")
    
    writedata = 'modelname'
    for j in range(0, len(labels)):
        writedata = writedata + ',' + labels[j]
    writedata = writedata + '\n'
    txtout.write(writedata)

    for i, mod in enumerate(MODELNAMES):
        writedata = mod 
        for j in range(0, len(labels)):
            writedata = writedata + ',' + (np.str(np.around(anomaly[i,j],2)))
        writedata = writedata + '\n'
        txtout.write(writedata)
    txtout.close()
   

    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    fileout = (fileoutstart + '/polar_amplification'  + plottype + '_anomaly.eps')
    plt.savefig(fileout)
    fileout = (fileoutstart + '/polar_amplification'  + plottype + '_anomaly.pdf')
    plt.savefig(fileout)
    plt.show()
    plt.close()


#####################################
def main():
    """
    Tha main control of the program to plot the
    polar amplification by temperature by latitude band

    """

    if LINUX_WIN == 'w':
        fileoutstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD + '\\')
        exptlsm = (FILESTART + 'regridded/PlioMIP2_Boundary_conds' +
                   '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'regridded/PlioMIP2_Boundary_conds' +
                   '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
    else:
        fileoutstart = '/nfs/hera1/earjcti/regridded/allplots/' + FIELD + '/'
        dataoutstart = '/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm = (FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh' +
                   '/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'PlioMIP2_Boundary_conds/Modern_std/' +
                   'Modern_std/Modern_std_LSM_v1.0.nc')


    ########################################################
    # setup: get the lsm for the land sea contrast plot

    exptland, exptsea = get_lsm(exptlsm)
    cntlland, cntlsea = get_lsm(cntllsm)
    maskall = np.ones(np.shape(exptland))


    #########################################################
    # need to get data from annual mean plot

    bandmax = [-60., -30., 0., 30., 60., 90.]
    bandmin = [-90., -60., -30., 0., 30., 60.]


    land_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))

    land_expt = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_expt = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_expt = np.zeros((len(MODELNAMES), len(bandmax)))

    land_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    
    land_expt_avg = np.zeros((len(MODELNAMES)))
    sea_expt_avg = np.zeros((len(MODELNAMES)))
    expt_avg = np.zeros((len(MODELNAMES)))

    land_cntl_avg = np.zeros((len(MODELNAMES)))
    sea_cntl_avg = np.zeros((len(MODELNAMES)))
    cntl_avg = np.zeros((len(MODELNAMES)))

    

    for modelno, modeluse in enumerate(MODELNAMES):

        # get mean data
        (exptcube, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME)
        (cntlcube, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME)

        # get data within bounds
        for i, band_upper in enumerate(bandmax):
            band_lower = bandmin[i]
            land_expt[modelno, i] = get_region(band_upper, band_lower,
                                               exptcube, exptland,
                                               grid_areas_expt)

            land_cntl[modelno, i] = get_region(band_upper, band_lower,
                                               cntlcube, cntlland,
                                               grid_areas_cntl)

            sea_expt[modelno, i] = get_region(band_upper, band_lower,
                                              exptcube, exptsea,
                                              grid_areas_expt)

            sea_cntl[modelno, i] = get_region(band_upper, band_lower,
                                              cntlcube, cntlsea,
                                              grid_areas_cntl)

            bands_expt[modelno, i] = get_region(band_upper, band_lower,
                                                exptcube, maskall,
                                                grid_areas_expt)

            bands_cntl[modelno, i] = get_region(band_upper, band_lower,
                                                cntlcube, maskall,
                                                grid_areas_cntl)
            
        #get average data for calculating polar amplification
        land_expt_avg[modelno] = get_region(90.0, -90.0, exptcube, exptland,
                                            grid_areas_expt)
        land_cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, cntlland,
                                            grid_areas_cntl)
        sea_expt_avg[modelno] = get_region(90.0, -90.0, exptcube, exptsea,
                                            grid_areas_expt)
        sea_cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, cntlsea,
                                            grid_areas_cntl)
        expt_avg[modelno] = get_region(90.0, -90.0, exptcube, maskall,
                                            grid_areas_expt)
        cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, maskall,
                                            grid_areas_cntl)

    land_anomaly = land_expt - land_cntl
    sea_anomaly = sea_expt - sea_cntl
    bands_anomaly = bands_expt - bands_cntl
    
    ##############################################################
    #  get pliomip1 data if required.  Note we will only get annual mean
    
    if PLIOMIP1 == 'y':
        pliomip1_mean, pliomip1_min, pliomip1_max = get_pliomip1_data()
    else:
        pliomip1_mean = 0
        pliomip1_min = 0
        pliomip1_max = 0

    #print polar amplification
    print('SH polar amplification')
    print('model, land amplification, sea amp, all amp')
    #for i, model in enumerate(MODELNAMES):
    #    print('bands',bandmax[0],bandmin[0])
    #    print(model, land_anomaly[i, 0] / (land_expt_avg[i] - land_cntl_avg[i]), 
    #          sea_anomaly[i, 0] / (sea_expt_avg[i] - sea_cntl_avg[i]),
    #          bands_anomaly[i,0] / (expt_avg[i] - cntl_avg[i]))
    all_sh_land_amp = [x1 / x2 for (x1, x2) in zip(land_anomaly[:,0], 
                                                   land_expt_avg - land_cntl_avg)]
    all_sh_sea_amp = [x1 / x2 for (x1, x2) in zip(sea_anomaly[:,0], 
                                                  sea_expt_avg - sea_cntl_avg)]
    all_sh_amp = [x1 / x2 for (x1, x2) in zip(bands_anomaly[:,0], 
                                              expt_avg - cntl_avg)]
    print('mean', 
          np.mean(land_anomaly[:, 0]) / np.mean(land_expt_avg[:] - land_cntl_avg[:]), 
          np.mean(sea_anomaly[:, 0]) / np.mean(sea_expt_avg[:] - sea_cntl_avg[:]),
          np.mean(bands_anomaly[:,0] / np.mean(expt_avg[:] - cntl_avg[:])))
    print('median', np.median(all_sh_land_amp), np.median(all_sh_sea_amp), 
          np.median(all_sh_amp))
    
    print('checking land',  np.mean(land_anomaly[:, 0]), np.mean(land_expt[:,0]),
          np.mean(land_cntl[:,0]))
          #,np.mean(land_expt_avg[:])-
          #np.mean(land_cntl_avg[:]))
    print('checking sea',  np.mean(sea_anomaly[:, 0]), np.mean(sea_expt[:,0]),
          np.mean(sea_cntl[:,0]))#,np.mean(sea_expt_avg[:])-
        #  np.mean(sea_cntl_avg[:]))
    print('checking avg',  np.mean(bands_anomaly[:, 0]),np.mean(bands_expt[:,0]),
          np.mean(bands_cntl[:,0]))#,np.mean(expt_avg[:])-
        #  np.mean(cntl_avg[:]))
      
    print(' ')    
    print('NH polar amplification')
    all_nh_land_amp = [x1 / x2 for (x1, x2) in zip(land_anomaly[:,5], 
                                                   land_expt_avg - land_cntl_avg)]
    all_nh_sea_amp = [x1 / x2 for (x1, x2) in zip(sea_anomaly[:,5], 
                                                  sea_expt_avg - sea_cntl_avg)]
    all_nh_amp = [x1 / x2 for (x1, x2) in zip(bands_anomaly[:,5], 
                                              expt_avg - cntl_avg)]
    print('model, land amp, sea amp, all amp')
    #for i, model in enumerate(MODELNAMES):
    #    print('bands',bandmax[5],bandmin[5])
    #    print(model, land_anomaly[i, 5] / (land_expt_avg[i] - land_cntl_avg[i]), 
    #          sea_anomaly[i, 5] / (sea_expt_avg[i] - sea_cntl_avg[i]),
    #          bands_anomaly[i,5] / (expt_avg[i] - cntl_avg[i]))
    print('mean', 
          np.mean(land_anomaly[:, 5]) / np.mean(land_expt_avg[:] - land_cntl_avg[:]), 
          np.mean(sea_anomaly[:, 5]) / np.mean(sea_expt_avg[:] - sea_cntl_avg[:]),
          np.mean(bands_anomaly[:,5] / np.mean(expt_avg[:] - cntl_avg[:])))
    print('median', np.median(all_nh_land_amp), np.median(all_nh_sea_amp), 
          np.median(all_nh_amp))

    # plot everything on one plot.

    plot_temp_by_lat(land_anomaly, bandmax, bandmin, 'Land', 
                     fileoutstart, 0, 0, 0)
    plot_temp_by_lat(sea_anomaly, bandmax, bandmin, 'Ocean', 
                     fileoutstart, 0, 0, 0)
    plot_temp_by_lat(bands_anomaly, bandmax, bandmin, '', 
                     fileoutstart, pliomip1_mean, pliomip1_min, pliomip1_max)



##########################################################
# DEFINITIONS

LINUX_WIN = 'l'
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'


MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]

PLIOMIP1 = 'y' # overplot PlioMIP1 models.

#MODELNAMES=['HadCM3','NorESM-L']
FIELD = 'NearSurfaceTemperature'
UNITS = 'degC'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
EXPTNAME = 'EOI400'
CNTLNAME = 'E280'

if FIELD == 'NearSurfaceTemperature':
    FILEOUT = FILESTART + 'regridded/alldata/data_for_arctic_amplification.txt'


main()
::::::::::::::
CEMAC/PLIOMIP2/average_HadISST_NOAA.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Jul 25 16:18:28 2019

@author: earjcti
This program will calculate a 30 year (1870-1900) average
of HadISST or HadSST4 data and write it to a file.
This can then be compared with modelled SST data

"""

import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import sys


def get_var():
    """
    gets variable names depending on whether we are using
    HadISST or HadSST4.0

    returns, FILENAME, OUTSTART, FILECUBE
    """

    if DATASET == 'HadISST':
        filename = '/nfs/hera1/earjcti/PLIOMIP2/HadISST/HadISST_sst.nc'
        varname = 'sea_surface_temperature'
        fieldname = 'SST'

    if DATASET == 'HadSST4':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/HadSST4/' +
                    'HadSST.4.0.0.0_median.nc')
        varname = 'Sea water temperature anomaly at a depth of 20cm'
        fieldname = 'SST'


    if DATASET == 'NOAAERSST5':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/NOAAERSST5/' +
                    'sst.mnmean.nc')
        varname = 'Monthly Means of Sea Surface Temperature'
        fieldname = 'SST'


    if DATASET == 'CRUTEMP':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/CRU_DATA/' +
                    'cru_ts4.04.1901.2019.tmp.dat.nc')
        varname = 'near-surface temperature'
        fieldname = 'NearSurfaceTemperature'


    if DATASET == 'CRUPRECIP':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/CRU_DATA/' +
                    'cru_ts4.04.1901.2019.pre.dat.nc')
        varname = 'precipitation'
        fieldname = 'TotalPrecipitation'


    outstart = ('/nfs/hera1/earjcti/regridded/'
                + DATASET + '/E280.' + fieldname + '.')
    filecube = iris.load_cube(filename, varname)


    return [filename, outstart, filecube]

def extract_nyrs(cube, startyear, endyear):
    """
    Extract data between startyear and endyear
    from a cube of monthly data

    Parameters:
    cube (iris cube): The cube containing a number of years
    startyear,endyear  (integer): years we wish to extract

    Returns:
    newcube (iris cube): The cube containing the subset of data
    """
    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        iris.coord_categorisation.add_year(t_slice, 'time', name='year')
        year = t_slice.coord('year').points

        if startyear <= year <= endyear:
            print('processing', year)
            t_slice.remove_coord('year')
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            # we have missing data at lsm and also some points are -1000.
            # change points that are -1000. to missing data
            newdata = np.ma.masked_where(t_slice2.data < -900., t_slice2.data)
            t_slice2.data = newdata
            cubelist.append(t_slice2)

    newcube = cubelist.concatenate_cube()

    return newcube


def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]

def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
        or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):
            mon_slice.coord('latitude').guess_bounds()
            mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):

        allmeancube.coord('latitude').guess_bounds()
        allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):

        yearmeancube.coord('latitude').guess_bounds()
        yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]


def textout(meanmon, stdevmon):
    """
    write out all the means to a text file
    this includes global means monthly means and latitudinal means
    """
    textfile = OUTSTART + 'data.txt'
    file1 = open(textfile, "w")
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2)) + ',' +
                np.str(np.round(stdevann, 3)) + '\n')

    # write monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    for i in range(0, 12):
        file1.write(np.str(i+1) + ','+np.str(np.round(meanmon[i], 2)) + ','
                    + np.str(np.round(stdevmon[i], 3))+'\n')

    # write latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')

    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i]) +
                    ',' + np.str(np.round(meanlat[i], 2)) +
                    ',' + np.str(np.round(stdevlat[i], 3)) + '\n')

    file1.close()

def plot_to_check(cube):
    """
    A test program to check that we have averaged properly.
    """


    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube=subcube_mean_mon.copy(data=)  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas = iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],
                             iris.analysis.MEAN, weights=grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt / 12)
    print(nyears)

    for i in range(0, nyears):
        tstart = i * 12
        tend = (i+1) * 12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color='r')

    # global mean from average

    grid_areas = iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean = mean_mon_data.collapsed(['latitude', 'longitude'],
                                            iris.analysis.MEAN, weights=grid_areas)

    plt.plot(temporal_mean.data, color='b', label='avg')
    plt.title('globavg HadISST')
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0] >= 32. >= bounds[i, 1] or bounds[i, 0] <= 32. < bounds[i, 1]):
            index = i

    subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color='r')

    #mean at 30N
    slice_30N = mean_mon_data.extract(iris.Constraint(latitude=32))
    mean_30N = slice_30N.collapsed(['longitude'], iris.analysis.MEAN)


    plt.plot(mean_30N.data, color='b', label='avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()

def writeout():
    """
    write the monthly mean and global mean cubes out to a netcdf file
    """

    outfile = OUTSTART + 'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)



##########################################################
# MAIN PROGRAM

"""
This program will average over the first 30 years of the HadISST
dataset and write results out to a file
"""

DATASET = 'CRUPRECIP' # HadISST HadSST4 NOAAERSST5 CRUTEMP CRUPRECIP

# read in HadISST data
FILENAME, OUTSTART, FILECUBE = get_var()

# extract the years from filecube likely 30 years
#cube30 = extract_nyrs(FILECUBE, 1870, 1899) # where available
cube30 = extract_nyrs(FILECUBE, 1901, 1930)

# create averages
 # add year and month time axis
iris.coord_categorisation.add_month_number(cube30, 'time', name='month')
iris.coord_categorisation.add_year(cube30, 'time', name='year')
mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(cube30)

# create info about each month and get average monthly data
# stdevmon is a numpy array of standard deviation, monthly_mean is a numpy array
# of means
monthly_mean = mon_avg(mean_mon_data)
monthly_standard_deviation = get_monthly_sd(cube30)


# get global and latitinal means for writing to text file
# plot for

meanann, stdevann, meanlat, stdevlat = area_means(mean_data, mean_year_data)

# write means to a text file
textout(monthly_mean, monthly_standard_deviation)

# write iris cubes out to a file
writeout()

# plot to check we have averaged properly
plot_to_check(cube30)
::::::::::::::
CEMAC/PLIOMIP2/calculate_land_sea_contrast_HadGEM3.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import netCDF4
import sys
#import os


###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """
    f = Dataset(LSM, "r")
    lsm_data = f.variables['lsm'][:]
    lats = f.variables['latitude'][:] 
    lons = f.variables['longitude'][:]
    f.close()
  
    lsm_data = np.squeeze(lsm_data)
    land_mask = lsm_data
    sea_mask = (lsm_data -1.0) * 1.0

    return land_mask, sea_mask, lats, lons

    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """

    f = Dataset(FILENAME_PLIO, "r")
    plio_data_all = f.variables[FIELDNAME][:]
    lats = f.variables['latitude'][:] 
    lons = f.variables['longitude'][:]
    f.close()

    plio_data = (np.mean(plio_data_all,axis=0)-273.15)

    f = Dataset(FILENAME_PI, "r")
    pi_data_all = f.variables[FIELDNAME][:]
    lats2 = f.variables['latitude'][:] 
    lons2 = f.variables['longitude'][:]
    f.close()

    pi_data = (np.mean(pi_data_all,axis=0)-273.15)


    for i, lat in enumerate(lats):
        if lat != lats2[i]:
            print('lats dont match', lat, i, lats2[i])
            sys.exit(0)

    for i, lon in enumerate(lons):
        if lon != lons2[i]:
           print('lons dont match', lon, i, lons2[i])
           sys.exit(0)

  
    return plio_data, pi_data, lats, lons


def get_global_avg(land_mask, sea_mask, dataarr, lats, lons):
    """
    gets global average temperature, and also global avg for
    the land and the ocean
    """
  
    grid_areas_lat = np.zeros(len(lats))
    for j, lat in enumerate(lats):
        grid_areas_lat[j] = np.cos(2. * np.pi * lat / 360.)

    print(np.shape(land_mask))
    global_mean = 0.
    global_mean_weights = 0.
    global_mean_land = 0.
    global_mean_land_weights = 0.
    global_mean_sea = 0.
    global_mean_sea_weights = 0.

    for j in range(0, len(lats)):
        for i in range(0, len(lons)):
            global_mean = global_mean + (dataarr[j, i] * grid_areas_lat[j])
            global_mean_weights = global_mean_weights + grid_areas_lat[j]
            if land_mask[j, i] == 1.0:
                global_mean_land = (global_mean_land + 
                                   (dataarr[j, i] * grid_areas_lat[j]))
                global_mean_land_weights = (global_mean_land_weights + 
                                            grid_areas_lat[j])
            else:
                global_mean_sea = (global_mean_sea + 
                                   (dataarr[j, i] * grid_areas_lat[j]))
                global_mean_sea_weights = (global_mean_sea_weights + 
                                            grid_areas_lat[j])

    global_mean = global_mean / global_mean_weights
    global_mean_land = global_mean_land / global_mean_land_weights
    global_mean_sea = global_mean_sea / global_mean_sea_weights

   
    return global_mean, global_mean_land, global_mean_sea, grid_areas_lat


def get_regional_landsea(rmax, rmin, land_mask, dataarr, lats, lons,
                         grid_areas):

    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    grid_areas_use = grid_areas * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j] = 0.0
    #grid_areas_land = grid_areas_use * land_cube.data
    #grid_areas_sea = grid_areas_use * sea_cube.data
    
    reg_mean = 0.
    reg_mean_weights = 0.
    reg_mean_land = 0.
    reg_mean_land_weights = 0.
    reg_mean_sea = 0.
    reg_mean_sea_weights = 0.

    for j in range(0, len(lats)):
        if grid_areas_use[j] != 0.0:
            for i in range(0, len(lons)):
                reg_mean = reg_mean + (dataarr[j, i] * grid_areas_use[j])
                reg_mean_weights = reg_mean_weights + grid_areas_use[j]
                if land_mask[j, i] == 1.0:
                    reg_mean_land = (reg_mean_land + 
                                     (dataarr[j, i] * grid_areas_use[j]))
                    reg_mean_land_weights = (reg_mean_land_weights + 
                                             grid_areas_use[j])
                else:
                    reg_mean_sea = (reg_mean_sea + 
                                    (dataarr[j, i] * grid_areas_use[j]))
                    reg_mean_sea_weights = (reg_mean_sea_weights + 
                                            grid_areas_use[j])
    print(rmax, rmin)
    print(rmax, rmin, reg_mean / reg_mean_weights)
    print(reg_mean, reg_mean_land, reg_mean_sea)
    print(reg_mean_weights, reg_mean_land_weights, reg_mean_sea_weights)
  
    reg_mean = reg_mean / reg_mean_weights
    reg_mean_land = reg_mean_land / reg_mean_land_weights
    reg_mean_sea = reg_mean_sea / reg_mean_sea_weights

    return [reg_mean, reg_mean_land, reg_mean_sea]
           
                                                       
def write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin):
    """
    write the information to a pandas dataframe
    """
    print(avg_T_anom, MODELNAME)

    data = [['Global', avg_T_anom], ['Global (over land)', avg_T_landanom],
            ['Global (over sea)', avg_T_seaanom]]

    for i, rmax in enumerate(regionmax):
        if rmax > 0:
            latrange = np.str(np.around(rmax)) + 'N'
        else:
            latrange = np.str(np.around(np.abs(rmax))) + 'S'
        if regionmin[i] > 0.:
            latrange = latrange + np.str(np.around(regionmin[i])) + 'N'
        else:
            latrange = latrange + np.str(np.around(np.abs(regionmin[i]))) + 'S'
        
        data.append(['glob_' + latrange, land_sea_anom[0,i]])
        data.append(['land_' + latrange, land_sea_anom[1,i]])
        data.append(['sea_' + latrange, land_sea_anom[2,i]])
        

    df = pd.DataFrame(data, columns = ['Simulated temperature', MODELNAME])
 

    # save dataframe as a excel file
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/IPCC/' + MODELNAME + 
          'mPWP_CMIP6_land_sea.csv')
    #df.to_excel(filename)
    df.to_csv(filename)


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM)

    # get land and sea mask
    land_mask, sea_mask, lsm_lats, lsm_lons = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    plio_data, pi_data, lats, lons = get_nsat_data()

    # check grid
    for i, lat in enumerate(lsm_lats):
        if lat != lats[i]:
           print('lsm lat doesnt match', i, lat, lats[i])
           sys.exit(0)
    for i, lon in enumerate(lsm_lons):
        if lon !=lons[i]:
           print('lsm lon doesnt match', i, lon, lons[i])
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, avg_land_T_plio, 
     avg_sea_T_plio, grid_areas) = get_global_avg(land_mask, sea_mask, 
                                                 plio_data, lats, lons)
                                    
    (avg_T_pi, avg_land_T_pi,
     avg_sea_T_pi, grid_areas) = get_global_avg(land_mask, sea_mask,
                                                pi_data, lats, lons)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask, pi_data,
                                                        lats, lons, 
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask, plio_data,
                                                        lats, lons,
                                                        grid_areas)

       
    
 # write to a spreadsheet
    land_sea_anom = land_sea_region_plio - land_sea_region_pi

    write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin)
    


    

   


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "HadGEM3"
FIELDNAMEIN = ['tas']


START = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
FILENAME_PI = START + 'climatologies/E280/atmos/clims_hadgem3_pi_airtemp_final.nc'
FILENAME_PLIO = START + 'climatologies/Eoi400/atmos/clims_hadgem3_pliocene_airtemp_final.nc'
FIELDNAME = 'temp'
LSM = START + 'hadgem3.mask.nc'
FIELDLSM = 'land_binary_mask'#

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

get_land_sea_contrast()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/calculate_land_sea_contrast.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import netCDF4
import sys
#import os


###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d

    ############################################
    if MODELNAME == 'IPSLCM5A' or MODELNAME == 'IPSLCM5A2':
        plio_lsm_cube = get_ipsl_lsm(LSM_PLIO, FIELDLSM)
        pi_lsm_cube = get_ipsl_lsm(LSM_PI, FIELDLSM)
    elif MODELNAME == 'HadGEM3':
        test = iris.fileformats.netcdf.load_cubes(LSM_PLIO, callback=None)
        print(test)
        for data in test:
            print(data)
        sys.exit(0)
        f = netCDF4.Dataset(LSM_PLIO, "r")
        print(f)
        sys.exit(0)
    else:
        plio_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PI, FIELDLSM))
        pi_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PLIO, FIELDLSM))
   
    plio_lsm_cube2 = change_to_2d(plio_lsm_cube)
    pi_lsm_cube2 = change_to_2d(pi_lsm_cube)
   
    plio_lsm_data = plio_lsm_cube2.data
    pi_lsm_data = pi_lsm_cube2.data

    if MODELNAME == 'IPSLCM6A':
        plio_lsm_data = plio_lsm_data / 100.0
        pi_lsm_data = pi_lsm_data / 100.0

    if MODELNAME == 'EC-Earth3.3':
        plio_lsm_data = np.where(plio_lsm_data > 0.5, 1.0, 0.0)
        pi_lsm_data = np.where(pi_lsm_data > 0.5, 1.0, 0.0)
  
  
    land_mask = np.zeros(np.shape(plio_lsm_data))
    sea_mask = np.zeros(np.shape(plio_lsm_data))

    for ix, plio_mask in np.ndenumerate(plio_lsm_data):
        if plio_mask == 1.0 and pi_lsm_data[ix] == 1.0:
            land_mask[ix] = 1.0
        #if pi_lsm_data[ix] > 0:
        #    land_mask[ix] = 1.0
        if plio_mask == 0.0 and pi_lsm_data[ix] == 0.0:
            sea_mask[ix] = 1.0

    land_cube = plio_lsm_cube2.copy(data=land_mask)
    land_cube.var_name = 'land_mask'
    land_cube.long_name = 'land_mask'
    print('getting sea mask')
    sea_cube = pi_lsm_cube2.copy(data=sea_mask)
    sea_cube.var_name = 'sea_mask'
    sea_cube.long_name = 'sea_mask'
    print('got sea mask')

    return land_cube, sea_cube

def get_hadcm3_data(filestart):
    """
    gets the nsat data from HadCM3 and MRI
    called by get_nsat_data
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if MODELNAME  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filestart + yearuse + '.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if MODELNAME  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if MODELNAME  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')
  
    if MODELNAME  == 'HadCM3':
        cube_temp.coord('ht').rename('surface')

    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))

    return cube

def get_ipslcm6a_data(file):
    """
    for ipslcm6a we have 200 years in the file.  but we only need 100 years
    """
    cube = iris.load_cube(file, FIELDNAME)
    # reduce number of years
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube100yr = cubelist.concatenate_cube()
  
    return cube100yr

def get_ipsl5_data(filename, exptname):
    """
    gets nsat data from ipsl
    there is a bit of an error in the file calendar so we will
    """
# copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        elif ncattr !='_FillValue':
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = 'Temperature 2m'

        cube = iris.load_cube('temporary.nc', fieldreq)

        cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm5a2_data(filename):
    """
    gets the data for ipslcm5a2
    and removes all auxillary coordinates
    """
    cubelist = iris.cubeList
    cube = iris.load_cube(filename, FIELDNAME)
    for coord in cube.aux_coords:
        coord.rename('toremove')
        cube.remove_coord('toremove')
    return cube

def get_giss_data(filenames):
    """
    gets giss data: this is in two files
    """ 
    allcubes = iris.cube.CubeList([])
    for file in filenames:
        cubetemp = iris.load_cube(file, FIELDNAME)
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
  
    return(cube)
    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """
        

    if MODELNAME == 'HadCM3' or MODELNAME == 'MRI-CGCM2.3':
        cube_plio = get_hadcm3_data(FILENAME_PLIO)
        cube_pi = get_hadcm3_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM6A':
        cube_plio = get_ipslcm6a_data(FILENAME_PLIO)
        cube_pi = get_ipslcm6a_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM5A':
        cube_pi = iris.load(FILENAME_PI)[0]
        cube_plio = get_ipsl5_data(FILENAME_PLIO,'Eoi400')
    elif MODELNAME == 'IPSLCM5A2':
        cube_plio = get_ipslcm5a2_data(FILENAME_PLIO)
        cube_pi = get_ipslcm5a2_data(FILENAME_PI)
    elif MODELNAME == 'GISS2.1G':
        cube_plio = get_giss_data(FILENAME_PLIO)
        cube_pi = get_giss_data(FILENAME_PI)
    else:
        cube_plio = iris.load_cube(FILENAME_PLIO, FIELDNAME)
        cube_pi = iris.load_cube(FILENAME_PI, FIELDNAME)
    
   
    cube_plio_avg = cube_plio.collapsed('time', iris.analysis.MEAN)
    cube_pi_avg = cube_pi.collapsed('time', iris.analysis.MEAN)
  
    return cube_plio_avg, cube_pi_avg


def get_global_avg(land_cube, sea_cube, cube_nsat):
    """
    get's global average temperature, and also global avg for
    the land and the ocean
    """


    if cube_nsat.coord('latitude').has_bounds():
        cube_nsat.coord('latitude').bounds
    else:
        cube_nsat.coord('latitude').guess_bounds()

    if cube_nsat.coord('longitude').has_bounds():
        cube_nsat.coord('longitude').bounds
    else:
        cube_nsat.coord('longitude').guess_bounds()

  
    grid_areas = iris.analysis.cartography.area_weights(cube_nsat)
    grid_areas_land = grid_areas * land_cube.data
    grid_areas_sea = grid_areas * sea_cube.data

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)

    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15
   
    return avg_temp_data, avg_temp_land, avg_temp_sea, grid_areas


def get_regional_landsea(rmax, rmin, land_cube, sea_cube, cube_nsat,
                         grid_areas_region):
    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    lats = cube_nsat.coord('latitude').points
    grid_areas_use = grid_areas_region * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j, :] = 0.0
    grid_areas_land = grid_areas_use * land_cube.data
    grid_areas_sea = grid_areas_use * sea_cube.data
    

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_use)
   
    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)
   
    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15

    return [avg_temp_data, avg_temp_land, avg_temp_sea]
           
                                                       
def write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin):
    """
    write the information to a pandas dataframe
    """
    print(avg_T_anom, MODELNAME)

    data = [['Global', avg_T_anom], ['Global (over land)', avg_T_landanom],
            ['Global (over sea)', avg_T_seaanom]]

    for i, rmax in enumerate(regionmax):
        if rmax > 0:
            latrange = np.str(np.around(rmax)) + 'N'
        else:
            latrange = np.str(np.around(np.abs(rmax))) + 'S'
        if regionmin[i] > 0.:
            latrange = latrange + np.str(np.around(regionmin[i])) + 'N'
        else:
            latrange = latrange + np.str(np.around(np.abs(regionmin[i]))) + 'S'
        
        data.append(['glob_' + latrange, land_sea_anom[0,i]])
        data.append(['land_' + latrange, land_sea_anom[1,i]])
        data.append(['sea_' + latrange, land_sea_anom[2,i]])
        

    df = pd.DataFrame(data, columns = ['Simulated temperature', MODELNAME])
 

    # save dataframe as a excel file
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/IPCC/' + MODELNAME + 
          'mPWP_CMIP6_land_sea.csv')
    #df.to_excel(filename)
    df.to_csv(filename)


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM_PLIO)

    # get land and sea mask
    land_mask_cube, sea_mask_cube = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    cube_nsat_plio, cube_nsat_pi = get_nsat_data()
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, 
     avg_land_T_plio, 
     avg_sea_T_plio,
     gridareas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                      cube_nsat_plio)

 

    (avg_T_pi, 
     avg_land_T_pi,
     avg_sea_T_pi,
     grid_areas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                     cube_nsat_pi)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))

    #plt.subplot(2,1,1)
    #V = [0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2]
    #qplt.contourf(cube_nsat_plio - cube_nsat_pi, levels=V)
    #plt.subplot(2,1,2)
    #qplt.contourf((sea_mask_cube + land_mask_cube),  levels=V)
    #plt.show()
    #sys.exit(0)
   


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_pi,
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_plio,
                                                        grid_areas)

       
    
 # write to a spreadsheet
    land_sea_anom = land_sea_region_plio - land_sea_region_pi

    write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin)
    


    

   

#############################################################################
def getnames():

# this program will get the names of the files and the field for each
# of the model

  
    # get names for each model

    if MODELNAME == 'CESM2':
        file_e280 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                         'f09_g17.CMIP6-piControl.' + 
                         '001.cam.h0.TREFHT.110001-120012.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                           'f09_g17.PMIP4-midPliocene-eoi400.' + 
                           '001.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                      'f09_g17.PMIP4-midPliocene-eoi400.001.' + 
                      'cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'COSMOS':
        file_e280 = (FILESTART + 'AWI/COSMOS/E280/E280.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        file_eoi400 = (FILESTART + 'AWI/COSMOS/Eoi400/Eoi400.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        fielduse =  "2m temperature"
        lsm_e280 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                    "E280_et_al/E280.slf.atm.nc")
        lsm_eoi400 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                      "Eoi400_et_al/Eoi400.slf.atm.nc")
        fieldlsm = "SLF"

    if MODELNAME == 'EC-Earth3.3':
        file_e280 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_surface.nc'
        file_eoi400 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_surface.nc'
        fielduse = 'Air temperature at 2m'
        lsm_e280 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc'
        lsm_eoi400 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc'
        fieldlsm = 'Land/sea mask'

    if MODELNAME == 'CESM1.2':
        file_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0701.0800.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if MODELNAME   ==  'MIROC4m':
        file_e280 = (FILESTART + 'MIROC4m/tas/MIROC4m_E280_Amon_tas.nc')
        file_eoi400 = (FILESTART + 'MIROC4m/tas/MIROC4m_Eoi400_Amon_tas.nc')
        fielduse = "tas"
        lsm_e280 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc')
        lsm_eoi400 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc')
        fieldlsm = "sftlf"

    if MODELNAME  == 'HadCM3':
        file_e280 = (FILESTART+'LEEDS/HadCM3/e280/NearSurfaceTemperature/' + 
                     'e280.NearSurfaceTemperature.')
        file_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/NearSurfaceTemperature/' + 
                     'eoi400.NearSurfaceTemperature.')
        fielduse = "TEMPERATURE AT 1.5M"
        lsm_e280 = (FILESTART+'LEEDS/HadCM3/e280/qrparm.mask.nc')
        lsm_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc')
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if MODELNAME == 'CCSM4':
        file_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0081.0180.nc')
        file_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.TREFHT.1001.1100.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'CCSM4_Utr':
        file_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                     'tas_Amon_CESM1.0.5_E280_r1i1p1f1_gn_275001-285012.nc')  
        file_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                       'tas_Amon_CESM1.0.5_Eoi400_r1i1p1f1_gn_190001-200012.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                    'land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC' + 
                    '_control_r1i1p1f1_gn.nc')
        lsm_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                      'land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_' + 
                      'f19g16_NESSC_control_r1i1p1f1_gn.nc')
        fieldlsm = 'LANDMASK[D=1]'
  
    if MODELNAME == 'CCSM4_UoT':
        start = FILESTART + 'UofT/UofT-CCSM4/'
        file_e280 = (start + '/E280/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_piControl_r1i1p1f1_gn_150101-160012.nc')  
        file_eoi400 = (start + '/Eoi400/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_midPliocene-eoi400_r1i1p1f1_gn_' + 
                       '160101-170012.nc') 
        fielduse = 'air_temperature'
        lsm_e280 = start + 'for_julia/E_mask.nc'
        lsm_eoi400 = start + 'for_julia/Eoi_mask.nc'
        fieldlsm = 'gridbox land fraction'
      
    if MODELNAME == 'NorESM-L':
       file_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_TREFHT.nc')
       file_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_TREFHT.nc')
       fielduse = 'Reference height temperature'
       lsm_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc')
       lsm_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc')
       fieldlsm = 'Fraction of sfc area covered by land'


    if MODELNAME  == 'MRI-CGCM2.3':
        file_e280 = (FILESTART + 'MRI-CGCM2.3/tas/e280.tas.')
        file_eoi400 = (FILESTART + 'MRI-CGCM2.3/tas/eoi400.tas.')
        fielduse = 'near surface air temperature [degC]'
        lsm_e280 = (FILESTART + 'MRI-CGCM2.3/sftlf.nc')
        lsm_eoi400 = lsm_e280
        fieldlsm = 'landsea mask [0 - 1]'


    if MODELNAME  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        mid = 'e280/tas_Amon_GISS-E2-1-G_piControl_r1i1p1f1_gn_'
        file_e280 = ([start + mid + '490101-495012.nc', 
                      start + mid + '495101-500012.nc'])
        mid = 'eoi400/tas_Amon_GISS-E2-1-G_midPliocene-eoi400_r1i1p1f1_gn_'
        file_eoi400 = ([start + mid + '305101-310012.nc',
                        start + mid + '310101-315012.nc'])
        fielduse = 'air_temperature'
        lsm_e280 = start + 'e280/NASA-GISS_PIctrl_all_fland.nc'
        lsm_eoi400 = start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc'
        fieldlsm = 'fland'

    if MODELNAME == 'NorESM1-F':
        file_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_TREFHT.nc'
        file_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_TREFHT.nc'
        lsm_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc'
        lsm_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc'
        fielduse = 'Reference height temperature'
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if MODELNAME == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        file_e280 = start + 'tas_Amon_IPSL-CM6A-LR_piControl_r1i1p1f1_gr_285001-304912.nc'
        file_eoi400 = (start + 'tas_Amon_IPSL-CM6A-LR_midPliocene-eoi400_' + 
                       'r1i1p1f1_gr_185001-204912.nc')
        lsm_e280 = start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc'
        lsm_eoi400 = start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc'
        fielduse = 'air_temperature'
        fieldlsm = 'land_area_fraction'

    if MODELNAME == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A/PI.NearSurfaceTemp_tas_3600_3699_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A/Eoi400.NearSurfaceTemp_tas_3581_3680_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Tas'
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if MODELNAME == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A2/PI.NearSurfaceTemp_tas_6110_6209_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A2/Eoi400.NearSurfaceTemp_tas_3381_3480_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Temperature 2m'
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if MODELNAME == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        file_e280 = start + 'climatologies/E280/clims_hadgem3_pi_airtemp_final.nc'
        file_eoi400 = start + 'climatologies/Eoi400/clims_hadgem3_pliocene_airtemp_final.nc'
        fielduse = 'temp'
        lsm_e280 = start + 'hadgem3.mask.nc'
        lsm_eoi400 = lsm_e280
        fieldlsm = 'land_binary_mask'
            
            
      
    retdata = [fielduse, file_e280, file_eoi400,
               fieldlsm, lsm_e280, lsm_eoi400]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "HadGEM3" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                  
FIELDNAMEIN = ['tas']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


# call program to get model dependent names
# fielduse,  and  filename
retdata = getnames()

FIELDNAME = retdata[0]
FILENAME_PI = retdata[1]
FILENAME_PLIO = retdata[2]
FIELDLSM = retdata[3]
LSM_PI = retdata[4]
LSM_PLIO = retdata[5]

print('fieldname is',FIELDNAME)

get_land_sea_contrast()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/CCSM4_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
"""
This program will average all of the CCSM4 models.
We will average the data file (text files) and also the mean average temperature file
"""

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys


#########################################################################
# stuff for doing text file is here  
def get_text_data(filename):
    """
    gets the text data for each filename
    returns globalmean, monthly mean, latitudinal mean
    """
    f=open(filename,"r")
    f1=f.readlines()
    f2 = [x.replace('\n', '') for x in f1]
            
    # get the means according to their position in the file
    all_mean_sd=f2[2]
    all_mon_mean_sd=f2[5:5+12]
    all_lat_mean_sd=f2[20:20+180]
           
    # extract global mean
    meanglob,sd=all_mean_sd.split(',')
    
   
    monmeans = np.zeros(12)
    latmeans = np.zeros(180)
    lats = np.zeros(180)
    # extract monthly means
    for x in all_mon_mean_sd:
        mon,mean,sd=x.split(',')
        monmeans[int(mon)-1]=float(mean)
            
            
    # extract latitude means
    for x in all_lat_mean_sd:
        lat,mean,sd=x.split(',')
        latss=int(float(lat)+89.5) # convert latitude to a subscript
               
        if mean != ' --' and mean != '--':
            latmeans[latss]=float(mean) # stores latitudinal means
        else:
            latmeans[latss]=np.nan
            
        lats[latss]=lat # stores latitudes
      
   
    return meanglob, monmeans, latmeans, lats
   
def writeout_textdata(mean_global, mean_mon, mean_lat, lats, expt):
    """
    writeout the text data to a file
    replace stdev with -999
    """
    
    textout = FILESTART + OUTMODEL + '/' + expt + '.' + FIELDNAME + '.data.txt'
    file1 =  open(textout, "w")

    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
   
    # write out global temperautre
    file1.write(np.str(np.round(mean_global, 2))+', -99.99\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(mean_mon[i], 2))+', -99.99\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(mean_lat)):
        file1.write(np.str(lats[i])+', '+np.str(np.round(mean_lat[i], 2))+', -99.99\n')

    file1.close()
    

def main_avg_text(expt):
    """
    loop over all the models and extract mean, monthlymeans, latitudinal means
    average all the means
    write out to a file in the format of the input.  (Put standard deviation to -999.999)
    """
    
    for i, model in enumerate(MODELNAMES):
        filename = FILESTART + model + '/' + expt + '.' + FIELDNAME + '.data.txt'
        
        globmean, monmeans, latmeans, lats = get_text_data(filename)
        
        if i == 0:
            allmeans = np.zeros(NMODELS)
            allmonmeans = np.zeros((NMODELS, len(monmeans)))
            alllatmeans = np.zeros((NMODELS, len(latmeans)))
        
        allmeans[i] = globmean
        allmonmeans[i, :] = monmeans
        alllatmeans[i, :] = latmeans
        
    mean_global = np.mean(allmeans)
    mean_mon = np.mean(allmonmeans, axis=0)
    mean_lat = np.mean(alllatmeans, axis=0)
    
    writeout_textdata(mean_global, mean_mon, mean_lat, lats, expt)
 
###############################################
## stuff for doing netcdf file is here
def main_avg_netcdf(expt):
    """
    loop over all the models and extract the global average netcdf file
    average all the means
    write out to a file in the format of the input.  
    """ 
    all_cubes=iris.cube.CubeList([])     
    for i, model in enumerate(MODELNAMES):
        print(i)
        filename = FILESTART + model + '/' + expt + '.' + FIELDNAME + '.allmean.nc'
        cube = iris.load_cube(filename)
        modelcube = resort_coords(cube, i)
        modelcube.data=modelcube.data.astype('float32') 
        all_cubes.append(modelcube)
    
    
    iris.experimental.equalise_cubes.equalise_attributes(all_cubes)
  
    cat_cubes = all_cubes.concatenate_cube()
    meancube = cat_cubes.collapsed(['model_level_number'], iris.analysis.MEAN)  
    
    fileout = FILESTART + OUTMODEL + '/' + expt + '.' + FIELDNAME + '.allmean.nc'
    iris.save(meancube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)
  
    
def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
       
       
     
    newcube=iris.util.new_axis(cube)
    newcube.add_dim_coord(iris.coords.DimCoord(levelno, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
   
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    newcube.rename('tas')
    
        
    return newcube      
    
    
def main():
    """ 
    main program
    1. average the text files for each of the models and writeout
    2. average the netcdf files from each of the models and writeout
    """
    
    for i, expt in enumerate(EXPTNAMES):
        avgtext = main_avg_text(expt)
        avgnetcdf = main_avg_netcdf(expt)
    
    
   

##########################################################
# fixed constants
        
LINUX_WIN='w'
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'#
else:
    FILESTART = ' '

MODELNAMES=['CCSM4-1deg', 'CCSM4-2deg','CCSM4-UoT']
NMODELS = len(MODELNAMES)
OUTMODEL = 'CCSM4-avg'

FIELDNAME='NearSurfaceTemperature'
EXPTNAMES=['EOI400','E280']
#EXPTNAMES=['EOI400']


main()::::::::::::::
CEMAC/PLIOMIP2/climate_sensitivity_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
import sys



#####################################
def  climate_sensitivity_analysis(modelnames,fieldname,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    else:
        filestart='/nfs/hera1/earjcti/regridded/'
     
        
    # set up a dictionary for the climate sensitivity
   
    # from my stuff
    #clim_sens ={'NorESM-L': 3.1,
    #             'NorESM1-F':2.29,
    #             'IPSLCM6A': 4.8,
    #             'IPSLCM5A':3.4,
    #             'HadCM3': 3.7,
    #             'MIROC4m':3.9,
    #             'COSMOS':4.1,
    #             'UofT':3.8,
    #             'EC-Earth3.1':3.2,
    #             'MRI-CGCM2.3':2.8,
    #             'CESM1.0.5': 3.1,
    #             'GISS': 3.31
    #             }
        
    # from Alan's table provided by authors
    clim_sens ={'NorESM-L': 3.1,
                 'NorESM1-F':2.3,
                 'IPSLCM6A': 4.8,
                 'IPSLCM5A2':3.6,
                 'IPSLCM5A':4.1,
                 'HadCM3': 3.5,
                 'MIROC4m':3.9,
                 'COSMOS':4.7,
                 'CCSM4-UoT':3.2,
                 'EC-Earth3.1':3.2,
                 'MRI2.3':2.8, # from my investigation
                 'CCSM4-2deg': 3.2,
                 'GISS2.1G': 3.3,
                 'CESM2': 5.3,
                 'CESM1.2' :4.1,
                 'CCSM4-1deg' :3.2
                 }
        
     
    # first get the data.  We need climate sensitivity, 
    # global temperature anomaly, latitude temperature anomaly
    # and gridbox by gridbox temperature anomaly
    climdiff=np.zeros(len(modelnames))
    climdiffmon=np.zeros((len(modelnames),12))
    climdifflat=np.zeros((len(modelnames),180))
    sensitivity_array=np.zeros(len(modelnames))
    alllats=np.arange(0,180,1)-89.5
    
    for mod in range(0,len(modelnames)):
        sensitivity_array[mod]=clim_sens.get(modelnames[mod])
        
        # get data from experiment file
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.data.txt'
        file1= open(fileexpt,"r")
        lines=list(file1)
       
        
        meanexpt,sdexpt=lines[2].split(",")
        monmeanexpt=np.zeros(12)
        latmeanexpt=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeanexpt[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeanexpt[index]=np.float(mean)
       
        
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.data.txt'
        file2= open(filecntl,"r")
        lines=list(file2)
        meancntl,sdexpt=lines[2].split(",")
        monmeancntl=np.zeros(12)
        latmeancntl=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeancntl[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeancntl[index]=np.float(mean)
        
        climdiff[mod]=np.float(meanexpt)-np.float(meancntl)
        
        climdiffmon[mod,:]=monmeanexpt-monmeancntl
        climdifflat[mod,:]=latmeanexpt-latmeancntl
        #print(modelnames[mod],climdifflat[mod])
    
     
    ########################################################
    # plot the climate sensitivity vs the global mean
    fig = plt.figure(figsize=(7.0, 4.0))
    plt.plot(climdiff,sensitivity_array,'x')
    plt.xlabel('Plio_Core - PI_CTL SAT anomaly',fontsize=15)
    plt.ylabel('ECS', fontsize=15)
    plt.xlim(np.floor(np.min(climdiff)),np.ceil(np.max(climdiff)))
    
    # do a linear regression
    print(modelnames)
    print(climdiff)
    print(sensitivity_array)
    print(np.mean(climdiff)*1.94, np.mean(sensitivity_array))
    sys.exit(0)
   
    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(climdiff, sensitivity_array)
    xarray=np.arange(0,5,1)
    yarray=intercept+(slope*xarray)
    plt.plot(xarray,yarray)
    plt.tick_params(axis='x',  labelsize=15)
    plt.tick_params(axis='y',  labelsize=15)
    #plt.title("R-squared: %2f, p-value: %2f" %'{:06.2f}'.format(r_value**2,), np.round(p_value, 2)), fontsize=15)
    plt.title("R-squared: " + np.str(np.around((r_value**2.), 2)) 
            + ",  p-value: " + np.str(np.around(p_value, 2)) , fontsize=15) 
    

    if redu == '':
        figtext = 'a)'
    else:
        figtext = 'a)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
  
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom' + 
             redu + '.eps')
    
    plt.tight_layout()
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    plt.close()

    rsq_std=r_value**2.
    
    ########################################################
    # plot the correlation between climate sensitivity vs the monthly mean
    rvals=np.zeros(12)
    pvals=np.zeros(12)
    for mon in range(0,12):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdiffmon[:,mon], sensitivity_array))
        rvals[mon]=r_value**2.
        pvals[mon]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    

    color = 'tab:red'
    ax1.set_xlabel('month', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=15)
    ax1.plot(np.arange(1,13,1), rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=15)
    ax1.tick_params(axis='x',  labelsize=15)
    #ax1.plot([0,13],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axi
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=15)  # we already handled the x-label with ax1
    ax2.plot(np.arange(1,13,1), pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=15)
    plt.title('ECS vs Plio_Core - PI_CTL by month', fontsize=15)
    
    
    if redu == '':
        figtext = 'c)'
    else:
        figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
   
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom' + 
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    plt.close()
    
    
     ########################################################
    # plot the correlation between climate sensitivity vs the latitudinal mean
    rvals=np.zeros(180)
    pvals=np.zeros(180)
    for lat in range(0,180):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdifflat[:,lat], sensitivity_array))
        rvals[lat]=r_value**2.
        pvals[lat]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))

    color = 'tab:red'
    ax1.set_xlabel('latitude', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=15)
    ax1.plot(alllats, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=15)
    ax1.tick_params(axis='x', labelsize=15)
    #ax1.plot([-90,90],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])
    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=15)  # we already handled the x-label with ax1
    ax2.plot(alllats, pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=15)
    ax2.set_ylim(0,0.1)
    plt.title('ECS vs Plio_Core - PI_CTL by latitude', fontsize=15)
    
    if redu == '':
        figtext = 'e)'
    else:
        figtext = 'c)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom' +
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    
    
    # now get the global data and do a correlation
    for mod in range(0,len(modelnames)):
        # get average anomaly
        
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.allmean.nc'
        exptcube=iris.load_cube(fileexpt)
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.allmean.nc'
        cntlcube=iris.load_cube(filecntl)
        
        ny,nx=np.shape(exptcube.data)
        if mod==0:
            anommap=np.zeros((len(modelnames),ny,nx))
           
       
        anommap[mod,:,:]=exptcube.data-cntlcube.data
    
        
    
    rsqmap=np.zeros((ny,nx))
    pvalmap = np.zeros((ny, nx))
    slopemap = np.zeros((ny,nx))
    interceptmap = np.zeros((ny,nx))
    for j in range(0,ny):
         for i in range(0,nx):
             slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(anommap[:,j,i], sensitivity_array))
             rsqmap[j,i]=r_value**2.
             pvalmap[j,i] = p_value
             slopemap[j,i] = slope
             interceptmap[j,i] = intercept
           
             
    rsqmapcube=exptcube.copy(data=rsqmap) 
    rsqmapcube.units=None
    
    slopecube=exptcube.copy(data=slopemap) 
    slopecube.units=None
    
    interceptcube=exptcube.copy(data=interceptmap) 
    interceptcube.units=None
    
    temparr = np.where(pvalmap < 0.05, 1, 0) 
    significance_cube = rsqmapcube.copy(data=temparr)
    
    
    plt.subplot(1,1,1)
    V=np.arange(0.0,1,0.05)
    qplt.contourf(rsqmapcube,V,cmap='YlGnBu')
    iplt.contourf(significance_cube, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(significance_cube, 1, hatches=[None, '\\\''], colors='none')
    #titlename=modeluse+' '+exptname+': '+field
    #bar=plt.colorbar(cs,orientation="horizontal")
    plt.title('correlation between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    
    #plt.title(titlename,fontsize=8)
    plt.gca().coastlines()
 
    if redu == '':
        figtext = 'g)'
    else:
        figtext = 'd)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe'+
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe' + 
             redu + '.pdf')
    plt.savefig(fileout)  
    plt.close()    
    
        
    # print out the slope and the intercept at each gridpoint
    plt.subplot(1,1,1)
    qplt.contourf(slopecube)
    plt.title('slope between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_slope_'+
                redu + '.eps')  
 
    plt.close()
    
    plt.subplot(1,1,1)
    qplt.contourf(interceptcube)
    plt.title('intercept between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_intercept_'+
                redu + '.eps')  
 
    plt.close()
    
    outfile = (filestart + 'allplots/' + 
              fieldname + '/climate_sensitivity_relationships'+
                redu + '.txt')
    
    txtfile1 = open(outfile,"w+") 
    txtfile1.write("longitude latitude rsq pvalue intercept slope \n")
    lons = interceptcube.coord('longitude').points
    lats = interceptcube.coord('latitude').points
    
    for j in range(0,ny):
         for i in range(0,nx):
             writestring = (np.str(np.around(lons[i],2)) + ',' + 
                            np.str(np.around(lats[j],2)) + ',' + 
                            np.str(np.around(rsqmap[j, i],2)) + ',' +
                            np.str(np.around(pvalmap[j, i],2)) + ',' +
                            np.str(np.around(interceptmap[j, i],2)) + ',' +
                            np.str(np.around(slopemap[j,i],2)) + '\n')
             txtfile1.write(writestring)
    txtfile1.close

##########################################################
# main program
        
filename=' '
linux_win='l'
#modelnames=['MIROC4m','COSMOS']   # MIROC4m  COSMOS UofT EC-Earth3.1

#modelnames=['CCSM4-2deg','COSMOS', 'CCSM4-1deg',
#            'EC-Earth3.1', 'CESM1.2',
#            'GISS2.1G','HadCM3',
#            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
#            'MIROC4m','MRI2.3',
#            'NorESM-L','NorESM1-F',
#            'CCSM4-UoT'
#            ]
#redu = ''

modelnames=['COSMOS', 'CESM1.2', 'CCSM4-1deg',
            'GISS2.1G','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI2.3',
            'NorESM-L','NorESM1-F',
            'CCSM4-UoT'
            ]
redu = '_redu'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
#fieldnames = ['SST']
units=['degC']
exptname='EOI400'
cntlname='E280'

for field in range(0,len(fieldnames)):
    climate_sensitivity_analysis(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)::::::::::::::
CEMAC/PLIOMIP2/climate_sensitivity.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

#os.environ["PROJ_LIB"] = r'C:\Users\julia\Miniconda2\pkgs\proj4-5.2.0-hc56fc5f_1003\Library\share'
#from mpl_toolkits.basemap import Basemap, shiftgrid
import sys



#####################################
def  climate_sensitivity_analysis(modelnames,fieldname,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
        datatext = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\data_for_7a-b.txt'
        netcdfout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\data_for_7c.nc'
    else:
        filestart='/nfs/hera1/earjcti/regridded/'
        datatext = '/nfs/hera1/earjcti/regridded/alldata/data_for_7a-b.txt'
        netcdfout = '/nfs/hera1/earjcti/regridded/alldata/data_for_7c.nc'
     
        
    # set up a dictionary for the climate sensitivity
   
    # from my stuff
    #clim_sens ={'NorESM-L': 3.1,
    #             'NorESM1-F':2.29,
    #             'IPSLCM6A': 4.8,
    #             'IPSLCM5A':3.4,
    #             'HadCM3': 3.7,
    #             'MIROC4m':3.9,
    #             'COSMOS':4.1,
    #             'UofT':3.8,
    #             'EC-Earth3.1':3.2,
    #             'MRI-CGCM2.3':2.8,
    #             'CESM1.0.5': 3.1,
    #             'GISS': 3.31
    #             }
        
    # from Alan's table provided by authors
    clim_sens ={'NorESM-L': 3.1,
                 'NorESM1-F':2.3,
                 'IPSLCM6A': 4.8,
                 'IPSLCM5A2':3.6,
                 'IPSLCM5A':4.1,
                 'HadCM3': 3.5,
                 'MIROC4m':3.9,
                 'COSMOS':4.7,
                 'CCSM4-UoT':3.2,
                 'EC-Earth3.1':3.2,
                 'EC-Earth3.3':4.3,
                 'MRI2.3':2.8, # from my investigation
                 'CCSM4-Utr': 3.2,
                 'GISS2.1G': 3.3,
                 'CESM2': 5.3,
                 'CESM1.2' :4.1,
                 'CCSM4' :3.2,
                 'CCSM4-avg' : 3.2
                 }
        
     
    # first get the data.  We need climate sensitivity, 
    # global temperature anomaly, latitude temperature anomaly
    # and gridbox by gridbox temperature anomaly
    climdiff=np.zeros(len(modelnames))
    climdiffmon=np.zeros((len(modelnames),12))
    climdifflat=np.zeros((len(modelnames),180))
    sensitivity_array=np.zeros(len(modelnames))
    alllats=np.arange(0,180,1)-89.5
    
    for mod in range(0,len(modelnames)):
        sensitivity_array[mod]=clim_sens.get(modelnames[mod])
        
        # get data from experiment file
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.data.txt'
        file1= open(fileexpt,"r")
        lines=list(file1)
       
        
        meanexpt,sdexpt=lines[2].split(",")
        monmeanexpt=np.zeros(12)
        latmeanexpt=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeanexpt[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeanexpt[index]=np.float(mean)
       
        
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.data.txt'
        file2= open(filecntl,"r")
        lines=list(file2)
        meancntl,sdexpt=lines[2].split(",")
        monmeancntl=np.zeros(12)
        latmeancntl=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeancntl[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeancntl[index]=np.float(mean)
        
        climdiff[mod]=np.float(meanexpt)-np.float(meancntl)
        
        climdiffmon[mod,:]=monmeanexpt-monmeancntl
        climdifflat[mod,:]=latmeanexpt-latmeancntl
        #print(modelnames[mod],climdifflat[mod])
    
     
    ########################################################
    # plot the climate sensitivity vs the global mean
   
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    ax1.plot(climdiff,sensitivity_array,'x')
    ax1.set_xlabel('Plio_Core - PI_CTL SAT anomaly',fontsize=15)
    ax1.set_ylabel('ECS', fontsize=15)
    ax1.set_xlim(np.floor(np.min(climdiff)),np.ceil(np.max(climdiff)))
    
    # do a linear regression
    print(modelnames)
    print(climdiff)
    print(sensitivity_array)
   
    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(climdiff, sensitivity_array)
    xarray=np.arange(0,10,1)
    yarray=intercept+(slope*xarray)
    ax1.plot(xarray,yarray)
    ax1.tick_params(axis='x',  labelsize=15)
    ax1.tick_params(axis='y',  labelsize=15)
    #plt.title("R-squared: " + np.str(np.around((r_value**2.), 2)) 
    #        + ",  p-value: " + np.str(np.around(p_value, 2)) , fontsize=15) 
    
    print('julia',slope, intercept, r_value, p_value)
   # sys.exit(0)
    figtext = 'a)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])
  
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom.png')
    
    #plt.tight_layout()
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom.pdf')
    plt.savefig(fileout)
    plt.close()

    rsq_std=r_value**2.
    
    f1 = openx=open(datatext,'w')
    f1.write('Data for Figure 7a\n')
    f1.write('model name, Plio_core - PI_Cntl, ECS\n')
    for i in range(0,len(modelnames)):
        f1.write(modelnames[i] + ',' + np.str(np.round(climdiff[i],2)) + ',' + 
                 np.str(np.round(sensitivity_array[i],2)) + '\n')
   

    
    ########################################################
    # plot the correlation between climate sensitivity vs the monthly mean
    rvals=np.zeros(12)
    pvals=np.zeros(12)
    for mon in range(0,12):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdiffmon[:,mon], sensitivity_array))
        rvals[mon]=r_value**2.
        pvals[mon]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

    color = 'tab:red'
    #ax1.set_xlabel('month', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=12)
    ax1.plot(labels, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=12)
    ax1.tick_params(axis='x',  labelsize=12)
    #ax1.plot([0,13],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axi
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=12)  # we already handled the x-label with ax1
    ax2.plot(labels, pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)
    #plt.title('ECS vs Plio_Core - PI_CTL by month', fontsize=15)
    
    
    figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
   
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom.pdf')
    plt.savefig(fileout)
    plt.close()
    
    
    
     ########################################################
    # plot the correlation between climate sensitivity vs the latitudinal mean
    rvals=np.zeros(180)
    pvals=np.zeros(180)
    for lat in range(0,180):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdifflat[:,lat], sensitivity_array))
        rvals[lat]=r_value**2.
        pvals[lat]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))

    color = 'tab:red'
    ax1.set_xlabel('latitude', fontsize=12)
    ax1.set_ylabel('Rsq', color=color, fontsize=12)
    ax1.plot(alllats, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=12)
    ax1.tick_params(axis='x', labelsize=12)
    #ax1.plot([-90,90],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])
    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=12)  # we already handled the x-label with ax1
    ax2.plot(alllats, pvals, color=color)
    #for i, lat in enumerate(alllats):
    #    print(lat, pvals[i], rvals[i])
   
    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)
    #ax2.set_ylim(0,0.1)
    #plt.title('ECS vs Plio_Core - PI_CTL by latitude', fontsize=15)
    
    figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom.pdf')
    plt.savefig(fileout)
    
    f1.write('\n')
    f1.write('Data for Figure 7b\n')
    f1.write('latitude, Pvalue, Rsqvalue\n')
    for i in range(0,len(alllats)):
        f1.write(np.str(np.round(alllats[i],1)) + ',' + np.str(np.round(pvals[i],3)) + ',' + 
                 np.str(np.round(rvals[i],2)) + '\n')
    f1.close()
    
    #############################################################################
    # now get the global data and do a correlation
    
    cubelist = iris.cube.CubeList([])
    for mod in range(0,len(modelnames)):
        # get average anomaly
        
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.allmean.nc'
        exptcube=iris.load_cube(fileexpt)
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.allmean.nc'
        cntlcube=iris.load_cube(filecntl)
        
        ny,nx=np.shape(exptcube.data)
        if mod==0:
            anommap=np.zeros((len(modelnames),ny,nx))
           
       
        anommap[mod,:,:]=exptcube.data-cntlcube.data
    
        
    
    rsqmap=np.zeros((ny,nx))
    pvalmap = np.zeros((ny, nx))
    slopemap = np.zeros((ny,nx))
    interceptmap = np.zeros((ny,nx))
    for j in range(0,ny):
         for i in range(0,nx):
             slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(anommap[:,j,i], sensitivity_array))
             rsqmap[j,i]=r_value**2.
             pvalmap[j,i] = p_value
             slopemap[j,i] = slope
             interceptmap[j,i] = intercept
           
             
    rsqmapcube=exptcube.copy(data=rsqmap) 
    rsqmapcube.units=None
    rsqmapcube.long_name = 'Rsq'
    rsqmapcube.standard_name = None
    rsqmapcube.var_name = 'Rsq'

    
    slopecube=exptcube.copy(data=slopemap) 
    slopecube.units=None
    
    interceptcube=exptcube.copy(data=interceptmap) 
    interceptcube.units=None
    
    temparr = np.where(pvalmap < 0.05, 1, 0) 
    significance_cube = rsqmapcube.copy(data=temparr)
    pval_cube = rsqmapcube.copy(data=pvalmap)
    pval_cube.units = None
    pval_cube.long_name = 'pvalue'
    pval_cube.standard_name = None
    pval_cube.var_name = 'pvalue'
    
    cubelist.append(rsqmapcube)
    cubelist.append(pval_cube)
    print(cubelist)
    iris.save(cubelist, netcdfout, netcdf_format='NETCDF3_CLASSIC')

    
    # plot the map with Rsq and the significance
    
    fig = plt.subplots(figsize=(7.0, 5.0))
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(0.0,1,0.05)
    
    qplt.contourf(rsqmapcube,V,cmap='YlGnBu')
    iplt.contourf(significance_cube, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(significance_cube, 1, hatches=[None, '\\\''], colors='none')
    #titlename=modeluse+' '+exptname+': '+field
    #bar=plt.colorbar(cs,orientation="horizontal")
    #plt.title('correlation between climate sensitivity and \n mPWP-PI anomaly '
    #          'at this point')
    plt.title('')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    
    #plt.title(titlename,fontsize=8)
    plt.gca().coastlines()
 
    figtext = 'c)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe.pdf')
    plt.savefig(fileout)  
    plt.close()    
    
        
    # print out the slope and the intercept at each gridpoint
    plt.subplot(1,1,1)
    qplt.contourf(slopecube)
    plt.title('slope between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_slope.eps')  
 
    plt.close()
    
    plt.subplot(1,1,1)
    
    qplt.contourf(interceptcube)
    plt.title('intercept between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_intercept.eps')  
 
    plt.close()
    
    outfile = (filestart + 'allplots/' + 
              fieldname + '/climate_sensitivity_relationships.txt')
    
    txtfile1 = open(outfile,"w+") 
    txtfile1.write("longitude latitude rsq pvalue intercept slope \n")
    lons = interceptcube.coord('longitude').points
    lats = interceptcube.coord('latitude').points
    
    for j in range(0,ny):
         for i in range(0,nx):
             writestring = (np.str(np.around(lons[i],2)) + ',' + 
                            np.str(np.around(lats[j],2)) + ',' + 
                            np.str(np.around(rsqmap[j, i],2)) + ',' +
                            np.str(np.around(pvalmap[j, i],2)) + ',' +
                            np.str(np.around(interceptmap[j, i],2)) + ',' +
                            np.str(np.around(slopemap[j,i],2)) + '\n')
             txtfile1.write(writestring)
    txtfile1.close

##########################################################
# main program
        
filename=' '
linux_win='l'
#modelnames=['MIROC4m','COSMOS']   # MIROC4m  COSMOS UofT EC-Earth3.1

modelnames=['CCSM4-Utr','COSMOS', 'CCSM4',
            'EC-Earth3.3', 'CESM1.2','CESM2',
            'GISS2.1G','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI2.3',
            'NorESM-L','NorESM1-F',
            'CCSM4-UoT'
            ]

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
#fieldnames = ['SST']
units=['degC']
exptname='EOI400'
cntlname='E280'

for field in range(0,len(fieldnames)):
    climate_sensitivity_analysis(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/create_grid.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 14 15:53:19 2019

#@author: earjcti

# create a blank netcdf file on the correct grid for putting the pliomip data on


import netCDF4
import numpy as np
from netCDF4 import Dataset

dataset=Dataset('one_lev_one_deg.nc', 'w',format='NETCDF3_CLASSIC') 
#create dimensions
level = dataset.createDimension('level', 1) 
latitude = dataset.createDimension('latitude', 180)
longitude = dataset.createDimension('longitude', 360) 
time = dataset.createDimension('time', None)

# create variables
times = dataset.createVariable('time', np.float64, ('time',)) 
levels = dataset.createVariable('level', np.int32, ('level',)) 
latitudes = dataset.createVariable('latitude', np.float32,('latitude',))
longitudes = dataset.createVariable('longitude', np.float32,('longitude',)) 
# Create the actual 4-d variable
dummy = dataset.createVariable('dummy', np.float32, ('time','level','latitude','longitude')) 

# Variable Attributes  
latitudes.units = 'degree_north'  
longitudes.units = 'degree_east'  
levels.units = 'Surface' 
dummy.units = 'None' 
times.units = 'hours since 0001-01-01 00:00:00'  
times.calendar = 'gregorian' 

# add variables
lats = np.linspace(-89.5,89.5,num=180) 
print(lats)
lons = np.arange(0,360,1.0)
print(len(lats)) 
print(len(lons))
latitudes[:] = lats  
longitudes[:] = lons 
dummy[:,:,:,:]=0.0
levels[0]=0

dataset.close()
::::::::::::::
CEMAC/PLIOMIP2/diagnostics_to_one_file.py
::::::::::::::
#!/usr/bin/env python3
# created 08/12/2022 by Julia
#
# This program will put all the regridded data for pliomip2 in one file 
# so they are easier for sharing.

import iris
from iris.cube import CubeList
import numpy as np

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F','HadGEM3'
            ]

FIELD = 'SST'

EXPTNAME = 'E280'

FILEINSTART = '/nfs/hera1/earjcti/regridded/'

cubelist = CubeList([])
for model in MODELNAMES:
    filename = FILEINSTART + model + '/' + EXPTNAME + '.SST.allmean.nc'
    cube = iris.load_cube(filename)
    print(cube)
    for coord in cube.coords():
        coord.bounds = None
    cube.long_name = model + '_' + FIELD
    cube.data = np.where(cube.data.mask == True, -99999., cube.data)
    cubelist.append(cube)
    


fileout = FILEINSTART + FIELD + '_' + EXPTNAME + '_allmodels.nc'
iris.save(cubelist,fileout,fill_value = -99999.)
::::::::::::::
CEMAC/PLIOMIP2/dmc_by_latitude.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
This program will do a DMC plot.  But it will be latitude vs deltSST

"""
import pandas as pd
import matplotlib as mp
import matplotlib.pyplot as plt
import numpy as np
import iris
import sys

def is_number(s):
    try:
        float(s)
        return True
    except ValueError:
        return False

def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints
    
def plot_data(lats, Tanom, stdev):
    """
    plots the data and the errorbars
    """
    
    stdevplot = np.zeros(len(stdev))
    for i in range(0, len(stdev)):
       numeric = is_number(stdev[i])
       if numeric:
            stdevplot[i] = stdev[i]
    
    print(stdevplot)

    plt.errorbar(lats, Tanom, yerr=stdevplot, fmt='o')
    

def get_multimodel_mean(fieldname):
    """
    gets the multimodel mean and calculates a zonal average
    """
    mmm_cube = iris.load_cube(MULTIMODELMEAN, fieldname)
    zm_cube = mmm_cube.collapsed(['longitude'], iris.analysis.MEAN)
    zm_cube_max = mmm_cube.collapsed(['longitude'], iris.analysis.MAX)
    zm_cube_min = mmm_cube.collapsed(['longitude'], iris.analysis.MIN)
    
    return zm_cube, zm_cube_max, zm_cube_min
    
def plot_zm(cubemean, cubemax, cubemin, max_cubemax, min_cubemin):
    """
    latitudinal plot + zonal range of multimodel mean
    cubemean, cubemax, cubemin are the mean and the range from the multimodel mean
    max_cubemax is the maximum longitude from the model with the maximum difference
    min_cubemin is the minimum longitude from the model with the minimum difference
    """
    
    lats = cubemean.coord('latitude').points
    datamean = cubemean.data
    datamax = cubemax.data
    datamin = cubemin.data
    
    fig, ax = plt.subplots() 
    ax.plot(lats, datamean)
    ax.fill_between(lats, min_cubemin.data, max_cubemax.data, alpha=0.4)
    ax.fill_between(lats, datamin, datamax, alpha=0.4)
    ax.set_ylim(-5.0,20.0)
   
    ax.set_xlabel('latitude')
    ax.set_ylabel('SST anomaly')
    


def main():
    """
    1. get the data
    2. get multimodel mean SST 
    3. plot the data on the same file as the MMM
    """

    lats, lons, data_tanom, data_stdev, Npoints = get_data()
    #
    zonal_mean_cube, zonal_max_cube, zonal_min_cube = get_multimodel_mean('SSTmean_anomaly')
    max_zonal_mean_cube, max_zonal_max_cube, max_zonal_min_cube = get_multimodel_mean('SSTmax_anomaly')
    min_zonal_mean_cube, min_zonal_max_cube, min_zonal_min_cube = get_multimodel_mean('SSTmin_anomaly')
   
   
    plot_zm(zonal_mean_cube, zonal_max_cube, 
            zonal_min_cube, max_zonal_max_cube, min_zonal_min_cube)
    plot_data(lats, data_tanom, data_stdev)
    
    
    plt.savefig(OUTNAME + '.eps')
    plt.savefig(OUTNAME + '.pdf')
    plt.close()

DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
MULTIMODELMEAN = '/nfs/hera1/earjcti/regridded/SST_multimodelmean.nc'
OUTNAME = '/nfs/hera1/earjcti/regridded/allplots/SST/dmc_by_latitude'

main()
::::::::::::::
CEMAC/PLIOMIP2/DMC_for_IPCC.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on August 2020


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
#import matplotlib as mp
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    import matplotlib as mpl
    import numpy as np
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=60, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

  
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    model_anom_cube, lsmplio_cube = get_model_data()

    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata() 
        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])

    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'H' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
main()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/DMC_for_IPCC_with_land.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    #plt.show()
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         land_lats, land_lons, land_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/DMC_for_IPCC_with_land_split.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 2021
# note this differs from DMC_for_IPCC_land in that it will produce 
# seperate plots for the land and the ocean
# it will also produce a plot where the ocean mmm and data disagree.


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons


def outside_x(x, lats, lons_shift, data_tanom, model_anom_cube):
    """
    reduces data points to those that are more than x degs away from the model
    """

    cubelats = model_anom_cube.coord('latitude').points
    cubelons = model_anom_cube.coord('longitude').points

    new_lats = []
    new_lons = []
    new_data = []

    for i, lat in enumerate(lats):
        lon = lons_shift[i]
      # find nearest latitude and lontiude to the value
        latix = np.abs(cubelats-lat).argmin()
        lonix = np.abs(cubelons-lon).argmin()

        model_slice  =  model_anom_cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
           
        modelanom = model_slice.data

        if np.abs(data_tanom[i] - modelanom) > x:
            new_lats.append(lat)
            new_lons.append(lon)
            new_data.append(data_tanom[i])
        
    return new_lats, new_lons, new_data


def plot(model_cube, mask_cube, lats, lons, data, fileout):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data 
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


  
    #plt.show()
    plt.savefig(fileout + '.png')
    plt.savefig(fileout + '.eps')
    plt.close()


def plot_2figs(model_cube, mask_cube, land_lats, land_lons, 
         land_data, ocean_lats, ocean_lons, ocean_data):
    """
    prepares a 2 part figure with land and ocean 
    """

    fig = plt.figure(figsize=[8.0,8.0])


    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    # OCEAN DATA
    # model
    ax = fig.add_subplot(2,1,1, projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('a) Ocean DMC')
    

    # overplot data   
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    plt.scatter(ocean_lons, ocean_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())
    plt.scatter(ocean_lons, ocean_lats, c=ocean_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
    print('length',np.shape(ocean_lons))
    sys.exit(0)


    # LAND DATA
    # model
    ax = fig.add_subplot(2,1,2, projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('b) Land DMC')
    

    # overplot data 
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())
    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

    # colorbar
  #  fig.tight_layout()
    fig.subplots_adjust(bottom=0.15)
    cbar_ax = fig.add_axes([0.15, 0.10, 0.75, 0.03])
    cbar = fig.colorbar(cs, cax=cbar_ax,orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C',fontsize=15)
    cbar.ax.tick_params(labelsize=10)

    plt.savefig('/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_ocn.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_ocn.eps')
    plt.close()

    
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        print('n mgca',len(lats))
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        print('n uk37',len(lats_UK37))
       
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)

    # put land and ocean on same figure
    plot_2figs(model_anom_cube, lsmplio_cube, land_lats, land_lons, 
         land_tanom, lats, lons_shift, data_tanom)
    
    # put land and ocean on seperate figures
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land'
    plot(model_anom_cube, lsmplio_cube, land_lats, land_lons, 
         land_tanom,fileout)
    
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/ocean'
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         fileout)

    # reduce points to those not within x deg of data
    x=2
    (llat_new, llon_new, ldata_new) = outside_x(x, land_lats, land_lons, 
                                               land_tanom, model_anom_cube)
    (olat_new, olon_new, odata_new) = outside_x(x, lats, lons_shift, 
                                               data_tanom, model_anom_cube)

    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_unmatched'
    plot(model_anom_cube, lsmplio_cube, llat_new, llon_new, 
         ldata_new,fileout)
    
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/ocean_unmatched'
    plot(model_anom_cube, lsmplio_cube, olat_new, olon_new, odata_new,
         fileout)

   
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/DMC_for_paper_with_land_alt.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            print(dfs.iloc[rl,0])
  
            if (dfs.iloc[rl,0] == 'Lake Baikal'):
                temps.append(temp - 5.8)
                print('LB found')
            elif (dfs.iloc[rl,0] == 'Lost Chicken Mine'):
                temps.append(temp - 4.0)
                print('LCM found')
            elif (dfs.iloc[rl,0] == 'James Bay Lowland'):
                temps.append(temp - 4.0)
                print('JBL found')
            else:
                temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    plt.savefig('/nfs/hera1/earjcti/regridded/alternative_IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/alternative_IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         land_lats, land_lons, land_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/emergent_constraints_old.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu)
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    print('mean',np.mean(new_clim_sens_redu))
    print('stdev',np.std(new_clim_sens_redu))
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
    

   
    return lats, lons, sstanom_min, sstanom_max


def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    mod_lat, mod_lon, mod_minsst, mod_maxsst =  readmodel()

                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
  
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
CEMAC/PLIOMIP2/emergent_constraints.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu,
                                                 proxy_sst_anom,
                                                 proxylat, proxylon)  
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    allsstanom = np.zeros((37, 17)) # the 17th is the MMM
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    line = lines[0]
    titles = line.split(',')
    modnames = titles[3:20]
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
      allsstanom[i-1, :] = np.asarray(vals[3:20], dtype = float)
    

   
    return lats, lons, sstanom_min, sstanom_max, allsstanom, modnames

def print_rmse(mod_allsst, proxy_allsst, modlats, modlons, 
               proxylats, proxylons, modnames):
    """
    just prints out the rmse for all the models

    """
    print('shape proxy', np.shape(proxy_allsst))
    print('shape data', np.shape(mod_allsst))
    npoints, nmods = np.shape(mod_allsst)
    
    f1 = open('ind_model_dmc.txt','w+')
    f1.write('model        rmse      bias   within 2deg/ 1deg/ 0.5deg \n')
    for i in range(0, nmods):
        sumsq = 0.0
        avger = 0.0
        count = 0.0
        within_1deg = 0
        within_2deg = 0
        within_05deg = 0
        for j in range(0, npoints):
            sumsq = sumsq + ((proxy_allsst[j] - mod_allsst[j, i])**2.0)
            avger = avger + (proxy_allsst[j] - mod_allsst[j, i])
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 1.0):
                within_1deg = within_1deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 2.0):
                within_2deg = within_2deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 0.5):
                within_05deg = within_05deg + 1
            count = count + 1.0
            
        rmse = np.sqrt(sumsq / count)
        avger = avger / count
      
        f1.write(modnames[i].ljust(12) + ',' +  np.str(np.around(rmse,2))
                 + ',   ' +  np.str(np.around(avger,2)) + ',   ' 
                 + np.str(within_2deg) +  ',' +  np.str(within_1deg)
                 +  ',' +  np.str(within_05deg) + '\n')
        print(modnames[i].ljust(12),',',np.around(rmse,2),
                 ',   ',np.around(avger,2),',   ',within_2deg,  
               '   ',within_1deg,',   ',within_05deg)
        
    f1.close()
   

def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens, 
               proxysst_full, proxy_fulllat, proxy_fulllon):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    (mod_lat, mod_lon, mod_minsst, mod_maxsst, 
    mod_allsst, modnames) =  readmodel()
    print_rmse(mod_allsst, proxysst_full, mod_lat, mod_lon, proxy_fulllat, 
               proxy_fulllon, modnames)
                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
CEMAC/PLIOMIP2/emergent_contstraints_old2.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu,
                                                 proxy_sst_anom,
                                                 proxylat, proxylon)  
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    allsstanom = np.zeros((37, 17)) # the 17th is the MMM
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    line = lines[0]
    titles = line.split(',')
    modnames = titles[3:20]
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
      allsstanom[i-1, :] = np.asarray(vals[3:20], dtype = float)
    

   
    return lats, lons, sstanom_min, sstanom_max, allsstanom, modnames

def print_rmse(mod_allsst, proxy_allsst, modlats, modlons, 
               proxylats, proxylons, modnames):
    """
    just prints out the rmse for all the models

    """
    print('shape proxy', np.shape(proxy_allsst))
    print('shape data', np.shape(mod_allsst))
    npoints, nmods = np.shape(mod_allsst)
    
    f1 = open('ind_model_dmc.txt','w+')
    f1.write('model        rmse      bias   within 2deg/ 1deg/ 0.5deg \n')
    for i in range(0, nmods):
        sumsq = 0.0
        avger = 0.0
        count = 0.0
        within_1deg = 0
        within_2deg = 0
        within_05deg = 0
        for j in range(0, npoints):
            sumsq = sumsq + ((proxy_allsst[j] - mod_allsst[j, i])**2.0)
            avger = avger + (proxy_allsst[j] - mod_allsst[j, i])
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 1.0):
                within_1deg = within_1deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 2.0):
                within_2deg = within_2deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 0.5):
                within_05deg = within_05deg + 1
            count = count + 1.0
            
        rmse = np.sqrt(sumsq / count)
        avger = avger / count
      
        f1.write(modnames[i].ljust(12) + ',' +  np.str(np.around(rmse,2))
                 + ',   ' +  np.str(np.around(avger,2)) + ',   ' 
                 + np.str(within_2deg) +  ',' +  np.str(within_1deg)
                 +  ',' +  np.str(within_05deg) + '\n')
        print(modnames[i].ljust(12),',',np.around(rmse,2),
                 ',   ',np.around(avger,2),',   ',within_2deg,  
               '   ',within_1deg,',   ',within_05deg)
        
    f1.close()
   

def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens, 
               proxysst_full, proxy_fulllat, proxy_fulllon):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    (mod_lat, mod_lon, mod_minsst, mod_maxsst, 
    mod_allsst, modnames) =  readmodel()
    print_rmse(mod_allsst, proxysst_full, mod_lat, mod_lon, proxy_fulllat, 
               proxy_fulllon, modnames)
                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
CEMAC/PLIOMIP2/extract_data_locations_monthly.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_02102019.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.outend = 'modeloutput_pliovar'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'modeloutput_CSCD_localities'
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'test_localities'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []

        with open(self.filenamein) as csvfile:
            print(self.filenamein)
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:#
                print(row)
                print(' ')
                print(' ')
                if count != 0: # not titleline
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period
        self.nmonths = 12

        if test == 'y':
            self.modelnames = ['NorESM-L']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]
        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros((len(self.lonlist), self.nmonths))
        model_data_near = np.zeros((len(self.lonlist), self.nmonths)) # values near the point
        near_distance = np.zeros((len(self.lonlist), self.nmonths)) # how far away we have to look to get data
        ngbox_avg = np.zeros((len(self.lonlist), self.nmonths)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            data_slice_months = data_slice.data
            for j in range(0,12):
                if -100. < data_slice_months[j] < 100.: 
                    model_data[i, j] = data_slice_months[j]
                    model_data_near[i, j] = model_data[i, j]
                else:
                    model_data[i, j] = float('NaN')
                    model_data_near[i, j] = float('NaN')

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            # check at month zero as we assume that lsm is same for all months
            while np.isnan(model_data_near[i, 0]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i, :] = neardata

            near_distance[i, :] = count_near_gb # how far away are we looking for data
            ngbox_avg[i, :] = ngboxes


        returndata = [model_data, model_data_near, near_distance, ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)

        sitenear = np.zeros((nmodels, npoints, self.nmonths)) # data near point
        sitevals = np.zeros((nmodels, npoints, self.nmonths)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints, self.nmonths)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints, self.nmonths)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.' + fieldnames + '.mean_month.nc')

            # get model points and how far away they are from data
            (sitevals[model, :, :], 
             sitenear[model, :, :], 
             sitenear_dist[model, :, :], 
             sitenear_ngbox_avg[model, :,: ]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'lat')
    sheet.write(0,1,'lon')
    for model in range(0,nmodels):
        sheet.write(0,2+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,latlist[i],style)
        sheet.write(i+1,1,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,2+model,datawrite[model,i],style)

    sheet.write(0,2+nmodels,'MMM')
    sheet.write(0,3+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,2+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,3+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

   
    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,4+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,4+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, monthno):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    print('writing out monthno',monthno, np.shape(eoi400))
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400[:, :, monthno])
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280[:, :, monthno])
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels, :, monthno]-e280[0:nmodels, :, monthno])

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near[:, :, monthno])
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near[:, :, monthno])
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels, :, monthno]-e280_near[0:nmodels, :, monthno])

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance[:, :, monthno])
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance[:, :, monthno])

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg[:, :, monthno])
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg[:, :, monthno])


    monthname = {0:'jan', 1:'feb', 2:'mar',3:'apr', 4:'may', 
                 5:'jun',6:'jul', 7:'aug', 8:'sep',9:'oct', 
                 10:'nov', 11:'dec' }

    fileoutwrite = fileout + monthname.get(monthno) + '.xls'

    # remove output file if it exists
    exists  =  os.path.isfile(fileoutwrite)
    if exists:
        os.remove(fileoutwrite)
    wb.save(fileoutwrite)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Erin' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
modelstart,outputfile,longitudes,latitudes,longitudes_alt = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points




##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
nmonths = len(eoi400[0, 0, :])

#######################################
# write data out to a workbook

for mon in range(0, 12):
    write_to_book(outputfile,longitudes,latitudes,longitudes_alt, mon)

###################################
# plot model points
for model in range(0,1):
        #plot points from january
    plotpoints(longitudes_alt,latitudes,eoi400[model,:,0])
          #plot points from july
    plotpoints(longitudes_alt,latitudes,eoi400[model,:,6])

::::::::::::::
CEMAC/PLIOMIP2/extract_data_locations_pliomip1.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations 
#   from Aislings PlioMIP1 data
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_alan.csv'
            self.latcolumn = 2
            self.loncolumn = 3
            self.outend = 'PlioMIP1output_pliovar.xls'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'PlioMIP1output_CSCD_localities.xls'

        if self.linuxwin == 'l':
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []

        with open(self.filenamein) as csvfile:
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:
                if count != 0: # not titleline
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.fileout,lonlist, latlist, lonlist_alt
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, field, latlist, lonlist, lonlist_alt, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.lonlist_alt=lonlist_alt
        self.period = period

        if test == 'y':
            self.modelnames = ['COSMOS']
        else:
            self.modelnames = ['COSMOS', 'GISS', 'HAD', 
                               'IPSL', 'MIROC', 'MRI',
                               'NOR'
                               ]

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, allcube_,fieldreq_):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        
        ncubes = len(allcube_)
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]

      
        print(cube,'found')
        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist_alt[i])).argmin()
            
            print(self.lonlist_alt[i],self.latlist[i],latix,lonix)

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            model_data[i] = data_slice.data
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = np.nan # if no data near set to nan

        return data_near,count_finite

    def get_fieldreq(self,model_):
        
        PeriodE280Use={
                       "COSMOS" : "Ctrl",
                       "GISS" : "Ctrl",
                       "HAD" : "ctrl",
                       "IPSL" : "ctrl",
                       "MIROC" : "ctrl",
                       "MRI" : "ctrl",
                       "NOR" : "ctrl"
                       }
        
        PeriodEoi400Use={
                         "COSMOS" : "Plio",
                         "GISS" : "Plio",
                         "HAD" : "plio",
                         "IPSL" : "plio",
                         "MIROC" : "plio",
                         "MRI" : "plio",
                         "NOR" : "plio"
                         }
        
        fieldname={
                   "COSMOS" : "SST",
                   "GISS" : "SST",
                   "HAD" : "sst",
                   "IPSL" : "sst",
                   "MIROC" : "sst",
                   "MRI" : "sst",
                   "NOR" : "sst"
                   }
         
        if self.period == 'E280':
            self.fieldreq = (model_ + '_' + 
                        PeriodE280Use.get(model_)+
                        '_' + fieldname.get(model_))
        if self.period == 'EOI400':
            self.fieldreq = (model_ + '_' + 
                        PeriodEoi400Use.get(model_)+
                        '_' + fieldname.get(model_)) 
            
            

    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
          
            filename = ('/nfs/hera1/earjcti/PLIOMIP/PlioMIP1_regridded.nc')
            self.get_fieldreq(self.modelnames[model])
            print('fieldreq is',self.fieldreq)
            
            allcube = iris.load(filename)
            
            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(allcube,self.fieldreq)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'lat')
    sheet.write(0,1,'lon')
    for model in range(0,nmodels):
        sheet.write(0,2+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,latlist[i],style)
        sheet.write(i+1,1,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,2+model,datawrite[model,i],style)

    # add multimodel mean and multimodel standard deviation
    sheet.write(0,2+nmodels,'MMM')
    sheet.write(0,3+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,2+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,3+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,4+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,4+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels])

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near)
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near)
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels]-e280_near[0:nmodels])

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance)
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance)

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg)
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Harry' # could have Harry or Erin
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
outputfile,longitudes,latitudes,longitudes_alt = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points


######################################
# setup a map and plot the points

#if linuxwin = ='l':
#    plotpoints(lonlist_alt,latlist,np.zeros(count))


##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,fieldnames,
                       latitudes,longitudes,longitudes_alt,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,fieldnames,
                       latitudes,longitudes,longitudes_alt,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt)

###################################
# plot model points
for model in range(0,len(modelnames_eoi400)):
    #plotpoints(lonlist,latlist,eoi400[model,:]-e280[model,:])

    plotpoints(longitudes_alt,latitudes,eoi400[model,:])

::::::::::::::
CEMAC/PLIOMIP2/extract_data_locations.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
import csv
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_02102019.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.sitecolumn = 1
            self.outend = 'modeloutput_pliovar.xls'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'modeloutput_CSCD_localities.xls'
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'test_localities.xls'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend

 
    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []
        sitelist = []

        with open(self.filenamein) as csvfile:
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:
                if count != 0: # not titleline
                    sitelist.append(row[self.sitecolumn])
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt, sitelist
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period

        if test == 'y':
            self.modelnames = ['NorESM1-F', 'HadCM3']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3','HadCM3_new',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]

        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            print(self.latlist[i], self.lonlist[i])
            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            
            data_slice_data = data_slice.data
            if -100. < data_slice_data < 100.: 
                model_data[i] = data_slice_data
            else:
                model_data[i] = float('NaN')
        
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.SST.allmean.nc')

            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite, sitename):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'site')
    sheet.write(0,1,'lat')
    sheet.write(0,2,'lon')
    for model in range(0,nmodels):
        sheet.write(0,3+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,sitename[i],style)
        sheet.write(i+1,1,latlist[i],style)
        sheet.write(i+1,2,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,3+model,datawrite[model,i],style)

    sheet.write(0,3+nmodels,'MMM')
    sheet.write(0,4+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,3+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,4+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,5+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,5+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, sitename):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400, sitename)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280, sitename)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels], sitename)

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near, sitename)
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near, sitename)
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels]-e280_near[0:nmodels], sitename)

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance, sitename)
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance, sitename)

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg, sitename)
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg, sitename)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Harry' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
(modelstart, outputfile, longitudes, 
 latitudes, longitudes_alt, sitenames) = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points


######################################
# setup a map and plot the points

#if linuxwin = ='l':
#    plotpoints(lonlist_alt,latlist,np.zeros(count))


##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt, sitenames)

###################################
# plot model points
#for model in range(0,len(modelnames_eoi400)):
    #plotpoints(lonlist,latlist,eoi400[model,:]-e280[model,:])

#    plotpoints(longitudes_alt,latitudes,eoi400[model,:])

::::::::::::::
CEMAC/PLIOMIP2/extract_HadCM3_MOC.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 18 14:13:50 2019

#@author: earjcti
#
# This program will extract the fields needed by Zhongshi Zhang for the
# PlioMIP MOC paper.  It will extract the monthly averages from
# steve hunters processed data 
#
#
#

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.plot as iplt
import sys

#####################################
def extract_fields(filename,fieldnames,fileoutstart,filefieldnames):

    # load required cubes
    cubes=iris.load_cubes(filename,fieldnames)
    
    for i in range(0,len(fieldnames)):
        fileout=fileoutstart+filefieldnames[i]+'.nc'
        if filefieldnames[i]=='SSS':
            subcube=cubes[i]
            cube_extract= subcube.extract(iris.Constraint(z2=5))
            iris.save(cube_extract,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
        else:
            iris.save(cubes[i],fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)


def avg_MOC(fileMOC,fileout,fieldname):
    cubes=iris.load(fileMOC,fieldname)
  
    print(np.shape(cubes[0]))
    print(cubes)
    print(cubes[0].data)
    avgcube=cubes[0].data
    for i in range(1,len(cubes)):
        subcube=cubes[i]
        subcube_1d=subcube[0,:,50]
        iplt.plot(subcube_1d,color='r')
        avgcube=avgcube+cubes[i].data
        cubedata=cubes[i].data
       
    avgcube=avgcube/len(cubes)
    # put average cube in subcube area
    subcube.data=avgcube
    subcube_1d=subcube[0,:,50]
    iplt.plot(subcube_1d,color='blue')
    iris.save(subcube,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
    
    

    
   
   

##########################################################
# main program

#####################################################
# this is extracting fields from a mean file

fieldnames=['OCN TOP-LEVEL TEMPERATURE K',
            'SALINITY (OCEAN) (PSU)',
            'AICE : ICE CONCENTRATION',
            'SALINITY (OCEAN) (PSU)',]
filefieldnames=['SST','SSS','iceconc','salinity']
#filename='C:\\Users\\julia\\OneDrive\\DATA\\HadCM3_DATA\\Eoi400_2400-2499_Monthly.nc'
#fileoutstart='C:\\Users\\julia\\OneDrive\\DATA\\HadCM3_DATA\\EOI400_2400_2499_Monthly_'

filename='/nfs/a103/palaeo_share/PlioMIP2/processed/Eoi400_2400-2499_Monthly.nc'
fileoutstart='/nfs/hera1/earjcti/um/tenvj/pk2/EOI400_2400_2499_Monthly_'
extract_fields(filename,fieldnames,fileoutstart,filefieldnames)

print('here')
filename='/nfs/a103/palaeo_share/PlioMIP2/processed/Preind_E280_2900-2999_Monthly.nc'
fileoutstart='/nfs/hera1/earjcti/um/tenvo/pk2/E280_2900_2999_Monthly_'
extract_fields(filename,fieldnames,fileoutstart,filefieldnames)


####################################################
# this is averaging MOC from the MOC scripts
#Basin=['Atlantic','Indian','Pacific','Global']

#for i in range(0,len(Basin)):
#    fieldname='Meridional Overturning Stream Function ('+Basin[i]+')'

#    fileMOC='/nfs/hera1/earjcti/um/tenvo/pk2/tenvoo@pgt*c1.nc'
#    fileout='/nfs/hera1/earjcti/um/tenvo/pk2/E280_avg_'+Basin[i]+'_MOC.nc'
#    avg_MOC(fileMOC,fileout,fieldname)

#    fileMOC='/nfs/hera1/earjcti/um/tenvj/pk2/tenvjo@pgo*c1.nc'
#    fileout='/nfs/hera1/earjcti/um/tenvj/pk2/EOI400_avg_'+Basin[i]+'_MOC.nc'
#    avg_MOC(fileMOC,fileout,fieldname)

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/extract_HadCM3_PlioMIP.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 18 14:13:50 2019

#@author: earjcti1
#
# This program will extract the fields needed for the PlioMIP2 database
#  It will extract the monthly averages from
# steve hunters processed data 
#
#
#

import os
import numpy as np
import scipy as sp
#import cf
import iris
from iris.cube import CubeList
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import sys
import warnings

warnings.filterwarnings("ignore")

def simplify_cube(cube):
    """
    gets cube and makes sure dimensions are longitude, latitude surface and
    t. 
    """    
    for coord in cube.coords():
        if coord.var_name == 'level275':
            coord.var_name = 'surface'
    
    cube.coord('surface').points = 0.0
    cube.coord('surface').units = 'm'
    cube.coord('surface').attributes = None
    
    cube.data = np.where(cube.data > 1.0E10, 0., cube.data)
    return cube

def get_evap(filename_):
    """
    will add up all the fluxes that make evaporation and returns
    total evaporation within a cube
    The fluxes are:
      evaporation from canopy
      evaporation from sea
      transpiration (this is exactly the same as evaporation from soil
                     I have checked some examples)
      sublim from surface
    """

    varnames_sec = ["EVAPORATION FROM SEA (GBM)   KG/M2/S",
                "TRANSPIRATION RATE           KG/M2/S"]
    
    varnames_ts = ["EVAP FROM CANOPY - AMOUNT   KG/M2/TS",
                "SUBLIM. FROM SURFACE (GBM)  KG/M2/TS"]
    
    
    for i, var in enumerate(varnames_sec):
        cube = iris.load_cube(filename_,var)
        cube = simplify_cube(cube)
        if i == 0:
            cubetot = cube
        else:
            cubetot = cubetot + cube
        
        
    for i, var in enumerate(varnames_ts):
        cube = iris.load_cube(filename_,var)
        cube = simplify_cube(cube)
        cube.data = cube.data / (30. * 60.)
        cube.units = 'kg m-2 s-1'
        cubetot = cubetot + cube
     
    return cubetot

#####################################
def extract_fields(filestart,expt,filetype,extra,startyear,endyear,timeperiod,
                   fileoutstart,varnamein,varnameout):

    # load required cubes
    #cubes=iris.load(filename)
    #print(cubes)
    #sys.exit(0)
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    
    
    # loop over years
    
   
    for year in range(startyear,endyear):
        allcubes=CubeList([])
        stringyear=np.str(year).zfill(2)
        for mon in range(0,len(monthnames)):
            print(mon)
            if format == '#':
                filename=(filestart + expt + filetype + 
                      extra + stringyear + monthnames[mon] + '+.nc')
            else:
                filename=(filestart + expt + filetype + 
                      extra + stringyear + monthnames[mon] + '.nc')
         
            if varnamein == 'TOTAL EVAPORATION':
                cube = get_evap(filename)
            else:
                print(filename,varnamein)
                cube=iris.load_cube(filename,varnamein)
                cube.coord('t').points=mon+1
            allcubes.append(cube)
            if mon==11:
                print(allcubes)
            
        #make sure the metadata on all cubes are the same
        iris.util.equalise_attributes(allcubes)
        catcube=allcubes.concatenate()[0]
        
        # if precipitation convert to mm/day
        if varnameout=='TotalPrecipitation':
            catcube.data=catcube.data * 60.*60.*24.
            catcube.long_name='TOTAL PRECIPITATION RATE    MM/DAY'
            catcube.units='mm/day'
       
        if varnameout=='evap':
            catcube.data=catcube.data * 60.*60.*24.
            catcube.long_name='TOTAL EVAPORATION    MM/DAY'
            catcube.units='mm/day'
      
        if varnameout=='so':
            catcube.data=(catcube.data * 1000.) + 35.
            catcube.long_name='SALMINTY (OCEAN)     (PSU)'
            catcube.units='ppt'
      
            
        # if temperature convert to Celcius
        print(varnameout)
        if varnameout=='SurfaceTemperature':
            print('convert to celcius')
            catcube.convert_units('celsius')
       
    
        stringyear=np.str(year).zfill(3)
        print(stringyear)
        fileout=fileoutstart+varnameout+'/'+timeperiod+'.'+varnameout+'.'+stringyear+'.nc'        
        iris.save(catcube,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
     
    
   

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning
            
fileextra = {
		"SURFACE TEMPERATURE AFTER TIMESTEP" : "a#pd",
        "TOTAL PRECIPITATION RATE     KG/M2/S" : "a#pd",
        "TEMPERATURE AT 1.5M": "a@pd",
        "OCN TOP-LEVEL TEMPERATURE          K" : "o@pf",
        "AICE : ICE CONCENTRATION" : "o@pf", 
                "TOTAL CLOUD AMOUNT - RANDOM OVERLAP" : "a#pd",
       
		"U COMPNT OF WIND ON PRESSURE LEVELS" : "a@pc",
		"V COMPNT OF WIND ON PRESSURE LEVELS" : "a@pc", 
        	"NET DOWN SURFACE SW FLUX: SW TS ONLY" : "a@pd",
		"NET DOWN SURFACE LW RAD FLUX" : "a@pd",
        	"SURFACE & B.LAYER HEAT FLUXES   W/M2" : "a@pd",
		"SURFACE LATENT HEAT FLUX        W/M2" : "a@pd",
        "OMEGA ON PRESSURE LEVELS" : "a@pc",
		"SPECIF HUM;P LEVS;U GRID.  USE MACRO" : "a@pc",
		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS" : "a@pc",
		"TEMPERATURE ON PRESSURE LEVELS" : "a@pc",
		"SURFACE LATENT HEAT FLUX        W/M2" : "a@pd",
        "DOWNWARD LW RAD FLUX: SURFACE" : "a@pd",
        "TOTAL DOWNWARD SURFACE SW FLUX" : "a@pd",
        "INCOMING SW RAD FLUX (TOA): ALL TSS" : "a@pd",
        "OUTGOING SW RAD FLUX (TOA)" : "a@pd",
        "OUTGOING LW RAD FLUX (TOA)" : "a@pd",
        "SURFACE & B.LAYER HEAT FLUXES   W/M2" : "a@pd",
		"PRESSURE AT MEAN SEA LEVEL" : "a@pd",
		"PSTAR AFTER TIMESTEP" : "a@pd",
		"TOTAL EVAPORATION" : "a@pd",
		"X-COMP OF SURF & BL WIND STRESS N/M2" : "a@pd",
                "Y-COMP OF SURF & BL WIND STRESS N/M2" : "a@pd",
		"TOTAL OCEAN U-VELOCITY      CM S**-1" : "o@pf",
		"TOTAL OCEAN V-VELOCITY      CM S**-1" : "o@pf",
		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S" : "o@pf",
                "POTENTIAL TEMPERATURE (OCEAN)  DEG.C" : "o@pf",
                "SALINITY (OCEAN)       (PSU-35)/1000" : "o@pf"
	}

shortname = {
		"SURFACE TEMPERATURE AFTER TIMESTEP" : "SurfaceTemperature",
        "TOTAL PRECIPITATION RATE     KG/M2/S" : "TotalPrecipitation",
        "TEMPERATURE AT 1.5M": "NearSurfaceTemperature",
        "TOTAL CLOUD AMOUNT - RANDOM OVERLAP" : "totcloud",
        "OCN TOP-LEVEL TEMPERATURE          K" : "SST",
        "AICE : ICE CONCENTRATION" : "SeaIceConc", 
		"U COMPNT OF WIND ON PRESSURE LEVELS" : "ua",
		"V COMPNT OF WIND ON PRESSURE LEVELS" : "va",
        	"NET DOWN SURFACE SW FLUX: SW TS ONLY" : "fsns",
		"NET DOWN SURFACE LW RAD FLUX" : "flns",
        "OMEGA ON PRESSURE LEVELS" : "wap",
		"SPECIF HUM;P LEVS;U GRID.  USE MACRO" : "spechumid",
		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS" : "zg",
		"TEMPERATURE ON PRESSURE LEVELS" : "ta",
		"SURFACE LATENT HEAT FLUX        W/M2" : "hfls",
        "DOWNWARD LW RAD FLUX: SURFACE" : "rlds",
        "TOTAL DOWNWARD SURFACE SW FLUX" : "rsds",
        "INCOMING SW RAD FLUX (TOA): ALL TSS" : "rsdt",
        "OUTGOING SW RAD FLUX (TOA)" : "rsut",
        "OUTGOING LW RAD FLUX (TOA)" : "rlut",
        "SURFACE & B.LAYER HEAT FLUXES   W/M2" : "surfheatflux",
        "CLEAR-SKY (II) UPWARD LW FLUX (TOA)" : "rlutcs",
                "CLEAR-SKY (II) UPWARD SW FLUX (TOA)" : "rsutcs",
		"PRESSURE AT MEAN SEA LEVEL" : "MSLP",
		"PSTAR AFTER TIMESTEP" : "ps",
		"TOTAL EVAPORATION" : "evap",
		"X-COMP OF SURF & BL WIND STRESS N/M2" : "tauu",
                "Y-COMP OF SURF & BL WIND STRESS N/M2" : "tauv",
		"TOTAL OCEAN U-VELOCITY      CM S**-1" : "uo",
		"TOTAL OCEAN V-VELOCITY      CM S**-1" : "vo",
		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S" : "wo",
                "POTENTIAL TEMPERATURE (OCEAN)  DEG.C" : "thetao",
                "SALINITY (OCEAN)       (PSU-35)/1000" : "so"
	}


exptname = {
        "e280" : "tenvo",
        "e400" : "tenvq",
        "e560":"tenvs",
        "eoi400" : "tenvj",
        "eoi350" : "tenvk",
        "eoi450" : "tenvl",
        "eoi280" : "tenvm"
      
}

extraname = {
        "e280" : "t",
        "e400" : "t",
        "eoi400" : "o",
        "eoi350" : "o",
        "eoi450" : "o",
        "eoi280" : "o",
        "e560" : "v",
        "xozza" : "p",
        "xozzb" : "o",
        "xozzc" : "o",
        "xozzd" : "o",
        "xozze" : "o",
        "xozzf" : "o",
        "xpkma" : "00000",
        "xpkmb" : "00000",
        "xpkmc" : "00000"}

#fieldname=["Y-COMP OF SURF & BL WIND STRESS N/M2","X-COMP OF SURF & BL WIND STRESS N/M2" 
#           ]
fieldname = ["SURFACE TEMPERATURE AFTER TIMESTEP",
#         "TOTAL PRECIPITATION RATE     KG/M2/S",
#        "TEMPERATURE AT 1.5M",
#         "OCN TOP-LEVEL TEMPERATURE          K",
        # "AICE : ICE CONCENTRATION", 
#          "TOTAL CLOUD AMOUNT - RANDOM OVERLAP",
       
#		"U COMPNT OF WIND ON PRESSURE LEVELS",
#		"V COMPNT OF WIND ON PRESSURE LEVELS",
#        	"NET DOWN SURFACE SW FLUX: SW TS ONLY",
#		"NET DOWN SURFACE LW RAD FLUX",
#        	"SURFACE & B.LAYER HEAT FLUXES   W/M2",
#		"SURFACE LATENT HEAT FLUX        W/M2",
#        "OMEGA ON PRESSURE LEVELS",
#		"SPECIF HUM;P LEVS;U GRID.  USE MACRO",
#		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS",
#		"TEMPERATURE ON PRESSURE LEVELS",
	#	"SURFACE LATENT HEAT FLUX        W/M2",
#        "DOWNWARD LW RAD FLUX: SURFACE",
#        "TOTAL DOWNWARD SURFACE SW FLUX",
#        "INCOMING SW RAD FLUX (TOA): ALL TSS",
#        "OUTGOING SW RAD FLUX (TOA)",
#        "OUTGOING LW RAD FLUX (TOA)",
#         "CLEAR-SKY (II) UPWARD SW FLUX (TOA)",
#         "CLEAR-SKY (II) UPWARD LW FLUX (TOA)",
#        "SURFACE & B.LAYER HEAT FLUXES   W/M2",
#		"PRESSURE AT MEAN SEA LEVEL",
#		"PSTAR AFTER TIMESTEP",
#		"TOTAL EVAPORATION",
#		"X-COMP OF SURF & BL WIND STRESS N/M2",
 #               "Y-COMP OF SURF & BL WIND STRESS N/M2",
#		"TOTAL OCEAN U-VELOCITY      CM S**-1",
#		"TOTAL OCEAN V-VELOCITY      CM S**-1",
#		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S"
#           "POTENTIAL TEMPERATURE (OCEAN)  DEG.C",
#             "SALINITY (OCEAN)       (PSU-35)/1000" 
	]
	       
#fieldname=["V COMPNT OF WIND ON PRESSURE LEVELS"]

linux_win='l'
startyear=0
endyear=100
timeperiod='xozzf'
expt=exptname.get(timeperiod,timeperiod)
extra=extraname.get(timeperiod)
format = '@'  # is the filename xxxxx#pd or xxxxx@pd

if linux_win=='w':
    filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\HadCM3\\'+exptname.get(timeperiod)+'/'
    fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\HadCM3_UPLOAD\\'+timeperiod+'/'
else:
    filestart='/nfs/hera1/earjcti/um/'+expt+'/pf/'
    fileoutstart='/nfs/hera1/earjcti/um/'+expt + '/'
    #fileoutstart='/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/'+timeperiod+'/'

for i in range(0,len(fieldname)):
    varnamein=fieldname[i]
    varnameout=shortname.get(varnamein)
    filetype=fileextra.get(varnamein,'a'+ format+ 'pd')

    extract_fields(filestart,expt,filetype,extra,startyear,endyear,timeperiod,fileoutstart,varnamein,varnameout)

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/extract_ipcc_data.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with all the data that we had in the supplementary and
have asked for it extracted as spreadsheet.  This program will do this. 

This is superceeded by calculate_land_sea_contrast.py

@author: julia
"""


import pandas as pd
import numpy as np
import sys

def get_global_temperature_anomaly():
    """
    
    extract model name and global_tanom from data_for_fig1a
    
    """
    
    data = [['CESM2',19.31,0.21,14.14,0.2],
            ['IPSLCM6A',16.02,0.23,12.54,0.25],
            ['COSMOS',16.83,0.48,13.5,0.46],
            ['EC-Earth3.3',18.17,0.15,13.33,0.19],
            ['CESM1.2',17.33,0.17,13.3,0.21],
            ['IPSLCM5A',14.38,0.22,12.09,0.27],
            ['MIROC4m',15.91,0.18,12.77,0.22],
            ['IPSLCM5A2',15.33,0.27,13.16,0.34],
            ['HadCM3',16.95,0.2,14.06,0.21],
            ['GISS2.1G',15.91,0.31,13.78,0.27],
            ['CCSM4',15.98,0.15,13.37,0.19],
            ['CCSM4-Utr',18.52,0.13,13.78,0.27],
            ['CCSM4-UoT',16.8,0.13,13.01,0.21],
            ['NorESM-L',14.57,0.13,12.48,0.14],
            ['MRI2.3',15.12,0.26,12.67,0.2],
            ['NorESM1-F',16.22,0.15,14.49,0.14]]
    
    modelnames = ['Simulated Temperature Changes', 'Fraction of Global Area']
    global_tanom = ['GLOBAL', 1.0]
    
    for row in data:
        modelnames.append(row[0])
        plio_temp = row[1]
        pi_temp = row[3]
        temp_anom = plio_temp - pi_temp
        global_tanom.append(temp_anom)
        
    
    return [modelnames, global_tanom]

def get_global_land_sea():
    """
    
    extract model name and global_tanom from data_for_fig3b
    
    """
    
    data = [['CESM2',6.58,4.69,4.23,3.52],
            ['IPSLCM6A',4.61,3.14,2.72,2.21],
            ['COSMOS',4.97,2.74,3.33,2.35],
            ['EC-Earth3.3',6.64,4.21,3.9,2.96],
            ['CESM1.2',5.09,3.72,2.89,2.46],
            ['IPSLCM5A',3.37,1.96,2.17,1.62],
            ['MIROC4m',4.63,2.6,2.95,2.14],
            ['IPSLCM5A2',3.15,1.87,1.99,1.51],
            ['HadCM3',4.47,2.34,2.97,1.69],
            ['GISS2.1G',2.57,2.08,1.46,1.09],
            ['CCSM4',3.51,2.36,1.64,1.42],
            ['CCSM4-Utr',5.6,4.5,2.9,2.67],
            ['CCSM4-UoT',4.77,3.51,2.32,2.13],
            ['NorESM-L',2.64,2.0,0.95,1.08],
            ['MRI2.3',3.71,2.04,2.04,1.42],
            ['NorESM1-F',2.52,1.52,1.13,1.07]]
    
    modelnames = ['Simulated Temperature Changes', 'Fraction of Global Area']
    global_landanom = ['GLOBAL (over land)', 1.0]
    global_seaanom = ['GLOBAL (over ocean)', 1.0]
    
    for row in data:
        modelnames.append(row[0])
        global_landanom.append(row[1])
        global_seaanom.append(row[2])
        
        
    land_dict = {} # set up dictionaries for new dataframe row
    sea_dict = {}
    for i, model in enumerate(modelnames):
        land_dict[model] = global_landanom[i]
        sea_dict[model] = global_seaanom[i]
    
        
    return [land_dict, sea_dict]


def get_latitude_bands_global(glob_l_s_ind):
    """
    data from data-for_supp_2
    """
    
    if glob_l_s_ind == 'g':
        data = [['CESM2',10.78,4.57,3.74,3.8,5.62,10.52],
                ['IPSLCM6A',8.68,2.87,2.14,2.5,3.79,7.73],
                ['COSMOS',6.87,2.01,2.45,2.76,3.88,7.26],
                ['EC-Earth3.3',8.45,3.37,3.07,3.67,6.64,11.36],
                ['CESM1.2',9.32,3.67,2.58,2.68,4.19,9.76],
                ['IPSLCM5A',4.11,1.29,1.5,2.03,3.04,5.18],
                ['MIROC4m',6.41,2.05,2.3,2.48,3.59,7.07],
                ['IPSLCM5A2',4.34,1.41,1.49,1.79,2.59,4.89],
                ['HadCM3',6.55,1.85,1.83,2.36,3.92,5.22],
                ['GISS2.1G',7.32,2.38,1.26,1.2,1.79,3.93],
                ['CCSM4',6.28,2.57,1.46,1.56,2.85,6.6],
                ['CCSM4-Utr',12.89,4.56,2.82,2.71,5.25,10.45],
                ['CCSM4-UoT',8.97,3.6,2.06,2.31,4.2,9.96],
                ['NorESM-L',7.59,2.26,1.11,1.06,1.66,4.82],
                ['MRI2.3',5.39,1.05,1.47,1.84,3.17,7.37],
                ['NorESM1-F',3.18,1.58,1.12,1.1,1.87,5.02]]
        titleend = ''
        frac = [0.067, 0.183, 0.25, 0.25, 0.183, 0.067]
        
    if glob_l_s_ind == 'l':
        data = [['CESM2',6.7,4.61,4.4,4.47,6.1,10.17],
                ['IPSLCM6A',4.55,2.61,2.6,3.01,4.03,7.34],
                ['COSMOS',5.82,2.11,3.9,3.31,3.92,7.32],
                ['EC-Earth3.3',5.24,3.92,4.13,4.68,7.14,10.7],
                ['CESM1.2',4.77,3.24,3.03,3.16,4.39,8.95],
                ['IPSLCM5A',1.63,1.61,1.98,2.5,2.97,4.97],
                ['MIROC4m',4.1,2.37,3.43,3.02,3.95,7.18],
                ['IPSLCM5A2',1.51,1.74,1.95,2.2,2.55,4.72],
                ['HadCM3',3.43,2.51,3.25,3.07,4.4,5.22],
                ['GISS2.1G',4.19,2.19,1.64,1.6,1.43,3.17],
                ['CCSM4',2.69,2.04,1.71,1.96,2.94,6.36],
                ['CCSM4-Utr',7.68,3.72,3.2,2.86,5.3,9.61],
                ['CCSM4-UoT',3.48,2.74,2.3,2.95,4.59,9.02],
                ['NorESM-L',5.31,1.71,1.14,0.94,1.71,4.67],
                ['MRI2.3',2.94,1.92,2.21,2.23,2.82,6.63],
                ['NorESM1-F',0.36,1.56,1.31,1.2,1.83,5.03]]
        titleend = ' (over land)'
        frac = [0.118, 0.312, 0.247, 0.196, 0.037, 0.09]
    
    if glob_l_s_ind == 's':
        data = [['CESM2',9.23,4.55,3.53,3.53,5.35,11.17],
                ['IPSLCM6A',6.91,2.87,2.02,2.33,3.87,8.69],
                ['COSMOS',3.85,1.98,2.01,2.55,4.04,7.36],
                ['EC-Earth3.3',6.53,3.31,2.77,3.27,6.41,12.45],
                ['CESM1.2',7.89,3.69,2.46,2.51,4.27,11.27],
                ['IPSLCM5A',1.78,1.25,1.36,1.86,3.37,5.09],
                ['MIROC4m',3.92,2.01,1.96,2.27,3.44,6.97],
                ['IPSLCM5A2',2.24,1.36,1.35,1.64,2.89,4.82],
                ['HadCM3',4.45,1.79,1.4,2.07,3.63,5.36],
                ['GISS2.1G',5.1,2.37,1.15,1.04,2.41,4.98],
                ['CCSM4',5.03,2.59,1.41,1.43,3.03,6.86],
                ['CCSM4-Utr',12.35,4.6,2.72,2.68,5.44,11.26],
                ['CCSM4-UoT',8.51,3.64,2.02,2.09,4.08,11.16],
                ['NorESM-L',5.25,2.28,1.11,1.15,1.91,4.96],
                ['MRI2.3',2.77,0.97,1.26,1.69,3.73,7.45],
                ['NorESM1-F',1.31,1.57,1.08,1.08,2.18,5.19]]
        titleend = ' (over sea)'
        frac = [0.045, 0.129, 0.251, 0.273, 0.244, 0.057]
    
    modelnames = ['Simulated Temperature Changes','Fraction of Global Area']
    l60S_90S = ['90-60oS ' + titleend, frac[5]]
    l30S_60S = ['60-30oS ' + titleend, frac[4]]
    l0_30S = ['30oS-0 ' + titleend, frac[3]]
    l30N_0N = ['0-30oN ' + titleend, frac[2]]
    l60N_30N = ['30-60oN ' + titleend, frac[1]]
    l90N_60N = ['60-90oN' + titleend, frac[0]]
    
    for row in data:
        modelnames.append(row[0])
        l60S_90S.append(row[1])
        l30S_60S.append(row[2])
        l0_30S.append(row[3])
        l30N_0N.append(row[4])
        l60N_30N.append(row[5])
        l90N_60N.append(row[6])
    
    dict_60S90S = {} # set up dictionaries for new dataframe row
    dict_30S60S = {}
    dict_0S30S = {}
    dict_30N0 = {}
    dict_60N30N = {}
    dict_90N60N = {}
    
    for i, model in enumerate(modelnames):
        print(l60S_90S[i], model, i)
        dict_60S90S[model] = l60S_90S[i]
        dict_30S60S[model] = l30S_60S[i]
        dict_0S30S[model] = l0_30S[i]
        dict_30N0[model] = l30N_0N[i]
        dict_60N30N[model] = l60N_30N[i]
        dict_90N60N[model] = l90N_60N[i]
    
        
    return [dict_60S90S, dict_30S60S, dict_0S30S, dict_30N0,
            dict_60N30N, dict_90N60N]

    
################################
modelnames, global_tanom = get_global_temperature_anomaly()
land_dfrow, sea_dfrow =  get_global_land_sea()
[glob_60S90S, glob_30S60S, glob_0S30S, glob_30N0,
            glob_60N30N, glob_90N60N] = get_latitude_bands_global('g')
[land_60S90S, land_30S60S, land_0S30S, land_30N0,
            land_60N30N, land_90N60N] = get_latitude_bands_global('l')
[sea_60S90S, sea_30S60S, sea_0S30S, sea_30N0,
            sea_60N30N, sea_90N60N] = get_latitude_bands_global('s')

#print(len(modelnames))
#print(len(global_tanom))
#print(modelnames)
#print(global_tanom)
print(land_dfrow)

# create the dataframe
df = pd.DataFrame([global_tanom], columns = modelnames, dtype=float)
df = df.append([land_dfrow], ignore_index=True)
df = df.append(sea_dfrow, ignore_index=True)
df = df.append(glob_90N60N,ignore_index= True)
df = df.append(land_90N60N,ignore_index= True)
df = df.append(sea_90N60N,ignore_index= True)
df = df.append(glob_60N30N, ignore_index= True)
df = df.append(land_60N30N, ignore_index= True)
df = df.append(sea_60N30N, ignore_index= True)
df = df.append(glob_30N0,ignore_index= True)
df = df.append(land_30N0,ignore_index= True)
df = df.append(sea_30N0,ignore_index= True)
df = df.append(glob_0S30S, ignore_index= True)
df = df.append(land_0S30S, ignore_index= True)
df = df.append(sea_0S30S, ignore_index= True)
df = df.append(glob_30S60S, ignore_index= True)
df = df.append(land_30S60S, ignore_index= True)
df = df.append(sea_30S60S, ignore_index= True)
df = df.append(glob_60S90S, ignore_index= True)
df = df.append(land_60S90S, ignore_index= True)
df = df.append(sea_60S90S, ignore_index= True)



# save dataframe as a csv file
df.to_csv('C:/Users/julia/OneDrive/WORK/MY_PAPERS/PlioMIP2/IPCC_box/' + 
          'mPWP_CMIP6_land_sea_by_latitude_jct.csv')
    
    


::::::::::::::
CEMAC/PLIOMIP2/get_means_from_pliomip1_and_regrid.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Jul  5 15:11:26 2019
@author: earjcti

 This program will get the means from the PlioMIP1 models that
 can be added to the pliomip2 figures

#
 """

#import os
import warnings
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#import matplotlib as mpl
#import matplotlib.cm as cm
#from matplotlib.colors import Normalize
import numpy as np
import iris
from iris.cube import CubeList
#import xlwt
#from xlwt import Workbook

warnings.filterwarnings("ignore")


###############################################################################

class Getmodeldata:
    """
    get all of the data from the model with the required averaing
    ie e280 or eoi400
    """
    def __init__(self, field, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day",
            "NearSurfaceTemperature" : "degC"
                        }


        self.fieldname = field
        self.period = period
        if period == 'E280':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Ctrl_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Ctrl_landmasks_p3grid.nc'
        if period == 'EOI400':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Plio_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Plio_landmasks_p3grid.nc'


        self.units = fieldunits.get(field)



    def get_fieldreq(self, model_):
        """
        All fields are in a single file.  This will get the fieldname
        required via a number of dictionaries
        """


        PeriodE280Use = {
            "CCSM" : "ctrl", "COSMOS" : "Ctrl", "GISS" : "Ctrl",
            "HAD" : "ctrl", "IPSL" : "ctrl", "MIROC" : "ctrl",
            "MRI" : "ctrl", "NOR" : "ctrl"
                        }

        PeriodEoi400Use = {
            "CCSM" : "plio", "COSMOS" : "plio", "GISS" : "Plio",
            "HAD" : "plio", "IPSL" : "plio", "MIROC" : "plio",
            "MRI" : "plio", "NOR" : "plio"
                          }

        fieldname_sst = {
            "CCSM" : "sst", "COSMOS" : "SST", "GISS" : "SST",
            "HAD" : "sst", "IPSL" : "sst", "MIROC" : "sst",
            "MRI" : "sst", "NOR" : "sst"
                        }

        fieldname_sat = {
            "CCSM" : "sat", "COSMOS" : "sat", "GISS" : "SAT",
            "HAD" : "SAT", "IPSL" : "sat", "MIROC" : "sat",
            "MRI" : "sat", "NOR" : "sat"
                        }


        if FIELDNAME == 'NearSurfaceTemperature':
            fielduse = fieldname_sat.get(model_)
        if FIELDNAME == 'TotalPrecipitation':
            fielduse = 'precip'
        if FIELDNAME == 'SST':
            fielduse = fieldname_sst.get(model_)

        if self.period == 'E280':
            fieldreq = (model_ + '_' +
                             PeriodE280Use.get(model_) + '_' + fielduse)
        if self.period == 'EOI400':
            fieldreq = (model_ + '_' +
                             PeriodEoi400Use.get(model_) + '_' + fielduse)


        return fieldreq

    def extract_cube(self, allcube_, fieldreq_):
        """
        will extract the cube from the list of cubes
        this is needed because the cube comes from varname not long name
        """
        ncubes = len(allcube_)

        # look for the cube
        cube_found = 'n'
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]
                cube_found = 'y'

        # if cube not found then change the capitalisation of the first
        # letter of the time period (ie plio ==> Plio,  Plio ==> plio)
        if cube_found == 'n':
            newstring = ' '
            if fieldreq_.find("Plio") > 0:
                newstring = fieldreq_.replace("Plio", "plio")
            if fieldreq_.find("plio") > 0:
                newstring = fieldreq_.replace("plio", "Plio")
            if fieldreq_.find("ctrl") > 0:
                newstring = fieldreq_.replace("ctrl", "Ctrl")
            if fieldreq_.find("Ctrl") > 0:
                newstring = fieldreq_.replace("Ctrl", "ctrl")
            for i in range(0, ncubes):
                if allcube_[i].var_name == newstring:
                    cube = allcube_[i]
                    cube_found = 'y'

        cube = iris.util.squeeze(cube)

        return cube

    def get_lsm(self, modname, datacube):
        """
        gets the lsm from the lsm file
        """

        alllsm = iris.load(self.lsm_file)
        ncubes = len(alllsm)
        fieldreq = modname + '_landmask'
        cube_found = 'n'

        time = {'E280': 'Ctrl',
                'EOI400' : 'Plio'}
        for i in range(0, ncubes):
            if alllsm[i].var_name == fieldreq:
                cube = alllsm[i]
                cube = iris.util.squeeze(cube)
                cube_found = 'y'

        # if cube not found then add the time period before the landseamask
        if cube_found == 'n':
            newstring = fieldreq.replace("landmask",
                                         time.get(self.period) + "_landmask")
            for i in range(0, ncubes):
                if alllsm[i].var_name == newstring:
                    cube = alllsm[i]
                    cube_found = 'y'

        if cube_found == 'n': #cube still not found then error
            print('cannot find lsm ' + fieldreq + ' or ' + newstring)
            sys.exit(0)

        # check coords of lsm cube are the same as the
        # coordinates of the data cube
        if np.array_equal(datacube.coord('longitude').points,
                          cube.coord('longitude').points):
            pass

        else:
            print('longitudes dont match')
            sys.exit(0)

        if np.array_equal(datacube.coord('latitude').points,
                          cube.coord('latitude').points):
            pass
        else:
            print('latitudes dont match')
            sys.exit(0)


        return np.squeeze(cube.data)

    def get_globalmean(self, cube):
        """
        calculates the area weighted global mean from the cube given
        """
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)

        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas)
        if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 270.:
            meancube.data = meancube.data - 273.15

        return meancube.data, grid_areas

    def get_latmean(self, cube, grid_areas, nbands):
        """
        calculates the area weighted mean in latitude bounds
        from the cube given
        """

        model_latbands = np.zeros((nbands))
        for boundno, thisbound in enumerate(LATBANDS):
            grid_areas_thisbound = np.zeros((np.shape(grid_areas)))
            for j, lat in enumerate(cube.coord('latitude').points):
                if thisbound[0] <= lat < thisbound[1]:
                    if cube.ndim == 3:
                        grid_areas_thisbound[:, j, :] = grid_areas[:, j, :]
                    if cube.ndim == 4:
                        grid_areas_thisbound[:, :, j, :] = grid_areas[:, :, j, :]
                    if cube.ndim == 2:
                        grid_areas_thisbound[j, :] = grid_areas[j, :]

            meancube = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_thisbound)

            if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 250.:
                meancube.data = meancube.data - 273.15

            model_latbands[boundno] = meancube.data


        return model_latbands

    def get_land_sea_means(self, cube, grid_areas, modname):
        """
        extract the land and the sea anomaly
        """

        # get lsm
        lsm = self.get_lsm(modname, cube)


        # get grid_areas (_20 means from 20N-20S)
        grid_areas_land = grid_areas * lsm
        grid_areas_sea = grid_areas - grid_areas_land
        grid_areas_land_20 = np.zeros(np.shape(grid_areas))
        grid_areas_sea_20 = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if -20 <= lat <= 20:
                grid_areas_land_20[j, :] = grid_areas_land[j, :]
                grid_areas_sea_20[j, :] = grid_areas_sea[j, :]

        mean_land = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_land)
        mean_sea = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_sea)
        mean_land_20 = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_land_20)
        mean_sea_20 = cube.collapsed(['longitude', 'latitude'],
                                     iris.analysis.MEAN,
                                     weights=grid_areas_sea_20)

        if FIELDNAME == 'NearSurfaceTemperature' and mean_land.data > 250.:
            mean_land.data = mean_land.data - 273.15
            mean_sea.data = mean_sea.data - 273.15
            mean_land_20.data = mean_land_20.data - 273.15
            mean_sea_20.data = mean_sea_20.data - 273.15

        return (mean_land.data, mean_sea.data, mean_land_20.data,
                mean_sea_20.data)

    def get_highlatmeans(self, cube, grid_areas, latval):
        """
        gets the means polewards of latval(north) and polewards of latval(S)
        """
        grid_areas_NH = np.zeros(np.shape(grid_areas))
        grid_areas_SH = np.zeros(np.shape(grid_areas))
        for j, lat in enumerate(cube.coord('latitude').points):
            if lat >= latval:
               grid_areas_NH[j, :] = grid_areas[j, :]
            if lat <= (-1.0) * latval:
                grid_areas_SH[j, :] = grid_areas[j, :]
                
        mean_NH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_NH)
        mean_SH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_SH)

        return (mean_NH.data, mean_SH.data)

    def extract_means(self):
        """
        the top level function for this class.  This will return all the
        means to the main program
        """
        # arrays to store data
        meanvals = np.zeros(len(MODELNAMES))
        meanvals_land = np.zeros(len(MODELNAMES))
        meanvals_sea = np.zeros(len(MODELNAMES))
        meanvals_land_20 = np.zeros(len(MODELNAMES))
        meanvals_sea_20 = np.zeros(len(MODELNAMES))
        meanvals_NH45 = np.zeros(len(MODELNAMES))
        meanvals_SH45 = np.zeros(len(MODELNAMES))
        meanvals_NH60 = np.zeros(len(MODELNAMES))
        meanvals_SH60 = np.zeros(len(MODELNAMES))

        nbands, nlims = np.shape(LATBANDS)
        latmeans = np.zeros((len(MODELNAMES), nbands))

        filename = (FILESTART + 'PlioMIP1_regridded.nc')
        allcubes = iris.load(filename)
        regridded_cubes = CubeList([])

        for i, model in enumerate(MODELNAMES):


            fieldreq = self.get_fieldreq(model)

            # extract the cube we want
            cube = self.extract_cube(allcubes, fieldreq)
            cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
            rg_cube = cube.regrid(cubegrid, iris.analysis.Nearest())
            if FIELDNAME == 'SST':
                newdata = np.where(rg_cube.data < 1E20, rg_cube.data, -999.999)
                newdata2 = np.where(newdata > -500, newdata, -999.999)
                regridded_cube= rg_cube.copy(data=newdata2)
                regridded_cubes.append(regridded_cube)
            else:
                regridded_cubes.append(rg_cube)
            
            meanvals[i], grid_areas = self.get_globalmean(cube)
            latmeans[i, :] = self.get_latmean(cube, grid_areas, nbands)

            (meanvals_land[i],
             meanvals_sea[i],
             meanvals_land_20[i],
             meanvals_sea_20[i]) = self.get_land_sea_means(cube, grid_areas, model)
            
            (meanvals_NH45[i], 
             meanvals_SH45[i]) = self.get_highlatmeans(cube,grid_areas, 45.0)
            
            (meanvals_NH60[i], 
             meanvals_SH60[i]) = self.get_highlatmeans(cube,grid_areas, 60.0)


        return (meanvals, latmeans, meanvals_land,
                meanvals_sea, meanvals_land_20, meanvals_sea_20,
                meanvals_NH45, meanvals_SH45,
                meanvals_NH60, meanvals_SH60, regridded_cubes)




# end of class Getmodeldata
##################################################################################
def get_nh_seascyc():
    """
    get the NH seasonal cycle from each model
    """


    shortfield = {"NearSurfaceTemperature" : "SAT",
                  "TotalPrecipitation" : "precip"}
    longfield_sat = {"CCSM" : "Reference height temperature",
                     "COSMOS" : "2m temperature",
                     "IPSL" : "t2m",
                     "MIROC": "tas",
                     "MRI" : "near surface air temperature [degC]"
                     }

    longfield_precip = {
        "CCSM" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "COSMOS" : "total precipitation",
        "IPSL" : "IPSL_precip",
        "MIROC": "MIROC_precip",
        "MRI" : "total precipitation [mm/day]"
                       }
    nh_means = np.zeros((len(MODELNAMES), 12))

    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + '/' + FIELDNAME + '/' +
                    model + '_Exp2_anom_' +
                    shortfield.get(FIELDNAME) + '_p3grid.nc')

        if FIELDNAME == "NearSurfaceTemperature":
            fieldreq = longfield_sat.get(model, "air_temperature")
        if FIELDNAME == "TotalPrecipitation":
            fieldreq = longfield_precip.get(model, "precipitation_flux")

        cube = iris.load_cube(filename, fieldreq)
        cube = iris.util.squeeze(cube)

        # get grid areas for seasonal average
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)
        grid_areas_nh = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if lat > 0:
                grid_areas_nh[:, j, :] = grid_areas[:, j, :]

        # get seasonal average
        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_nh)

        nh_means[i, :] = meancube.data
        plt.plot(meancube.data, label=model)
    plt.plot(np.mean(nh_means, axis=0), label='mean')
    plt.legend()
    #plt.show()

    return nh_means


def write_global_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write global means from each model to a text file

    """

    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)

    filetext.write("modelname, global mean EOI400," +
                   "global mean E280, anomaly \n")

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(np.mean(modelmean_eoi400), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_e280), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_anomaly), 3)) + '\n')



def write_lat_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write the latitudinal mean from each model to a textfile
    """


    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)
    mean_eoi400 = np.mean(modelmean_eoi400, axis=0)
    mean_e280 = np.mean(modelmean_e280, axis=0)
    mean_anomaly = mean_eoi400 - mean_e280

    filetext.write("modelname, latband mean EOI400," +
                   "latband mean E280, latband anomaly \n")
    filetext.write("bands are " + np.str(LATBANDS) + '\n')

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(mean_eoi400, 3)) + ',' +
                   np.str(np.around(mean_e280, 3)) +',' +
                   np.str(np.around(mean_anomaly, 3)) + '\n')

def write_nh_seascycle(filetext, seasanom):
    """
    write the NH seasonal anomaly from each model to a textfile
    """


    filetext.write("modelname, [jan feb mar apr may jun jul aug sep oct nov dec] \n")
    mean_anomaly = np.mean(seasanom, axis=0)

    for i, model in enumerate(MODELNAMES_FULL):
        anom = np.str(np.around(seasanom[i], 3))

        filetext.write((model + ',' + anom + '\n'))

    filetext.write('MEAN,' +  np.str(np.around(mean_anomaly, 3)) + '\n')

def write_landsea_means(filetext, landmean_eoi400_allmodels,
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, region):
    """
    write the land sea averages to a text file
    """
    filetext.write("modelname" + region + "[mean_ocean_eoi400, meanocean_e280, meanocean_anom, " +
                   "mean_land_eoi400, mean_land_e280, mean_land_anom ] \n")

    eoi400_sea_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels), 3))
    e280_sea_mean = np.str(np.around(np.mean(seamean_e280_allmodels), 3))
    eoi400_land_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels), 3))
    e280_land_mean = np.str(np.around(np.mean(landmean_e280_allmodels), 3))
    sea_anom_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels)
                                     - np.mean(seamean_e280_allmodels), 3))
    land_anom_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels)
                                      - np.mean(landmean_e280_allmodels), 3))

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400_sea = np.str(np.around(seamean_eoi400_allmodels[i], 3))
        e280_sea = np.str(np.around(seamean_e280_allmodels[i], 3))
        eoi400_land = np.str(np.around(landmean_eoi400_allmodels[i], 3))
        e280_land = np.str(np.around(landmean_e280_allmodels[i], 3))
        sea_anom = np.str(np.around((seamean_eoi400_allmodels[i]
                                     - seamean_e280_allmodels[i]), 3))
        land_anom = np.str(np.around((landmean_eoi400_allmodels[i]
                                      - landmean_e280_allmodels[i]), 3))


        filetext.write(model + ',' + eoi400_sea + ',' + e280_sea + ',' + sea_anom +
                       ',' + eoi400_land + ',' + e280_land + ',' + land_anom + '\n')

    filetext.write('MEAN,' + eoi400_sea_mean + ',' + e280_sea_mean + ',' + sea_anom_mean +
                   ',' + eoi400_land_mean + ',' + e280_land_mean + ',' + land_anom_mean + '\n')

def write_hemisphere_anom(filetext, NH_anomaly45,SH_anomaly45,
                          NH_anomaly60, SH_anomaly60):
    """
    writes the anomalies polewards of 45N and 45S and 60S and 60N
    """
    filetext.write("modelname, 45N-90N_anom, 45S-90S_anom, 60N-90N_anom, " +
                   "60S-90S_anom \n")
    for i, model in enumerate(MODELNAMES_FULL):

        filetext.write(model + ',' + np.str(np.around(NH_anomaly45[i], 3)) + ',' +
                       np.str(np.around(SH_anomaly45[i],3)) + ',' +
                       np.str(np.around(NH_anomaly60[i],3)) + ',' +
                       np.str(np.around(SH_anomaly60[i],3)) + '\n')
    filetext.write('MEAN,' + np.str(np.around(np.mean(NH_anomaly45), 3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly45),3)) + ',' +
                   np.str(np.around(np.mean(NH_anomaly60),3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly60),3)) +  '\n')


def write_regridded(regriddedcubes,exptname):
    """
    write the data on a 1x1 grid to a file
    """
    fileoutstart = FILESTART + FIELDNAME + '_regridded/'
    for i, model in enumerate(MODELNAMES_FULL):
        filename = (fileoutstart + FIELDNAME + '_' + model + '_' + 
                    exptname + '.nc')
        iris.save(regriddedcubes[i], filename, netcdf_format="NETCDF3_CLASSIC",
                  fill_value = -999.999)

    

    
  
def main():

    # get data means and latitudinal anomalies
    modeldata = Getmodeldata(FIELDNAME, 'EOI400')
    (globalmean_eoi400_allmodels,
     latmean_eoi400_allmodels,
     landmean_eoi400_allmodels,
     seamean_eoi400_allmodels,
     landmean20_eoi400_allmodels,
     seamean20_eoi400_allmodels,
     NH45_eoi400_anomaly, SH45_eoi400_anomaly,
     NH60_eoi400_anomaly, SH60_eoi400_anomaly,
     regridded_eoi400_cubes) = modeldata.extract_means()

    modeldata = Getmodeldata(FIELDNAME, 'E280')
    (globalmean_e280_allmodels,
     latmean_e280_allmodels,
     landmean_e280_allmodels,
     seamean_e280_allmodels,
     landmean20_e280_allmodels,
     seamean20_e280_allmodels,
     NH45_e280_anomaly, SH45_e280_anomaly,
     NH60_e280_anomaly, SH60_e280_anomaly,
     regridded_e280_cubes) = modeldata.extract_means()


    # write regridded data
    write_regridded(regridded_eoi400_cubes,'Expt2')
    write_regridded(regridded_e280_cubes,'Cntl')
    sys.exit(0)


    # get seasonal cycle
    NH_seas_anom = get_nh_seascyc()


    # write to text

    filetext = open((FILESTART + '/means_for_' + FIELDNAME + '.txt'), "w+")
    write_global_means(filetext, globalmean_eoi400_allmodels,
                       globalmean_e280_allmodels)
    write_lat_means(filetext, latmean_eoi400_allmodels,
                    latmean_e280_allmodels)
    write_nh_seascycle(filetext, NH_seas_anom)
    write_landsea_means(filetext, landmean_eoi400_allmodels, # global land sea
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, "global")
    write_landsea_means(filetext, landmean20_eoi400_allmodels, # global land sea
                        seamean20_eoi400_allmodels, landmean20_e280_allmodels,
                        seamean20_e280_allmodels, "20N-20S")
    write_hemisphere_anom(filetext, NH45_eoi400_anomaly - NH45_e280_anomaly,
                          SH45_eoi400_anomaly - SH45_e280_anomaly,
                          NH60_eoi400_anomaly - NH60_e280_anomaly,
                          SH60_eoi400_anomaly - SH60_e280_anomaly)

    filetext.close()

   
####################################
# definitions

LINUX_WIN = 'l'
#FIELDNAME = 'TotalPrecipitation'
#FIELDNAME = 'NearSurfaceTemperature'
FIELDNAME = 'SST'
if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/PLIOMIP_old/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\PLIOMIP1\\'

MODELNAMES = ['CCSM', 'COSMOS', 'GISS', 'HAD', 'IPSL',
              'MIROC', 'MRI', 'NOR']

CONVERT_MODELS = {'CCSM' : 'CCSM4',
                  'GISS' : 'GISS-E2-R',
                  'HAD'  : 'HadCM3',
                  'IPSL' : 'IPSLCM5A',
                  'MIROC': 'MIROC4m',
                  'MRI' : 'MRI2.3',
                  'NOR' : 'NORESM-L'}

MODELNAMES_FULL = []
for MOD in MODELNAMES:
    MODELNAMES_FULL.append(CONVERT_MODELS.get(MOD, MOD))

LATBANDS = [[-90., -60.], [-60., -30.], [-30., 0.], [0., 30],
            [30., 60.], [60., 90.]]


main()

::::::::::::::
CEMAC/PLIOMIP2/get_means_from_pliomip1.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Jul  5 15:11:26 2019
@author: earjcti

 This program will get the means from the PlioMIP1 models that
 can be added to the pliomip2 figures

#
 """

#import os
import warnings
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#import matplotlib as mpl
#import matplotlib.cm as cm
#from matplotlib.colors import Normalize
import numpy as np
import iris
#import xlwt
#from xlwt import Workbook

warnings.filterwarnings("ignore")


###############################################################################

class Getmodeldata:
    """
    get all of the data from the model with the required averaing
    ie e280 or eoi400
    """
    def __init__(self, field, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day",
            "NearSurfaceTemperature" : "degC"
                        }


        self.fieldname = field
        self.period = period
        if period == 'E280':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Ctrl_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Ctrl_landmasks_p3grid.nc'
        if period == 'EOI400':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Plio_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Plio_landmasks_p3grid.nc'


        self.units = fieldunits.get(field)



    def get_fieldreq(self, model_):
        """
        All fields are in a single file.  This will get the fieldname
        required via a number of dictionaries
        """


        PeriodE280Use = {
            "CCSM" : "ctrl", "COSMOS" : "Ctrl", "GISS" : "Ctrl",
            "HAD" : "ctrl", "IPSL" : "ctrl", "MIROC" : "ctrl",
            "MRI" : "ctrl", "NOR" : "ctrl"
                        }

        PeriodEoi400Use = {
            "CCSM" : "plio", "COSMOS" : "plio", "GISS" : "Plio",
            "HAD" : "plio", "IPSL" : "plio", "MIROC" : "plio",
            "MRI" : "plio", "NOR" : "plio"
                          }

        fieldname_sst = {
            "CCSM" : "sst", "COSMOS" : "SST", "GISS" : "SST",
            "HAD" : "sst", "IPSL" : "sst", "MIROC" : "sst",
            "MRI" : "sst", "NOR" : "sst"
                        }

        fieldname_sat = {
            "CCSM" : "sat", "COSMOS" : "sat", "GISS" : "SAT",
            "HAD" : "SAT", "IPSL" : "sat", "MIROC" : "sat",
            "MRI" : "sat", "NOR" : "sat"
                        }


        if FIELDNAME == 'NearSurfaceTemperature':
            fielduse = fieldname_sat.get(model_)
        if FIELDNAME == 'TotalPrecipitation':
            fielduse = 'precip'

        if self.period == 'E280':
            fieldreq = (model_ + '_' +
                             PeriodE280Use.get(model_) + '_' + fielduse)
        if self.period == 'EOI400':
            fieldreq = (model_ + '_' +
                             PeriodEoi400Use.get(model_) + '_' + fielduse)


        return fieldreq

    def extract_cube(self, allcube_, fieldreq_):
        """
        will extract the cube from the list of cubes
        this is needed because the cube comes from varname not long name
        """
        ncubes = len(allcube_)

        # look for the cube
        cube_found = 'n'
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]
                cube_found = 'y'

        # if cube not found then change the capitalisation of the first
        # letter of the time period (ie plio ==> Plio,  Plio ==> plio)
        if cube_found == 'n':
            newstring = ' '
            if fieldreq_.find("Plio") > 0:
                newstring = fieldreq_.replace("Plio", "plio")
            if fieldreq_.find("plio") > 0:
                newstring = fieldreq_.replace("plio", "Plio")
            if fieldreq_.find("ctrl") > 0:
                newstring = fieldreq_.replace("ctrl", "Ctrl")
            if fieldreq_.find("Ctrl") > 0:
                newstring = fieldreq_.replace("Ctrl", "ctrl")
            for i in range(0, ncubes):
                if allcube_[i].var_name == newstring:
                    cube = allcube_[i]
                    cube_found = 'y'

        cube = iris.util.squeeze(cube)

        return cube

    def get_lsm(self, modname, datacube):
        """
        gets the lsm from the lsm file
        """

        alllsm = iris.load(self.lsm_file)
        ncubes = len(alllsm)
        fieldreq = modname + '_landmask'
        cube_found = 'n'

        time = {'E280': 'Ctrl',
                'EOI400' : 'Plio'}
        for i in range(0, ncubes):
            if alllsm[i].var_name == fieldreq:
                cube = alllsm[i]
                cube = iris.util.squeeze(cube)
                cube_found = 'y'

        # if cube not found then add the time period before the landseamask
        if cube_found == 'n':
            newstring = fieldreq.replace("landmask",
                                         time.get(self.period) + "_landmask")
            for i in range(0, ncubes):
                if alllsm[i].var_name == newstring:
                    cube = alllsm[i]
                    cube_found = 'y'

        if cube_found == 'n': #cube still not found then error
            print('cannot find lsm ' + fieldreq + ' or ' + newstring)
            sys.exit(0)

        # check coords of lsm cube are the same as the
        # coordinates of the data cube
        if np.array_equal(datacube.coord('longitude').points,
                          cube.coord('longitude').points):
            pass

        else:
            print('longitudes dont match')
            sys.exit(0)

        if np.array_equal(datacube.coord('latitude').points,
                          cube.coord('latitude').points):
            pass
        else:
            print('latitudes dont match')
            sys.exit(0)


        return np.squeeze(cube.data)

    def get_globalmean(self, cube):
        """
        calculates the area weighted global mean from the cube given
        """
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)

        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas)
        if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 270.:
            meancube.data = meancube.data - 273.15

        return meancube.data, grid_areas

    def get_latmean(self, cube, grid_areas, nbands):
        """
        calculates the area weighted mean in latitude bounds
        from the cube given
        """

        model_latbands = np.zeros((nbands))
        for boundno, thisbound in enumerate(LATBANDS):
            grid_areas_thisbound = np.zeros((np.shape(grid_areas)))
            for j, lat in enumerate(cube.coord('latitude').points):
                if thisbound[0] <= lat < thisbound[1]:
                    if cube.ndim == 3:
                        grid_areas_thisbound[:, j, :] = grid_areas[:, j, :]
                    if cube.ndim == 4:
                        grid_areas_thisbound[:, :, j, :] = grid_areas[:, :, j, :]
                    if cube.ndim == 2:
                        grid_areas_thisbound[j, :] = grid_areas[j, :]

            meancube = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_thisbound)

            if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 250.:
                meancube.data = meancube.data - 273.15

            model_latbands[boundno] = meancube.data


        return model_latbands

    def get_land_sea_means(self, cube, grid_areas, modname):
        """
        extract the land and the sea anomaly
        """

        # get lsm
        lsm = self.get_lsm(modname, cube)


        # get grid_areas (_20 means from 20N-20S)
        grid_areas_land = grid_areas * lsm
        grid_areas_sea = grid_areas - grid_areas_land
        grid_areas_land_20 = np.zeros(np.shape(grid_areas))
        grid_areas_sea_20 = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if -20 <= lat <= 20:
                grid_areas_land_20[j, :] = grid_areas_land[j, :]
                grid_areas_sea_20[j, :] = grid_areas_sea[j, :]

        mean_land = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_land)
        mean_sea = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_sea)
        mean_land_20 = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_land_20)
        mean_sea_20 = cube.collapsed(['longitude', 'latitude'],
                                     iris.analysis.MEAN,
                                     weights=grid_areas_sea_20)

        if FIELDNAME == 'NearSurfaceTemperature' and mean_land.data > 250.:
            mean_land.data = mean_land.data - 273.15
            mean_sea.data = mean_sea.data - 273.15
            mean_land_20.data = mean_land_20.data - 273.15
            mean_sea_20.data = mean_sea_20.data - 273.15

        return (mean_land.data, mean_sea.data, mean_land_20.data,
                mean_sea_20.data)

    def get_highlatmeans(self, cube, grid_areas, latval):
        """
        gets the means polewards of latval(north) and polewards of latval(S)
        """
        grid_areas_NH = np.zeros(np.shape(grid_areas))
        grid_areas_SH = np.zeros(np.shape(grid_areas))
        for j, lat in enumerate(cube.coord('latitude').points):
            if lat >= latval:
               grid_areas_NH[j, :] = grid_areas[j, :]
            if lat <= (-1.0) * latval:
                grid_areas_SH[j, :] = grid_areas[j, :]
                
        mean_NH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_NH)
        mean_SH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_SH)

        return (mean_NH.data, mean_SH.data)

    def extract_means(self):
        """
        the top level function for this class.  This will return all the
        means to the main program
        """
        # arrays to store data
        meanvals = np.zeros(len(MODELNAMES))
        meanvals_land = np.zeros(len(MODELNAMES))
        meanvals_sea = np.zeros(len(MODELNAMES))
        meanvals_land_20 = np.zeros(len(MODELNAMES))
        meanvals_sea_20 = np.zeros(len(MODELNAMES))
        meanvals_NH45 = np.zeros(len(MODELNAMES))
        meanvals_SH45 = np.zeros(len(MODELNAMES))
        meanvals_NH60 = np.zeros(len(MODELNAMES))
        meanvals_SH60 = np.zeros(len(MODELNAMES))

        nbands, nlims = np.shape(LATBANDS)
        latmeans = np.zeros((len(MODELNAMES), nbands))

        filename = (FILESTART + 'PlioMIP1_regridded.nc')
        allcubes = iris.load(filename)

        for i, model in enumerate(MODELNAMES):


            fieldreq = self.get_fieldreq(model)

            # extract the cube we want
            cube = self.extract_cube(allcubes, fieldreq)
            meanvals[i], grid_areas = self.get_globalmean(cube)
            latmeans[i, :] = self.get_latmean(cube, grid_areas, nbands)

            (meanvals_land[i],
             meanvals_sea[i],
             meanvals_land_20[i],
             meanvals_sea_20[i]) = self.get_land_sea_means(cube, grid_areas, model)
            
            (meanvals_NH45[i], 
             meanvals_SH45[i]) = self.get_highlatmeans(cube,grid_areas, 45.0)
            
            (meanvals_NH60[i], 
             meanvals_SH60[i]) = self.get_highlatmeans(cube,grid_areas, 60.0)


        return (meanvals, latmeans, meanvals_land,
                meanvals_sea, meanvals_land_20, meanvals_sea_20,
                meanvals_NH45, meanvals_SH45,
                meanvals_NH60, meanvals_SH60)




# end of class Getmodeldata
##################################################################################
def get_nh_seascyc():
    """
    get the NH seasonal cycle from each model
    """


    shortfield = {"NearSurfaceTemperature" : "SAT",
                  "TotalPrecipitation" : "precip"}
    longfield_sat = {"CCSM" : "Reference height temperature",
                     "COSMOS" : "2m temperature",
                     "IPSL" : "t2m",
                     "MIROC": "tas",
                     "MRI" : "near surface air temperature [degC]"
                     }

    longfield_precip = {
        "CCSM" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "COSMOS" : "total precipitation",
        "IPSL" : "IPSL_precip",
        "MIROC": "MIROC_precip",
        "MRI" : "total precipitation [mm/day]"
                       }
    nh_means = np.zeros((len(MODELNAMES), 12))

    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + '/' + FIELDNAME + '/' +
                    model + '_Exp2_anom_' +
                    shortfield.get(FIELDNAME) + '_p3grid.nc')

        if FIELDNAME == "NearSurfaceTemperature":
            fieldreq = longfield_sat.get(model, "air_temperature")
        if FIELDNAME == "TotalPrecipitation":
            fieldreq = longfield_precip.get(model, "precipitation_flux")

        cube = iris.load_cube(filename, fieldreq)
        cube = iris.util.squeeze(cube)

        # get grid areas for seasonal average
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)
        grid_areas_nh = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if lat > 0:
                grid_areas_nh[:, j, :] = grid_areas[:, j, :]

        # get seasonal average
        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_nh)

        nh_means[i, :] = meancube.data
        plt.plot(meancube.data, label=model)
    plt.plot(np.mean(nh_means, axis=0), label='mean')
    plt.legend()
    #plt.show()

    return nh_means


def write_global_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write global means from each model to a text file

    """

    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)

    filetext.write("modelname, global mean EOI400," +
                   "global mean E280, anomaly \n")

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(np.mean(modelmean_eoi400), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_e280), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_anomaly), 3)) + '\n')



def write_lat_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write the latitudinal mean from each model to a textfile
    """


    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)
    mean_eoi400 = np.mean(modelmean_eoi400, axis=0)
    mean_e280 = np.mean(modelmean_e280, axis=0)
    mean_anomaly = mean_eoi400 - mean_e280

    filetext.write("modelname, latband mean EOI400," +
                   "latband mean E280, latband anomaly \n")
    filetext.write("bands are " + np.str(LATBANDS) + '\n')

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(mean_eoi400, 3)) + ',' +
                   np.str(np.around(mean_e280, 3)) +',' +
                   np.str(np.around(mean_anomaly, 3)) + '\n')

def write_nh_seascycle(filetext, seasanom):
    """
    write the NH seasonal anomaly from each model to a textfile
    """


    filetext.write("modelname, [jan feb mar apr may jun jul aug sep oct nov dec] \n")
    mean_anomaly = np.mean(seasanom, axis=0)

    for i, model in enumerate(MODELNAMES_FULL):
        anom = np.str(np.around(seasanom[i], 3))

        filetext.write((model + ',' + anom + '\n'))

    filetext.write('MEAN,' +  np.str(np.around(mean_anomaly, 3)) + '\n')

def write_landsea_means(filetext, landmean_eoi400_allmodels,
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, region):
    """
    write the land sea averages to a text file
    """
    filetext.write("modelname" + region + "[mean_ocean_eoi400, meanocean_e280, meanocean_anom, " +
                   "mean_land_eoi400, mean_land_e280, mean_land_anom ] \n")

    eoi400_sea_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels), 3))
    e280_sea_mean = np.str(np.around(np.mean(seamean_e280_allmodels), 3))
    eoi400_land_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels), 3))
    e280_land_mean = np.str(np.around(np.mean(landmean_e280_allmodels), 3))
    sea_anom_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels)
                                     - np.mean(seamean_e280_allmodels), 3))
    land_anom_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels)
                                      - np.mean(landmean_e280_allmodels), 3))

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400_sea = np.str(np.around(seamean_eoi400_allmodels[i], 3))
        e280_sea = np.str(np.around(seamean_e280_allmodels[i], 3))
        eoi400_land = np.str(np.around(landmean_eoi400_allmodels[i], 3))
        e280_land = np.str(np.around(landmean_e280_allmodels[i], 3))
        sea_anom = np.str(np.around((seamean_eoi400_allmodels[i]
                                     - seamean_e280_allmodels[i]), 3))
        land_anom = np.str(np.around((landmean_eoi400_allmodels[i]
                                      - landmean_e280_allmodels[i]), 3))


        filetext.write(model + ',' + eoi400_sea + ',' + e280_sea + ',' + sea_anom +
                       ',' + eoi400_land + ',' + e280_land + ',' + land_anom + '\n')

    filetext.write('MEAN,' + eoi400_sea_mean + ',' + e280_sea_mean + ',' + sea_anom_mean +
                   ',' + eoi400_land_mean + ',' + e280_land_mean + ',' + land_anom_mean + '\n')

def write_hemisphere_anom(filetext, NH_anomaly45,SH_anomaly45,
                          NH_anomaly60, SH_anomaly60):
    """
    writes the anomalies polewards of 45N and 45S and 60S and 60N
    """
    filetext.write("modelname, 45N-90N_anom, 45S-90S_anom, 60N-90N_anom, " +
                   "60S-90S_anom \n")
    for i, model in enumerate(MODELNAMES_FULL):

        filetext.write(model + ',' + np.str(np.around(NH_anomaly45[i], 3)) + ',' +
                       np.str(np.around(SH_anomaly45[i],3)) + ',' +
                       np.str(np.around(NH_anomaly60[i],3)) + ',' +
                       np.str(np.around(SH_anomaly60[i],3)) + '\n')
    filetext.write('MEAN,' + np.str(np.around(np.mean(NH_anomaly45), 3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly45),3)) + ',' +
                   np.str(np.around(np.mean(NH_anomaly60),3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly60),3)) +  '\n')

def main():

    # get data means and latitudinal anomalies
    modeldata = Getmodeldata(FIELDNAME, 'EOI400')
    (globalmean_eoi400_allmodels,
     latmean_eoi400_allmodels,
     landmean_eoi400_allmodels,
     seamean_eoi400_allmodels,
     landmean20_eoi400_allmodels,
     seamean20_eoi400_allmodels,
     NH45_eoi400_anomaly, SH45_eoi400_anomaly,
     NH60_eoi400_anomaly, SH60_eoi400_anomaly) = modeldata.extract_means()

    modeldata = Getmodeldata(FIELDNAME, 'E280')
    (globalmean_e280_allmodels,
     latmean_e280_allmodels,
     landmean_e280_allmodels,
     seamean_e280_allmodels,
     landmean20_e280_allmodels,
     seamean20_e280_allmodels,
     NH45_e280_anomaly, SH45_e280_anomaly,
     NH60_e280_anomaly, SH60_e280_anomaly) = modeldata.extract_means()

    # get seasonal cycle
    NH_seas_anom = get_nh_seascyc()


    # write to text

    filetext = open((FILESTART + '/means_for_' + FIELDNAME + '.txt'), "w+")
    write_global_means(filetext, globalmean_eoi400_allmodels,
                       globalmean_e280_allmodels)
    write_lat_means(filetext, latmean_eoi400_allmodels,
                    latmean_e280_allmodels)
    write_nh_seascycle(filetext, NH_seas_anom)
    write_landsea_means(filetext, landmean_eoi400_allmodels, # global land sea
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, "global")
    write_landsea_means(filetext, landmean20_eoi400_allmodels, # global land sea
                        seamean20_eoi400_allmodels, landmean20_e280_allmodels,
                        seamean20_e280_allmodels, "20N-20S")
    write_hemisphere_anom(filetext, NH45_eoi400_anomaly - NH45_e280_anomaly,
                          SH45_eoi400_anomaly - SH45_e280_anomaly,
                          NH60_eoi400_anomaly - NH60_e280_anomaly,
                          SH60_eoi400_anomaly - SH60_e280_anomaly)

    filetext.close()


####################################
# definitions

LINUX_WIN = 'l'
#FIELDNAME = 'TotalPrecipitation'
FIELDNAME = 'NearSurfaceTemperature'
if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/PLIOMIP_old/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\PLIOMIP1\\'

MODELNAMES = ['CCSM', 'COSMOS', 'GISS', 'HAD', 'IPSL',
              'MIROC', 'MRI', 'NOR']

CONVERT_MODELS = {'CCSM' : 'CCSM4',
                  'GISS' : 'GISS-E2-R',
                  'HAD'  : 'HadCM3',
                  'IPSL' : 'IPSLCM5A',
                  'MIROC': 'MIROC4m',
                  'MRI' : 'MRI2.3',
                  'NOR' : 'NORESM-L'}

MODELNAMES_FULL = []
for MOD in MODELNAMES:
    MODELNAMES_FULL.append(CONVERT_MODELS.get(MOD, MOD))

LATBANDS = [[-90., -60.], [-60., -30.], [-30., 0.], [0., 30],
            [30., 60.], [60., 90.]]


main()

::::::::::::::
CEMAC/PLIOMIP2/globalmean_timeseries.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 21 2020

#
# This program will produce a regridded 1X1degree timeseries of a given field.  
# We will remove the annual cycle in order to look for interannual variability 
# etc.  


import numpy as np
from netCDF4 import Dataset
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
from scipy import stats
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname, lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse == "sst":
        if exptname == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'

        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data = np.ma.array(cube.data, mask=mymask)
    return cube

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u = unit.Unit('days since 0800-01-01 00:00:00',
                      calendar=unit.CALENDAR_360_DAY)
        if model == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model == 'HadCM3' and fielduse == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model == 'HadCM3' and fielduse == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube = cube_temp.extract(iris.Constraint(surface=0.))

    return cube

def get_noresm_ocn(exptnamein, fieldnamein):
    """
    get noresm ocean things.  

    """
    
    print('need to do this')
    print('it is in monthly data and on a tripolar grid')
    sys.exit(0)

    return cube

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src, Dataset("temporary.nc", "w", format='NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name, src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name, dimension in src.dimensions.items():

            if name != 'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name, (len(dimension)))

        # copy all file data
        for name, variable in src.variables.items():
            print('name is', name, variable)
            print('datatype',variable.datatype)
            print('dimensions',variable.dimensions)
            if name not in ('time_counter_bnds', 'time_centered'):
                x = dst.createVariable(name, variable.datatype,
                                       variable.dimensions)
                if name == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:] = ((src.variables[name][:] / (60.*60.*24))
                                              -(src.variables[name][0] / (60.*60.*24)) + 15.)
                else:
                    dst.variables[name][:] = src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    #print(ncattr, attribute, exptname)
                    if ncattr == 'calendar' and exptname == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr == 'units' and name == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            if ncattr != "_FillValue":
                                dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein in ('ts', 'tas'):
            cube.convert_units('Celsius')

        if exptname == 'Eoi400':
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_360_DAY)
        else:
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return cube

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return cube

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return cube

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return cube


def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """

    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]

    # put units as celcius if required
    if fielduse == 'tas':
        cube.units = 'Celsius'

    cube2 = iris.util.new_axis(cube, 'time')

    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """

    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

    cube.add_dim_coord(iris.coords.DimCoord(points,
                                            standard_name='time', long_name='time',
                                            var_name='time',
                                            units=u,
                                            bounds=None,
                                            coord_system=None, circular=False), 0)

    if fieldnamein == 'pr':
        cube.data = cube.data * 60. *60. *24. *1000.
        cube.name = 'Total precipitation'
        cube.long_name = 'Total precipitation'
        cube.units = 'mm/day'


    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]

    return singlecube

def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """

    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
        cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
        cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
        if fieldnameout == 'TotalPrecipitation':
            cube = cube1 + cube2
        if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
            cube1.convert_units('Celsius')
            cubedata = np.where((cube2.data > 0.01),
                                -1.8, cube1.data)
            cube = cube1.copy(data=cubedata)
    else:
        cube = get_cesm12_singlecube(filename, fielduse)

    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b.e21.B1850.f09_g17.' +
                           'PMIP4-midPliocene-eoi400.001.'+
                           'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' +
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280':
            #if modelname == 'CESM2':
            #    filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
           #                'b.e21.B1850.f09_g17.' +
           #                'CMIP6-piControl.001.cam.h0.'+
           #                'LANDFRAC.1300.1399.nc')
           # else:
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                       'b.e12.B1850.f09_g16.preind.' +
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data = np.ma.array(cube.data, mask=mymask)


    return cube



######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube

    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years


    """

    if modelname in ('CCSM4', 'CESM1.2', 'CESM2','NorESM1-F','NorESM-L'):
        print('CCSM',cube.coord('month').points)
        months = cube.coord('month').points
        months = months -1
        for i, month in enumerate(months):
            if month == 0: months[i] = 12
        print(months)
        cube.coord('month').points = months
 
   


    return cube

######################################################
def cube_avg(cube):
    """
    Extract global annual averaged data from an array

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanyear (numpy array): the global mean of the field

    """

    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    
    meanyearcube.coord('latitude').guess_bounds()
    meanyearcube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(meanyearcube)
    
    yearglobavg_cube = (meanyearcube.collapsed(['longitude', 'latitude'],
                                       iris.analysis.MEAN, weights = grid_areas))
    global_avg = yearglobavg_cube.data
  

    return yearglobavg_cube.data




##############################################
def get_timeseries_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

    # outfile
    if linux_win == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/' + modelname +
                    '/timeseries/' + exptnameout + '.' + fieldnameout + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
                    + modelname + '\\timeseries\\' + exptnameout 
                    + '.' + fieldnameout + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


    #####################################
    # get all data in a single cube
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3'): # all fields in one file
        cube = get_ecearth_cube(exptnamein, lsmstart)
    elif modelname in ('HadCM3', 'MRI2.3'):
        cube = get_hadcm3_cube(modelname)
    elif modelname in ('IPSLCM5A', 'IPSLCM5A2') and fieldnamein != 'tos':
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif modelname in ('NorESM1-F', 'NorESM-L') and fieldnamein == 'tos':
        cube = get_noresm_ocn(exptnamein, fieldnamein)
    elif modelname == 'IPSLCM6A':
        cube = get_ipslcm6()
    elif modelname in ('MIROC4m', 'tos'):
        cube = get_miroc_tos()
    elif modelname == 'GISS2.1G':
        cube = get_giss()
    elif modelname == 'CCSM4-Utr':
        cube = get_ccsm4_2deg()
    elif modelname in ('CESM1.2', 'CCSM4', 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube = get_ccsm4_uot(fieldnamein)
    else:
        cube = iris.load_cube(filename)

    

    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid

    if ((modelname == 'CCSM4-UoT')
            or (modelname == 'IPSLCM5A' and fieldnamein == 'tos')
            or (modelname == 'IPSLCM5A2' and fieldnamein == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A',
                     'IPSLCM5A2', 'IPSLCM6A', 'CCSM4-Utr', 'GISS2.1G'):
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if modelname in ('NorESM1-F', 'NorESM-L', 'CESM1.2', 'CESM2', 'CCSM4'):

       # if precipitation is in m/s convert to mm/day
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. * 60. * 24. * 1000.
            cube.data = cube.data * 60. * 60. * 24. * 1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if modelname in ('CCSM4-UoT', 'NorESM1-F', 'NorESM-L', 'IPSLCM6A',
                     'EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A', 'IPSLCM5A2',
                     'HadCM3', 'GISS2.1G'):
         # convert to celcius
        if fieldnamein in ('tas', 'tos'):
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname in ('COSMOS', 'MIROC4m', 'IPSLCM6A', 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate


    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube, 'time', name='month')
    iris.coord_categorisation.add_year(regridded_cube, 'time', name='year')


     # correct the start month if required
    regridded_cube = correct_start_month(regridded_cube)
    
    # calculate averages
    mean_year_array = cube_avg(regridded_cube)
    
    return mean_year_array




#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                     }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }

    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }

    NorESM_FIELDS = {"pr" : "PRECT",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "sst"
                    }

    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                      }

    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA = {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                    "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }

    CESM2_EXTRA = {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                   "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                   }

    CCSM4_EXTRA = {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                   "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                   }

    ECearth_EXPT = {"Eoi400": "mPlio",
                    "E280":"PI"
                   }

    CESM12_EXPT = {"Eoi400": "PlioMIP2",
                   "E280":"PI"
                   }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
                     "E280":"PI"
                    }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }

    CESM2_TIME = {"E280" : ".110001-120012",
                  "Eoi400" : ".1101.1200"
                 }

    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
                     "E280":"3600_3699"
                    }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
                       "E280":"6110_6209",
                      }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
                     "E280":"piControl_r1i1p1f1_gr_285001-304912",
                     }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                         "E280":"piControl_r1i1p1f1_gn_285001-304912",
                        }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
                  "E280":"piControl_r1i1p1f1_gn_490101-495012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
                 }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
                       "E280":"piControl_r1i1p1f1_gr1_150101-160012",
                       "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                      }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
                  "E280":"piControl_r1i1p1f1_gn_495101-500012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                  }
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"}
    cosmos_version = {"tas": "",
                      "pr": "",
                      "tos":"_remapbil"}

    # get names for each model
    if modelname == 'MIROC4m':
        filename = filestart + modelname + '/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename + fielduse + '/MIROC4m_'+exptnamein
                    + '_' + atm_ocn_ind.get(fieldnamein) + '_' + fielduse + '.nc')
    if modelname == 'COSMOS':
        if linux_win == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename + exptnamein + '.' + fielduse +
                    '_CMIP6_name_' + fieldnamein +
                    '_2650-2749_monthly_mean_time_series' +
                    cosmos_version.get(fieldnamein) + '.nc')
    if modelname == 'CCSM4-UoT':
        if linux_win == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' +
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart + 'UofT-CCSM4\\' + exptnamein + '\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)

        filename = (filename + fielduse +
                    '_' + atm_ocn_ind.get(fieldnamein) +
                    '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart + 'LEEDS/HadCM3/' + exptuse + '/' + fielduse + '/'
                    + exptuse + '.' + fielduse + '.')
    if modelname == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                    +exptuse+'.'+fielduse+'.')
    if modelname == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname
                    + '_'
                    + ECearth_EXPT.get(exptnamein)
                    + fileend)
    if modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse 
                        + '_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse + '_monthly_TS.nc')

    if modelname == 'NorESM1-F' or modelname == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' +fielduse + '/'
                    + modelname + '_' + exptnamein + '.' + fielduse + '.')
        else:
            filename = (filestart + modelname + '/' + modelname + '_' +
                    exptnamein + '_' + fielduse + '.nc')
    if modelname == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' + fielduse +
                        '_Omon_IPSL-CM6A-LR_' + IPSLCM6A_TIME_ALT.get(exptnamein) 
                        + '_rectilinear.nc')
        else:
            filename = (filestart + modelname + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) + '_IPSL-CM6A-LR_' 
                        + IPSLCM6A_TIME.get(exptnamein) + '.nc')
    if modelname == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart + modelname + '/' + exptuse + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME1.get(exptnamein)
                        + '.nc')
        filename.append(filestart + modelname + '/' + exptuse + '/' 
                        + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME2.get(exptnamein) + '.nc')

    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein

    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                      CESM2_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECL' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']

        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'TS' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'ICEFRAC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']

        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' +
                      CCSM4_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)


    retdata = [fielduse, filename]
    return retdata

def checkdrift(plio_ts, pi_ts):
    """
    this program needs a pliocene timeseries and a preindustrial timeseries
    it will calculate the linear regression and use the slope to find
    the expected drift over 100 years (in the format ??degC / century)
    
    it will do this for the pliocene the preindustrial and the anomaly
    """
    
    nyears = len(plio_ts)
    allyears = np.arange(0.0, nyears, 1.0)
    nyears_pi = len(pi_ts)
    allyears_pi = np.arange(0.0, nyears_pi, 1.0)
    nyears_min = np.min([nyears, nyears_pi])
    allyears_min = np.arange(0.0, nyears_min, 1.0)
     
    print(nyears, nyears_pi, nyears_min)
  
    print(modelname)
    print('=======')
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears_min, plio_ts[0:nyears_min])
    print('pliocene drift = ' + np.str(np.around((slope * 100.),2)) 
          + 'dec C / centuary')
    
    
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears_min, pi_ts[0:nyears_min])
    print('pi drift = ' + np.str(np.around((slope * 100.), 2))
          + 'dec C / centuary')
    
    anomaly = plio_ts[0:nyears_min] - pi_ts[0:nyears_min]
    allyears = np.arange(0.0, nyears_min, 1.0)
    
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears, anomaly)
    print('anomaly drift = ' +  np.str(np.around((slope * 100.),2) )
              + 'dec C / centuary')


##########################################################
# main program

filename =  ' '
linux_win =  'l'
modelname = "CESM2" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"
            }

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"
            }

fieldname = {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']

fieldnamein = 'tas'
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


if (modelname in ('IPSLCM5A', 'IPSLCM5A2', 'CCSM4-Utr') 
    and fieldnamein == 'tos'):
        filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
if modelname in ('IPSLCM6A', 'GISS2.1G'):
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/'

# call program to get model dependent names
# fielduse,  and  filename and process for eoi400
fielduse, filename = getnames(modelname, filestart, fieldnamein, 'Eoi400')
fieldnameout = fieldname.get(fieldnamein)
exptnameout = exptname.get('Eoi400')
plio_timeseries = get_timeseries_data(fieldnamein, 'Eoi400')

# call program to get model dependent names
# fielduse,  and  filename and process for e280
fielduse, filename = getnames(modelname, filestart, fieldnamein, 'E280')
fieldnameout = fieldname.get(fieldnamein)
exptnameout = exptname.get('E280')
pi_timeseries = get_timeseries_data(fieldnamein, 'E280')

checkdrift(plio_timeseries, pi_timeseries)




::::::::::::::
CEMAC/PLIOMIP2/means_all_models_pre_hadgem.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import iris.util
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

import sys


def plotmean_newaxis(cube, modelno_):
     tempcube=iris.util.new_axis(cube)
     tempcube.add_dim_coord(iris.coords.DimCoord(modelno_, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
     return tempcube
 

def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
         
    newcube = plotmean_newaxis(cube, [levelno])
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    
        
    return newcube
###########################################
def get_NH_mean(modelname, expt, field):
     """
     gets the mean of the NH for model - modelname
                             and expt - experiment name (ie PI)
     returns a numpy array of length 12 with the average for each month
     """
     # read data into iris cube
     filename = (FILESTART + modelname + 
                  '/' + expt + '.' + field + 
                  '.mean_month.nc')
     
     if (modelname == 'GISS2.1G' or modelname == 'IPSLCM6A'
         or (modelname == 'IPSLCM5A' and field == 'SST')
         or (modelname == 'IPSLCM5A2' and field == 'SST')
         or (modelname == 'NorESM-L' and field == 'SST')
         or (modelname == 'NorESM1-F' and field == 'SST')):
          cubes = iris.load(filename)
          cube = cubes[0]
     else:
          cube = iris.load_cube(filename, field)
        
     # get weights and average over NH
     cube.coord('latitude').guess_bounds()
     cube.coord('longitude').guess_bounds()
     grid_areas = iris.analysis.cartography.area_weights(cube)
     grid_areas_nh = np.zeros(grid_areas.shape)
     for j, lat in enumerate(cube.coord('latitude').points):
          if lat > 0:
               grid_areas_nh[:, j, :] = grid_areas[:, j, :]

     cube_nh = (cube.collapsed(['longitude', 'latitude'],
                iris.analysis.MEAN, weights = grid_areas_nh))
     
     return cube_nh.data
 
def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART[:-10] + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    
    # means are at the start of the file
    allanoms_list = []
    for i in range(1, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break    # we have now got all the means
        modname, eoi400, e280, anom = line.split(',')
        if modname == 'MEAN':
            meananom=anom
        else:
            allanoms_list.append(anom)
            
    # find line which contains 'jan feb mar ' which is the start of the seasonal cycle
    string = 'jan feb mar'
    min_seas_cyc = np.zeros(12) + 1000.
    max_seas_cyc = np.zeros(12) - 1000.
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        modname, anomstr = line.split(',')
        anom_arr = np.array(anomstr.strip('[]').split(), dtype=float)
        if modname == 'MEAN':
            mean_seas_cyc = anom_arr
        else:
            for j in range(0,12):
                min_seas_cyc[j] = np.min([anom_arr[j], min_seas_cyc[j]]) 
                max_seas_cyc[j] = np.max([anom_arr[j], max_seas_cyc[j]]) 
    

    allanoms = np.asarray(allanoms_list, dtype=float)
    
        
    return allanoms, min_seas_cyc, max_seas_cyc, mean_seas_cyc
  
   

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units,individual_plot):
   
    
    
    names = {"EOI400" : "Plio_Core",
             "E280" : "PI_Ctrl"
            }
    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }
    
    # set up lists to store all values for both experiment and control    
    model_global_mean=[]
    model_global_sd=[]
    
    # store months in a numpy array (model, experiment,month)
    monmeans=np.zeros((len(modelnames),2,12))   
    monsd=np.zeros((len(modelnames),2,12))
    
    # store latitudes in a numpy array (model, experiment,latitude)
    latmeans=np.zeros((len(modelnames),2,180))   
    latsd=np.zeros((len(modelnames),2,180))
    lats=np.zeros(180)
     
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        filenames=[]
        filenames.append(FILESTART+modeluse+'/'+exptname+'.'+field+'.data.txt')
        filenames.append(FILESTART+modeluse+'/'+cntlname+'.'+field+'.data.txt')
       
        # set up temporary lists to store data from each experiment
        means=[]
        sds=[]
        nmon=12
       
        for fileno in range(0,len(filenames)):
            
            f=open(filenames[fileno],"r")
            f1=f.readlines()
            f2 = [x.replace('\n', '') for x in f1]
            
            # get the means according to their position in the file
            all_mean_sd=f2[2]
            all_mon_mean_sd=f2[5:5+12]
            all_lat_mean_sd=f2[20:20+180]
           
            # extract global mean
            mean,sd=all_mean_sd.split(',')
            means.append(mean)
            sds.append(sd)
            
            # extract monthly means 
            for x in all_mon_mean_sd:
                mon,mean,sd=x.split(',')
                monmeans[modelno,fileno,int(mon)-1]=float(mean)
                monsd[modelno,fileno,int(mon)-1]=float(sd)
            
            
            # extract latitude means
            for x in all_lat_mean_sd:
                lat,mean,sd=x.split(',')
                latss=int(float(lat)+89.5) # convert latitude to a subscript
               
                if mean != ' --' and mean != '--':
                    latmeans[modelno,fileno,latss]=float(mean) # stores latitudinal means
                else:
                    latmeans[modelno,fileno,latss]=np.nan
                if sd != ' --' and sd != '--' :
                    latsd[modelno,fileno,latss]=float(sd)
                else:
                    latsd[modelno,fileno,latss]=np.nan
                lats[latss]=lat # stores latitudes
            
            
        model_global_mean.append(means)
        model_global_sd.append(sds)
           
    ############################################################
    # get the monthly means for the NH
    # 
    # store months in a numpy array (model, experiment,month)
    # ss 1 is experiment ss 2 is control

    monmeans_NH = np.zeros((len(modelnames),2,12))   
    monsd = np.zeros((len(modelnames),2,12))

    for i, model in enumerate(modelnames):
         expt_mon_mean = get_NH_mean(model, exptname, field)
         monmeans_NH[i, 0, :] = expt_mon_mean

         cntl_mon_mean = get_NH_mean(model, cntlname, field)
         monmeans_NH[i, 1, :] = cntl_mon_mean

    
    #===============================================================
    # if pliomip1 is set get pliomip1 data
    if PLIOMIP1 == 'y':
        (mean_pliomip1, min_seas_pliomip1,
         max_seas_pliomip1, mean_seas_pliomip1) = get_pliomip1_data(field)
    
    #############################################################
    # plot the global mean and error bars from each model.
    
   
    expt_global_mean=[float(item[0]) for item in model_global_mean]
    expt_global_2sigma=([float(item[0])*2.0 for item in model_global_sd])

    cntl_global_mean=[float(item[1]) for item in model_global_mean]
    cntl_global_2sigma=([float(item[1])*2.0 for item in model_global_sd])
  
    
   
   
    #fig,ax=plt.subplots(2,1,1)
    ax=plt.subplot(2,1,1)
   
    ax.errorbar(modelnames,expt_global_mean,
                yerr=expt_global_2sigma,fmt='x',label=names.get(exptname))
    ax.errorbar(modelnames,cntl_global_mean,
                yerr=cntl_global_2sigma,fmt='x',label=names.get(cntlname))
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.15*box.height), box.width * 0.8, box.height])
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    titlename='Global mean ' + namefield.get(field)
    plt.title(titlename)
    ax.tick_params(axis='x',labelbottom='False')
    plt.ylabel(units)
    
    
    
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)],'x')
    print('means',[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    anomalies = [x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)]
    print('multimodelmean = ',
          np.mean(expt_global_mean) - np.mean(cntl_global_mean), np.mean(anomalies))
    print('multimodelmedian = ', np.median(anomalies))
    print('percentiles 10/50/90',np.percentile(anomalies, 10),
          np.percentile(anomalies,50), np.percentile(anomalies,90))
    sys.exit(0)
    
    sorted_anomalies = np.sort(anomalies)
    print(sorted_anomalies)
    #print('means % change',[((x1 - x2) *100 / x2) for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    #print('multimodelmean = ',(np.mean(expt_global_mean) - np.mean(cntl_global_mean)) *100. / np.mean(cntl_global_mean))
     # if pliomip1 is set overplot pliomip1 means as grey horizontal bars
    if (PLIOMIP1 == 'y' and field !='SST'):
        for mean_mod in mean_pliomip1:
            ax.axhline(y=mean_mod, color='grey', alpha=0.4)
    # Shrink axis as appropriate
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, box.height*0.8])
    plt.title('Global mean ' + namefield.get(field) + ' anomaly')
    plt.ylabel(units)
    plt.xticks(rotation='90', fontsize=8)
    #plt.xticks(x, labels, rotation='vertical')
   

    fileout=FILESTART+'allplots/'+field+'/global_means.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/global_means.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out data
    
    txtfile1 = open(FILEOUT,"w+")
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_5a \n")
        
    txtfile1.write('modelname, Pliocore_global_mean, pliocore_global_mean_2sigma, ' + 
                   'picntl_global_mean, picntl_global_mean_2sigma \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + np.str(np.around(expt_global_mean[i],2)) + 
                       ',' + np.str(np.around(expt_global_2sigma[i],2)) + 
                       ',' + np.str(np.around(cntl_global_mean[i],2)) + 
                       ',' + np.str(np.around(cntl_global_2sigma[i],2)) + '\n'))
        
    txtfile1.write('\n')
    
      
    
    ################################################################
    #  plot the NH seasonal cycle from each model
   

    plt.subplot(2,1,1)
    for i in range(0,len(monmeans_NH[:,0,0])):
        # plot experiment data
       
        plt.plot(monmeans_NH[i,0,:],color='r')
        # plot control data
        plt.plot(monmeans_NH[i,1,:],color='b')
        plt.title('NH annual cycle of '+field)
        plt.ylabel(units)
        
    plt.subplot(2,1,2)
    plt.plot(np.mean(monmeans_NH[:,0,:],axis=0),label=exptname,color='r')
    plt.plot(np.mean(monmeans_NH[:,1,:],axis=0),label=cntlname,color='b')
    plt.ylabel(units)
    plt.xlabel('month')    
    plt.legend()
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.pdf'
    plt.savefig(fileout)
    plt.close
    
    
    ax=plt.subplot(1,1,1)
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    for i in range(0,len(monmeans_NH[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
           ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i])
        else:
            ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i],
                    linestyle='dashed')
        
        
    ax.plot(labels,np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0),
        color='black',linestyle='dashed',linewidth=2,label='avg')
    
    print('monthmeans',np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0))
   
    # plot pliomip1 data if appropriate
    if PLIOMIP1 == 'y':
        ax.plot(labels, mean_seas_pliomip1, color='black', linestyle='dotted',
                linewidth=2, label='PlioMIP1')
        ax.fill_between(labels, min_seas_pliomip1, max_seas_pliomip1, alpha=0.2, 
                        color="grey")
        
    
    plt.title(exptname+'-'+cntlname+': NH '+ field + ' anomaly')
    plt.ylabel(units)
    #plt.xlabel('month') 
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' NH anomaly')
    
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_3a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_6a \n")
       
        
    txtfile1.write('modelname, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + 
                        np.str(np.around((monmeans_NH[i,0,0] - monmeans_NH[i,1,0]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,1] - monmeans_NH[i,1,1]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,2] - monmeans_NH[i,1,2]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,3] - monmeans_NH[i,1,3]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,4] - monmeans_NH[i,1,4]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,5] - monmeans_NH[i,1,5]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,6] - monmeans_NH[i,1,6]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,7] - monmeans_NH[i,1,7]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,8] - monmeans_NH[i,1,8]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,9] - monmeans_NH[i,1,9]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,10] - monmeans_NH[i,1,10]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,11] - monmeans_NH[i,1,11]),2)) + '\n'))
        
    txtfile1.write('\n')
    txtfile1.close

    ###################################################################
    # plot the latitudinal range from each model
    
    # won't print this out as it doesn't look very useful.
    # absolute value of temperature by latitude
    #plt.subplot(2,1,1)
    #for i in range(0,len(latmeans[:,0,0])):
    #    # plot experiment data
    #   
    #    plt.plot(latmeans[i,0,:],lats,color='r')
    #    # plot control data
    #    plt.plot(latmeans[i,1,:],lats,color='b')
    #    plt.title('latitudinal average of '+field)
    #    plt.xlabel(units)
    #    
    #plt.subplot(2,1,2)
    #plt.plot(np.mean(latmeans[:,0,:],axis=0),lats,label=exptname,color='r')
    #plt.plot(np.mean(latmeans[:,1,:],axis=0),lats,label=cntlname,color='b')
    #plt.xlabel(units)
    #plt.ylabel('latitude')    
    #plt.legend()
    
    
    ax=plt.subplot(1,1,1)
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    for i in range(0,len(latmeans[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i])
        else:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i],
                    linestyle='dashed')
    ax.plot(np.mean(latmeans[:,0,:],axis=0)-np.mean(latmeans[:,1,:],axis=0),
             lats,label='avg',color='black',linestyle='dashed',linewidth=2)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' anomaly')
    plt.xlabel(units)
    plt.ylabel('latitude') 
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
  
     # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1c \n")
    elif field == 'SST':
        txtfile1.write("zonal mean data for SST \n")
   
   
        for i, model in enumerate(modelnames):
            txtfile1.write(model + '\n')
            txtfile1.write('latitude, Tanom \n')
            for j, lat in enumerate(lats):
                txtfile1.write(np.str(np.around(lat,2)) + ',' + 
                               np.str(np.around(latmeans[i,0,j] - latmeans[i,1,j],2)) + '\n')
        
        txtfile1.write('\n')
        
    txtfile1.close
   
    #########################################################
    # plot a picture of the monthly anomaly from each model
    
    
    if field=='SurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='NearSurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='SST':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -5.
            datamax = 32.
            dataincr = 2.
    if field=='TotalPrecipitation':
            valmin=-1.4
            valmax=1.6
            incr=0.2
            cmapname='RdBu'
            
            datamin = 0.
            datamax = 5.
            dataincr = 0.1
    Vanom=np.arange(valmin,valmax,incr)
    Vdata= np.arange(datamin, datamax, dataincr)
    
   
    
    #########################################################
    # plot a picture of the change from each model and the 
    # multimodelmean
    
    
    anom_cubes=iris.cube.CubeList([])
    expt_cubes=iris.cube.CubeList([])
    cntl_cubes=iris.cube.CubeList([])
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        
        exptcube=iris.load_cube(exptfile)
        cntlcube=iris.load_cube(cntlfile)
       
       
        if modeluse == 'EC-Earth3.1' and field == 'SST':
           cntlcube.coord('latitude').bounds = None
           cntlcube.coord('longitude').bounds = None
           
        if modeluse == 'CCSM4-UoT' and field == 'TotalPrecipitation':
           cntlcube.coord('latitude').var_name = 'latitude'
           cntlcube.coord('longitude').var_name = 'longitude'
           exptcube.coord('latitude').var_name = 'latitude'
           exptcube.coord('longitude').var_name = 'longitude'
           cntlcube.coord('latitude').long_name = None
           cntlcube.coord('longitude').long_name = None
           exptcube.coord('latitude').long_name = None
           exptcube.coord('longitude').long_name = None
           cntlcube.coord('latitude').points = cntlcube.coord('latitude').points.astype('float32')
           cntlcube.coord('longitude').points = cntlcube.coord('longitude').points.astype('float32')
           exptcube.coord('latitude').points =  exptcube.coord('latitude').points .astype('float32')
           exptcube.coord('longitude').points = exptcube.coord('longitude').points.astype('float32')
        
        diffcube=exptcube-cntlcube
        
        # check float 32 for concatenation
        diffcube.data=diffcube.data.astype('float32') 
        exptcube.data=exptcube.data.astype('float32') 
        cntlcube.data=cntlcube.data.astype('float32') 
        if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                diffcube.units='Celsius'
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                diffcube.convert_units('Celsius')
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')
        
        # remove scalar coordinates so that we can concatenate
        # also add a new axis with the model number
        
        newcube = resort_coords(diffcube,modelno)
        newdata = newcube.data
        anom_cubes.append(newcube)    
    
        newcube = resort_coords(exptcube,modelno)
        newcube.rename(field)
        print(modeluse)
        print('MINIMUM',np.min(newcube.data))
        expt_cubes.append(newcube)
        
        newcube = resort_coords(cntlcube,modelno)
        newcube.rename(field)
        cntl_cubes.append(newcube)          
         
        
        # plot individual values if required
        if individual_plot=='y':
            plt.subplot(1,2,1)
            cs=iplt.contourf(exptcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+exptname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
        
            plt.subplot(1,2,2)
            cs=iplt.contourf(cntlcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
       
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.pdf'
            plt.savefig(fileout)
            plt.close()
        
            # plot a picture of the anomaly from each of the models
            plt.subplot(1,1,1)
            
            
            cs=iplt.contourf(exptcube-cntlcube,Vanom,extend='both',cmap=cmapname)
            titlename=modeluse+' '+exptname+'-'+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.pdf'
            plt.savefig(fileout)
            plt.close()
        
   
       
    #############################    
    # get the multi-modelmean and standard deviation
    
   
    iris.experimental.equalise_cubes.equalise_attributes(expt_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_cubes)
    allpliocube = expt_cubes.concatenate_cube()
   
    meanplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanplio.rename(field + 'mean_mPWP')
    maxplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxplio.rename(field + 'max_mPWP')
    minplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MIN)
    minplio.rename(field + 'min_mPWP')
    stdplio = allpliocube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdplio.rename(field + 'std_mPWP')
    
    allpicube = cntl_cubes.concatenate_cube()
    meanpi = allpicube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpi.rename(field + 'mean_pi')
    maxpi = allpicube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxpi.rename(field + 'max_pi')
    minpi = allpicube.collapsed(['model_level_number'], iris.analysis.MIN)
    minpi.rename(field + 'min_pi')
    stdpi = allpicube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdpi.rename(field + 'std_pi')
    
    allmeancube=anom_cubes.concatenate_cube() # all model mean anomalies
    meancube=allmeancube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meancube.rename(field + 'mean_anomaly')
    maxcube=allmeancube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxcube.rename(field + 'max_anomaly')
    mincube=allmeancube.collapsed(['model_level_number'], iris.analysis.MIN)
    mincube.rename(field + 'min_anomaly')
    stdcube=allmeancube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdcube.rename(field + 'anomaly_multimodel_stddev')
    
    ###############################################
    # write out the mean and standard deviation to a netcdf file
    
    cubelist = iris.cube.CubeList([meanplio, stdplio, maxplio, minplio,
                                   meanpi, stdpi, maxpi, minpi,
                                   meancube, stdcube, maxcube, mincube])
    fileout = (FILESTART + field + '_multimodelmean.nc')
    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    
    
    ###########################
    # plot the mean value and standard deviation
    
   
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(valmin,valmax,incr)
    mycmap = plt.cm.get_cmap(cmapname,len(V+2))
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    if linux_win == 'l':
        exptlsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        exptlsm = FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.figtext(0.02, 0.97,'b)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(namefield.get(field) +' anomaly: multimodel mean')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.pdf')
    plt.savefig(fileout)
    plt.close()
    
    if field=='NearSurfaceTemperature':
        V=np.arange(0,5.0,incr)
        textout = 'd)'
    if field=='TotalPrecipitation':
        textout = 'c)'
        V=np.arange(0,1.3, 0.1)
    ax = plt.axes(projection = ccrs.PlateCarree())
    qplt.contourf(stdcube, V,extend='both',cmap='plasma')
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.title(namefield.get(field) +' anomaly:Standard Deviation')
    plt.figtext(0.02, 0.97,textout,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.pdf')
    plt.savefig(fileout)
    plt.close()
     

    cubelist = iris.cube.CubeList([meancube, stdcube])
    iris.save(cubelist,FILEOUTNC,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
        
        
    ##########################################################
    # plot the monthly anomaly from each model
    if linux_win=='l' and individual_plot=='y':
        for modelno in range(0,len(modelnames)):
            modeluse=modelnames[modelno]
            exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.mean_month.nc'
            cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.mean_month.nc'
            exptcube=iris.load_cube(exptfile)
            cntlcube=iris.load_cube(cntlfile)
     
            for mon in range(0,12):
                anom=exptcube.data[mon,:,:]-cntlcube.data[mon,:,:]
                lat=exptcube.coord('latitude').points
                lon=exptcube.coord('longitude').points
                lons,lats=np.meshgrid(lon,lat)
                map=Basemap(llcrnrlon=0.0,urcrnrlon=360.0,
                         llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',
                         resolution='c')
                x, y = map(lons, lats)
                map.drawcoastlines()
                V=np.arange(valmin,valmax,incr)
                cs = map.contourf(x,y,anom,V,cmap=cmapname,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
                plt.title(modeluse+':'+'month is '+str(mon+1))
                cbar.set_label(units)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.eps')
                plt.savefig(fileout)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.pdf')
                plt.savefig(fileout)
                plt.close()
          
        
  


##########################################################
# main program
        
filename=' '
linux_win='l'

modelnames=['CESM2', 'HadGEM3','IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]

#modelnames = ['CCSM4-Utr']
            
PLIOMIP1 = 'n'
#modelnames=['COSMOS',
#            'CCSM4-UoT']   

fieldnames=['TotalPrecipitation']
units=['mm/day']
#fieldnames=['NearSurfaceTemperature']
#units=['degC']
#fieldnames=['SST']
#units=['degC']
exptname='EOI400'
cntlname='E280'
individual_plot='n' # do you want to plot the anomalies for all of the individual models

if linux_win=='w':
   FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded/'
else:
   FILESTART='/nfs/hera1/earjcti/regridded/'

FILEOUT = FILESTART + 'dummy.txt'
FILEOUTNC = FILESTART + 'dummy.nc'

for field in range(0,len(fieldnames)):

    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'alldata/data_for_5a_6a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_5b_5c.nc'
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'alldata/data_for_1a_1c_3a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_1b_1d.nc'
    if fieldnames[field] == 'SST':
        FILEOUT = FILESTART + 'alldata/zonal_mean_SST_data.txt'
  
   
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field],individual_plot)

#sys.exit(0)
\
::::::::::::::
CEMAC/PLIOMIP2/means_all_models.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import iris.util
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

import sys


def plotmean_newaxis(cube, modelno_):
     tempcube=iris.util.new_axis(cube)
     tempcube.add_dim_coord(iris.coords.DimCoord(modelno_, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
     return tempcube
 

def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
         
    newcube = plotmean_newaxis(cube, [levelno])
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    
        
    return newcube
###########################################
def get_NH_mean(modelname, expt, field):
     """
     gets the mean of the NH for model - modelname
                             and expt - experiment name (ie PI)
     returns a numpy array of length 12 with the average for each month
     """
     # read data into iris cube
     filename = (FILESTART + modelname + 
                  '/' + expt + '.' + field + 
                  '.mean_month.nc')
     
     if (modelname == 'GISS2.1G' or modelname == 'IPSLCM6A'
         or (modelname == 'IPSLCM5A' and field == 'SST')
         or (modelname == 'IPSLCM5A2' and field == 'SST')
         or (modelname == 'NorESM-L' and field == 'SST')
         or (modelname == 'NorESM1-F' and field == 'SST')):
          cubes = iris.load(filename)
          cube = cubes[0]
     else:
          print(filename,field)
          cube = iris.load_cube(filename, field)
        
     # get weights and average over NH
     cube.coord('latitude').guess_bounds()
     cube.coord('longitude').guess_bounds()
     grid_areas = iris.analysis.cartography.area_weights(cube)
     grid_areas_nh = np.zeros(grid_areas.shape)
     for j, lat in enumerate(cube.coord('latitude').points):
          if lat > 0:
               grid_areas_nh[:, j, :] = grid_areas[:, j, :]

     cube_nh = (cube.collapsed(['longitude', 'latitude'],
                iris.analysis.MEAN, weights = grid_areas_nh))
     
     return cube_nh.data
 
def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART[:-10] + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    print('julia',PLIOMIP1_FILE)
    sys.exit(0)
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    
    # means are at the start of the file
    allanoms_list = []
    for i in range(1, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break    # we have now got all the means
        modname, eoi400, e280, anom = line.split(',')
        if modname == 'MEAN':
            meananom=anom
        else:
            allanoms_list.append(anom)
            
    # find line which contains 'jan feb mar ' which is the start of the seasonal cycle
    string = 'jan feb mar'
    min_seas_cyc = np.zeros(12) + 1000.
    max_seas_cyc = np.zeros(12) - 1000.
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        modname, anomstr = line.split(',')
        anom_arr = np.array(anomstr.strip('[]').split(), dtype=float)
        if modname == 'MEAN':
            mean_seas_cyc = anom_arr
        else:
            for j in range(0,12):
                min_seas_cyc[j] = np.min([anom_arr[j], min_seas_cyc[j]]) 
                max_seas_cyc[j] = np.max([anom_arr[j], max_seas_cyc[j]]) 
    

    allanoms = np.asarray(allanoms_list, dtype=float)
    
        
    return allanoms, min_seas_cyc, max_seas_cyc, mean_seas_cyc
  
   

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units,individual_plot):
   
    
    
    names = {"EOI400" : "Plio_Core",
             "E280" : "PI_Ctrl"
            }
    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }
    
    # set up lists to store all values for both experiment and control    
    model_global_mean=[]
    model_global_sd=[]
    
    # store months in a numpy array (model, experiment,month)
    monmeans=np.zeros((len(modelnames),2,12))   
    monsd=np.zeros((len(modelnames),2,12))
    
    # store latitudes in a numpy array (model, experiment,latitude)
    latmeans=np.zeros((len(modelnames),2,180))   
    latsd=np.zeros((len(modelnames),2,180))
    lats=np.zeros(180)
     
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        print(modeluse)
        filenames=[]
        filenames.append(FILESTART+modeluse+'/'+exptname+'.'+field+'.data.txt')
        filenames.append(FILESTART+modeluse+'/'+cntlname+'.'+field+'.data.txt')
       
        # set up temporary lists to store data from each experiment
        means=[]
        sds=[]
        nmon=12
       
        for fileno in range(0,len(filenames)):
            
            f=open(filenames[fileno],"r")
            f1=f.readlines()
            f2 = [x.replace('\n', '') for x in f1]
            
            # get the means according to their position in the file
            all_mean_sd=f2[2]
            all_mon_mean_sd=f2[5:5+12]
            all_lat_mean_sd=f2[20:20+180]
           
            # extract global mean
            mean,sd=all_mean_sd.split(',')
            means.append(mean)
            sds.append(sd)
            
            # extract monthly means 
            for x in all_mon_mean_sd:
                mon,mean,sd=x.split(',')
                monmeans[modelno,fileno,int(mon)-1]=float(mean)
                monsd[modelno,fileno,int(mon)-1]=float(sd)
            
            
            # extract latitude means
            for x in all_lat_mean_sd:
                lat,mean,sd=x.split(',')
                latss=int(float(lat)+89.5) # convert latitude to a subscript
               
                if mean != ' --' and mean != '--':
                    latmeans[modelno,fileno,latss]=float(mean) # stores latitudinal means
                else:
                    latmeans[modelno,fileno,latss]=np.nan
                if sd != ' --' and sd != '--' :
                    latsd[modelno,fileno,latss]=float(sd)
                else:
                    latsd[modelno,fileno,latss]=np.nan
                lats[latss]=lat # stores latitudes
            
            
        model_global_mean.append(means)
        model_global_sd.append(sds)
           
    ############################################################
    # get the monthly means for the NH
    # 
    # store months in a numpy array (model, experiment,month)
    # ss 1 is experiment ss 2 is control

    monmeans_NH = np.zeros((len(modelnames),2,12))   
    monsd = np.zeros((len(modelnames),2,12))

    for i, model in enumerate(modelnames):
         expt_mon_mean = get_NH_mean(model, exptname, field)
         monmeans_NH[i, 0, :] = expt_mon_mean

         cntl_mon_mean = get_NH_mean(model, cntlname, field)
         monmeans_NH[i, 1, :] = cntl_mon_mean

    
    #===============================================================
    # if pliomip1 is set get pliomip1 data
    if PLIOMIP1 == 'y':
        (mean_pliomip1, min_seas_pliomip1,
         max_seas_pliomip1, mean_seas_pliomip1) = get_pliomip1_data(field)
    
    #############################################################
    # plot the global mean and error bars from each model.
    
   
    expt_global_mean=[float(item[0]) for item in model_global_mean]
    expt_global_2sigma=([float(item[0])*2.0 for item in model_global_sd])

    cntl_global_mean=[float(item[1]) for item in model_global_mean]
    cntl_global_2sigma=([float(item[1])*2.0 for item in model_global_sd])
  
    
   
   
    #fig,ax=plt.subplots(2,1,1)
    ax=plt.subplot(2,1,1)
   
    ax.errorbar(modelnames,expt_global_mean,
                yerr=expt_global_2sigma,fmt='x',label=names.get(exptname))
    ax.errorbar(modelnames,cntl_global_mean,
                yerr=cntl_global_2sigma,fmt='x',label=names.get(cntlname))
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.15*box.height), box.width * 0.8, box.height])
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    titlename='Global mean ' + namefield.get(field)
    plt.title(titlename)
    ax.tick_params(axis='x',labelbottom='False')
    plt.ylabel(units)
    
    
    
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)],'x')
    print('means',[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    anomalies = [x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)]
    print('multimodelmean = ',
          np.mean(expt_global_mean) - np.mean(cntl_global_mean), np.mean(anomalies))
    print('multimodelmedian = ', np.median(anomalies))
    print('percentiles 10/50/90',np.percentile(anomalies, 10),
          np.percentile(anomalies,50), np.percentile(anomalies,90))
    print('percentiles 5/95/90',np.percentile(anomalies, 5),
          np.percentile(anomalies,95))
      
    
    sorted_anomalies = np.sort(anomalies)
    print(sorted_anomalies)
    print('means % change',[((x1 - x2) *100 / x2) for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    print('multimodelmean = ',(np.mean(expt_global_mean) - np.mean(cntl_global_mean)) *100. / np.mean(cntl_global_mean))
     # if pliomip1 is set overplot pliomip1 means as grey horizontal bars
    if (PLIOMIP1 == 'y' and field !='SST'):
        for mean_mod in mean_pliomip1:
            ax.axhline(y=mean_mod, color='grey', alpha=0.4)
    # Shrink axis as appropriate
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, box.height*0.8])
    plt.title('Global mean ' + namefield.get(field) + ' anomaly')
    plt.ylabel(units)
    plt.xticks(rotation='90', fontsize=8)
    #plt.xticks(x, labels, rotation='vertical')
   

    fileout=FILESTART+'allplots/'+field+'/global_means.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/global_means.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out data
    
    txtfile1 = open(FILEOUT,"w+")
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_5a \n")
        
    txtfile1.write('modelname, Pliocore_global_mean, pliocore_global_mean_2sigma, ' + 
                   'picntl_global_mean, picntl_global_mean_2sigma \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + np.str(np.around(expt_global_mean[i],2)) + 
                       ',' + np.str(np.around(expt_global_2sigma[i],2)) + 
                       ',' + np.str(np.around(cntl_global_mean[i],2)) + 
                       ',' + np.str(np.around(cntl_global_2sigma[i],2)) + '\n'))
        
    txtfile1.write('\n')
    
      
    
    ################################################################
    #  plot the NH seasonal cycle from each model
   

    plt.subplot(2,1,1)
    for i in range(0,len(monmeans_NH[:,0,0])):
        # plot experiment data
       
        plt.plot(monmeans_NH[i,0,:],color='r')
        # plot control data
        plt.plot(monmeans_NH[i,1,:],color='b')
        plt.title('NH annual cycle of '+field)
        plt.ylabel(units)
        
    plt.subplot(2,1,2)
    plt.plot(np.mean(monmeans_NH[:,0,:],axis=0),label=exptname,color='r')
    plt.plot(np.mean(monmeans_NH[:,1,:],axis=0),label=cntlname,color='b')
    plt.ylabel(units)
    plt.xlabel('month')    
    plt.legend()
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.pdf'
    plt.savefig(fileout)
    plt.close
    
    
    ax=plt.subplot(1,1,1)
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    for i in range(0,len(monmeans_NH[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
           ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i])
        else:
            ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i],
                    linestyle='dashed')
        
        
    ax.plot(labels,np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0),
        color='black',linestyle='dashed',linewidth=2,label='avg')
    
    print('monthmeans',np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0))
   
    # plot pliomip1 data if appropriate
    if PLIOMIP1 == 'y':
        ax.plot(labels, mean_seas_pliomip1, color='black', linestyle='dotted',
                linewidth=2, label='PlioMIP1')
        ax.fill_between(labels, min_seas_pliomip1, max_seas_pliomip1, alpha=0.2, 
                        color="grey")
        
    
    plt.title(exptname+'-'+cntlname+': NH '+ field + ' anomaly')
    plt.ylabel(units)
    #plt.xlabel('month') 
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' NH anomaly')
    
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_3a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_6a \n")
       
        
    txtfile1.write('modelname, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + 
                        np.str(np.around((monmeans_NH[i,0,0] - monmeans_NH[i,1,0]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,1] - monmeans_NH[i,1,1]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,2] - monmeans_NH[i,1,2]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,3] - monmeans_NH[i,1,3]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,4] - monmeans_NH[i,1,4]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,5] - monmeans_NH[i,1,5]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,6] - monmeans_NH[i,1,6]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,7] - monmeans_NH[i,1,7]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,8] - monmeans_NH[i,1,8]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,9] - monmeans_NH[i,1,9]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,10] - monmeans_NH[i,1,10]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,11] - monmeans_NH[i,1,11]),2)) + '\n'))
        
    txtfile1.write('\n')
    txtfile1.close

    ###################################################################
    # plot the latitudinal range from each model
    
    # won't print this out as it doesn't look very useful.
    # absolute value of temperature by latitude
    #plt.subplot(2,1,1)
    #for i in range(0,len(latmeans[:,0,0])):
    #    # plot experiment data
    #   
    #    plt.plot(latmeans[i,0,:],lats,color='r')
    #    # plot control data
    #    plt.plot(latmeans[i,1,:],lats,color='b')
    #    plt.title('latitudinal average of '+field)
    #    plt.xlabel(units)
    #    
    #plt.subplot(2,1,2)
    #plt.plot(np.mean(latmeans[:,0,:],axis=0),lats,label=exptname,color='r')
    #plt.plot(np.mean(latmeans[:,1,:],axis=0),lats,label=cntlname,color='b')
    #plt.xlabel(units)
    #plt.ylabel('latitude')    
    #plt.legend()
    
    
    ax=plt.subplot(1,1,1)
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    for i in range(0,len(latmeans[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i])
        else:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i],
                    linestyle='dashed')
    ax.plot(np.mean(latmeans[:,0,:],axis=0)-np.mean(latmeans[:,1,:],axis=0),
             lats,label='avg',color='black',linestyle='dashed',linewidth=2)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' anomaly')
    plt.xlabel(units)
    plt.ylabel('latitude') 
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
  
     # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1c \n")
    elif field == 'SST':
        txtfile1.write("zonal mean data for SST \n")
   
   
        for i, model in enumerate(modelnames):
            txtfile1.write(model + '\n')
            txtfile1.write('latitude, Tanom \n')
            for j, lat in enumerate(lats):
                txtfile1.write(np.str(np.around(lat,2)) + ',' + 
                               np.str(np.around(latmeans[i,0,j] - latmeans[i,1,j],2)) + '\n')
        
        txtfile1.write('\n')
        
    txtfile1.close
   
    #########################################################
    # plot a picture of the monthly anomaly from each model
    
    
    if field=='SurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='NearSurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='SST':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -5.
            datamax = 32.
            dataincr = 2.
    if field=='TotalPrecipitation':
            valmin=-1.4
            valmax=1.6
            incr=0.2
            cmapname='RdBu'
            
            datamin = 0.
            datamax = 5.
            dataincr = 0.1
    Vanom=np.arange(valmin,valmax,incr)
    Vdata= np.arange(datamin, datamax, dataincr)
    
   
    
    #########################################################
    # plot a picture of the change from each model and the 
    # multimodelmean
    
    
    anom_cubes=iris.cube.CubeList([])
    expt_cubes=iris.cube.CubeList([])
    cntl_cubes=iris.cube.CubeList([])
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        
        exptcube=iris.load_cube(exptfile)
        cntlcube=iris.load_cube(cntlfile)
       
       
        if modeluse == 'EC-Earth3.1' and field == 'SST':
           cntlcube.coord('latitude').bounds = None
           cntlcube.coord('longitude').bounds = None
           
        if modeluse == 'CCSM4-UoT' and field == 'TotalPrecipitation':
           cntlcube.coord('latitude').var_name = 'latitude'
           cntlcube.coord('longitude').var_name = 'longitude'
           exptcube.coord('latitude').var_name = 'latitude'
           exptcube.coord('longitude').var_name = 'longitude'
           cntlcube.coord('latitude').long_name = None
           cntlcube.coord('longitude').long_name = None
           exptcube.coord('latitude').long_name = None
           exptcube.coord('longitude').long_name = None
           cntlcube.coord('latitude').points = cntlcube.coord('latitude').points.astype('float32')
           cntlcube.coord('longitude').points = cntlcube.coord('longitude').points.astype('float32')
           exptcube.coord('latitude').points =  exptcube.coord('latitude').points .astype('float32')
           exptcube.coord('longitude').points = exptcube.coord('longitude').points.astype('float32')
        
        diffcube=exptcube-cntlcube
        
        # check float 32 for concatenation
        diffcube.data=diffcube.data.astype('float32') 
        exptcube.data=exptcube.data.astype('float32') 
        cntlcube.data=cntlcube.data.astype('float32') 
        if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'HadGEM3')
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                diffcube.units='Celsius'
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                diffcube.convert_units('Celsius')
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')
        
        # remove scalar coordinates so that we can concatenate
        # also add a new axis with the model number
        
        newcube = resort_coords(diffcube,modelno)
        newdata = newcube.data
        anom_cubes.append(newcube)    
    
        newcube = resort_coords(exptcube,modelno)
        newcube.rename(field)
        print(modeluse)
        print('MINIMUM',np.min(newcube.data))
        expt_cubes.append(newcube)
        
        newcube = resort_coords(cntlcube,modelno)
        newcube.rename(field)
        cntl_cubes.append(newcube)          
         
        
        # plot individual values if required
        if individual_plot=='y':
            plt.subplot(1,2,1)
            cs=iplt.contourf(exptcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+exptname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
        
            plt.subplot(1,2,2)
            cs=iplt.contourf(cntlcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
       
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.pdf'
            plt.savefig(fileout)
            plt.close()
        
            # plot a picture of the anomaly from each of the models
            plt.subplot(1,1,1)
            
            
            cs=iplt.contourf(exptcube-cntlcube,Vanom,extend='both',cmap=cmapname)
            titlename=modeluse+' '+exptname+'-'+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.pdf'
            plt.savefig(fileout)
            plt.close()
        
   
       
    #############################    
    # get the multi-modelmean and standard deviation
    
   
    iris.experimental.equalise_cubes.equalise_attributes(expt_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_cubes)
    allpliocube = expt_cubes.concatenate_cube()
   
    meanplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanplio.rename(field + 'mean_mPWP')
    maxplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxplio.rename(field + 'max_mPWP')
    minplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MIN)
    minplio.rename(field + 'min_mPWP')
    stdplio = allpliocube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdplio.rename(field + 'std_mPWP')
    
    allpicube = cntl_cubes.concatenate_cube()
    meanpi = allpicube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpi.rename(field + 'mean_pi')
    maxpi = allpicube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxpi.rename(field + 'max_pi')
    minpi = allpicube.collapsed(['model_level_number'], iris.analysis.MIN)
    minpi.rename(field + 'min_pi')
    stdpi = allpicube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdpi.rename(field + 'std_pi')
    
    allmeancube=anom_cubes.concatenate_cube() # all model mean anomalies
    meancube=allmeancube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meancube.rename(field + 'mean_anomaly')
    maxcube=allmeancube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxcube.rename(field + 'max_anomaly')
    mincube=allmeancube.collapsed(['model_level_number'], iris.analysis.MIN)
    mincube.rename(field + 'min_anomaly')
    stdcube=allmeancube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdcube.rename(field + 'anomaly_multimodel_stddev')
    
    ###############################################
    # write out the mean and standard deviation to a netcdf file
    
    cubelist = iris.cube.CubeList([meanplio, stdplio, maxplio, minplio,
                                   meanpi, stdpi, maxpi, minpi,
                                   meancube, stdcube, maxcube, mincube])
    fileout = (FILESTART + field + '_multimodelmean.nc')
    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    
    
    ###########################
    # plot the mean value and standard deviation
    
   
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(valmin,valmax,incr)
    mycmap = plt.cm.get_cmap(cmapname,len(V+2))
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    if linux_win == 'l':
        exptlsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        exptlsm = FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.figtext(0.02, 0.97,'b)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(namefield.get(field) +' anomaly: multimodel mean')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.pdf')
    plt.savefig(fileout)
    plt.close()
    
    if field=='NearSurfaceTemperature':
        V=np.arange(0,5.0,incr)
        textout = 'd)'
    if field=='TotalPrecipitation':
        textout = 'c)'
        V=np.arange(0,1.3, 0.1)
    ax = plt.axes(projection = ccrs.PlateCarree())
    qplt.contourf(stdcube, V,extend='both',cmap='plasma')
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.title(namefield.get(field) +' anomaly:Standard Deviation')
    #plt.figtext(0.02, 0.97,textout,
    # horizontalalignment='left',
    # verticalalignment='top',
    # fontsize=20)
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.pdf')
    plt.savefig(fileout)
    plt.close()
     

    cubelist = iris.cube.CubeList([meancube, stdcube])
    iris.save(cubelist,FILEOUTNC,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
        
        
    ##########################################################
    # plot the monthly anomaly from each model
   
    expt_month_cubes=iris.cube.CubeList([])
    cntl_month_cubes=iris.cube.CubeList([])

    for modelno in range(0,len(modelnames)):
         modeluse=modelnames[modelno]
         exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.mean_month.nc'
         cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.mean_month.nc'

         cube=iris.load_cube(exptfile)
         cube.data=cube.data.astype('float32') 
         exptcube = resort_coords(cube,modelno)
         exptcube.rename(field)
        
         cube=iris.load_cube(cntlfile)
         cube.data=cube.data.astype('float32') 
         cntlcube = resort_coords(cube,modelno)
         cntlcube.rename(field)

         if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'HadGEM3')
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')

         expt_month_cubes.append(exptcube)
         cntl_month_cubes.append(cntlcube)

         if linux_win=='l' and individual_plot=='y':
   
            for mon in range(0,12):
                anom=exptcube.data[mon,:,:]-cntlcube.data[mon,:,:]
                lat=exptcube.coord('latitude').points
                lon=exptcube.coord('longitude').points
                lons,lats=np.meshgrid(lon,lat)
                map=Basemap(llcrnrlon=0.0,urcrnrlon=360.0,
                         llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',
                         resolution='c')
                x, y = map(lons, lats)
                map.drawcoastlines()
                V=np.arange(valmin,valmax,incr)
                cs = map.contourf(x,y,anom,V,cmap=cmapname,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
                plt.title(modeluse+':'+'month is '+str(mon+1))
                cbar.set_label(units)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.eps')
                plt.savefig(fileout)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.pdf')
                plt.savefig(fileout)
                plt.close()
        
    iris.experimental.equalise_cubes.equalise_attributes(expt_month_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_month_cubes)
     
    allpimonthcubes = cntl_month_cubes.concatenate_cube()
    meanpimonth = allpimonthcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpimonth.rename(field + 'mean_pi')
    meanpimonth.add_dim_coord(iris.coords.DimCoord(np.arange(1,13),
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                bounds = None,
                coord_system = None,  circular = False), 0)
   
  
    allpliomonthcubes = expt_month_cubes.concatenate_cube()
    meanpliomonth = allpliomonthcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpliomonth.rename(field + 'mean_plio')
    meanpliomonth.add_dim_coord(iris.coords.DimCoord(np.arange(1,13),
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                bounds = None,
                coord_system = None,  circular = False), 0)


    meananommonth = meanpliomonth - meanpimonth
    meananommonth.rename(field + 'plio - pi')
   
    cubelist = iris.cube.CubeList([meanpimonth, meanpliomonth, meananommonth])
    fileout = (FILESTART + field + '_multimodelmean_month.nc')

    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
  
    print('end of program')


##########################################################
# main program
        
filename=' '
linux_win='l'

modelnames=['CESM2', 'HadGEM3','IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]

#modelnames = ['CCSM4-Utr']
            
PLIOMIP1 = 'y'
#modelnames=['COSMOS',
#            'CCSM4-UoT']   

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
units=['degC']
#fieldnames=['SST']
#units=['degC']
exptname='EOI400'
cntlname='E280'
individual_plot='n' # do you want to plot the anomalies for all of the individual models

if linux_win=='w':
   FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded/'
else:
   FILESTART='/nfs/hera1/earjcti/regridded/'

FILEOUT = FILESTART + 'dummy.txt'
FILEOUTNC = FILESTART + 'dummy.nc'

for field in range(0,len(fieldnames)):

    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'alldata/data_for_5a_6a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_5b_5c.nc'
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'alldata/data_for_1a_1c_3a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_1b_1d.nc'
    if fieldnames[field] == 'SST':
        FILEOUT = FILESTART + 'alldata/zonal_mean_SST_data.txt'
  
   
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field],individual_plot)

#sys.exit(0)
\
::::::::::::::
CEMAC/PLIOMIP2/model_rankings.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on October 2019


#@aauthor: earjcti
"""
This program will plot model diagnostics against model information
the information it will use is
1. year of first use
2. IPCC report where it was first used
3. resolution
"""


import numpy as np
import scipy as sp
import iris
import matplotlib.pyplot as plt



#####################################
def plot_regress(field1, field2, cluster, xtitle, ytitle, fileout, xmin, xmax):
    """
    plots the regression of the field vs the ranking
    """
    # plot the climate sensitivity vs the global mean

   
    ax = plt.subplot(111)
    for i, fielduse in enumerate(field2):
        model = MODELNAMES[i]
        print('j1',i,'j2', field1[i], 'j3',fielduse,'j4', MODELNAMES[i])
        if i % 4 == 0: # i divides 4 with no remainder
            ax.scatter(field1[i], fielduse, label = model) 
        elif i % 4 == 1 :
            ax.scatter(field1[i], fielduse, label = model, marker='^') 
        elif i % 4 == 2 :
            ax.scatter(field1[i], fielduse, label = model, marker='<') 
        else:
            ax.scatter(field1[i], fielduse, label = model, marker='v') 
   
    plt.xlabel(xtitle)
    plt.ylabel(ytitle)
    #plt.legend()

    plt.xlim(xmin, xmax)


    slope, intercept, r_value, p_value, std_err = (
        sp.stats.linregress(field1, field2))

    xarray = np.arange(xmin, xmax, 1)
    yarray = intercept+(slope*xarray)
    #ax.plot(xarray,yarray)
    rsq_val_small = np.str(np.around(r_value **2, 2))
    p_value_small = np.str(np.around(p_value, 2))

    if FIELDNAME == 'NearSurfaceTemperature':
       if xtitle == 'year':
           subplotno = 'a'
       else:
           subplotno = 'a'
    if FIELDNAME == 'TotalPrecipitation':
       if xtitle == 'year':
           subplotno = 'b'
       else:
           subplotno = 'b'
    plt.title((subplotno + ") R-squared: "  + rsq_val_small + 
              "  p-value: " + p_value_small))
              

    box = ax.get_position()
    ax.set_position([box.x0, box.y0,
                     box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))

    #plt.show()
    plt.savefig(fileout+'.eps')
    plt.savefig(fileout+'.pdf')
    plt.close()


def get_model_years():
    """
    get the year when the model was first used from a dictionary
    """

    year = {'NorESM-L': 2011,
            'NorESM1-F':2017,
            'IPSLCM6A': 2018,
            'IPSLCM5A2':2017,
            'IPSLCM5A':2010,
            'HadCM3': 1997,
            'MIROC4m':2004,
            'COSMOS':2009,
            'CCSM4-UoT':2011,
            'EC-Earth3.3':2019,
            'MRI2.3': 2006, # from my investigation
            'CCSM4-Utr': 2011,
            'GISS2.1G': 2019,
            'CESM1.2' :2013,
            'CESM2': 2019,
            'CCSM4' :2011
            }

    allyears = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        allyears[i] = year.get(model)

    return allyears

def get_resolution():
    """
    get the resolution of the model
    """

    xres_a = {'NorESM-L': 3.75,
              'NorESM1-F':2.5,
              'IPSLCM6A': 2.5,
              'IPSLCM5A2':3.75,
              'IPSLCM5A':3.75,
              'HadCM3': 3.75,
              'MIROC4m':2.8,
              'COSMOS':3.75,
              'CCSM4-UoT':1.25,
              'EC-Earth3.3':1.125,
              'MRI-CGCM2.3':2.8, # from my investigation
              'CESM4-Utr': 2.5,
              'CESM2' : 1.25,
              'GISS': 2.5,
              'CESM1.2' :1.25,
              'CCSM4' :1.25
              }

    yres_a = {'NorESM-L': 3.75,
              'NorESM1-F':1.9,
              'IPSLCM6A': 1.26,
              'IPSLCM5A2':1.9,
              'IPSLCM5A':1.9,
              'HadCM3': 2.5,
              'MIROC4m':2.8,
              'COSMOS':3.75,
              'CCSM4-UoT':0.9,
              'EC-Earth3.3':1.125,
              'MRI-CGCM2.3':2.8, # from my investigation
              'CESM-Utr': 1.9,
              'CESM2' : 0.95,
              'GISS': 2.0,
              'CESM1.2' :0.9,
              'CCSM4' :0.9
              }

    horiz_boxes = {'NorESM-L': 4608,
                   'NorESM1-F':13824,
                   'IPSLCM6A': 20592,
                   'IPSLCM5A2':9216,
                   'IPSLCM5A':9216,
                   'HadCM3': 7008,
                   'MIROC4m':8192,
                   'COSMOS':4608,
                   'CCSM4-UoT':55296,
                   'EC-Earth3.3':51200,
                   'MRI2.3':8192, # from my investigation
                   'CCSM4-Utr': 13824,
                   'GISS2.1G': 12960,
                   'CESM1.2' :55296,
                   'CESM2' : 55296,
                   'CCSM4' :55296
                   }

    horiz_gb_atm = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        horiz_gb_atm[i] = horiz_boxes.get(model)

    return horiz_gb_atm

def get_cluster():
    """
    this will put models of the same family into a cluster
    """

    year ={'NorESM-L': 1,
           'NorESM1-F': 1,
           'IPSLCM6A': 2,
           'IPSLCM5A2':2,
           'IPSLCM5A':2,
           'HadCM3': 0,
           'MIROC4m': 0,
           'COSMOS': 0,
           'CCSM4-UoT':3,
           'EC-Earth3.3': 0,
           'MRI2.3': 0, # from my investigation
           'CCSM4-Utr': 3,
           'GISS2.1G': 0,
           'CESM1.2' : 3,
           'CESM2' : 3, 
           'CCSM4' : 3
           }

    allyears = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        allyears[i] = year.get(model)

    return allyears

def get_model_info():
    """
    gets information about the model
    returns modelyears
            cluster this will group models of the same family
    """
    years = get_model_years()
    resolution = get_resolution()
    cluster = get_cluster()

    return [years, resolution, cluster]

def read_means_file(filename):
    """
    read all the data from the file containing the means
    returns the mean
    """

    file1 = open(filename, "r")
    lines = list(file1)

    meanval, sdval = lines[2].split(",")

    meanint = np.float(meanval)

    return meanint

def get_anomaly():
    """
    gets the anomaly in the field for each of the models
    """

    climdiff = np.zeros(len(MODELNAMES))

    for i, model in enumerate(MODELNAMES):

        print(FIELDNAME)
        print(model)
        meanexpt = read_means_file(FILESTART + model + '/EOI400.' +
                                   FIELDNAME + '.data.txt')
        meancntl = read_means_file(FILESTART + model + '/E280.' +
                                   FIELDNAME + '.data.txt')

        climdiff[i] = meanexpt - meancntl


    return climdiff



def main():
    """
    1. get model information.  For example the year of the model
    2. second the anomaly in the field
    3. plot
    """
    modelyears, model_res, modelcluster = get_model_info()

    fieldanom = get_anomaly()

    # regress anomaly against model year
    plot_regress(modelyears, fieldanom, modelcluster, 'year',
                 FIELDNAME + ' anom',
                 FILESTART + '/allplots/' + FIELDNAME +
                 '/' + FIELDNAME + '_anom_vs_modelyear' + SUBSCRIPT,
                 1990., 2020.)

    # regress anomaly against model resolution
    plot_regress(model_res, fieldanom, modelcluster, 'horizontal gridboxes',
                 FIELDNAME + ' anom',
                 FILESTART + '/allplots/' + FIELDNAME +
                 '/' + FIELDNAME + '_anom_vs_nbox_atm' + SUBSCRIPT,
                 5000., 60000.)



##########################################################
# main program

LINUX_WIN = 'l'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/regridded/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]
SUBSCRIPT = ''

#MODELNAMES = ['CCSM4-UoT',
#              'CCSM4',
#              'CESM1.2',
#              'IPSLCM6A',
#              'IPSLCM5A2',
#              'IPSLCM5A',
#              'NorESM-L',
#              'NorESM1-F',
#              'COSMOS',
#              'GISS',
#              'HadCM3',
#              'MIROC4m',
#              'MRI-CGCM2.3'
#            ]
#SUBSCRIPT = '_redu'

FIELDNAME = 'NearSurfaceTemperature'
#FIELDNAME = 'TotalPrecipitation'


main()
::::::::::::::
CEMAC/PLIOMIP2/noregrid_ocn_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019

#based on regrid_ocn_50yr_avg - but will calculate the means without regridding

#
#

import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    print('julia',fielduse)
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    print(cube.data)
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm_400(fieldname):
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename, fieldname)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    print(allcube)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def getmeans_data(fieldnamein, exptnamein):
    """
    gets the means from  the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A_origgrid'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        print('before',filename)
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F' and exptnamein == 'E400'):
        cube100 = get_noresm_400(fielduse)
    else:
        cube100 = iris.load_cube(filename)

   
     

    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = cube.coord('time').points
        newpoints = origpoints/24.
        cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A' or modelname == 'HadGEM3'
             or modelname  == 'IPSLCM5A2'
             or modelname  == 'IPSLCM6A_origgrid'
             or modelname == 'CCSM4-Utr' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            cube.data = cube.data * 60. *60. *24.
            cube.name = 'Total precipitation'
            cube.long_name = 'Total precipitation'
            cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4'):
        print('cube.units',cube.units)
        print('j1',cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            cube.data = cube.data * 60. *60. *24. *1000.
            print('j2',cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',cube.data[:,0])
            cube.name = 'Total precipitation'
            cube.long_name = 'Total precipitation'
            cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A_origgrid' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A_origgrid' or 
        modelname  == 'EC-Earth3.1'):
          cube.coord('time').units = refdate


       
    print(cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(cube,  'time',  name = 'year')
    
    # correct the start month if required
    cube=correct_start_month(cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  cube.extract(iris.Constraint(month = 1))
    feb_slice  =  cube.extract(iris.Constraint(month = 2))
    mar_slice  =  cube.extract(iris.Constraint(month = 3))
    apr_slice  =  cube.extract(iris.Constraint(month = 4))
    may_slice  =  cube.extract(iris.Constraint(month = 5))
    jun_slice  =  cube.extract(iris.Constraint(month = 6))
    jul_slice  =  cube.extract(iris.Constraint(month = 7))
    aug_slice  =  cube.extract(iris.Constraint(month = 8))
    sep_slice  =  cube.extract(iris.Constraint(month = 9))
    oct_slice  =  cube.extract(iris.Constraint(month = 10))
    nov_slice  =  cube.extract(iris.Constraint(month = 11))
    dec_slice  =  cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the new dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                    "E400": "b.e21.B1850.f09_g17.CMIP6-piControl.400.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : ".110001-120012",
                  "E400" : ".0801.0900",
                  "Eoi400" : ".110001.120012"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        if linux_win  == 'l':
#            filename = filestart + 'UofT/'
#            filename = (filename + 'UofT-CCSM4/for_julia/' + 
#                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
            filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        
        
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
        if exptnamein == 'E400':
            filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_TREFHT_PRECT_month.nc')
            fielduse = NorESM_FIELDS.get(fieldnamein + 'E400')
            if fieldnamein == 'tos':
                filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_SST_month.nc')
          
            

    if modelname  == 'IPSLCM6A_origgrid':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+'IPSLCM6A/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(exptnamein, filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+'IPSLCM6A/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            if exptnamein == 'Eoi400' or exptnamein == 'E400':
                filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename = [filename1, filename2]
                fielduse = ['PRECC', 'PRECL']
            if exptnamein == 'E280':
                filename = (filestart + 'NCAR/b.e21.B1850.f09_g17.' + 
                            'CMIP6-piControl.001.cam.h0.PRECT.110001-120012.nc')
                fielduse = 'PRECT'
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            print(exptnamein)
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
        if fieldnamein =='totcloud':
            filestart='/nfs/hera1/earjcti/PLIOMIP2/CESM2/clt_Amon_CESM2_'
            fielduse = 'clt'
            if exptnamein == 'Eoi400':
                filename = (filestart + 'midPliocene-eoi400_r1i1p1f1_'+
                            'gn_015101-020012.nc')
            if exptnamein == 'E280':
                filename = (filestart +'piControl_r1i1p1f1_gn_090001-099912.nc')
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "IPSLCM6A_origgrid" 

                   # note that if you change the names of any of the models
                   # listed below make sure you carry it through to the program

                   # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A_origgrid GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

fieldnamein = ['pr']
#fieldnamein = ['tos'] # ocean tempeature or sst
exptnamein = ['E280']

#fieldnamein = ['tos']
#exptnamein = ['Eoi400', 'E280','E560']
#exptnamein = ['E400']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A_origgrid' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
        print(fielduse,filename)
       

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
#        sys.exit(0)
        getmeans_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/PLIOVAR_gradients.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the SST meridional gradient temperature change
vs global SSTtemperature change for each of the PlioMIP models

changed on October 17th to add the data to the figure

"""

import sys
import iris
import iris.quickplot as qplt
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


class GetProxy:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = FILESTARTP + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = FILESTARTP +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = FILESTARTP + 'pliovar_metadata_global_02102019.csv'
        self.pifile = FILESTARTP + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        # put the temperature anomalies into latitude bounds
        self.boundmin = -90. + (np.arange(0, 12) * 15.)
        self.boundmax = -75. + (np.arange(0, 12) * 15)
       
        boundtemp, nval = self.put_data_to_bounds(self.sitetemp.values - self.temppi)
        boundtemp_bs, nval_bs = self.put_data_to_bounds(self.sitetemp_bs.values - self.temppi)
       

        
        return boundtemp, boundtemp_bs, self.boundmin, self.boundmax, nval, nval_bs
    
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
    
    
    def put_data_to_bounds(self, tanom):
       """
       we now have the longitude, latitude and temperature of each datapoint
       we now put them into 15deg latitude bounded regions (defined by self.bounds)
       and find the average temperature in each region
       also return the number of points in each region
       """  
      
       boundtemp = np.zeros(12)
       count_boundtemp = np.zeros(12)
      
       for i in range(0, self.nsites):    
           # if temperature is a number add the temperature to the 
           # bound region
         
           if np.isfinite(tanom[i]):
               for bound in range(0,12):
              
                   if ((self.boundmin[bound] < self.lat[i]) &
                       (self.lat[i] <= self.boundmax[bound])):
            
                           boundtemp[bound] = (boundtemp[bound] + tanom[i])
                           count_boundtemp[bound] = count_boundtemp[bound] + 1
                           
                   
       # get average
      
       boundtemp = boundtemp / count_boundtemp
       
       print('bound temp',boundtemp)
       print('nbound',count_boundtemp)
       print(self.boundmin)
      
      
       return boundtemp, count_boundtemp
       
        
    
# end of class   
 
######################################################################
def combine_mgca_uk37(temp_uk37, temp_mgca, n_uk37, n_mgca,
                     boundmin_uk37, boundmax_uk37, boundmin_mgca, boundmax_mgca):
    """
    we have lots of arrays which show average temperature (temp_????) in a bounded
    box (boundmin_???? - boundmax_????).  We also have the number of points that make
    up the average temperature.
    
    We would like to average these depending on how many points are in the box
    
    returns: an array which contains the average combined temperature of uk37 and mg/ca
    """
    
    nvals = len(boundmin_uk37)
    temp_combined = np.zeros(nvals)
        
    for i in range(0, nvals):
        j = np.where(boundmin_mgca == boundmin_uk37[i])
        
        print(i, n_uk37[i], n_mgca[i])
        temp_combined[i] = (((np.nan_to_num(temp_uk37[i]) * n_uk37[i]) +
                            (np.nan_to_num(temp_mgca[j]) * n_mgca[j])) / 
                            (n_uk37[i] + n_mgca[j]))
    
    return temp_combined
 
#####################################################
def weightdata(tanom, lowerbounds, upperbounds):
        """
        we have some proxy data.  We would like to weight it to find the
        average value in each region.  
        Currently the proxy data is in latitudinal bands
        we want to find the global average and the (30S-30N) - (60N-75N)
        gradients
        
        input: temperature anomaly data, 
        lowervalue of the latituinal bound
        uppervalue of the latitudinal bound
        
        returns: merid_dy (30S-30N) - (60N-75N) - weighted by ocean area
                 glob_avg (average of (60S-75N) - weighted by ocean area)
        """
       
        # get land sea mask from pi model
        lsmcube = iris.load_cube(FILESTART + 'HadCM3/E280.SST.allmean.nc')
        lsmcube.data = lsmcube.data / lsmcube.data
        # get weights and multiply by lsm
        lsmcube.coord('latitude').guess_bounds()
        lsmcube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(lsmcube)
        lsmcube.data = lsmcube.data * grid_areas
       
        
        # find weights area between each lowerbound and upper bound
        weights = np.zeros(len(tanom))
        for i, lat in enumerate(lsmcube.coord('latitude').points):
            latcube = lsmcube.extract(iris.Constraint(latitude=lat))
            latdata = np.sum(latcube.data)
            for bound in range(0, len(lowerbounds)):
                if lowerbounds[bound] < lat <=upperbounds[bound]:
                   weights[bound] = weights[bound] + latdata
        
        # global average = temperautre in each bound * weights / total of weights

        
        globavg = np.nansum(weights * tanom) / np.nansum(weights)
        print('weights and tanom')
        print('weights',weights)
        print('tanom',tanom)
        print('upperbound',upperbounds)
        
        
        # get average in each bound
        
        tropavg_deep = bound_avg(-15., 15., lowerbounds, upperbounds, weights, tanom)
        tropavg = bound_avg(-30., 30., lowerbounds, upperbounds, weights, tanom)
        tropavg_nh = bound_avg(0., 30.,lowerbounds, upperbounds, weights, tanom)
        highlatavg = bound_avg(60., 75.,lowerbounds, upperbounds, weights, tanom)
        
        # get average polewards of 45degrees
        weightcount_midhighlat = 0.
        midhighlatavg = 0.
        for bound in range(0, len(lowerbounds)):
            # do poleward of 45 deg
            if ((lowerbounds[bound] >=45. or upperbounds[bound] <=-45.)
                 and (np.isfinite(tanom[bound]))):
                midhighlatavg = midhighlatavg + (weights[bound] * tanom[bound])
                weightcount_midhighlat = weightcount_midhighlat + weights[bound]
        
        midhighlatavg = midhighlatavg / weightcount_midhighlat
        
        # get gradients
        merid_dy = tropavg - highlatavg
        
        merid_dy_nh = tropavg_nh - highlatavg
        
        merid_dy_45 = tropavg_deep - midhighlatavg
        
    
        return merid_dy, merid_dy_nh, merid_dy_45, globavg
    
def bound_avg(minval, maxval, lowerbounds, upperbounds, weights, tanom):
    """
    averages the temperature over the range minval, maxval
    returns:  weighted averaged temperautre
    """
    
    avgval = 0.
    weightcount = 0.
      
    for bound in range(0, len(lowerbounds)):
        if lowerbounds[bound] >= minval and upperbounds[bound] <= maxval:
            avgval = avgval + (weights[bound] * tanom[bound]) 
            weightcount = weightcount + weights[bound]
    avgval = avgval / weightcount
    
    return avgval
            
############################################################
def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if field == 'SST' or field == 'NearSurfaceTemperature':
        if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'
                or modeluse == 'CESM1.0.5'):
            cube.units = 'Celsius'
        else:
            cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    if modeluse == 'EC-Earth3.1' and field == 'SST':
        cube.coord('latitude').bounds = None
        cube.coord('longitude').bounds = None

    cube.cell_methods = None

    return cube

def globmean(cube):
    """
    returns the global mean value of a SST cube (single value)
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    return tempcube.data

def getgradient(cube):
    """
    gets the gradient in the cube.  This is the average SST 60-75N
    minus the average SST equatorward of 30N
    input : cube
    output : gradient_et (30N-30S) - (60N-75N)
             gradient_na (30N-30S) - (60N-75N)(290E-5E)
             gradient_na_nh (30N-0S) - (60N-75N)(290E-5E)
    """

    grid_areas = iris.analysis.cartography.area_weights(cube)
    grid_areas_tropics = np.zeros(np.shape(grid_areas))
    grid_areas_deeptropics = np.zeros(np.shape(grid_areas))
    grid_areas_tropics_nh = np.zeros(np.shape(grid_areas))
    grid_areas_et = np.zeros(np.shape(grid_areas)) # 60N-75N
    grid_areas_na = np.zeros(np.shape(grid_areas)) # 60N - 75N 290E-360E
    grid_areas_45 = np.zeros(np.shape(grid_areas))

    nlat = len(cube.coord('latitude').points)
    nlon = len(cube.coord('longitude').points)
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points

    for j in range(0, nlat):
        if ((lats[j] >= 60.) and (lats[j] <= 75)):
            grid_areas_et[j, :] = grid_areas[j, :]
            for i in range(0, nlon):
                if (lons[i] >= 290 or lons[i]<=5.):
                    grid_areas_na[j, i] = grid_areas[j, i]
                else:
                    grid_areas_na[j, i] = 0.0
        else:
            grid_areas_et[j, :] = 0.0
            grid_areas_na[j, :] = 0.0

        if ((lats[j] <= 30.) and (lats[j] >= -30.)):
            grid_areas_tropics[j, :] = grid_areas[j, :]
        else:
            grid_areas_tropics[j, :] = 0.0
            
        if ((lats[j] <= 15.) and (lats[j] >= -15.)):
            grid_areas_deeptropics[j, :] = grid_areas[j, :]
        else:
            grid_areas_deeptropics[j, :] = 0.0
            
        if ((lats[j] >= 45.) or (lats[j] <= -45.)):
            grid_areas_45[j, :] = grid_areas[j, :]
        else:
            grid_areas_45[j, :] = 0.0
            
        if ((lats[j] <= 30.) and (lats[j] >= 0.)):
            grid_areas_tropics_nh[j, :] = grid_areas[j, :]
        else:
            grid_areas_tropics_nh[j, :] = 0.0

    temptrop = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_tropics)
    temptropdeep = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_deeptropics)
    temptrop_nh = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_tropics_nh)
    tempet = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_et)
    temp45 = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_45)
    tempna = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_na)

    gradient_et = temptrop.data - tempet.data 
    gradient_na = temptrop.data - tempna.data 
    gradient_nh = temptrop_nh.data - tempna.data
    gradient_deep_extra = temptropdeep.data - temp45.data
    
    print(np.sum(grid_areas))  
    print(np.sum(grid_areas_tropics)/np.sum(grid_areas))  
    print(np.sum(grid_areas_deeptropics)/np.sum(grid_areas))  
    print(np.sum(grid_areas_et)/np.sum(grid_areas))  
    print(np.sum(grid_areas_na)/np.sum(grid_areas)) 
    print(np.sum(grid_areas_45)/np.sum(grid_areas)) 
   
    
    return gradient_et, gradient_na, gradient_nh, gradient_deep_extra

def get_model_data(modelname):
    """
    1. gets the pliocene and the preindustrial SST data for each file
    2. calculates the global SSTA
    3. calculates the gradient as (SST polewards of 60deg) - (SST equatorward of 30deg)
    4. calculates the mPWP - PI gradient (gradient produced in 3.)

    input modelname
    output modTanom = the global mPWP-PI SSTA
           mod_gradanom = the mPWP (meridional SST gradient) minus the PI (meridional SST gradient)

    """

    #1. get data
    cubepi = get_data(FILESTART + modelname + '/E280.' + FIELDNAME + '.allmean.nc',
                      FIELDNAME, modelname)
    cubeplio = get_data(FILESTART + modelname + '/EOI400.' + FIELDNAME + '.allmean.nc',
                        FIELDNAME, modelname)
    #2 global mean anomaly
    meanpi = globmean(cubepi)
    meanplio = globmean(cubeplio)
    tempSSTA = meanplio - meanpi

    # calculate gradient
    gradpi_et, gradpi_na, gradpi_nh, gradpi_15NS_45NS = getgradient(cubepi)
    gradplio_et, gradplio_na, gradplio_nh, gradplio_15NS_45NS = getgradient(cubeplio)
    gradSSTA_et = gradplio_et - gradpi_et
    gradSSTA_na = gradplio_na - gradpi_na
    gradSSTA_nh = gradplio_nh - gradpi_nh
    gradSSTA_15NS_45NS = gradplio_15NS_45NS - gradpi_15NS_45NS

    return tempSSTA, gradSSTA_et, gradSSTA_na, gradSSTA_nh, gradSSTA_15NS_45NS

def plotdata(global_data, merid_grad_data, Tanom_model, gradanom_model, fileout):
    """
    this will plot the data and the model output
    we are plotting the global temerature anomaly vs the meridional gradient
    temperautre anomaly for both model and data
    """
    
    plt.scatter(global_data, merid_grad_data,
                label='proxy data', color='black',
                marker = 's')
    
    for i, model in enumerate(MODELNAMES):
         # add this to scatterplot
        if i < 6:
            plt.scatter(Tanom_model[i], gradanom_model[i], label=model)
        else:
            plt.scatter(Tanom_model[i], gradanom_model[i], label=model, marker='^')

    plt.xlabel('global mean SSTA')
    plt.ylabel('change in meridional gradient')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.tight_layout()

   
    plt.savefig(fileout + 'pdf')
    plt.savefig(fileout + 'png')
    plt.close()
    
    # write data to textfile
    txtout = open(fileout + 'txt', 'w+')

    txtout.write('model, global_mean, merid_gradient \n')
    txtout.write('proxy_data,' + 
                 np.str(np.around(global_data, 2)) + ',' + 
                 np.str(np.around(merid_grad_data,2)) + '\n')
    for i, model in enumerate(MODELNAMES):
        txtout.write(model + ',' + 
                     np.str(np.around(Tanom_model[i],2)) + ',' + 
                     np.str(np.around(gradanom_model[i],2)) + '\n')
    txtout.write('multimodelmean,' + 
                     np.str(np.around(np.mean(Tanom_model),2)) + ',' + 
                     np.str(np.around(np.mean(gradanom_model),2)) + '\n')
    txtout.close


def main():
    """
    1. Call a program that will get the data to plot
    2. Weight the proxy data by area of each latitude banc
    3. Get the output (global SST, meridional gradient SST anomaly)
       for each of the models
    4. Plot the data on a symplot
    """

    # 1. get data
    obj = GetProxy('t1', 'UK37') # get data for t1 timeslice
    (t1_temp, t1_temp_bs, 
     boundmin, boundmax,
     nval, nval_bs) = obj.get_proxydata()
    
    
    if MG_CA == 'y': # also get Mg/Ca data
        obj = GetProxy('t1', 'MGCA') # get data for t1 timeslice
        (t1_temp_mgca, t1_temp_bs_mgca, 
         boundmin_mgca, boundmax_mgca,
         nval_mgca, nval_bs_mgca)  = obj.get_proxydata()
       
        print('doing t1_comb')
        t1_comb = (combine_mgca_uk37(t1_temp, t1_temp_mgca, nval, nval_mgca,
                                     boundmin, boundmax, boundmin_mgca, boundmax_mgca))
        print('doing t1_comb_bs')
        t1_comb_bs = (combine_mgca_uk37(t1_temp_bs, t1_temp_bs_mgca, nval_bs, nval_bs_mgca,
                                     boundmin, boundmax, boundmin_mgca, boundmax_mgca))
        
        t1_temp = t1_comb
        t1_temp_bs = t1_comb_bs
        print('about to exit')
        
     
    
    # 2. weight proxydata (standard and bayspline)
    
    (merid_grad, merid_grad_nh, 
     merid_grad_15_45, global_ssta) = weightdata(t1_temp, boundmin, boundmax)
    
    (merid_grad_bs, merid_grad_nh_bs, 
     merid_grad_15_45_bs, global_ssta_bs) = weightdata(t1_temp_bs, boundmin, boundmax)
   
    
    # 3. get model results 
    Tanom = np.zeros(len(MODELNAMES))
    gradanom_et = np.zeros(len(MODELNAMES))
    gradanom_na = np.zeros(len(MODELNAMES))
    gradanom_na_nh = np.zeros(len(MODELNAMES))
    gradanom_15_45 = np.zeros(len(MODELNAMES))
    
    for i, model in enumerate(MODELNAMES):
        (Tanom[i], gradanom_et[i], 
         gradanom_na[i], 
         gradanom_na_nh[i],
         gradanom_15_45[i]) = get_model_data(model)
   
    #4. plot standard (gradient 30N-30S - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '.'
    plotdata(global_ssta, merid_grad, Tanom, gradanom_na, fileout)
    
    #plot standard (gradient 30N-0N - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_nh.'
    plotdata(global_ssta, merid_grad_nh, Tanom, gradanom_na_nh, fileout)
    
    #plot Bayspline (gradient 30N-30S - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_bayspline.'
    plotdata(global_ssta_bs, merid_grad_bs, Tanom, gradanom_na, fileout)
    
    
    #plot Bayspline (gradient 30N-0N - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_bayspline_nh.'
    plotdata(global_ssta_bs, merid_grad_nh_bs, Tanom, gradanom_na_nh, fileout)
    
    # plot original (gradient 15N-15S, 45N/S-90N/S)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_15NS_45NS-90NS.'
    plotdata(global_ssta, merid_grad_15_45, Tanom, gradanom_15_45, fileout)
    
    # plot Bayspline (gradient 15N-15S, 45N/S-90N/S)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_15NS_45NS-90NS_bayspline.'
    plotdata(global_ssta_bs, merid_grad_15_45_bs, Tanom, gradanom_15_45, fileout)



    return

    

# variable definition
LINUX_WIN = 'l'
FIELDNAME = 'SST'

if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    FILESTARTP = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/' # where proxy data is
else:
    FILESTART = 'C:/Users/julia/OneDrive/WORK/DATA/regridded/'
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    FILESTARTP = 'C:/Users/julia/OneDrive/WORK/DATA/proxydata/'

UNITS = 'deg C'
TIMEPERIODS = ['pi', 'mPWP']
MODELNAMES = ['CCSM4', 'CCSM4-UoT',
              'CCSM4-Utr',  
              'CESM1.2','CESM2',
              'COSMOS', 'EC-Earth3.3', 
              'GISS2.1G', 'HadCM3',
              'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
              'MIROC4m', 'MRI2.3',
              'NorESM-L', 'NorESM1-F'
             ]


MG_CA = 'y'
if MG_CA == 'y':
    MGCASS = '_mgca'
else:
    MGCASS = ''

main()
::::::::::::::
CEMAC/PLIOMIP2/plot_individual_models_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models 
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#import Basemap
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys


def getmeanfield(fieldname, period):
    """
    get the mean values from the mean value file 
    
    inputs: fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
            period (probably mPWP PI or anomaly)

    returns the mean value from the multuimodelmean.nc file
    """
    
    meanfile = ('/nfs/hera1/earjcti/regridded/' + fieldname + 
                '_multimodelmean.nc')
    meanfield = fieldname + 'mean_' + period
    
    cube = iris.load_cube(meanfile, meanfield)

    return cube


def getmodelfield(modelname, fieldname, period):
     """
     get the mean values from the model data
     inputs: modelname (ie HadCM3)
             fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
             period (likely EOI400 or E280)
     returns:  a cube contatining the mean data from the model
     
     """
     
     modfile = ('/nfs/hera1/earjcti/regridded/' + modelname + '/' + 
                period + '.' + fieldname + '.allmean.nc')
    
     tempcube = iris.load(modfile)
     cube = tempcube[0]
     cube.units = UNITS
     
     return cube

class Plotalldata:   
    def __init(self):
        self.data = []
        
        
    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
              modelnames : the names of all the models
        """
        self.nmodels = len(modelnames)
        self.filestart = ('/nfs/hera1/earjcti/regridded/allplots/' +
                          fieldname + '/individual')
    
        for i in range(0, self.nmodels):
            
            cubedata = anom_cubes[i].data
            self.latitudes = anom_cubes[i].coord('latitude').points
            lon = anom_cubes[i].coord('longitude').points
            self.datatoplot, self.longitudes = (shiftgrid(180.,
                                                          cubedata,
                                                          lon,
                                                          start=False))
            self.model = modelnames[i]
            self.plotmap(i)
        

        return      
    
    def plotmap(self, i):
        """
        will plot the data in a map format
        
        """
        
        plotpos = np.mod(i, 8) + 1
        plt.subplot(3, 3, plotpos)
        lons, lats = np.meshgrid(self.longitudes, self.latitudes)
        
        map=Basemap(llcrnrlon=-180.0, urcrnrlon=180.0, 
                    llcrnrlat=-90.0, urcrnrlat=90.0, 
                    projection='cyl',resolution='l')
   
        map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines()
    
        V = np.arange(-5.0, 6.0, 1.0)    
        cs = map.contourf(x, y, self.datatoplot, V, cmap='RdBu_r',
                          extend='both')
        plt.title(self.model)
        
        print(i,self.nmodels)
        if plotpos == 8 or (i + 1) == self.nmodels:
            plt.subplot(3, 3, 9)
            plt.gca().set_visible(False)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.set_label(UNITS)
            
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                        + '.eps')
            plt.savefig(fileout, bbox_inches='tight')
            
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                    + '.png')
            
            plt.savefig(fileout, bbox_inches='tight')
        
        
##########################################################
# main program
# set up variable information

fieldname='NearSurfaceTemperature'
filename=' '
linux_win='l'


modelnames=['CESM1.0.5','COSMOS','EC-Earth3.1','GISS','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI-CGCM2.3',
            'NorESM-L','NorESM1-F',
            'UofT',
            ]

#modelnames=['GISS']
UNITS = 'Celsius'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']

# set up cubelists to store data
mpwp_anom_cubes=iris.cube.CubeList([])
pi_anom_cubes=iris.cube.CubeList([])
anom_anom_cubes=iris.cube.CubeList([])


#################################################
# get mean data
mean_plio_cube = getmeanfield(fieldname, 'mPWP')
mean_pi_cube = getmeanfield(fieldname, 'pi')
mean_anom_cube = getmeanfield(fieldname, 'anomaly')


for model in range(0,len(modelnames)):
    model_plio_cube = getmodelfield(modelnames[model], fieldname, 'EOI400')
    model_pi_cube = getmodelfield(modelnames[model], fieldname, 'E280')
    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_anom_cubes.append(model_plio_cube - mean_plio_cube)
    pi_anom_cubes.append(model_pi_cube - mean_pi_cube)
    anom_anom_cubes.append(model_anom_cube - model_pi_cube)
    
##################################################
# plot the cubes for the model anomalies relative to the mean

obj = Plotalldata()
obj.plotdata(fieldname, mpwp_anom_cubes, modelnames)

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/plot_individual_models.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import sys
import numpy as np
#import matplotlib as mp
import matplotlib.pyplot as plt
#from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#import netCDF4
#from netCDF4 import Dataset, MFDataset
import iris
import iris.analysis.cartography
import iris.coord_categorisation

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def getmeanfield(period):
    """
    get the mean values from the mean value file

    inputs: fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
            period (probably mPWP PI or anomaly)

    returns the mean value from the multuimodelmean.nc file
    """

    meanfile = (FILESTART + 'regridded/' + FIELDNAME +
                '_multimodelmean.nc')
    meanfield = FIELDNAME + 'mean_' + period

    cube = iris.load_cube(meanfile, meanfield)


    return cube


def getmodelfield(modelname, period):
    """
    get the mean values from the model data
    inputs: modelname (ie HadCM3)
            period (likely EOI400 or E280)
    returns:  a cube contatining the mean data from the model

    """

    modfile = (FILESTART + 'regridded/' + modelname + '/' +
               period + '.' + FIELDNAME + '.allmean.nc')

    tempcube = iris.load(modfile)
    cube = tempcube[0]
    cube.units = UNITS

    #this will make all the dimensions of all the cubes match.


    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube


class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, timeperiod, anom_cubes):
        self.nmodels = len(MODELNAMES)
        self.filestart = (FILESTART + '/regridded/allplots/' +
                          FIELDNAME + '/' + timeperiod + '_individual')
        self.timeperiod = timeperiod
        self.anom_cubes = anom_cubes

        if (FIELDNAME == 'NearSurfaceTemperature'
            or FIELDNAME == 'SST'):
                self.valmin = -5.
                self.valmax = 6.
                self.diff = 1.
                self.colormap = 'RdBu_r'

        if FIELDNAME == 'TotalPrecipitation':
            self.valmin = -2.
            self.valmax = 2.1
            self.diff = 0.2
            self.colormap = 'RdBu'


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        fig = plt.figure(figsize=(11.0, 8.5))
        for i in range(0, self.nmodels):

            cubedata = self.anom_cubes[i].data
            latitudes = self.anom_cubes[i].coord('latitude').points
            lon = self.anom_cubes[i].coord('longitude').points
            datatoplot, longitudes = (shiftgrid(180., cubedata,
                                                lon, start=False))
            #if (np.mod(i, 8) + 1) == 1:
            #    title_ = (MODELNAMES[i] + ':' +
            #              self.timeperiod + ' (model - MMM)')
            #else:
            #    title_ = (MODELNAMES[i])

            title_ = (MODELNAMES[i])
            self.plotmap(i, title_,
                         datatoplot, longitudes, latitudes, fig)


        return

    def plotmap(self, i, titlename, datatoplot, longitudes, latitudes, fig):
        """
        will plot the data in a map format

        """

        xplot = 4
        yplot = 4


        plotpos = np.mod(i, xplot * yplot) + 1
        plt.subplot(xplot, yplot, plotpos)
        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=-90.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)

        V = np.arange(self.valmin, self.valmax, self.diff)
        cs = map.contourf(x, y, datatoplot, V, cmap=self.colormap,
                          extend='both')
        plt.title(titlename)


        if plotpos == (xplot * yplot) or (i + 1) == self.nmodels:
             # Shrink current axis by 20% and put a legend to the right
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.82, top=0.9,
                                wspace=0.1, hspace=0.0)

            cb_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])
           
            cbar = fig.colorbar(cs, cax=cb_ax, orientation='vertical')
            #cbar = plt.colorbar(fig, orientation='horizontal')
            #fig.colorbar(fix, ax=axs[:, col], shrink=0.6)
            cbar.ax.tick_params(labelsize=15)
            cbar.set_label(UNITS, fontsize=15)
            print('plotted colorbar')
            #plt.show()
            #plt.tight_layout()
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.eps')
            plt.savefig(fileout, bbox_inches='tight')

            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.pdf')

            plt.savefig(fileout, bbox_inches='tight')
            plt.close()


##########################################################
# main program
# set up variable information

#FIELDNAME = 'NearSurfaceTemperature'
#UNITS = 'Celsius'
FIELDNAME = 'SST'
UNITS = 'Celsius'
#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'
#FIELDNAME = 'SST'
LINUX_WIN = 'l'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
             ]

#MODELNAMES = ['NorESM-L']



# set up cubelists to store data
mpwp_anom_cubes = iris.cube.CubeList([])
pi_anom_cubes = iris.cube.CubeList([])
anom_anom_cubes = iris.cube.CubeList([])


#################################################
# get mean data
mean_plio_cube = getmeanfield('mPWP')
mean_pi_cube = getmeanfield('pi')
mean_anom_cube = getmeanfield('anomaly')


for model, modelname in enumerate(MODELNAMES):
    model_plio_cube = getmodelfield(modelname, 'EOI400')
    model_pi_cube = getmodelfield(modelname, 'E280')
    print(modelname)
    if modelname == 'EC-Earth3.1' and FIELDNAME == 'SST':
       model_pi_cube.coord('latitude').bounds = None
       model_pi_cube.coord('longitude').bounds = None

    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_anom_cubes.append(model_plio_cube - mean_plio_cube)
    pi_anom_cubes.append(model_pi_cube - mean_pi_cube)
    anom_anom_cubes.append(model_anom_cube - mean_anom_cube)

##################################################
# plot the cubes for the model anomalies relative to the mean

obj = Plotalldata('mPWP', mpwp_anom_cubes)
obj.plotdata()

obj = Plotalldata('PI', pi_anom_cubes)
obj.plotdata()

obj = Plotalldata('mPWP-PI', anom_anom_cubes)
obj.plotdata()

#
::::::::::::::
CEMAC/PLIOMIP2/precip_plot_for_IPCC_with_data.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap


def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """
    colors = [(84, 48, 5), (113, 70, 16), (143, 93, 27), (173, 115, 38),
              (195, 137, 60), (206, 160, 97), (216, 182, 135),
              (227, 204, 173), (238, 226, 211), (248, 248, 247),
              (212, 230, 229), (176, 212, 209), (140, 194, 190),
              (103, 176, 170), (67, 158, 150), (44, 135, 127),
              (29, 110, 100), (14, 85, 74), (0, 60, 48)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube


def get_model_data():
    """
    read in precipitation data and return
    """

    (lsm_cube, lsm_plio_cube) = get_lsm()

    precip_cube = iris.load_cube(PRECIP_MMM_FILE, 
                                 'TotalPrecipitationmean_anomaly')
  
   
    return precip_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns precipitation
    """

    dfs = pd.read_excel(PRECIP_DATAFILE)
    sites = []
    lats = []
    lons = []
    precip = []
   
    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        precip_file = dfs.iloc[rl,11]
        if precip_file == '1000**':
            precip_file = 1000.
        print(rl, precip_file)
        if np.isfinite(precip_file):
           sites.append(dfs.iloc[rl, 0])
           lats.append(dfs.iloc[rl, 2])
           lons.append(dfs.iloc[rl, 3])
           precip.append(precip_file / 365.) # mm/year to mm/day

    return lats, lons, precip

def get_cru_precip(lats, lons):
    """
    get's the cru precip at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUPRECIP/' + 
               'E280.TotalPrecipitation.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_precip = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_precip[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_precip[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_precip[i] = np.nanmean(surround)

        # convert from mm/month to mm/day
        cru_precip[i] = cru_precip[i] * 12.0 / 365. 
           
    return cru_precip


   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -1.4
    vmax = 1.4
    incr = 0.1
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    #cbar = plt.colorbar(cs,  orientation= 'horizontal',
    #                    ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar = plt.colorbar(cs,  orientation= 'horizontal')
   
    cbar.set_label('mm/day')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI precipitation anomaly')
    

    # overplot data 
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
 
    plt.scatter(lons, lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Panom.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Panom_.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsm_cube = get_model_data()

    # get land observations and cru precipitation at land points
    
    lats, lons, land_precip = get_land_obs()
    cru_land_precip = get_cru_precip(lats, lons)

    print('land precip obs',land_precip)
    print('cru precip',cru_land_precip)
   
    data_panom = land_precip - cru_land_precip
    print('precip anom',data_panom)
    
  
    plot(model_anom_cube, lsm_cube, lats, lons, data_panom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')

PRECIP_MMM_FILE = FILESTART + 'regridded/TotalPrecipitation_multimodelmean.nc'  
PRECIP_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_HadGEM3_clim.py
::::::::::::::
import iris 
import numpy as np
import numpy.ma as ma

def regrid_HadGEM(field, period):

    """
    creates the netcdf averaged files from HG3
    """

    periodalt = {'E280' : 'pi',
                 'Eoi400' : 'pliocene'}

    if field == 'NearSurfaceTemperature':
        fileend = ('/atmos/clims_hadgem3_' + periodalt.get(period) + 
                    '_airtemp_final.nc')
    if field == 'TotalPrecipitation':
        fileend = ('/atmos/clims_hadgem3_' + periodalt.get(period) + 
                    '_precip_final.nc')
  
    if field == 'SST':
       fileend = ('/ocean/clims_hadgem3_' + periodalt.get(period) + 
                    '_sst')
       if period == 'Eoi400':
           fileend = fileend + '_final.nc'
       else:
           fileend = fileend + '.nc'

    filein = ('/nfs/hera1/pliomip2/data/HadGEM3_new/climatologies/' 
          + period + fileend)

    cube = iris.load_cube(filein)
    cube.long_name = field
    if field == 'NearSurfaceTemperature':
        cube.data = cube.data - 273.15
        cube.units = 'Celsius'
    if field == 'TotalPrecipitation':
        cube.data = cube.data *60.*60.*24.
        cube.units = 'mm/day'
    print(cube)

    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())
    print('cubegrid',cubegrid)
    print('regridded_cube',regridded_cube)
   
    if field == 'SST':
        regridded_data = np.ma.asarray(regridded_cube.data) 
        
        for index, x in np.ndenumerate(regridded_data):
            if not np.isfinite(x):
                regridded_data.mask[index] = True
        
       
  

    fileout = ('/nfs/hera1/earjcti/regridded100/HadGEM3/' + period.upper() 
               + '.' + 
           field + '.mean_month.nc')

    iris.save(regridded_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    avg_cube =  regridded_cube.collapsed(['time'], iris.analysis.MEAN)
   
    fileout = ('/nfs/hera1/earjcti/regridded100/HadGEM3/' 
               + period.upper() + '.' + 
               field + '.allmean.nc')

    iris.save(avg_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    return (regridded_cube, avg_cube)


########################################
def avg_NorESM_SST(model, period):
    """
    averages NorESM that the NorESM group regridded
    """

    filename = ('/nfs/hera1/pliomip2/data/' + model + 
                 '/' + model + '_' + period + '.sst.climo.nc')
    cubeorig = iris.load_cube(filename)
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    cube = cubeorig.regrid(cubegrid, iris.analysis.Linear())


    fileout = ('/nfs/hera1/earjcti/regridded100/'+ model +'/' 
               + period.upper() +  
               '.SST.mean_month.nc')

    iris.save(cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    avg_cube =  cube.collapsed(['time'], iris.analysis.MEAN)

    fileout = ('/nfs/hera1/earjcti/regridded100/' + model + '/' + period.upper() + 
               '.SST.allmean.nc')

    iris.save(avg_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    return cube, avg_cube

################################################
def means_to_txt(modelname, annmeancube, avgcube, field, period):
    """ 
    creates the text file from HadGEM3
    """ 

    textout = ('/nfs/hera1/earjcti/regridded100/' + modelname 
               + '/' + period.upper() +
               '.' + field + '.data.txt')
    file1 =  open(textout, "w")

    # get mean field for cube

    avgcube.coord('latitude').guess_bounds()
    avgcube.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(avgcube)
    tempcube = avgcube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data 


    # get mean for each latitude
    tempcube = avgcube.collapsed(['longitude'], iris.analysis.MEAN)
    
    meanlat = tempcube.data 
    meanlat = np.squeeze(meanlat)
  
    
    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2))+', 0.0\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    annmeancube.coord('latitude').guess_bounds()
    annmeancube.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(annmeancube)
    tempcube = annmeancube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    if field == 'NearSurfaceTemperature':
        meanmon = tempcube.data -273.15
    else:
        meanmon = tempcube.data 

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', 0.0\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(avgcube.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', 0.0\n')

    file1.close()


#############################################

periods = ['Eoi400','E280']
fields = ['NearSurfaceTemperature','SST']
#fields = ['TotalPrecipitation']

for period in periods:
    for field in fields:
        (annmeancube, avgcube) = regrid_HadGEM(field, period)
        means_to_txt('HadGEM3',annmeancube,avgcube, field, period)


######
# also average noresm here because we need to do it somewhe
#models = ['NorESM1-F','NorESM-L']
#for period in periods:
#    for model in models:
#        annmeancube, avgcube = avg_NorESM_SST(model, period)
#        means_to_txt(model, annmeancube, avgcube, 'SST', period)
        
::::::::::::::
CEMAC/PLIOMIP2/regrid_HCM3_50_year_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will is a bit like the regridding program for PlioMIP.
# However it will regrid non PlioMIP experiments for HadCM3
# and calculate all the means.
# There is also the option to calculate the means without regridding
# 
# Before this program preprocess data using extract_HadCM3_PlioMIP.py

import numpy as np
import iris
from iris.cube import CubeList
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
#from iris.experimental.equalise_cubes import equalise_attributes
import cf_units as unit
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################

def get_hadcm3_cube():
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = CubeList([])
  
    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+fieldnamein + '/' + exptnamein + '.' + fieldnamein + '.' + yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

       # if i ==startyear:
       #     allcubes = iris.cube.CubeList([])
   
        allcubes.append(cubetemp)

    iris.util.equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

  
    return(cube)


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = '100_'
    else:
        regridded = '_'

    if REGRID == 'y':
        regridded = 'regridded' + regridded
   

    # outfile
    if linux_win  == 'l':
        print(regridded,  exptnameout, fieldnameout)
        if REGRID == 'n':
            outstart = (filename + fieldnameout + '/means/' )
        if REGRID == 'y':
            outstart = (filename + fieldnameout + '/regriddedmeans/' )
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    cube100 = get_hadcm3_cube()
    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if REGRID == 'n':
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    if fieldnamein  == 'tas' or fieldnamein  == 'SST':
        regridded_cube.convert_units('Celsius')
        cube.convert_units('Celsius')

  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    print('writing outfile',outfile)
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int64(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()



##########################################################
# main program

filename  =  ' '
linux_win  =  'l'

# this is regridding where all results are in a single file
avg100yr = 'n'

fieldnamein = 'NearSurfaceTemperature'
exptnamein = 'xozzf'
REGRID = 'n'
startyear = 0
endyear = 100
 
 
if linux_win  == 'l':
    filestart = '/nfs/hera1/earjcti/um/' + exptnamein + '/' 
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

fielduse = fieldnamein
filename = filestart
     

fieldnameout = fieldnamein
exptnameout = exptnamein
regrid_data(fieldnamein, exptnamein)
::::::::::::::
CEMAC/PLIOMIP2/regrid_noresm.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019
"""
The NorESM group have now uploaded averaged data for SST.  We will need to 
get this in the required format for our paper.  
"""


import numpy as np
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import sys


##############################################
def write_avg_to_file(moncube, anncube):
    """
    average the data and write to a file
    """


    textout = FILEDIR + PERIOD + '.SST.data.txt'

    file1 =  open(textout, "w")

    # 1, globalmean

    anncube.coord('latitude').guess_bounds()
    anncube.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(anncube)
    tempcube = anncube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data
    stdevann=-999.999


    # get mean for each latitude
    tempcube = anncube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)
    stdevlat = np.zeros(np.shape(meanlat)) - 1000.



    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    moncube.coord('latitude').guess_bounds()
    moncube.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(moncube)
    tempcube = moncube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data
    stdevmon = np.zeros(12) - 1000.

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(anncube.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    
    
def main():
    """
    reprocesses the NorESM SST data.  
    We have a monthly averaged netcdf file.  
    We need an annual averaged netcdf file + a text file containing all the averages
    """

    filein = FILEDIR + PERIOD + '.SST.mean_month.nc'
    cube = iris.load_cube(filein, 'Ocean surface temperature')
    
    # write mean cube to a file
    meancube = cube.collapsed(['time'],  iris.analysis.MEAN)
    iris.save(meancube, FILEDIR + PERIOD + '.SST.allmean.nc', 
              netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)
    
    # get text data

    write_avg_to_file(cube, meancube)
    print('end of prog')
   
    
##########################################################
# main program


print('start')
LINUX_WIN = 'l'
MODELNAME = 'NorESM1-F'
PERIOD = 'E280' #EOI400 E280

if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'
    
FILEDIR = (FILESTART + 'regridded/' + MODELNAME + '/')
main()
::::::::::::::
CEMAC/PLIOMIP2/regrid_ocn_100yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
        if exptname  == 'E280':
            lsmfile = lsmstart+modelname+'/lsm.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        for name,  dimension in src.dimensions.iteritems():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.iteritems():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube


def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        print(cube.units)
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                       'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b.e12.B1850.f09_g16.preind.' + 
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

   
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear


        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if modelname  == 'EC-Earth3.1': # all fields in one file
        cube = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI-CGCM2.3'):
        cube = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube = get_miroc_tos()
    elif (modelname  == 'GISS'):
        cube = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        cube = get_cesm12(exptnamein)
    else:
        cube = iris.load_cube(filename)


    ndim = cube.ndim




    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid UofTdata or a field that was originally on a tripolar grid
    print('julia1')
    if ((modelname   == 'UofT-CCSM4')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname  == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'UofT' or modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' or modelname == 'CESM1.2'
        or modelname == 'CCSM4-1deg'):
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            cube.data = cube.data* 60. *60. *24. *1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'UofT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname  == 'UofT':
        # we need to add the missing time coordinate
        points = (np.arange(0, 1200)*30)+15. # go for middle of month
        u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

        regridded_cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    elif (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
          modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate



       # end of Uof T loop

    print('julia2')
    #print(regridded_cube.coord('time'))

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".0806.0905"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'UofT':
        if linux_win  == 'l':
            filename = filestart+modelname+'/'
            filename = filename+'UofT-CCSM4/'+exptnamein+'/'+atm_ocn_ind.get(fieldnamein)+'/'
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '_'+atm_ocn_ind.get(fieldnamein)+'_'+exptnamein+
                      '_'+modelname+'-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI-CGCM2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1':
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        print(fielduse)
        filename = (filestart+'EC-Earth3.1/'
                  +ECearth_EXPT.get(exptnamein)
                  +'.EC-Earth3.1.surface.nc')
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "IPSLCM5A2" # MIROC4m  COSMOS UofT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS
                   # new to this version CCSM4-2deg, CESM1.2
                   # CCSM4-1deg

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr']
#exptnamein = ['Eoi400']

#fieldnamein = ['pr']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['tos']
exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_ocn_50yr_avg_pre_HadGEM.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    
    if fieldnamein  == 'pr':
       cube.data = cube.data * 60. *60. *24. *1000.
       cube.name = 'Total precipitation'
       cube.long_name = 'Total precipitation'
       cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e21.B1850.f09_g17.' +
                           'CMIP6-piControl.001.cam.h0.'+
                           'LANDFRAC.1300.1399.nc')
            else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'
       or modelname == 'CESM2'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4-1deg'
          or modelname == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    else:
        cube100 = iris.load_cube(filename)


  

    ###########################################
    # reduce number of years to 50

    cube = reduce_years(cube100)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4-1deg'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }
    
    CESM2_TIME = {"E280" : ".1300.1399",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        if linux_win  == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        filename = (filename +  fielduse +
                      '_' + atm_ocn_ind.get(fieldnamein) +
                      '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CESM2" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['Eoi400','E280']

fieldnamein = ['tos']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_ocn_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# note a similar program which calculates the means without regridding is
# noregrid_ocn_50yr_avg.py


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    print('julia',fielduse)
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    print(cube.data)
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm_400(fieldname):
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename, fieldname)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    print(allcube)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        print('before',filename)
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F' and exptnamein == 'E400'):
        cube100 = get_noresm_400(fielduse)
    else:
        cube100 = iris.load_cube(filename)

   
     

    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A' or modelname == 'HadGEM3'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-Utr' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                    "E400": "b.e21.B1850.f09_g17.CMIP6-piControl.400.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : ".110001-120012",
                  "E400" : ".0801.0900",
                  "Eoi400" : ".110001.120012"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        if linux_win  == 'l':
#            filename = filestart + 'UofT/'
#            filename = (filename + 'UofT-CCSM4/for_julia/' + 
#                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
            filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        
        
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
        if exptnamein == 'E400':
            filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_TREFHT_PRECT_month.nc')
            fielduse = NorESM_FIELDS.get(fieldnamein + 'E400')
            if fieldnamein == 'tos':
                filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_SST_month.nc')
          
            

    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            if exptnamein == 'Eoi400' or exptnamein == 'E400':
                filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename = [filename1, filename2]
                fielduse = ['PRECC', 'PRECL']
            if exptnamein == 'E280':
                filename = (filestart + 'NCAR/b.e21.B1850.f09_g17.' + 
                            'CMIP6-piControl.001.cam.h0.PRECT.110001-120012.nc')
                fielduse = 'PRECT'
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            print(exptnamein)
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
        if fieldnamein =='totcloud':
            filestart='/nfs/hera1/earjcti/PLIOMIP2/CESM2/clt_Amon_CESM2_'
            fielduse = 'clt'
            if exptnamein == 'Eoi400':
                filename = (filestart + 'midPliocene-eoi400_r1i1p1f1_'+
                            'gn_015101-020012.nc')
            if exptnamein == 'E280':
                filename = (filestart +'piControl_r1i1p1f1_gn_090001-099912.nc')
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "NorESM1-F" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

#fieldnamein = ['tas']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['tos']
#exptnamein = ['Eoi400', 'E280','E560']
exptnamein = ['E400']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
#        sys.exit(0)
        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_ocn_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
        if exptname  == 'E280':
            lsmfile = lsmstart+modelname+'/lsm.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    
    if fieldnamein  == 'pr':
       cube.data = cube.data * 60. *60. *24. *1000.
       cube.name = 'Total precipitation'
       cube.long_name = 'Total precipitation'
       cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        print(cube.units)
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                       'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b.e12.B1850.f09_g16.preind.' + 
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if modelname  == 'EC-Earth3.1': # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    else:
        cube100 = iris.load_cube(filename)


  

    ###########################################
    # reduce number of years to 50

    cube = reduce_years(cube100)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname  == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' or modelname == 'CESM1.2'
        or modelname == 'CCSM4-1deg'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    #print(regridded_cube.coord('time'))

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        if linux_win  == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        filename = (filename +  fielduse +
                      '_' + atm_ocn_ind.get(fieldnamein) +
                      '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1':
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        print(fielduse)
        filename = (filestart+'EC-Earth3.1/'
                  +ECearth_EXPT.get(exptnamein)
                  +'.EC-Earth3.1.surface.nc')
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CCSM4-2deg" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # new to this version CCSM4-2deg, CESM1.2
                   # CCSM4

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['Eoi400','E280']

#fieldnamein = ['pr']
fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_ocn_tripolar.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#Created on June2019 2019


#
# This very simple program will convert data that is on a tripolar grid onto a 
# rectilinear grid.  It will still need to be passed through regrid_ocn in order
# to calculate means etc.


import numpy as np
import cf
import iris
#import cfplot as cfp
#import matplotlib.pyplot as plt
from netCDF4 import Dataset, MFDataset
#import sys
#import os


def reformat_with_iris(exptnamein):
    """
    we are going to load the data as an iris cube and add some
    auxillary coordinates and then write out to a file called temporary.nc
    
    the data is currently in filename
    """
    
    print('reformatting ',filename)
    
    origcube = iris.load_cube(filename, 'Near-Surface Air Temperature')
    latcube = iris.load_cube(filename, 'array of t-grid latitudes')
    loncube = iris.load_cube(filename, 'array of t-grid longitudes')
    

    print(origcube)
   
    # promote the auxillary coordinates to dimension coordinates
    nt, ny, nx = origcube.shape
    origcube.coord('nlat').points=np.arange(0,ny,1)
    origcube.coord('nlat').rename('y')
    origcube.coord('y').var_name='y'
    origcube.coord('y').long_name=None
    origcube.coord('y').units=None
    origcube.coord('nlon').points=np.arange(0,nx,1)
    origcube.coord('nlon').rename('x')
    origcube.coord('x').var_name='x'
    origcube.coord('x').long_name=None
    origcube.coord('x').units=None
    print('j3',origcube)
    iris.util.promote_aux_coord_to_dim_coord(origcube, 'y')
    iris.util.promote_aux_coord_to_dim_coord(origcube, 'x')
    print(origcube.coord('x'))
    
 
    
    
    # add an auxillary coordinate for latitude and longitude these are 
    # 2d coordinates
    loncoord=iris.coords.AuxCoord(loncube.data,standard_name='longitude', 
                                  long_name='Longitude',var_name='nav_lon',
                                  units='degrees_east')
    latcoord=iris.coords.AuxCoord(latcube.data,standard_name='latitude', 
                                  long_name='Latitude',var_name='nav_lat',
                                  units='degrees_north')


    origcube.add_aux_coord(loncoord,[1,2])
    origcube.add_aux_coord(latcoord,[1,2])
    
    print('j6')
    print(origcube)
    print(origcube.coord('latitude'))
    #sys.exit(0)
    
    
    iris.save(origcube, exptnamein + '_temporary.nc', 
              fill_value=2.0E20)
    print('saved')

    


#####################################
def regrid_data(fieldnamein,fieldnameout,exptnamein,exptnameout,filename,modelname,linux_win,fielduse,filenameout):

   
    
    print('moodelname is',modelname)
    print('filename is',filename)
    print('fielduse is',fielduse)
      
    
    if ((modelname=='IPSLCM5A') and exptnamein=='Eoi400'):
       # there is a bit of an error in the file calendar so we will 
       # copy the data to a new file but without the error
       with Dataset(filename) as src, Dataset("temporary.nc", "w",format='NETCDF3_CLASSIC') as dst:
        # copy attributes
            for name in src.ncattrs():
                dst.setncattr(name, src.getncattr(name))
                #print('att',name)   
                # copy dimensions
            for name, dimension in src.dimensions.iteritems():
              
                if name != 'tbnds':   # don't copy across time counter bounds
                   # if fielduse=='SeasurfaceTemp_sst' and name=='y':
                   #     name='latitude'
                   # if fielduse=='SeasurfaceTemp_sst' and name=='x':
                   #     name='longitude'
                    dst.createDimension(name, (len(dimension)))
           
                     
            # copy all file data 
            for name, variable in src.variables.iteritems():
                print('name is',name,variable)
                if name !='time_counter_bnds' and name!='time_centered':
                    x = dst.createVariable(name, variable.datatype, 
                                               variable.dimensions)
                       
                    if name=='time_counter':
                    # convert from seconds to days and start at middle of month
                        dst.variables[name][:] = (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                    else:
                        dst.variables[name][:] = src.variables[name][:]
                    # copy attributes for this variable
                    for ncattr in src.variables[name].ncattrs():
                        attribute=src.variables[name].getncattr(ncattr)
                        #print('j2',name,ncattr,attribute)
                           
                        if ncattr=='calendar' and exptnamein=='Eoi400':
                            dst.variables[name].setncattr(ncattr,'360_day')
                        else:
                            if (ncattr=='units' and name=='time_counter'):
                            # change units from seconds to days
                                dst.variables[name].setncattr(ncattr,attribute.replace('seconds','days'))
                            else:
                                dst.variables[name].setncattr(ncattr,attribute) 
               
      
    
        
       origf=cf.read_field('temporary.nc')
       print('read copied dataset')
    else:
       if (modelname == 'IPSLCM6A'):
           print (filename)
           origf = cf.read_field(filename, select='sea_surface_temperature')
       elif (modelname == 'CESM1.0.5'):
           #origf = cf.read(filename)
           #print(origf)
           #sys.exit(0)
           reformat_with_iris(exptnamein) # the data is not in a very good format
                                # we will reformat and write to temporary.nc
           
           print('about to read temporaray.nc')
           
           origf=cf.read_field(exptnamein + '_temporary.nc', )
           # iterate over auxillary coordinates and see if they are
           # longitud eor latitude
           for aux in origf.auxs():
               print (origf.auxs(aux))
           a=origf.aux('latitude')
           a.units=('degrees_north')
           print(a)
           
           b=origf.aux('longitude')
           b.units=('degrees_east')
           print(b)
           print(origf)
           #sys.exit(0)  
               
           #for key, aux in self.auxs(ndim=2).iteritems():
           #     if aux.Units.islongitude:
           #sys.exit(0)
           print (origf)
           print (origf.data_axes())
           print (origf.coords())
           
         
       elif (modelname == 'HadGEM3'):
           
       else:
           origf=cf.read_field(filename)
        
        
    print (origf)
   
    
    gridf=cf.read_field('one_lev_one_deg.nc')
    #print  gridf
    
    print('about to regrid')
   
    # assume tripoplar grid
    if modelname == 'CESM1.0.5':
       regridf=origf.regrids(gridf,
                          method='bilinear',
                          src_axes={'X': 'ncdim%x','Y':'ncdim%y'},
                          src_cyclic=True)
    else:   
        regridf=origf.regrids(gridf,
                          method='bilinear',
                          src_axes={'X': 'ncdim%x','Y':'ncdim%y'},
                          src_cyclic=True)
    print('regridded')
    
    # see if we can remove auxillary coordinates
    if (modelname=='IPSLCM5A' or modelname=='IPSLCM5A2'):
        regridf.remove_item(description='T',role='a')
   
    
   
    # write to a temporary file so that we can read in as an iris cube and do all our iris analysis 
    # in exactly the same way as before
    
    print(filename)
    print(filenameout)
    cf.write(regridf,filenameout,fmt='NETCDF4_CLASSIC')
 
   
  
    
   
  

#############################################################################
def getnames(modelname,fieldname,exptname): 
    """
    parameters: 
        modelname - the model
        fieldname - the name of the field ie 'tos'
        exptname - the name of the experiment ie [Eoi400]
        
    returns:
        fielduse - the name of the field in the file
        filename - the input file
        filenameout = the output file
    
    this program will get the names of the files and the field for each
    of the models   
    
    """
    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                    "tas" : "NearSurfaceAirTemp",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "SeaSurfaceTemp"
                    }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }
    
    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }
    
    NorESM_FIELDS = {"pr" : "PRECT",
                     "ts" : "TREFHT",
                     "sic" : "SeaIceAreaFraction"
                     }
    
     
    CESM105_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }

    ECearth_EXPT = {"Eoi400" : "mPlio",
                    "E280" : "PI"
                    }
    
    IPSLCM5A_EXPT = {"Eoi400" : "Eoi400",
                     "E280" : "PI"
                     }
    
    IPSLCM5A_TIME = {"Eoi400" : "3581_3680",
                     "E280" : "3600_3699"
                     }
    
    IPSLCM5A21_TIME = {"Eoi400" : "3381_3480",
                     "E280" : "6110_6209",
                     }
    
    IPSLCM6A_TIME = {"Eoi400" : "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                     "E280":"piControl_r1i1p1f1_gn_285001-304912",
                     }
    
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"
                   }

    # get names for each model
   
    if modelname == 'COSMOS':
        if linux_win=='l':
            filename=filestartin+'/AWI/COSMOS/'
            filename=filename+exptname+'/'
        else:
            filenameout=filestartout+'/COSMOS/'
        fielduse=COSMOS_FIELDS.get(fieldname)
        filename=(filename+exptname+'.'+fielduse+
                      '_CMIP6_name_'+fieldname+
                      '_2650-2749_monthly_mean_time_series.nc')
        filenameout=(filestartout+'COSMOS/'+exptname+'.'+fielduse+
                      '_CMIP6_name_'+fieldname+
                      '_2650-2749_monthly_mean_time_series_rectilinear.nc')
   
   
    if modelname=='IPSLCM5A' or modelname=='IPSLCM5A2':
        exptuse=exptname_l.get(exptname)
        if modelname=='IPSLCM5A':
            timeuse=IPSLCM5A_TIME.get(exptname)
        if modelname=='IPSLCM5A2':
            timeuse=IPSLCM5A21_TIME.get(exptname)
        fielduse=IPSLCM5A_FIELDS.get(fieldname)
        filename=(filestartin+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptname)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')
        filenameout=(filestartout+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptname)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        print(filename)
        print(filenameout)
        
        
   
    if modelname=='IPSLCM6A':
        fielduse=MIROC_FIELDS.get(fieldname)
        filename=(filestartin+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptname)+'.nc')
        filenameout=(filestartout+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptname)+'_rectilinear.nc')
        
    if modelname == 'CESM1.0.5':
        print (filestartin)
        print (modelname)
        print (exptname)
        print (CESM105_FIELDS.get(fieldname))
        
        filename = (filestartin + modelname + '/' + exptname + '/' +
                  exptname + '_' + CESM105_FIELDS.get(fieldname) +
                  '.nc')
        fielduse = fieldname
        filenameout = (filestartout + modelname + '/' + exptname + '/' +
                  exptname + '_' + CESM105_FIELDS.get(fieldname) +
                  '.nc')
    
        print(filename)
        print(filenameout)
        #sys.exit(0)

    if modelname == 'HadGEM3':
        ending = 'midPliocene-eoi400_r1i1p1f1_gn_233401-239312.nc'
        filename = (filestartin + 'HadGEM3/tos_Omon_HadGEM3-GC31-LL_' + 
                   ending)
        fielduse = 'sea_surface_temperature'
        filenameout = filestartin + 'HadGEM3/regridded_' + ending
   
    
    return [fielduse,filename,filenameout]


##########################################################
# main program

filename=' '
linux_win='l'
modelname='HadGEM3' # IPSLCM5A, IPSLCM5A2
                   #                      IPSLCM6A
                   # 'CESM.1.0.5 HadGEM3
                   
#modelname = 'IPSLCM5A'

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

# only need fields that are on a tripolar grid like orca
fieldname = {
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein=['ts','pr']
#exptnamein=['Eoi400','E280']

#fieldnamein=['pr']
fieldnamein=['tos'] # ocean tempeature or sst
#exptnamein=['E280','Eoi400']

exptnamein=['E280']
if linux_win=='l':
    filestart='/nfs/hera1/pliomip2/data/'
    if (modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2' 
        or modelname == 'COSMOS' or modelname =='CESM1.0.5'):
        filestartin='/nfs/hera1/pliomip2/data/'
    if modelname=='IPSLCM6A' or modelname == 'HadGEM3':
        filestartin='/nfs/hera1/earjcti/PLIOMIP2/'
    filestartout='/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    
    


for expt in range(0,len(exptnamein)):
    for field in range(0,len(fieldnamein)):

        # call program to get model dependent names
        # fielduse, and  filename 
        fielduse, filename, filenameout = (getnames
                                           (modelname,fieldnamein[field],
                                            exptnamein[expt]))
        
        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])

        print ('filename',filename)
        
        regrid_data(fieldnamein[field],fieldnameout,exptnamein[expt],exptnameout,
                    filename,modelname,linux_win,fielduse,filenameout)

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_timeseries.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 21 2020

#
# This program will produce a regridded 1X1degree timeseries of a given field.  
# We will remove the annual cycle in order to look for interannual variability 
# etc.  


import numpy as np
from netCDF4 import Dataset
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname, lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse == "sst":
        if exptname == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'

        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data = np.ma.array(cube.data, mask=mymask)
    return cube

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u = unit.Unit('days since 0800-01-01 00:00:00',
                      calendar=unit.CALENDAR_360_DAY)
        if model == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model == 'HadCM3' and fielduse == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model == 'HadCM3' and fielduse == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube = cube_temp.extract(iris.Constraint(surface=0.))

    return cube

def get_noresm_ocn(exptnamein, fieldnamein):
    """
    get noresm ocean things one file per month

    """
    print(modelname, exptnamein)
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    fileend = '_remap.nc'
   
    
    if exptnamein == 'Eoi400' and modelname == 'NorESM1-F': 
        startyear = 2400
        endyear = 2500
    if exptnamein == 'E280' and modelname == 'NorESM1-F': 
        startyear = 1900
        endyear = 2000
    if exptnamein == 'E280' and modelname == 'NorESM-L': 
        startyear = 2100
        endyear = 2200
    if exptnamein == 'Eoi400' and modelname == 'NorESM-L': 
        startyear = 1100
        endyear = 1200
  
    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        for i, mon in enumerate(months):
            file = filename + np.str(year) + '-' + mon + fileend
            cubetemp = iris.load_cube(file, 'Ocean surface temperature')
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
    print(cube)
  
  
    return cube

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src, Dataset("temporary.nc", "w", format='NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name, src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name, dimension in src.dimensions.items():

            if name != 'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name, (len(dimension)))

        # copy all file data
        for name, variable in src.variables.items():
            print('name is', name, variable)
            if name not in ('time_counter_bnds', 'time_centered'):
                x = dst.createVariable(name, variable.datatype,
                                       variable.dimensions)
                if name == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:] = ((src.variables[name][:] / (60.*60.*24))
                                              -(src.variables[name][0] / (60.*60.*24)) + 15.)
                else:
                    dst.variables[name][:] = src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr == 'calendar' and exptname == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr == 'units' and name == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein in ('ts', 'tas'):
            cube.convert_units('Celsius')

        if exptname == 'Eoi400':
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_360_DAY)
        else:
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return cube

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return cube

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return cube

def get_HadGEM3():
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    # eoi400
    #startyear = 2334
    #endyear = 2434

    #e280
    startyear=1950
    endyear = 2050
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        #if year < 2394: 
        #   extra = 'v963'
        #else:
        #    extra = 'x150'
        #e280
        extra='q637'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    return cube



def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """

    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]

    # put units as celcius if required
    if fielduse == 'tas':
        cube.units = 'Celsius'

    cube2 = iris.util.new_axis(cube, 'time')

    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """

    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

    cube.add_dim_coord(iris.coords.DimCoord(points,
                                            standard_name='time', long_name='time',
                                            var_name='time',
                                            units=u,
                                            bounds=None,
                                            coord_system=None, circular=False), 0)

    if fieldnamein == 'pr':
        cube.data = cube.data * 60. *60. *24. *1000.
        cube.name = 'Total precipitation'
        cube.long_name = 'Total precipitation'
        cube.units = 'mm/day'


    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]

    return singlecube

def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """

    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
        cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
        cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
        if fieldnameout == 'TotalPrecipitation':
            cube = cube1 + cube2
        if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
            cube1.convert_units('Celsius')
            cubedata = np.where((cube2.data > 0.01),
                                -1.8, cube1.data)
            cube = cube1.copy(data=cubedata)
    else:
        cube = get_cesm12_singlecube(filename, fielduse)

    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b.e21.B1850.f09_g17.' +
                           'PMIP4-midPliocene-eoi400.001.'+
                           'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' +
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280':
            #if modelname == 'CESM2':
            #    filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
           #                'b.e21.B1850.f09_g17.' +
           #                'CMIP6-piControl.001.cam.h0.'+
           #                'LANDFRAC.1300.1399.nc')
           # else:
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                       'b.e12.B1850.f09_g16.preind.' +
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data = np.ma.array(cube.data, mask=mymask)


    return cube



######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube

    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years


    """

    if modelname in ('CCSM4', 'CESM1.2', 'CESM2'):
        print('CCSM',cube.coord('month').points)
        months = cube.coord('month').points
        months = months -1
        for i, month in enumerate(months):
            if month == 0: months[i] = 12
        print(months)
        cube.coord('month').points = months
 
    else:
        startyear = (cube.coord('year').points[0])
        endyear = (cube.coord('year').points[-1])
        # count the number of months that have the same year as the first index
        nstart = 0
        nend = 0
        for i in range(0, 12):
            if cube.coord('year').points[i] == startyear:
                nstart = nstart+1
        for i in range(-13, 0):
            if cube.coord('year').points[i] == endyear:
                nend = nend+1
        if nend != 12 or nstart != 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
            if nend + nstart == 12:
                for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                    cube.coord('year').points[i] = startyear


                else:

                    print('you have a partial year somewhere')
                    print('correct input data to provide full years')
                    print(nend, nstart)
                    sys.exit(0)


    return cube

######################################################
def cube_avg(cube):
    """
    Extract monthly averaged data from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months

    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)

    meanmonthcube.long_name = fieldnameout

    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')

    return meanmonthcube


def remove_ann_cycle(cube, mon_avg_cube):
    """
    removes the annual cycle stored in mon_avg_cube from cube
    """

    cubedata = cube.data
    for i, monthno in enumerate(cube.coord('month').points):
        mon_avg_data = (mon_avg_cube.extract(iris.Constraint(month=monthno))).data
        cubedata[i, :, :] = cubedata[i, :, :] - mon_avg_data


    timeseries_cube = cube.copy(data=cubedata)

    return timeseries_cube


##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

    # outfile
    if linux_win == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/' + modelname +
                    '/timeseries/' + exptnameout + '.' + fieldnameout + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
                    + modelname + '\\timeseries\\' + exptnameout 
                    + '.' + fieldnameout + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


    #####################################
    # get all data in a single cube
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3'): # all fields in one file
        cube = get_ecearth_cube(exptnamein, lsmstart)
    elif modelname in ('HadCM3', 'MRI2.3'):
        cube = get_hadcm3_cube(modelname)
    elif modelname in ('IPSLCM5A', 'IPSLCM5A2') and fieldnamein != 'tos':
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif modelname in ('NorESM1-F', 'NorESM-L') and fieldnamein == 'tos':
        cube = get_noresm_ocn(exptnamein, fieldnamein)
    elif modelname == 'IPSLCM6A':
        cube = get_ipslcm6()
    elif modelname in ('MIROC4m', 'tos'):
        cube = get_miroc_tos()
    elif modelname == 'GISS2.1G':
        cube = get_giss()
    elif modelname == 'CCSM4-Utr':
        cube = get_ccsm4_2deg()
    elif modelname in ('CESM1.2', 'CCSM4', 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'HadGEM3'):
        cube = get_HadGEM3()  
    else:
        cube = iris.load_cube(filename)

  
    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid

    if ((modelname == 'CCSM4-UoT')
            or (modelname == 'IPSLCM5A' and fieldnamein == 'tos')
            or (modelname == 'IPSLCM5A2' and fieldnamein == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A',
                     'IPSLCM5A2', 'IPSLCM6A', 'CCSM4-Utr', 'GISS2.1G'):
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if modelname in ('NorESM1-F', 'NorESM-L', 'CESM1.2', 'CESM2', 'CCSM4'):

       # if precipitation is in m/s convert to mm/day
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. * 60. * 24. * 1000.
            cube.data = cube.data * 60. * 60. * 24. * 1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if modelname in ('CCSM4-UoT', 'NorESM1-F', 'NorESM-L', 'IPSLCM6A',
                     'EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A', 'IPSLCM5A2',
                     'HadCM3', 'GISS2.1G'):
         # convert to celcius
        if fieldnamein in ('tas', 'tos'):
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname in ('COSMOS', 'MIROC4m', 'IPSLCM6A', 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate


    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube, 'time', name='month')
    iris.coord_categorisation.add_year(regridded_cube, 'time', name='year')


     # correct the start month if required
    regridded_cube = correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_cube = cube_avg(regridded_cube)



    # remove annual cyble
    anom_cube = remove_ann_cycle(regridded_cube, mean_mon_cube)

    # to check we have removed the average properly get the monthly
    # average of the anomaly cube it should be zero

    new_mean_mon_cube = cube_avg(anom_cube)
    qplt.contourf(new_mean_mon_cube[2, :, :], levels=np.arange(-0.01, 0.011, 0.001), extend='both')
    plt.show()



    # write the cubes out to a file

    outfile = outstart+'timeseries_no_ann_cycle.nc'
    iris.save(anom_cube, outfile, netcdf_format='NETCDF3_CLASSIC', fill_value=2.0E20)




#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                     }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }

    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }

    NorESM_FIELDS = {"pr" : "PRECT",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "sst"
                    }

    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                      }

    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA = {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                    "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }

    CESM2_EXTRA = {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                   "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                   }

    CCSM4_EXTRA = {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                   "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                   }

    ECearth_EXPT = {"Eoi400": "mPlio",
                    "E280":"PI"
                   }

    CESM12_EXPT = {"Eoi400": "PlioMIP2",
                   "E280":"PI"
                   }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
                     "E280":"PI"
                    }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }

    CESM2_TIME = {"E280" : ".110001-120012",
                  "Eoi400" : ".1101.1200"
                 }

    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
                     "E280":"3600_3699"
                    }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
                       "E280":"6110_6209",
                      }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
                     "E280":"piControl_r1i1p1f1_gr_285001-304912",
                     }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                         "E280":"piControl_r1i1p1f1_gn_285001-304912",
                        }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
                  "E280":"piControl_r1i1p1f1_gn_490101-495012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
                 }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
                       "E280":"piControl_r1i1p1f1_gr1_150101-160012",
                       "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                      }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
                  "E280":"piControl_r1i1p1f1_gn_495101-500012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                  }
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"}
    cosmos_version = {"tas": "",
                      "pr": "",
                      "tos":"_remapbil"}

    # get names for each model
    if modelname == 'MIROC4m':
        filename = filestart + modelname + '/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename + fielduse + '/MIROC4m_'+exptnamein
                    + '_' + atm_ocn_ind.get(fieldnamein) + '_' + fielduse + '.nc')
    if modelname == 'COSMOS':
        if linux_win == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename + exptnamein + '.' + fielduse +
                    '_CMIP6_name_' + fieldnamein +
                    '_2650-2749_monthly_mean_time_series' +
                    cosmos_version.get(fieldnamein) + '.nc')
    if modelname == 'CCSM4-UoT':
        if linux_win == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' +
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart + 'UofT-CCSM4\\' + exptnamein + '\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)

        filename = (filename + fielduse +
                    '_' + atm_ocn_ind.get(fieldnamein) +
                    '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart + 'LEEDS/HadCM3/' + exptuse + '/' + fielduse + '/'
                    + exptuse + '.' + fielduse + '.')
    if modelname == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                    +exptuse+'.'+fielduse+'.')
    if modelname == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname
                    + '_'
                    + ECearth_EXPT.get(exptnamein)
                    + fileend)
    if modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse 
                        + '_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse + '_monthly_TS.nc')

    if modelname == 'NorESM1-F' or modelname == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/sst_regrid_1degree/'
                    + modelname + '_' + exptnamein + '.' + fielduse + '.')
    if modelname == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' + fielduse +
                        '_Omon_IPSL-CM6A-LR_' + IPSLCM6A_TIME_ALT.get(exptnamein) 
                        + '_rectilinear.nc')
        else:
            filename = (filestart + modelname + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) + '_IPSL-CM6A-LR_' 
                        + IPSLCM6A_TIME.get(exptnamein) + '.nc')
    if modelname == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart + modelname + '/' + exptuse + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME1.get(exptnamein)
                        + '.nc')
        filename.append(filestart + modelname + '/' + exptuse + '/' 
                        + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME2.get(exptnamein) + '.nc')

    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein

    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                      CESM2_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECL' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']

        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'TS' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'ICEFRAC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']

        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' +
                      CCSM4_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)


    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if exptnamein == 'Eoi400':
            filename = filestart + 'Eoi400/ocean/sst_sal_temp/new_nemo_b'
        if exptnamein == 'E280':
            filename = filestart + 'E280/ocean/sst_sal_temp/new_nemo_b'
       
        fielduse = 'sea_surface_temperature'

    print(fielduse, filename)
    retdata = [fielduse, filename]
    return retdata


##########################################################
# main program

filename =  ' '
linux_win =  'l'
modelname = "HadGEM3" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"
            }

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"
            }

fieldname = {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['E280']

fieldnamein = ['tos']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if (modelname in ('IPSLCM5A', 'IPSLCM5A2', 'CCSM4-Utr')
            and fieldnamein[field] == 'tos'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if modelname in ('IPSLCM6A', 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]


        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/regrid_winds_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on june 24 2020; copied from regrid_ocn_50yr_avg.py


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 50 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#



import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)

    print(allcubes)
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()

 
    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 
                                                 'longitude',
                                                 'air_pressure'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot():
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    print(filename)
    cubes = iris.load(filename)
    cube = cubes[0]
    pressures = cubes[1].data
    
    print(cube)
    print(pressures)
    
    
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)

    cube.add_dim_coord(iris.coords.DimCoord(pressures,
                standard_name = 'air_pressure',  long_name = 'pressure',
                var_name = 'pressure',
                units = 'Pa',
                bounds = None,
                coord_system = None,  circular = False), 1)


    return cube

    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
        if allcube[i].var_name == "lev":
            cubelev = allcube[i]

    if exptnamein == 'E280' or MODELNAME == 'CCSM4' or MODELNAME == 'CESM1.2':
        pressures = cubelev.data * 100.
        print(pressures)
        cube.add_dim_coord(iris.coords.DimCoord(pressures,
                standard_name = 'air_pressure',  long_name = 'pressure',
                var_name = 'pressure',
                units = 'Pa',
                bounds = None,
                coord_system = None,  circular = False), 1)

    if exptname == 'E280' and MODELNAME == 'CESM1.2':
        points = (np.arange(0, 1200)*30)+15. # go for middle of month
        u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of
        cube.coord('time').points = points
        cube.coord('time').units = u
        iris.util.promote_aux_coord_to_dim_coord(cube, 'time')
       
         #cube.add_dim_coord(iris.coords.DimCoord(points,
         #       standard_name = 'time',  long_name = 'time',
         #       var_name = 'time',
         #       units = u,
         #       bounds = None,
         #       coord_system = None,  circular = False), 0)

    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube, exptnamein):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (MODELNAME  == 'NorESM1-F' or MODELNAME  == 'NorESM-L'
       or MODELNAME == 'CESM1.2' or MODELNAME == 'CCSM4'
       or (MODELNAME == 'CESM2' and exptnamein == 'E280')):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = FIELDNAMEOUT
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = FIELDNAMEOUT

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=FIELDNAMEOUT

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=FIELDNAMEOUT
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=FIELDNAMEOUT

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(exptnamein, filename, fielduse, constraint):
    """
    regrid the data
    """


    print('moodelname is', MODELNAME)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if LINUX_WIN  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+MODELNAME+'/'+ EXPTNAMEOUT +'.'+
        FIELDNAMEOUT + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +MODELNAME+'\\' + EXPTNAMEOUT + '.' + FIELDNAMEOUT + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (MODELNAME  == 'EC-Earth3.1' or
       MODELNAME == 'EC-Earth3.3'): # all fields in one file
        cube100 = iris.load_cube(filename, fielduse)
    elif (MODELNAME  == 'HadCM3' or MODELNAME  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(MODELNAME)
    elif ((MODELNAME  == 'IPSLCM5A' or MODELNAME  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (MODELNAME  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (MODELNAME  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (MODELNAME  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (MODELNAME  == 'CESM1.2' 
          or MODELNAME == 'CCSM4'
          or MODELNAME == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (MODELNAME == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot()
    else:
        cube100 = iris.load_cube(filename)

    print(cube100)

    cube_level = cube100.extract(constraint)
   
    ###########################################
    # reduce number of years to 50

    print(cube_level)
    #sys.exit(0)
    cube = reduce_years(cube_level)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((MODELNAME   == 'CCSM4-UoT')
        or (MODELNAME  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (MODELNAME  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if MODELNAME  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (MODELNAME  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'
   
        
    if (MODELNAME  == 'COSMOS' or MODELNAME  == 'MIROC4m' or
        MODELNAME  == 'IPSLCM6A' or 
        MODELNAME  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube, exptnamein)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg ' + FIELDNAMEIN)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    
    COSMOS_FIELDS  = {"ua" : "u-velocity", "va" : "v-velocity"}

    ECearth_FIELDS  = {"ua" : "U component of wind", 
                       "va" : "V component of wind" }

    HadCM3_FIELDS  = {"ua" : "U COMPNT OF WIND ON PRESSURE LEVELS", 
                      "va" : "V COMPNT OF WIND ON PRESSURE LEVELS" }

    IPSLCM5A_FIELDS  = { }

    NorESM_FIELDS = {"ua" : "U", "va" : "V"}
    
    CCSM42_FIELDS = {"ua" : "U", "va": "V" }
    
    CESM12_FIELDS = {"ua": "U", "va": "V"}
    
    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b.e12.B1850.f09_g16.preind.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0707.0806",
                   "Eoi400" : ".1101.1200"
                   }
    
    CESM2_TIME = {"E280" : ".1201.1300",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    
    # get names for each model
    if MODELNAME   ==  'MIROC4m':
        filename = FILESTART + MODELNAME + '/'
        fielduse = FIELDNAMEIN
        filename = (filename + fielduse + '/MIROC4m_' + 
                    exptnamein + '_Amon_' + fielduse + '.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
   

    if MODELNAME   ==  'COSMOS':
        if LINUX_WIN  == 'l':
            filename = FILESTART + 'AWI/COSMOS/'
            filename = filename + exptnamein + '/'
        else:
            filename = FILESTART + '/COSMOS/'
        fielduse = COSMOS_FIELDS.get(FIELDNAMEIN)
        filename = (filename + exptnamein + '.' + FIELDNAMEIN +
                    '_2650-2749_monthly_mean_time_series.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
   

    if MODELNAME   ==  'CCSM4-UoT':
        if LINUX_WIN  == 'l':
            filename = FILESTART + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/Amon/')
        else:
            filename = FILESTART+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = FIELDNAMEIN
        
        filename = (filename +  fielduse +
                      '_Amon_' + exptnamein + '_UofT-CCSM4_gr.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
        

    if MODELNAME  == 'HadCM3':
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = HadCM3_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + 'LEEDS/HadCM3/' + exptuse
                    + '/' + FIELDNAMEIN + '/'
                    + exptuse + '.' + FIELDNAMEIN + '.')
        constraint = iris.Constraint(p = LEVEL)

    if MODELNAME  == 'MRI2.3':
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = FIELDNAMEIN + str(np.int(LEVEL))
        
        filename = (FILESTART + 'MRI-CGCM2.3/' + fielduse + 
                    '/' + exptuse + '.' + fielduse + '.')
        constraint = None
      
    if MODELNAME  == 'EC-Earth3.1' or MODELNAME == 'EC-Earth3.3':
        fileend = '_uv.nc'
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = ECearth_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + MODELNAME + '/'
                    + MODELNAME 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
        constraint = iris.Constraint(air_pressure = LEVEL * 100)
   

    if MODELNAME  == 'IPSLCM5A' or MODELNAME  == 'IPSLCM5A2':
        exptuse = EXPTNAME_L.get(exptnamein)
        if MODELNAME  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if MODELNAME  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART+MODELNAME+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +IPSLCM5A_FIELD.get(fielduse)+'_'+timeuse+'_monthly_TS.nc')
        constraint = None

    if MODELNAME  == 'NorESM1-F' or MODELNAME  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + MODELNAME + '/' + MODELNAME + '_' 
                    + exptnamein + '_' + fielduse + '.nc')
        constraint = iris.Constraint(pressure = LEVEL)

    if MODELNAME  == 'IPSLCM6A':
        fielduse = FIELDNAMEIN
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/' + MODELNAME 
                    + '/' + fielduse + '_' + 'Amon_IPSL-CM6A-LR_'
                   + IPSLCM6A_TIME.get(exptnamein)+'.nc')
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
    if MODELNAME  == 'GISS2.1G':
        fielduse = FIELDNAMEIN
        exptuse = EXPTNAME_L.get(exptnamein)
        filename = []
        filename.append('/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
                        + exptuse + '/' + fielduse + '_Amon'
                        + '_GISS-E2-1-G_' 
                        + GISS_TIME1.get(exptnamein) + '.nc')
        filename.append('/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
                        + exptuse + '/' + fielduse + '_Amon'
                        + '_GISS-E2-1-G_' 
                        + GISS_TIME2.get(exptnamein) + '.nc')
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
       

    if MODELNAME == 'CCSM4-Utr':
        filename=(FILESTART + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(FIELDNAMEIN) +
                  '.nc')
        fielduse = FIELDNAMEIN
        
    if MODELNAME == 'CESM1.2':
        filename=(FILESTART + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CESM12_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
        

    
    if MODELNAME == 'CESM2':
        print(exptnamein)
        print(CESM2_TIME.get(exptnamein))
        print(CESM2_EXTRA.get(exptnamein))
        print(CESM12_FIELDS.get(FIELDNAMEIN))

        filename=(FILESTART + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CESM2_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
               
            
    if MODELNAME == 'CCSM4':
        filename = (FILESTART + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
        
    print(fielduse,filename,constraint)
    retdata = [fielduse, filename, constraint]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "IPSLCM5A" # MIROC4m  COSMOS CCSM4UoT -EC-Earth3.3
                   # HadCM3 MRI
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

EXPTNAME  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

EXPTNAME_L  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}



# this is regridding where all results are in a single file
FIELDNAMEIN = 'va'
LEVEL = 850.
EXPTNAMEIN = ['Eoi400', 'E280', 'E400']
#EXPTNAMEIN = ['E400']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for exptname in EXPTNAMEIN:


    # call program to get model dependent names
    # fielduse,  and  filename
    retdata = getnames(exptname)

    fielduse = retdata[0]
    filename = retdata[1]
    lev_constraint = retdata[2]
    print(fielduse, filename, lev_constraint)
    #sys.exit(0)

    FIELDNAMEOUT = FIELDNAMEIN + '_' + np.str(LEVEL)
    EXPTNAMEOUT = EXPTNAME.get(exptname)

    regrid_data(exptname, filename, fielduse, lev_constraint)

::::::::::::::
CEMAC/PLIOMIP2/sea_SAT_relationships.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
#Created on 05/03/2020


#@author: earjcti

This program will plot the land temperature relationships
requested by IPCC in particular

- Land vs sea: I suggest reporting both (1) all sea vs GMAT, and 
(2) sea from 60S - 60N vs GMST. 
These metrics are also needed to help inform GMAT reconstructions from the proxy data, 
which are primarily marine-based.


"""

import warnings
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
from scipy import stats

warnings.filterwarnings("ignore")

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match.
    """

    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube

def get_lsm(filein):
    """
    returns a  numpy array of land points and one of sea points
    """
    tempcube = iris.load_cube(filein)
    cubegrid = iris.load_cube('one_lev_one_deg.nc')
    lsmcube = tempcube.regrid(cubegrid, iris.analysis.Linear())
    landpoints = lsmcube.data
    seapoints = (lsmcube.data - 1.0) * (-1.0)

    return landpoints, seapoints

def get_mean_data(model, expt, field):
    """
    gets the cube of mean data for a single model

    Parameters
    ----------
    model : the name of the model we are interested in
    expt : whether it is the experiment or the control

    Returns
    -------
    a cube with the mean data from this file
    grid_areas = the size of the grid for averaging
    """

    filename = (FILESTART + 'regridded/' + model
                + '/' + expt + '.' + field + '.allmean.nc')

    print(field, filename)
   
    cube = iris.load_cube(filename)
    cube2 = resort_coords(cube)

    cube2.coord('latitude').guess_bounds()
    cube2.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube2)

    return cube2, grid_areas


def get_region(latmin, latmax, cube, mask, grid_areas):
    """
    Gets the average temeprature over land within the
    bounded range

    Parameters
    ----------
    latmin, latmax : the range of latitudes we are extracting data from
    cube : the cube containing average temperatures t
    mask : numpy array containing the mask we want.  Could be a land mask
           a sea mask or ones everywhere (ie all points)
    grid_areas : the size of each gridpoint for weighting

    Returns
    -------
    region_avg : scalar containing the average temperature over the
                     required region

    """

    grid_areas_mask = grid_areas * mask
    grid_areas_band = np.zeros(grid_areas.shape)
    lats = cube.coord('latitude').points

    for j, lat in enumerate(lats):
        if latmin <= lat <= latmax:
            grid_areas_band[j, :] = grid_areas_mask[j, :]

    region_avg = cube.collapsed(['longitude', 'latitude'],
                                iris.analysis.MEAN,
                                weights=grid_areas_band)

    return region_avg.data

def scatter_sea_vs_global(allanom, seaanom, plottype, txtfile):
    """
    plot the global temperature anomaly vs the 
    ocean temperature anomaly for theregion on one plot
    also outputs the regression equation

    Parameters
    ----------
    allanom : temperature anomaly from the globe
    seaanom : temperature anomaly from the sea
    plot type: '' - global or latitude range

    Returns
    -------
    None.

    """

    titlename = 'MPWP - PI: global vs ocean ' + plottype + ' temperature '
    ax = plt.subplot(1, 1, 1)

    for i, model in enumerate(MODELNAMES):
        if i % 4 == 0: # i divides 4 with no remainder
            ax.scatter(seaanom[i], allanom[i], label = model) 
        elif i % 4 == 1 :
            ax.scatter(seaanom[i], allanom[i], label = model, marker='^') 
        elif i % 4 == 2 :
            ax.scatter(seaanom[i], allanom[i], label = model, marker='<') 
        else:
            ax.scatter(seaanom[i], allanom[i], label = model, marker='v') 
    #plt.title(titlename)
    plt.xlabel('ocean temperature ' + plottype + ' anomaly (' + UNITS + ')')
    plt.ylabel('global SAT ' + ' anomaly (' + UNITS + ')')


    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout = (FILEOUTSTART + '/sea_SAT'  + plottype + '_anomaly.eps')
    plt.savefig(fileout)
    fileout = (FILEOUTSTART + '/sea_SAT'  + plottype + '_anomaly.pdf')
    #plt.show()
    plt.savefig(fileout)
    plt.close()


    # print out the regression equation and the data to a file
    print(plottype)
    print('=======')
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(seaanom, allanom)
    print('GMSAT = ' + np.str(np.around(slope, 3)) + 
          'x OSAT + ' + np.str(np.around(intercept, 3)))
    print('pvalue = ' + np.str(np.around(p_value, 3)) + 
          ' rvalue = ' + np.str(np.round(r_value, 3)) + 
          ' rsq = ' + np.str(np.round(r_value * r_value, 3)))
    print('   ')

    # print out the values to a file
    
    txtfile.write("modelname, global mean SAT, oceanSAT " + plottype + '\n')
    for i, model in enumerate(MODELNAMES):
        txtfile.write((model + ',' + np.str(np.around(allanom[i],3)) + 
                       ',' + np.str(np.around(seaanom[i],3)) + '\n'))
    
      

#####################################
def main():
    """
    Tha main control of the program to plot the
    polar amplification by temperature by latitude band

    """

    if LINUX_WIN == 'w': 
        exptlsm = (FILESTART + 'regridded/PlioMIP2_Boundary_conds' +
                   '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'regridded/PlioMIP2_Boundary_conds' +
                   '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
    else:
        exptlsm = (FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh' +
                   '/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'PlioMIP2_Boundary_conds/Modern_std/' +
                   'Modern_std/Modern_std_LSM_v1.0.nc')


    ########################################################
    # setup: get the lsm for the land sea contrast plot

    exptland, exptsea = get_lsm(exptlsm)
    cntlland, cntlsea = get_lsm(cntllsm)
    fullmask = np.ones(np.shape(exptsea))
    

    #########################################################
    # need to get data from annual mean plot

    sea_anomaly = np.zeros((len(MODELNAMES)))
    sea_6060_anomaly = np.zeros((len(MODELNAMES))) # from 60N-60S
    all_anomaly = np.zeros((len(MODELNAMES)))
    
    for modelno, modeluse in enumerate(MODELNAMES):

        # get mean data
        (exptcube_SAT, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME, FIELD_SAT)
        (cntlcube_SAT, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME, FIELD_SAT)
        (exptcube_SST, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME, FIELD_SST)
        (cntlcube_SST, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME, FIELD_SST)
        
        # get all data
        expt_data = get_region(-90.0, 90.0, exptcube_SAT, 
                               fullmask, grid_areas_expt)
        cntl_data = get_region(-90.0, 90.0, cntlcube_SAT, 
                               fullmask, grid_areas_cntl)
        print(expt_data, cntl_data)
        all_anomaly[modelno] = expt_data - cntl_data
        

        # get sea anomaly
        expt_data = get_region(-90.0, 90.0, exptcube_SST, 
                               exptsea, grid_areas_expt)
        cntl_data = get_region(-90.0, 90.0, cntlcube_SST, 
                               cntlsea, grid_areas_cntl)
        sea_anomaly[modelno] = expt_data - cntl_data

        # get sea anomaly from 60N-60S
        expt_data = get_region(-60.0, 60.0, exptcube_SST, 
                               exptsea, grid_areas_expt)
        cntl_data = get_region(-60.0, 60.0, cntlcube_SST, 
                               cntlsea, grid_areas_cntl)
        sea_6060_anomaly[modelno] = expt_data - cntl_data

   # plot everything on one plot and write results to a text file
    filetext = open((DATAOUTSTART + '/data_for_sea_SAT_anomaly.txt'), "w+")
   
    scatter_sea_vs_global(all_anomaly, sea_anomaly, '', filetext)
    scatter_sea_vs_global(all_anomaly, sea_6060_anomaly, '_60N-60S_', filetext)
    
    filetext.close  

##########################################################
# DEFINITIONS

LINUX_WIN = 'l'
FIELD_SAT = 'NearSurfaceTemperature'
FIELD_SST = 'SST'
UNITS = 'degC'
EXPTNAME = 'EOI400'
CNTLNAME = 'E280'

MODELNAMES=['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]


if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    FILEOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD_SAT + '\\')
    DATAOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD_SAT + '\\')
else:
    FILESTART = '/nfs/hera1/earjcti/'
    FILEOUTSTART = '/nfs/hera1/earjcti/regridded/allplots/' + FIELD_SAT + '/'
    DATAOUTSTART = '/nfs/hera1/earjcti/regridded/alldata/'

FILEOUT = FILESTART + 'regridded/alldata/data_for_sea_SAT_relationships.txt'


main()
::::::::::::::
CEMAC/PLIOMIP2/statistical_significance_of_changes.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sep 21 2019

@author: earjcti

This will plot the MMM precipitation and temperature anomalies.  It will hatch the
statistically robust changes based on the followin method (Mba et al 2018)

1. Roughly 80% of models must agree on the direction of the change
   (if we have 13 models then 10 must agree)
2. The ((ensemble mean change) / (the ensemble standard deviation))>1.
   (note I am not sure whether to get the ensemble standard deviation from
    pi or the mPWP maybe I will try both)

"""

import sys
import iris
import iris.quickplot as qplt
import iris.plot as iplt
#from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
import netCDF4


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if field == 'SST' or field == 'NearSurfaceTemperature':
        if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
            cube.units = 'Celsius'
        else:
            cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    if modeluse == 'EC-Earth3.1' and field == 'SST':
        cube.coord('latitude').bounds = None
        cube.coord('longitude').bounds = None

    cube.cell_methods = None

    return cube



def get_ind_model_anomaly():
    """
    gets the anomaly data for all the models puts them into a list of cubes for
    returning to the calling program
    """

    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        cubepi = get_data(FILESTART + model + '/E280.' + FIELDNAME + '.allmean.nc',
                          FIELDNAME, model)
        cubeplio = get_data(FILESTART + model + '/EOI400.' + FIELDNAME + '.allmean.nc',
                            FIELDNAME, model)
        cubediff = cubeplio - cubepi
        cubelist.append(cubediff)


    return cubelist

def check_sign(cubelist):
    """
    we are checking that a number (NSIGN) of cubes has the same sign in a field
    for example if the field is temperature we may want to see that 10/13 models all show the
    same sign of temperature change

    input a list of cubes from models
    output a numpy array which is 1 where at least 'NSIGN' models have the same sign
           and 0 when they don't
    """

    cube = cubelist[0]
    ydim, xdim = np.shape(cube.data)
    posarr = np.zeros((ydim, xdim))
    negarr = np.zeros((ydim, xdim))

    for cubeno, cube in enumerate(cubelist):
        cubedata = cube.data
        temparr = np.ma.where(cubedata > 0, 1, 0) # temporary array
        posarr = posarr + temparr
        temparr = np.ma.where(cubedata < 0, 1, 0)
        negarr = negarr + temparr

    # if there are more than NSIGN elements in posarr or negarr than the same sign arr is set
    sign_arr = np.ma.where(posarr >= NSIGN, 1, 0)
    temparr = np.ma.where(negarr >= NSIGN, 1, 0)
    sign_arr = sign_arr + temparr
    newcube = cube.copy(data=sign_arr)

    return newcube

def check_large_anomaly(ratiocube):
    """
    if ratio is greater than 1 set to 1, otherwise set to zero
    """
    temparr = np.ma.where(ratiocube.data > 1, 1, 0)
    newcube = ratiocube.copy(data=temparr)

    return newcube


def main():
    """
    1. get the change in the field from all the models
    2. find out where > 70% of the models agree in the sign of the change (region1)
    3. get mean change in the field
    4. get the standard deviation for the control climate
    5. find out where mean change / standard deviation > 1 (region2)
    6. plot, hatching where region 1 and region2 are satisfied
    """

    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }

    modelcubelist = get_ind_model_anomaly()
    cube_sign = check_sign(modelcubelist) # checks where a given number of them are the correct sign
    meancube = iris.load_cube(FILESTART + FIELDNAME + '_multimodelmean.nc',
                              FIELDNAME + 'mean_anomaly')  # get mean
    stdevcube = iris.load_cube(FILESTART + FIELDNAME + '_multimodelmean.nc',
                               FIELDNAME + 'std_pi') # get standard deviation
    cube_mean_std_sign = check_large_anomaly(meancube / stdevcube)

    #qplt.contourf(cube_sign)
    ax = plt.axes(projection = ccrs.PlateCarree())
    if FIELDNAME == 'TotalPrecipitation':
       
        qplt.contourf(meancube, np.arange(-1.4, 1.6, 0.2), cmap='RdBu', extend='both')
        plt.figtext(0.02, 0.97,'d)',
                   horizontalalignment='left',
                   verticalalignment='top',
                   fontsize=20)
    else:
        qplt.contourf(meancube, np.arange(0, 5.5, 0.5), cmap='Reds', extend='both')
    iplt.contourf(cube_sign, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(cube_mean_std_sign, 1, hatches=[None, 3 * '\\\''], colors='none')
    plt.gca().coastlines()
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    plt.title(namefield.get(FIELDNAME) +' anomaly: multimodel mean')

    plt.savefig(OUTSTART + FIELDNAME + 'robust_anomalies.eps')
    plt.savefig(OUTSTART + FIELDNAME + 'robust_anomalies.pdf')
    plt.close()

    OUTNC = FILESTART + 'dummy.nc'
    if FIELDNAME == 'NearSurfaceTemperature':
        OUTNC = FILESTART + 'alldata/data_for_fig2.nc'
    if FIELDNAME == 'TotalPrecipitation':
        OUTNC = FILESTART + 'alldata/data_for_fig5d.nc'
        
    print(OUTNC)
    
    cubelist = iris.cube.CubeList([meancube, cube_sign, cube_mean_std_sign])
    iris.save(cubelist, OUTNC)        
    


    return



# variable definition
LINUX_WIN = 'l'

#FIELDNAME = 'NearSurfaceTemperature'
FIELDNAME = 'SST'
UNITS = 'deg C'

#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'


if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    OUTSTART = ' '


MODELNAMES = ['CCSM4-Utr', 'COSMOS', 'CESM1.2', 'CESM2','CCSM4',
              'EC-Earth3.3', 'GISS2.1G', 'HadCM3',
              'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
              'MIROC4m', 'MRI2.3',
              'NorESM-L', 'NorESM1-F',
              'CCSM4-UoT'
             ]
#ODELNAMES = ['EC-Earth3.1']
NSIGN = np.floor(len(MODELNAMES) * 0.8) # *0.8 is 80%

main()
::::::::::::::
CEMAC/PLIOMIP2/temperature_diagnostics.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#import Basemap
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match. 
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
       
    return cube  

def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
            
    # find line which contains 'modelnameglobal[mean_ocean_eoi400'
    # which is the start of the land sea contrast
    string = 'modelnameglobal[mean_ocean_eoi400'
    land_amplification = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, mean_ocean_eoi400, meanocean_e280, meanocean_anom, 
        mean_land_eoi400, mean_land_e280, mean_land_anom) = line.split(',')
        amp = np.float(mean_land_anom) / np.float(meanocean_anom)
        if model != 'MEAN':
            land_amplification.append(amp)
            
    land_amp_arr = np.asarray(land_amplification, dtype=float)
    
    # find line which contains 'modelname20N-20S[mean_ocean_eoi400'
    # which is the start of the land sea contrast over the tropics
    string = 'modelname20N-20S[mean_ocean_eoi400'
    index=0
    land_amplification_20 = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
     
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, mean_ocean_eoi400, meanocean_e280, meanocean_anom, 
        mean_land_eoi400, mean_land_e280, mean_land_anom) = line.split(',')
        amp = np.float(mean_land_anom) / np.float(meanocean_anom)
        if model != 'MEAN':
            land_amplification_20.append(amp)
            
    land_amp20_arr = np.asarray(land_amplification_20, dtype=float)
    
    # find line which contains '45N-90N_anom'
    # which is the start of the fields averaged over certain regions
    string = '45N-90N_anom'
    index=0
    NH_SH_ratio45 = []
    PA_NH_60 = []
    PA_SH_60 = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
     
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, anom_45_90N, anom_45_90S, 
         anom_60_90N, anom_60_90S) = line.split(',')
        NH_SH_ratio = np.float(anom_45_90N) / np.float(anom_45_90S)
      
        if model != 'MEAN':
            NH_SH_ratio45.append(NH_SH_ratio)
            PA_NH_60.append(anom_60_90N)
            PA_SH_60.append(anom_60_90S)
   
    NH_SH_ratio45_arr = np.asarray(NH_SH_ratio45, dtype=float)
    PA_NH_60_arr = np.asarray(PA_NH_60, dtype = float)
    PA_SH_60_arr = np.asarray(PA_SH_60, dtype = float)
    
        
    return (land_amp_arr, land_amp20_arr, NH_SH_ratio45_arr, 
            PA_NH_60_arr, PA_SH_60_arr)

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\'+field+'\\'
        exptlsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
        cntllsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
    else:
        fileoutstart='/nfs/hera1/earjcti/regridded/allplots/'+field+'/'
        dataoutstart='/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm=FILESTART+'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
        cntllsm=FILESTART+'PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
     
 
    ########################################################
    # setup: get the lsm for the land sea contrast plot
    
    tempcube=iris.load_cube(exptlsm)
    cubegrid=iris.load_cube('one_lev_one_deg.nc')
    exptlsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
    exptsea=(exptlsmcube.data - 1.0)*(-1.0)
    
   
    tempcube=iris.load_cube(cntllsm)
    cntllsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
    cntlsea=(cntllsmcube.data - 1.0)*(-1.0)
    
 
    #########################################################
    # need to get data from annual mean plot
    
    nh_anomaly=np.zeros(len(modelnames))
    sh_anomaly=np.zeros(len(modelnames))
    nh_anomaly_extratropics=np.zeros(len(modelnames))
    sh_anomaly_extratropics=np.zeros(len(modelnames))
    nh_anomaly_polar=np.zeros(len(modelnames))
    sh_anomaly_polar=np.zeros(len(modelnames))
    all_anomaly=np.zeros(len(modelnames))
    land_anomaly=np.zeros(len(modelnames))
    sea_anomaly=np.zeros(len(modelnames))
    land_anomaly_tropics=np.zeros(len(modelnames))
    sea_anomaly_tropics=np.zeros(len(modelnames))
    
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+'regridded/'+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+'regridded/'+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        cube=iris.load_cube(exptfile)
        exptcube = resort_coords(cube)
        cube=iris.load_cube(cntlfile)
        cntlcube = resort_coords(cube)
    
        #######################################
        # compare NH vs SH temperature
        exptcube.coord('latitude').guess_bounds()
        exptcube.coord('longitude').guess_bounds()
        cntlcube.coord('latitude').guess_bounds()
        cntlcube.coord('longitude').guess_bounds()
        grid_areas_expt = iris.analysis.cartography.area_weights(exptcube)
        grid_areas_cntl = iris.analysis.cartography.area_weights(cntlcube)
        
        # exptcube
        nlat=len(exptcube.coord('latitude').points)
        lats=exptcube.coord('latitude').points
        
        grid_areas_nh=np.zeros(grid_areas_expt.shape)
        grid_areas_sh=np.zeros(grid_areas_expt.shape)
        grid_areas_nh_extratropics=np.zeros(grid_areas_expt.shape)
        grid_areas_sh_extratropics=np.zeros(grid_areas_expt.shape)
        grid_areas_nh_polar=np.zeros(grid_areas_expt.shape)
        grid_areas_sh_polar=np.zeros(grid_areas_expt.shape)
        polarval=60.0
        for j in range(0,nlat):
            if lats[j] <0:
                grid_areas_sh[j,:]=grid_areas_expt[j,:]
            else:
                grid_areas_nh[j,:]=grid_areas_expt[j,:]
            if lats[j] < -45.0:
                grid_areas_sh_extratropics[j,:]=grid_areas_expt[j,:]
            if lats[j] > 45.0:
                grid_areas_nh_extratropics[j,:]=grid_areas_expt[j,:]
            if lats[j] <= -1.0*polarval:
                grid_areas_sh_polar[j,:]=grid_areas_expt[j,:]
            if lats[j] >= polarval:
                grid_areas_nh_polar[j,:]=grid_areas_expt[j,:]
           
        expt_nh = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh)
        expt_sh = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh)
        expt_anom = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN,weights=grid_areas_expt)
        expt_nh_extratropics = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_extratropics)
        expt_sh_extratropics = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_extratropics)
        expt_nh_polar = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_polar)
        expt_sh_polar = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_polar)
        
        
        
        #cntlcube
        nlat=len(cntlcube.coord('latitude').points)
        lats=cntlcube.coord('latitude').points
        grid_areas_nh=np.zeros(grid_areas_cntl.shape)
        grid_areas_sh=np.zeros(grid_areas_cntl.shape)
        for j in range(0,nlat):
            if lats[j] <0:
                grid_areas_sh[j,:]=grid_areas_cntl[j,:]
            else:
                grid_areas_nh[j,:]=grid_areas_cntl[j,:]
           
        cntl_nh = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh)
        cntl_sh = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh)
        cntl_anom = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN,weights=grid_areas_cntl)
        cntl_nh_extratropics = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_extratropics)
        cntl_sh_extratropics = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_extratropics)
        cntl_nh_polar = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_polar)
        cntl_sh_polar = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_polar)
        
        
        nh_anomaly[modelno]=expt_nh.data-cntl_nh.data
        sh_anomaly[modelno]=expt_sh.data-cntl_sh.data
        nh_anomaly_extratropics[modelno]=expt_nh_extratropics.data-cntl_nh_extratropics.data
        sh_anomaly_extratropics[modelno]=expt_sh_extratropics.data-cntl_sh_extratropics.data
        nh_anomaly_polar[modelno]=expt_nh_polar.data-cntl_nh_polar.data
        sh_anomaly_polar[modelno]=expt_sh_polar.data-cntl_sh_polar.data
        all_anomaly[modelno]=expt_anom.data-cntl_anom.data
      
        
        #######################################
        # compare land with sea (globally and for tropics)
        
        
        # expt
        # first check grid
        for i in range(0,len(exptcube.coord('latitude').points)):
            if exptcube.coord('latitude').points[i] !=exptlsmcube.coord('latitude').points[i]:
                print('differences in lsm and gridded data',i)
                sys.exit(0)
       
    
        for i in range(0,len(exptcube.coord('longitude').points)):    
            if exptcube.coord('longitude').points[i] != exptlsmcube.coord('longitude').points[i]:
                print('differences in lsm and gridded data',i,exptcube.coord('longitude').points[i],
                  exptlsmcube.coord('longitude').points[i])
                sys.exit(0)
            
        grid_areas_land=grid_areas_expt * exptlsmcube.data  
        grid_areas_sea=grid_areas_expt * exptsea
        
        expt_land = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_land)
        expt_sea = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sea)
        
        # get grid areas and experiment means for tropics
        nlat=len(exptcube.coord('latitude').points)
        lats=exptcube.coord('latitude').points
        grid_areas_land_tropics=np.zeros(grid_areas_land.shape)
        grid_areas_sea_tropics=np.zeros(grid_areas_sea.shape)
        for j in range(0,nlat):
            if ((lats[j] < 20.) and (lats[j] > -20.)):
                grid_areas_land_tropics[j,:]=grid_areas_land[j,:]
                grid_areas_sea_tropics[j,:]=grid_areas_sea[j,:]
    
        expt_land_tropics = exptcube.collapsed(['longitude', 'latitude'],
                                               iris.analysis.MEAN, weights=grid_areas_land_tropics)
        expt_sea_tropics = exptcube.collapsed(['longitude', 'latitude'], 
                                              iris.analysis.MEAN, weights=grid_areas_sea_tropics)
    
        # cntl
        # first check grid
        for i in range(0,len(cntlcube.coord('latitude').points)):
            if cntlcube.coord('latitude').points[i] !=cntllsmcube.coord('latitude').points[i]:
                print('differences in lsm and gridded data',i)
                sys.exit(0)
       
    
        for i in range(0,len(cntlcube.coord('longitude').points)):    
            if cntlcube.coord('longitude').points[i] != cntllsmcube.coord('longitude').points[i]:
                print('differences in lsm and gridded data',i,cntlcube.coord('longitude').points[i],
                  cntllsmcube.coord('longitude').points[i])
                sys.exit(0)
            
        grid_areas_land=grid_areas_cntl * cntllsmcube.data  
        grid_areas_sea=grid_areas_cntl * cntlsea
    
        cntl_land = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_land)
        cntl_sea = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sea)
    
        # get grid areas and experiment means for tropics
        nlat=len(cntlcube.coord('latitude').points)
        lats=cntlcube.coord('latitude').points
        grid_areas_land_tropics=np.zeros(grid_areas_land.shape)
        grid_areas_sea_tropics=np.zeros(grid_areas_sea.shape)
        for j in range(0,nlat):
            if ((lats[j] < 20.) and (lats[j] > -20.)):
                grid_areas_land_tropics[j,:]=grid_areas_land[j,:]
                grid_areas_sea_tropics[j,:]=grid_areas_sea[j,:]
    
        cntl_land_tropics = cntlcube.collapsed(['longitude', 'latitude'],
                                               iris.analysis.MEAN, weights=grid_areas_land_tropics)
        cntl_sea_tropics = cntlcube.collapsed(['longitude', 'latitude'], 
                                              iris.analysis.MEAN, weights=grid_areas_sea_tropics)
    
    
        land_anomaly[modelno]=expt_land.data-cntl_land.data
        sea_anomaly[modelno]=expt_sea.data-cntl_sea.data
    
        land_anomaly_tropics[modelno]=expt_land_tropics.data-cntl_land_tropics.data
        sea_anomaly_tropics[modelno]=expt_sea_tropics.data-cntl_sea_tropics.data
    
    # get data from PlioMIP1 if required
    if PLIOMIP1 == 'y':
        (pliomip1_landsea_amp, 
         pliomip1_tropics_landsea_amp,
         pliomip1_NH_SH_ratio45,
         pliomip1_PA_NH,
         pliomip1_PA_SH) = get_pliomip1_data(field)
   
    
    # plot NH SH contrast
    ax=plt.subplot(1,1,1)
    ax.plot(modelnames,nh_anomaly,'x',label='NH anomaly')
    ax.plot(modelnames,sh_anomaly,'x',label='SH anomaly')
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    plt.xticks(rotation='90')
    plt.title('mPWP - PI anomaly for each hemisphere')
    fileout=fileoutstart+'/hemisphere_difference.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/hemisphere_difference.pdf'
    plt.savefig(fileout)
    plt.close()
    
    
    # plot NH SH contrast for extratropics
    ax=plt.subplot(2,1,1)
    ax.plot(modelnames,nh_anomaly_extratropics,'x',label='>45N anom')
    ax.plot(modelnames,sh_anomaly_extratropics,'x',label='<45S anom')
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    #plt.xticks(rotation='45')
    ax.tick_params(axis='x',labelbottom='False')
    plt.title('Plio_core - PI_Ctl; extratropical NH/SH anomaly')
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,nh_anomaly_extratropics/sh_anomaly_extratropics
            ,'x',label='>45N/<45S')
    ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
    
    if PLIOMIP1 == 'y':
       for mod_p1 in pliomip1_NH_SH_ratio45:
            ax.axhline(y=mod_p1, color='grey', alpha=0.4)
    
    
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.4*box.height), box.width * 0.8, (0.9*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel('factor')
    plt.xticks(rotation='90')
    #plt.title('mPWP - PI anomaly')
    
    fileout=fileoutstart+'/hemisphere_difference_extratropics.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/hemisphere_difference_extratropics.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out hemisphere difference extratropics
    
    txtout = open(FILEOUT, "w+") 
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3c \n')
        
        writedata = ("model_name, nh_anom_et, sh_anom_et \n")
    
        txtout.write(writedata)
        for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(nh_anomaly_extratropics[i],2)) + ',' + 
                         np.str(np.around(sh_anomaly_extratropics[i],2)) + '\n')
            txtout.write(writedata)
   
     # plot polar amplification 
    ax=plt.subplot(1,1,1)
    labelname='>'+np.str(np.int(polarval))+'N amplification'
    ax.plot(modelnames,nh_anomaly_polar/all_anomaly,'x',label=labelname)
    labelname='<'+np.str(np.int(polarval))+'S amplification'
    ax.plot(modelnames,sh_anomaly_polar/all_anomaly,'x',label=labelname)
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.7, (0.7*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel('factor')
    plt.xticks(rotation='90')
    ax.axhline(y=1.0, xmin=0.0, xmax=len(modelnames), color='r')
    #ax.tick_params(axis='x',labelbottom='False')
    plt.title('Plio_Core - PI_Ctl; polar amplification factor')
    plt.figtext(0.02, 0.97,'d)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    print('NH polar amplification', np.mean(nh_anomaly_polar) / np.mean(all_anomaly))
    print('SH polar amplification', np.mean(sh_anomaly_polar) / np.mean(all_anomaly))
    print('total polar amplification', np.mean(nh_anomaly_polar + sh_anomaly_polar) / (2.0 * np.mean(all_anomaly)))
    print('all polar amplification', (nh_anomaly_polar + sh_anomaly_polar) / (2.0 * all_anomaly))
    nh_amps = [x1 / x2 for (x1, x2) in zip(nh_anomaly_polar, all_anomaly)] 
    sh_amps = [x1 / x2 for (x1, x2) in zip(sh_anomaly_polar, all_anomaly)] 
    print('nh amps unsorted', nh_amps)
    print('nh amps',np.sort(nh_amps))
    print('nh median', np.median(nh_amps))
    print('sh median', np.median(sh_amps))
    print('nh percentiles 10/50/90',np.percentile(nh_amps, 10),
          np.percentile(nh_amps,50), np.percentile(nh_amps,90))
    print('sh percentiles 10/50/90',np.percentile(sh_amps, 10),
          np.percentile(sh_amps,50), np.percentile(sh_amps,90))
   
   
    
    fileout=fileoutstart+'/polar_amplification_'+np.str(np.int(polarval))+'.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/polar_amplification_'+np.str(np.int(polarval))+'.pdf'
    plt.savefig(fileout)
    plt.close()
    
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3d \n')
        
        writedata = ("model_name, nh_anom_polar, sh_anom_polar, global_anom \n")
    
        txtout.write(writedata)
        for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(nh_anomaly_polar[i],2)) + ',' + 
                         np.str(np.around(sh_anomaly_polar[i],2)) + ',' + 
                         np.str(np.around(all_anomaly[i],2)) + 
                         '\n')
            txtout.write(writedata)
    
    
    # plot land sea contrast
    if field != 'TotalPrecipitation':
        ax=plt.subplot(2, 1, 1)
    else:
        ax = plt.subplot(1, 1, 1)
    ax.plot(modelnames,land_anomaly,'x',label='Land anomaly')
    ax.plot(modelnames,sea_anomaly,'x',label='Sea anomaly')
    box = ax.get_position()
    if field != 'TotalPrecipitation':
        ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
        ax.tick_params(axis='x',labelbottom='False')
        plt.figtext(0.02, 0.97,'b)',
                       horizontalalignment='left',
                       verticalalignment='top',
                       fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    plt.ylabel(units)
   
    #plt.xticks(rotation='45')
    plt.title('Plio_Core - PI_Ctrl global land-sea anomaly')
    
    
    # plot land sea contrast tropics]
    
    ax2=plt.subplot(2,1,2)
    ax2.plot(modelnames,land_anomaly_tropics,'x',label='Land anomaly')
    ax2.plot(modelnames,sea_anomaly_tropics,'x',label='Sea anomaly')
    box = ax2.get_position()
    ax2.set_position([box.x0, box.y0+(0.4*box.height), box.width * 0.8, (0.9*box.height)])
    ax2.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    plt.xticks(rotation='90')
    plt.title('Plio_Core - PI_Ctrl 20N-20S land-sea anomaly')
    
    print('sea anomaly tropics',np.mean(sea_anomaly_tropics))

    fileout=fileoutstart+'/land_sea_contrast.pdf'
    plt.savefig(fileout)
    fileout=fileoutstart+'/land_sea_contrast.eps'
    plt.savefig(fileout)
    plt.close()
    
    # write out
    
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3b \n')
    else:
        txtout.write('data for 6b \n' )
        
    writedata = ("model_name, land_anomaly, sea_anomaly, " + 
                 "tropical_land_anomaly, tropical_sea_anomaly\n")
    
    
    txtout.write(writedata)
    for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(land_anomaly[i],2)) + ',' + 
                         np.str(np.around(sea_anomaly[i],2)) + ',' + 
                         np.str(np.around(land_anomaly_tropics[i],2)) + ',' + 
                         np.str(np.around(sea_anomaly_tropics[i],2)) + '\n')
            print(writedata)
            txtout.write(writedata)
        
    txtout.close
    print('mean land anomaly', np.mean(land_anomaly))
    print('mean sea anomaly', np.mean(sea_anomaly))
    
    ######################################
    # plot land sea contrast as a factor
    factor_land=land_anomaly / sea_anomaly
    factor_land_tropics=land_anomaly_tropics / sea_anomaly_tropics
   
    if field != 'NearSurfaceTemperature':
        ax=plt.subplot(2,1,1)
    else:
        ax = plt.subplot(1, 1, 1)
    ax.plot(modelnames,factor_land,'x')
    ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
    if PLIOMIP1 == 'y':
        for mod_amp in pliomip1_landsea_amp:
            ax.axhline(y=mod_amp, color='grey', alpha=0.4)
           
    box = ax.get_position()
    if field != 'NearSurfaceTemperature':
        ax.set_position([box.x0 + (0.1 * box.width),
                         box.y0+(0.3*box.height), 
                         box.width * 0.8, (0.8*box.height)])
        ax.tick_params(axis='x',labelbottom='False')
    else:
        ax.set_position([box.x0, box.y0+(0.3*box.height), 
                         box.width, (0.7*box.height)])
    plt.figtext(0.02, 0.97,'b)',
                horizontalalignment='left',
                verticalalignment='top',
                fontsize=20)
    plt.xticks(rotation='90')
    plt.ylabel('land amplification')
   
    #plt.xticks(rotation='45')
    plt.title('mPWP - PI; land_anomaly / sea anomaly')
    
    if field != 'NearSurfaceTemperature':
        ax=plt.subplot(2,1,2)
        ax.plot(modelnames,factor_land_tropics,'x')
        ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
        if PLIOMIP1 == 'y':
            for mod_amp in pliomip1_tropics_landsea_amp:
                ax.axhline(y=mod_amp, color='grey', alpha=0.4)
        
        box = ax.get_position()
        ax.set_position([box.x0 + (0.1 * box.width), 
                         box.y0+(0.3*box.height), 
                        box.width * 0.8, (0.9*box.height)])
        plt.ylabel('land amplification')
        plt.xticks(rotation='90')
        plt.title('mPWP - PI (20N-20S); land_anomaly / sea anomaly')
        plt.figtext(0.02, 0.97,'b)',
                   horizontalalignment='left',
                   verticalalignment='top',
                   fontsize=20)
    fileout=fileoutstart+'/land_sea_amplification.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/land_sea_amplification.pdf'
    plt.savefig(fileout)
    plt.close()
   
    
   
   
    
   
    
    
    
        
################################################################
def plotmap(modelnames,field,exptname,cntlname,linux_win,units):  
    # this subprogram will 
    # 1. read in the data from each map.  
    # 2. It will normalise the data by subtracting the mean and dividing by the spatial standard deviation
    # 3. it will then plot the field change by number of standard deviations above and below the m2an

    normalized_cubes=iris.cube.CubeList([])
    standard_dev=[]
    if linux_win=='w':
        FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
        fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\'+field+'\\'
        exptlsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        FILESTART='/nfs/hera1/earjcti/'
        fileoutstart='/nfs/hera1/earjcti/regridded/allplots/'+field+'/'
        dataoutstart='/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm=FILESTART+'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
      
    
    lsmcube=iris.load_cube(exptlsm)
        
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        print(modeluse)
        exptfile=FILESTART+'regridded/'+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+'regridded/'+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        cube=iris.load_cube(exptfile)
        exptcube = resort_coords(cube)
        exptcube.data=exptcube.data.astype('float32') # change to float32 for concatentation later
       
       
        for coord in exptcube.coords():
            if coord.standard_name !='longitude' and coord.standard_name !='latitude':
                exptcube.remove_coord(coord)
                
        cube=iris.load_cube(cntlfile)
        cntlcube = resort_coords(cube)
        cntlcube.data=cntlcube.data.astype('float32')
        
        for coord in cntlcube.coords():
            if coord.standard_name !='longitude' and coord.standard_name !='latitude':
                cntlcube.remove_coord(coord)
    
        
        diffcube=exptcube-cntlcube
       
        #print(exptcube.coord('surface'))
      
        #######################################
        # get mean and spatial standard deviation
        diffcube.coord('latitude').guess_bounds()
        diffcube.coord('longitude').guess_bounds()
       
        grid_areas_diff = iris.analysis.cartography.area_weights(diffcube)
       
        diffmean = diffcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_diff)
        diffsd = diffcube.collapsed(['longitude', 'latitude'], iris.analysis.STD_DEV)
        
        standard_dev.append(diffsd.data)
       
        ########################################
        # normalise the cube by dividing by the mean and std dev
        # and plot
        
        normcube=(diffcube-diffmean)/diffsd
        print(normcube)
      
        
        V=np.arange(-4,4.5,0.5)
        mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
        newcolors=mycmap(np.linspace(0,1,len(V+2)))
        white=([1,1,1,1])
        print((len(V)/2)-1,(len(V)/2)+2)
        print((np.ceil(len(V)/2)-1),np.floor((len(V)/2)+2))
        
        newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
        
        mycmap=ListedColormap(newcolors)
           
        #Draw the contour w
       
        qplt.contourf(normcube, V,extend='both',cmap=mycmap)
        #qplt.contourf(normcube, V,extend='both',cmap='RdBu_r')
        
       
        #sys.exit()
        qplt.contour(lsmcube,1,colors='black')
        plt.title(modeluse+': '+field+'\n No of stddev from mean'
                  +'('+(np.str(np.round(diffsd.data,1)))+units+')')

        if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/'+modeluse+'.eps'
            plt.savefig(fileout)
            
        fileout=fileoutstart+'Map_normalised//'+modeluse+'.pdf'
        plt.savefig(fileout)
        plt.close()
        
        #normcube.Coord(1, standard_name='model', long_name='model', var_name='model', units='1', 
       #                       bounds=None, attributes=None, coord_system=None)
        tempcube=iris.util.new_axis(normcube)
        tempcube.add_dim_coord(iris.coords.DimCoord(modelno, 
                standard_name='model_level_number', long_name='model', 
                var_name='model', 
                units=None,
                bounds=None,
                coord_system=None, circular=False),0)
        # tempcube needs to be dtype=float64
        
        
      
        normalized_cubes.append(tempcube)
        
    
    ################################################
    # END OF MODEL LOOP
    equalise_attributes(normalized_cubes)
   
    print(normalized_cubes)
    print(normalized_cubes[0])
    print(normalized_cubes[1])
    print(normalized_cubes[0].dtype)
    print(normalized_cubes[1].dtype)
    allnormcube=normalized_cubes.concatenate_cube()
    
    meancube=allnormcube.collapsed(['model_level_number'], iris.analysis.MEAN)
    maxcube=allnormcube.collapsed(['model_level_number'], iris.analysis.MAX)
    mincube=allnormcube.collapsed(['model_level_number'], iris.analysis.MIN)
    mediancube=allnormcube.collapsed(['model_level_number'], iris.analysis.MEDIAN)
    
    ###########################
    # plot the mean value
    
    V=np.arange(-2.5,2.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    print(standard_dev)
    print(np.amin(standard_dev))
    print(np.amax(standard_dev))
    print(minval,maxval)
    #sys.exit(0)
    plt.title(field+'\n Mean No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/meandiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//meandiff.pdf'
    plt.savefig(fileout)
    plt.close()
    plt.show()
        
    
    ###########################
    # plot the median value
    
        
    qplt.contourf(mediancube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Median No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/mediandiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//mediandiff.pdf'
    plt.savefig(fileout)
    plt.close()
    plt.show()
        
    
    ###########################
    # plot the maximum value
    
    V=np.arange(-4.5,4.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(maxcube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Maximum No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/maxdiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//maxdiff.pdf'
    plt.savefig(fileout)
    plt.close()
   
    
    ###########################
    # plot the minimum value
    
    V=np.arange(-2.5,2.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(mincube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Minimum No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/mindiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//mindiff.pdf'
    plt.savefig(fileout)
    plt.close()
   

        
         


##########################################################
# main program

filename=' '
linux_win='l'
if linux_win=='w':
    FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART='/nfs/hera1/earjcti/'
        

modelnames=['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]


#modelnames=['HadCM3','NorESM-L']
fieldnames=['NearSurfaceTemperature']
#units=['degC']
#
#fieldnames=['TotalPrecipitation']
units=['mm/day']
exptname='EOI400'
cntlname='E280'
PLIOMIP1 = 'n'

for field in range(0,len(fieldnames)):
    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'regridded/alldata/data_for_6b.txt'
        
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'regridded/alldata/data_for_3b_3c_3d.txt'
       
    
    # will plot NH vs SHPlio_enh_LSM_v1.0Plio_enh_LSM_v1.0
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])
     
    # plotmap will show where the field is larger or smaller than the mean  
    plotmap(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)
::::::::::::::
CEMAC/PLIOMIP2/temperature_gradients.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the SST zonal and meridional gradients acropss the atlantic and 
the pacific

"""

import sys
import iris
import iris.quickplot as qplt
#from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    print(modeluse)
    if modeluse == 'MMM':
        print(filereq,field)
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def get_within_region(cube, region):
    """
    This routine will mask out all the regions that are not 
    input: a temperature cube, a region name
    output: a new cube with all data not within the region masked out.
    """


    if region == 'TROPATL':
        latmin = -20.
        latmax = 20.
        lonmin = 300.
        lonmax = 360.
        
    if region == 'TROPPAC':
        latmin = -20.
        latmax = 20.
        lonmin = 140.
        lonmax = 260.
        
    if region == 'MERIDATL':
        latmin = -70.
        latmax = 70.
        lonmin = 290.
        lonmax = 360.
        
    if region == 'MERIDPAC':
        latmin = -70.
        latmax = 60.
        lonmin = 150.
        lonmax = 260.
        
        
    cubedata = cube.data
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points
    

    for j in range(0, len(lats)):
        cubedata[j, :] = np.ma.masked_where(lons < lonmin, 
                                            cubedata[j, :])
        cubedata[j, :] = np.ma.masked_where(lons > lonmax, 
                                            cubedata[j, :])
  
        
    for i in range(0, len(lons)):
        cubedata[:, i] = np.ma.masked_where(lats < latmin, 
                                            cubedata[:, i])
        cubedata[:, i] = np.ma.masked_where(lats > latmax, 
                                            cubedata[:, i])
        
    newcube = cube.copy(data=cubedata)
    
    #if region == 'MERIDPAC':
    #plot
    #    contour = qplt.contourf(newcube)
    #    plt.gca().coastlines()
    #    plt.show()
    #    sys.exit(0)

    return [newcube, lonmin, lonmax, latmin, latmax]



class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    it can either plot the zonal mean or the meridional mean
    """
    def __init__(self, region, timeperiod, lonmin, lonmax, latmin, latmax):

        """
        inputs are:
                    
        """

        fullregname  =  {
                         "TROPATL" : "Atlantic ",
                         "TROPPAC" : "Pacific ",
                         "MERIDATL" : "Atlantic ",
                         "MERIDPAC": "Pacific "}
    
        colorreq = {
                    "mPWP" : "red",
                    "pi" : "blue",
                    "mPWP-pi" : "green"}
        
        hatchreq = {
                    "mPWP" : "",
                    "pi" : "",
                    "mPWP-pi" : ""}
        
        self.region = region
        self.timeperiod = timeperiod
        self.regiontitle = fullregname.get(region)
        self.colorreq = colorreq.get(timeperiod)
        self.hatchreq = hatchreq.get(timeperiod)


        if region == 'TROPATL':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(latmin)) + 
            #                    '-' + np.str(np.int(latmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 10.
            else:
                self.valmin = 20.
                self.valmax = 30.
                
        if region == 'TROPPAC':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(latmin)) + '-' 
            #                    + np.str(np.int(latmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 10.
            else:
                self.valmin = 20.
                self.valmax = 32.
                
        if region == 'MERIDATL':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(lonmin)) + 
            #                    '-' + np.str(np.int(lonmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = -0.
                self.valmax = 20.
            else:
                self.valmin = -10.
                self.valmax = 30.
                
        if region == 'MERIDPAC':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(lonmin)) + '-' 
            #                    + np.str(np.int(lonmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 20.
            else:
                self.valmin = -5.
                self.valmax = 35.
            



    def plotzm(self, cube, cubemin, cubemax, ax, colorname):
        """
        plot the zonal mean
        """
    

        cube_zm = cube.collapsed('latitude', iris.analysis.MEAN)
        cube_zm_min = cubemin.collapsed('latitude', iris.analysis.MEAN)
        cube_zm_max = cubemax.collapsed('latitude', iris.analysis.MEAN)
        lons = cube_zm.coord('longitude').points
        
        #cube_zm_var = cubevar.collapsed('latitude', iris.analysis.MEAN)
        #zm_std = np.sqrt(cube_zm_var.data)
        #cube_zm_2sigma = cube_zm_var.copy(data=zm_std * 2)
        ax.plot(lons, cube_zm.data, label=self.timeperiod, color=self.colorreq)
        ax.fill_between(lons, cube_zm_min.data, cube_zm_max.data, alpha=0.2, 
                        color=self.colorreq,
                        hatch=self.hatchreq)
        plt.title('20N-20S Mean SST:' +  self.regiontitle)
        ax.set_xlabel('longitude')
        ax.set_ylabel('deg C', color=colorname)
        ax.set_ylim(self.valmin, self.valmax)
        ax.tick_params(axis='y', labelcolor=colorname)
        
    
        return

    def plotmm(self, cube, cubemin, cubemax, ax, colorname, fig):
        """
        plot the meridional mean from the cube
        """

        cube_mm = cube.collapsed('longitude', iris.analysis.MEAN)
        cube_mm_min = cubemin.collapsed('longitude', iris.analysis.MEAN)
        cube_mm_max = cubemax.collapsed('longitude', iris.analysis.MEAN)
        lats = cube_mm.coord('latitude').points
        
        for i, lat in enumerate(lats):
            print(lat, cube_mm.data[i], cube_mm_min.data[i], cube_mm_max.data[i])
        ax.plot(cube_mm.data, lats, label=self.timeperiod, color=self.colorreq)
        ax.fill_betweenx(lats, cube_mm_min.data, cube_mm_max.data, alpha=0.2, 
                        color=self.colorreq,
                        hatch=self.hatchreq)
        fig.suptitle('Zonal Mean SST: ' +  self.regiontitle)
        ax.set_xlabel('deg C', color=colorname)
        ax.set_ylabel('latitude')
        ax.set_xlim(self.valmin, self.valmax)
        ax.tick_params(axis='x', labelcolor=colorname)
   
    
        return


# emd of class

def main_time(timeperiod, region):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = (FILESTART + FIELDNAME + '_multimodelmean.nc')

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')
    
    max_cube = get_data(filename, FIELDNAME + 'max_' + timeperiod, 
                         'MMM')
    
    min_cube = get_data(filename, FIELDNAME + 'min_' + timeperiod, 
                         'MMM')
    
    if timeperiod == 'anomaly':
        sd_cube = get_data(filename, 'SSTanomaly_multimodel_stddev', 
                         'MMM')
    else:
        sd_cube = get_data(filename, FIELDNAME + 'std_' + timeperiod, 
                         'MMM')
    variance_cube = np.square(sd_cube)

    # get the mean value within the region and the standard deviation within the 
    # region
    regioncube, lonmin, lonmax, latmin, latmax = get_within_region(mean_cube, region)
    regioncubemax, lonmin, lonmax, latmin, latmax = get_within_region(max_cube, region)
    regioncubemin, lonmin, lonmax, latmin, latmax = get_within_region(min_cube, region)
    regionvariance, lonmin, lonmax, latmin, latmax = get_within_region(variance_cube, region)


    return regioncube, regionvariance, regioncubemin, regioncubemax, lonmin, lonmax, latmin, latmax

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    cubelist = iris.cube.CubeList([])
    
    figno =  {"MERIDATL": "a)",
            "MERIDPAC": "b)",
            "TROPATL": "c)",
            "TROPPAC": "d)"
    }
    
    for j, region in enumerate(REGIONNAMES):
        
        
        
        # get cubes for each timeperiod and plot
        fig, ax1 = plt.subplots() 
        for i in range(0, len(TIMEPERIODS)):
            (cube_time_region, cube_region_variance, 
             cube_region_min, cube_region_max,
             lonmin, lonmax, latmin, latmax)  = main_time(TIMEPERIODS[i], region)
            
            cubelist.append(cube_time_region)
            cubelist.append(cube_region_min)
            cubelist.append(cube_region_max)
       
            plobj = Plotalldata(region, TIMEPERIODS[i], lonmin, lonmax, latmin, latmax)
            if region[0:3] == 'TRO': 
                plobj.plotzm(cube_time_region, cube_region_min,
                             cube_region_max,
                             ax1, 'black')
                outname = OUTSTART + 'zonalmean' + region
                
    
            
                
            if region[0:3] == 'MER':             
                plobj.plotmm(cube_time_region, cube_region_min,
                             cube_region_max,
                             ax1,'black', fig)
                outname = OUTSTART + 'mm_' + region
        
       
        box = ax1.get_position()
        #print(box)
        #sys.exit(0)
        #ax1.set_position([box.x0+(box.height*0.2), box.y0, 1.0, 1.0])
        # plot zonal or meridional mean for the difference
        
        (cubediff_region, cubediff_region_variance, 
         cubediff_region_min, cubediff_region_max,
             lonmin, lonmax, latmin, latmax)  = main_time('anomaly', region)
        print(np.mean(cubediff_region_max.data))
       
        
        plobjdiff = Plotalldata(region, TIMEPERIODS[1] + '-' + TIMEPERIODS[0],
                                lonmin, lonmax, latmin, latmax)
       
        if region[0:3] == 'TRO':             
           ax2 = ax1.twinx() 
           
           plobjdiff.plotzm(cubediff_region, cubediff_region_min,
                            cubediff_region_max, ax2, 'green') 
           
           print('julia',np.mean(cubediff_region.data), 
                 np.mean(cubediff_region_min.data),
                 np.mean(cubediff_region_max.data)
                 )
           #sys.exit(0)
           
           
        if region[0:3] == 'MER':             
            ax2 = ax1.twiny() 
            ax2.xaxis.set_ticks_position("bottom")
            ax2.xaxis.set_label_position("bottom")
            ax2.spines["bottom"].set_position(("axes", -0.2))
            plobjdiff.plotmm(cubediff_region, cubediff_region_min,
                             cubediff_region_max, ax2, 'green', fig) 
           
         # plot the legend and close the plot 
        
        #fig.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        #fig.legend()
        plt.figtext(0.02, 0.97,figno.get(region),
                    horizontalalignment='left',
                    verticalalignment='top',
                    fontsize=20)
        #plt.subplots_adjust(top=0.85)
        fig.tight_layout(rect=[0, 0.03, 1, 0.95])
        #plt.show()
        #sys.exit(0)
        plt.savefig(outname + '.eps')
        plt.savefig(outname + '.pdf')
       
        iris.save(cubelist, OUTWRITE, netcdf_format='NETCDF3_CLASSIC', fill_value=1.0E20)        
    
       
    return    
        
        

# variable definition
LINUX_WIN = 'l'
FIELDNAME = 'SST'

if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    OUTWRITE = FILESTART + 'alldata/data_for_fig4.nc'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    OUTSTART = FILESTART + 'allplots\\' + FIELDNAME + '\\'
    OUTWRITE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_fig4.nc'
    

REGIONNAMES = ['TROPATL', 'TROPPAC', 'MERIDATL', 'MERIDPAC']
#REGIONNAMES = ['TROPATL']

UNITS = 'deg C'
#TIMEPERIODS = ['pi']
TIMEPERIODS = ['pi', 'mPWP']


main()
::::::::::::::
CEMAC/PLIOMIP2/temperature_isotherms_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the multimodel mean near surface air temperature with
+/-2 degree isotherms marked out.  It is to understand which regions
are likely to support thermofrost

"""

import sys
import iris
from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def plotmean_newaxis(cube, modelno_):
    """
    this will add a new axis to the cube which contains the model number
    this is needed for concatenation
    """

    tempcube_ = iris.util.new_axis(cube)
    tempcube_.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                 standard_name='model_level_number',
                                                 long_name='model',
                                                 var_name='model',
                                                 units=None,
                                                 bounds=None,
                                                 coord_system=None,
                                                 circular=False), 0)
    return tempcube_

def get_within_pm2deg(cube):
    """
    This routine will mask out all the regions that are not within
    + or - 2degrees
    input: a temperature cube
    output: a new cube with all data not within +/-deg masked out.
    """

    cubedata = cube.data

    newdata = np.ma.masked_outside(cubedata, -2.0, 2.0, copy=True)

    newcube = cube.copy(data=newdata)

    return newcube

class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, cube, timeperiod_, fieldname, modelname,
                 outname):

        """
        inputs are:
                    modelname - like 'HadCM3' or 'MMM' multimodelmean
                    cube - a single cube containing the data
        """


        self.titlename = timeperiod_ + ' ' + modelname
        self.cube = cube
        self.outname = outname

        if fieldname == 'NearSurfaceTemperature':
            self.valmin = -30.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'RdBu_r'
            self.use_cbar = 'y'
            
        if fieldname == 'MMM':
            self.valmin = 0.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'gist_ncar_r'
            self.use_cbar = 'n'

        if fieldname == 'TEMP +/- 2DEG':
            self.valmin = -10.
            self.valmax = -5.0
            self.diff = 1.0
            self.colormap = 'jet'
            self.use_cbar = 'n'



    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        cubedata = self.cube.data
        latitudes = self.cube.coord('latitude').points
        lon = self.cube.coord('longitude').points
        datatoplot, longitudes = (shiftgrid(180., cubedata,
                                            lon, start=False))


        self.plotmap(datatoplot, longitudes, latitudes)


        return

    def plotmap(self, datatoplot, longitudes, latitudes):
        """
        will plot the data in a map format

        """

        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=0.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        lonmap, latmap = map(lons, lats)
        map.drawcoastlines()

        values = np.arange(self.valmin, self.valmax, self.diff)
        contourplot = map.contourf(lonmap, latmap, datatoplot, values,
                                   cmap=self.colormap,
                                   extend='both')
        plt.title(self.titlename)

        if self.use_cbar == 'y':
            cbar = plt.colorbar(contourplot, orientation='horizontal')
            cbar.set_label(UNITS, size=10)
            cbar.ax.tick_params(labelsize=8, labelrotation=60)


        fileout = (self.outname + '.eps')
        plt.savefig(fileout, bbox_inches='tight')

        fileout = (self.outname + '.png')#

        plt.savefig(fileout, bbox_inches='tight')
        plt.close()

def main_model_ind():
    """
    get the data from the individual models
    plot the multimodel mean and the individual models on the same 
    figure
    """
    
    # get a mean value of all the cubes which are within +/- 2deg
    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + model + '/' + exptname.get(timeperiod) +
                    '.' + FIELDNAME + '.allmean.nc')

        mod_cube = get_data(filename, FIELDNAME, model)
        mod_cube_2deg = get_within_pm2deg(mod_cube)

        if i == 0:
            tempcube_orig = mod_cube_2deg.copy(data=mod_cube_2deg.data)
        else:
            tempcube_orig.data = mod_cube_2deg.data
        tempcube = plotmean_newaxis(tempcube_orig, i)
        cubelist.append(tempcube)


    twodeg_cube = cubelist.concatenate_cube()
    any_in_range_cube = twodeg_cube.collapsed(['model_level_number'],
                                              iris.analysis.MEAN)


# now we want to plot in blue if multimodel mean is in the range and
# in red if only some of the models are in range

    data_mmm = permafrost_mean_cube.data
    data_any = any_in_range_cube.data
    ysize, xsize = np.shape(data_mmm)
    data_both = np.zeros(np.shape(data_mmm))
    for i in range(0, xsize):
        for j in range(0, ysize):
            if np.ma.is_masked(data_mmm[j, i]):
                data_both[j, i] = data_any[j, i]
            else:
                data_both[j, i] = -100.

    all_mean_cube = any_in_range_cube.copy(data=data_both)

    plioobj = Plotalldata(all_mean_cube, timeperiod, 'TEMP +/- 2DEG',
                          '+/- 2deg: any (red) MMM (blue)',
                          OUTSTART + timeperiod +'_within_2deg')
    plioobj.plotdata()
    
    
    
def main_time(timeperiod):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    toplot: y if we want to plot, n if we don't
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = ('/nfs/hera1/earjcti/regridded/'
                + FIELDNAME + '_multimodelmean.nc')

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')

    permafrost_mean_cube = get_within_pm2deg(mean_cube)

# now we need to get all the models and see if any of the values
# are within +/-degC
    if len(MODELNAMES) > 1:
        main_model_ind()


    return permafrost_mean_cube

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    if len(TIMEPERIODS) ==1 : # just do main_timeperiod
        dummy = main_time(TIMEPERIODS[0])
        
    if len(TIMEPERIODS) == 2:  # plotting two timeperiods on same figure
        cube1 = main_time(TIMEPERIODS[0])
        cube2 = main_time(TIMEPERIODS[1])
        
        data1 = cube1.data
        data2 = cube2.data
        ysize, xsize = np.shape(data1)
        databoth = np.zeros((ysize, xsize))
      
        for i in range(0, xsize):
            for j in range(0, ysize):
                if np.ma.is_masked(data1[j, i]):
                    pass
                else:
                    data1[j, i] = 10
                    databoth[j, i] = databoth[j, i] + data1[j, i]
                if np.ma.is_masked(data2[j, i]):
                    pass
                else:
                    data2[j, i] = 20
                    databoth[j, i] = databoth[j, i] + data2[j, i]
       
        databoth = np.where(databoth == 0, np.nan, databoth) 
        print(databoth[:,0])

        cubeboth = cube1.copy(data=databoth)
        
        plotobj = Plotalldata(cubeboth, '', 'MMM',
                              '+/- 2deg: mPWP (green), PI (orange), both (blue)',
                              OUTSTART + 'MMM_within_2deg')
        plotobj.plotdata()
        plt.show()
        
        

# variable definition
FILESTART = ('/nfs/hera1/earjcti/regridded/')
FIELDNAME = 'NearSurfaceTemperature'
OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
UNITS = 'deg C'
#TIMEPERIODS = ['pi']
#TIMEPERIODS = ['mPWP']
TIMEPERIODS = ['pi','mPWP']

if len(TIMEPERIODS) > 1:
    MODELNAMES = []
else:
    MODELNAMES = ['CESM1.0.5', 'COSMOS', 'EC-Earth3.1', 'GISS', 'HadCM3',
                  'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                  'MIROC4m', 'MRI-CGCM2.3',
                  'NorESM-L', 'NorESM1-F',
                  'UofT'
                  ]

main()
::::::::::::::
CEMAC/PLIOMIP2/temperature_isotherms.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the multimodel mean near surface air temperature with
isotherms (at a given level) marked out.  It is to understand which regions
are likely to support thermofrost

"""

import sys
import os
import iris
import numpy as np
import matplotlib.pyplot as plt

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def plotmean_newaxis(cube, modelno_):
    """
    this will add a new axis to the cube which contains the model number
    this is needed for concatenation
    """

    tempcube_ = iris.util.new_axis(cube)
    tempcube_.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                 standard_name='model_level_number',
                                                 long_name='model',
                                                 var_name='model',
                                                 units=None,
                                                 bounds=None,
                                                 coord_system=None,
                                                 circular=False), 0)
    return tempcube_

class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, cubelist, timeperiod_, fieldname, modelname,
                 outname):

        """
        inputs are:
                    modelname - like 'HadCM3' or 'MMM' multimodelmean
                    cube - a single cube containing the data
        """


        self.titlename = timeperiod_ + ' ' + modelname
        self.cubelist = cubelist
        self.outname = outname

        if fieldname == 'NearSurfaceTemperature':
            self.valmin = -30.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'RdBu_r'
            self.use_cbar = 'y'
            
        if fieldname == 'MMM':
            self.valmin = 0.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'gist_ncar_r'
            self.use_cbar = 'n'

     


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """

        cube = self.cubelist[0]
        latitudes = cube.coord('latitude').points
        lon = cube.coord('longitude').points

        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 30., 180.)
        plt.savefig(self.outname + '_Asia.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_Asia.png', bbox_inches='tight')
        plt.close()
        
        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 180., 360.)
        plt.savefig(self.outname + '_America.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_America.png', bbox_inches='tight')
        plt.close()
        
        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 0., 360.)
        plt.savefig(self.outname + '_Globe.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_Globe.png', bbox_inches='tight')
        plt.close()


        return

    def plotmap(self, longitudes, latitudes, left, right):
        """
        will plot the data in a map format

        """

        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=left, urcrnrlon=right,
                      llcrnrlat=0.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        lonmap, latmap = map(lons, lats)
        map.drawcoastlines()

        for i, cube in enumerate(self.cubelist):
            cubedata = cube.data
            contourplot = map.contour(lonmap, latmap, cube.data, 
                                      ISOTHERM_NEEDED,
                                   colors=COLORUSE[i], linewidths=1.5,
                                   linestyles='solid')

        plt.title(self.titlename)
    



def main_model_ind():
    """
    get the data from the individual models
    plot the multimodel mean and the individual models on the same 
    figure
    """
    
    # get a mean value of all the cubes which are within +/- 2deg
    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + model + '/' + exptname.get(timeperiod) +
                    '.' + FIELDNAME + '.allmean.nc')

        mod_cube = get_data(filename, FIELDNAME, model)
        mod_cube_2deg = get_within_pm2deg(mod_cube)

        if i == 0:
            tempcube_orig = mod_cube_2deg.copy(data=mod_cube_2deg.data)
        else:
            tempcube_orig.data = mod_cube_2deg.data
        tempcube = plotmean_newaxis(tempcube_orig, i)
        cubelist.append(tempcube)


    twodeg_cube = cubelist.concatenate_cube()
    any_in_range_cube = twodeg_cube.collapsed(['model_level_number'],
                                              iris.analysis.MEAN)


# now we want to plot in blue if multimodel mean is in the range and
# in red if only some of the models are in range

    data_mmm = permafrost_mean_cube.data
    data_any = any_in_range_cube.data
    ysize, xsize = np.shape(data_mmm)
    data_both = np.zeros(np.shape(data_mmm))
    for i in range(0, xsize):
        for j in range(0, ysize):
            if np.ma.is_masked(data_mmm[j, i]):
                data_both[j, i] = data_any[j, i]
            else:
                data_both[j, i] = -100.

    all_mean_cube = any_in_range_cube.copy(data=data_both)

    plioobj = Plotalldata(all_mean_cube, timeperiod, 'TEMP +/- 2DEG',
                          '+/- 2deg: any (red) MMM (blue)',
                          OUTSTART + timeperiod +'_within_2deg')
    plioobj.plotdata()
    
    
    
def main_time(timeperiod):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    toplot: y if we want to plot, n if we don't
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = (FILESTART  + FIELDNAME + '_multimodelmean.nc')
    print(filename)

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')

# now we need to get all the models and see if any of the values
# are within +/-degC
    #if len(MODELNAMES) > 1:
    #    main_model_ind()


    return mean_cube

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    cubelist_times = iris.cube.CubeList([])
    if len(TIMEPERIODS) ==1 : # just do main_timeperiod
        cube = main_time(TIMEPERIODS[0])
        cubelist_times.append(cube)
        
    if len(TIMEPERIODS) == 2:  # plotting two timeperiods on same figure
    
        cube1 = main_time(TIMEPERIODS[0])
        cubelist_times.append(cube1)
        cube2 = main_time(TIMEPERIODS[1])
        cubelist_times.append(cube2)
        
        data1 = cube1.data
        data2 = cube2.data
        
        
       
    # plot isotherm contour
        title = np.str(ISOTHERM_NEEDED) + 'deg: mPWP (red), PI (blue)'
        plotobj = Plotalldata(cubelist_times, '', 'MMM',
                          title,
                          OUTSTART + 'MMM_' + np.str(ISOTHERM_NEEDED) + 'deg')
        plotobj.plotdata()
       
        

# variable definition
LINUX_WIN = 'l'
ISOTHERM_NEEDED = 0.0

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/regridded/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    
FIELDNAME = 'NearSurfaceTemperature'
OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
UNITS = 'deg C'
#TIMEPERIODS = ['pi']
#TIMEPERIODS = ['mPWP']
TIMEPERIODS = ['pi','mPWP']
COLORUSE = ['blue','red']

#if len(TIMEPERIODS) > 1:
#    MODELNAMES = []
#else:
#    MODELNAMES = ['CESM1.0.5', 'COSMOS', 'EC-Earth3.1', 'GISS', 'HadCM3',
#                  'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
#                  'MIROC4m', 'MRI-CGCM2.3',
#                  'NorESM-L', 'NorESM1-F',
#                  'UofT'
#                  ]

main()
::::::::::::::
CEMAC/suzie/means_and_standard_deviations.py
::::::::::::::
#NAME
#    means_and_standard_deviations.py
#PURPOSE 
#    Ruza wants me to calculate the means from suzies experiments.
#    The experiments are in
#        /nfs/lise/ee14s2r/work/um
#    The output is on /nfs/annie/earpal/database/experiments/exptid/climatology
#
#
# Julia 10.11.2022


# Import necessary libraries

import os
import numpy as np
import iris
from iris.cube import CubeList
from iris import coord_categorisation
import sys

def get_all_fields():
    """
    gets all the fieldnames in the filetype from the first file
    """

    #if MONTH == 'ann':
    #    if TYPE == 'pg':
    #        filename = (INFILE_START + str(STARTYEAR).zfill(4) + 'c1+.nc')
    #    else:
    #        filename = (INFILE_START + str(STARTYEAR).zfill(4) + 'ja+.nc')
    #else:
    #    filename = (INFILE_START + str(STARTYEAR).zfill(4) + MONTHIN.get(MONTH)
    #               + '+.nc')
    #cubes = iris.load(filename)
    #varnames = []
    #for cube in cubes:
    #    varnames.append(cube.var_name)
    #print(varnames)
    #sys.exit(0)

    if TYPE == 'pd':
        varnames = ['totCloud_mm_ua', 'solar_mm_s3_srf', 'downSol_mm_TOA', 'upSol_mm_s3_TOA',  'downSol_Seaice_mm_s3_srf', 'longwave_mm_s3_srf', 'olr_mm_s3_TOA', 'ilr_mm_s3_srf', 'sh_mm_hyb', 'taux_mm_hyb', 'tauy_mm_hyb',  'u_mm_10m', 'v_mm_10m', 'evap_mm_srf', 'canopyEvap_mm_can',  'evapsea_mm_srf', 'lh_mm_srf', 'temp_mm_1_5m', 'srfSublim_mm_srf','transpiration_mm_srf', 'precip_mm_srf',  'sm_mm_soil', 'soiltemp_mm_soil',  'p_mm_msl', 'p_mm_srf',  'snowdepth_mm_srf', 'temp_mm_srf',  'iceconc_mm_srf', 'icedepth_mm_srf' ]

    if TYPE == 'pc':
        varnames = ['u_mm_p',  'v_mm_p', 'ht_mm_p', 'temp_mm_p',]

    if TYPE == 'pf':
        if EXPT == 'xpdea':
            varnames = ['field645_mm_dpth', 'field646_mm_dpth', 'ucurrTot_mm_dpth','vcurrTot_mm_dpth', 'uVelSeaice_mm_uo',  'vVelSeaice_mm_uo', 'temp_mm_uo','dSeaiceSnowDepthdt_mm_uo',  'temp_mm_dpth', 'salinity_mm_dpth', 'streamFn_mm_uo', 'mixLyrDpth_mm_uo',  'iceconc_mm_uo', 'icedepth_mm_uo', 'PLE_mm_uo', 'outflow_mm_uo', 'temp_mm_uo_1', 'temp_mm_uo_2', 'temp_mm_uo_3', 'temp_mm_uo_4', 'otracer16_mm_dpth',   'otracer16_mm_dpth_1', 'otracer16_mm_dpth_2',  'otracer16_mm_dpth_3', 'otracer16_mm_dpth_4', 'otracer16_mm_dpth_5']
        else:
            varnames = ['field645_mm_dpth', 'field646_mm_dpth', 'ucurrTot_mm_dpth','vcurrTot_mm_dpth', 'srfSalFlux_mm_uo', 'uVelSeaice_mm_uo',  'vVelSeaice_mm_uo', 'temp_mm_uo', 'dSeaiceDepthdttherm_mm_uo', 'dSeaiceSnowDepthdttherm_mm_uo', 'temp_mm_dpth', 'salinity_mm_dpth', 'streamFn_mm_uo', 'mixLyrDpth_mm_uo',  'iceconc_mm_uo', 'icedepth_mm_uo', 'PLE_mm_uo', 'outflow_mm_uo', 'temp_mm_uo_1', 'temp_mm_uo_2', 'temp_mm_uo_3', 'temp_mm_uo_4', 'otracer16_mm_dpth',   'otracer16_mm_dpth_1', 'otracer16_mm_dpth_2',  'otracer16_mm_dpth_3', 'otracer16_mm_dpth_4', 'otracer16_mm_dpth_5']

    if TYPE == 'pg':
        varnames = ['W_mm_dpth', 'insitu_T_mm_dpth',  'W_ym_dpth',  'ucurrTot_ym_dpth', 'vcurrTot_ym_dpth', 'srfSalFlux_ym_uo', 'temp_ym_dpth', 'salinity_ym_dpth', 'otracer16_ym_dpth', 'otracer16_ym_dpth_1', 'streamFn_ym_uo', 'iceconc_ym_uo', 'icedepth_ym_uo', 'anomSaltFlux_ym_uo' ]

    return varnames

def get_average_for_variable(variable):
    """
    gets the average for this variable over all the files
    """

    allcubes = CubeList([])
    for year in range(STARTYEAR, ENDYEAR+1):
        filename = (INFILE_START + str(year).zfill(4) + MONTHIN.get(MONTH) 
                    + '+.nc')
        cube = iris.load_cube(filename,variable)
        cube.coord('t').attributes =None
        allcubes.append(cube)

    print(allcubes)
    iris.util.equalise_attributes(allcubes)
    iris.util.unify_time_units(allcubes)
    #print(variable)
    #print(allcubes[0].coord('t').metadata)
    #print(allcubes[1].coord('t').metadata)
    #print(allcubes[2].coord('t').metadata)
    concatcube = allcubes.concatenate_cube(allcubes)
    #print(concatcube)
    meancube = concatcube.collapsed('t',iris.analysis.MEAN)
    meancube.coord('t').bounds = None
    meancubedata = meancube.data
    data2 = np.ma.where(meancubedata < 1.0E30, meancubedata, 1.0E20)
    meancube2 = meancube.copy(data=data2)
   
    stdevcube = concatcube.collapsed('t',iris.analysis.STD_DEV)
    stdevcube.coord('t').bounds = None
    sdcubedata = stdevcube.data
    data3 = np.ma.where(sdcubedata < 1.0E30, sdcubedata, 1.0E20)
    sdcube2 = stdevcube.copy(data=data3)

   
    return meancube2, sdcube2


def get_average_for_variable_pg(variable):
    """
    gets the average for this variable over all the files if it is pg
    (not this is seperate because a few variables were in the pg as 12 months)
    """

    allcubes = CubeList([])
    for year in range(STARTYEAR, ENDYEAR+1):
        filename = (INFILE_START + str(year).zfill(4) +  'c1+.nc')
        cube = iris.load_cube(filename,variable)
        # check how many times
        try:
            times = cube.coord('t').points
            tname='t'
        except:
            times = cube.coord('t_1').points
            tname='t_1'

        if len(times) >=2:
            tgt1=True
        else:
            tgt1=False
        if tgt1:
            iris.coord_categorisation.add_month_number(cube, tname, name='month')
        cube.coord(tname).attributes = None
        if tgt1:
            cube.coord('month').attributes = None
        allcubes.append(cube)
      
    iris.util.equalise_attributes(allcubes)
    iris.util.unify_time_units(allcubes)
    #print(variable)
    #print(allcubes[0].coord('t'))
    #print(allcubes[1].coord('t'))
    #print(allcubes[2].coord('t'))
    #print(allcubes)
    concatcube = allcubes.concatenate_cube(allcubes)
    if tgt1:
        meancube = concatcube.aggregated_by(['month'],iris.analysis.MEAN)
        stdevcube = concatcube.aggregated_by(['month'],iris.analysis.STD_DEV)
   
    else:
        meancube = concatcube.collapsed(tname,iris.analysis.MEAN)
        stdevcube = concatcube.collapsed(tname,iris.analysis.STD_DEV)
   
    meancube.coord(tname).bounds = None
    meancubedata = meancube.data
    data2 = np.ma.where(meancubedata < 1.0E30, meancubedata, 1.0E20)
    meancube2 = meancube.copy(data=data2)
   
    stdevcube.coord(tname).bounds = None
    sdcubedata = stdevcube.data
    data3 = np.ma.where(sdcubedata < 1.0E30, sdcubedata, 1.0E20)
    sdcube2 = stdevcube.copy(data=data3)

   
    return meancube2, sdcube2
    
def get_avgs():
    """
    gets data and averages
    """

    varnames = get_all_fields()  # gets the fields in the files
    allmncubes = CubeList([])
    allstcubes = CubeList([])
  
    for i, name in enumerate(varnames):
        print(name)
        avgcube,stdevcube = get_average_for_variable(name)
        allmncubes.append(avgcube)
        allstcubes.append(stdevcube)

    print(allmncubes)
    fileout = (FILEOUTSTART + EXPT + ATM_OCN.get(TYPE) + '.' + TYPE + 
               'cl' + MONTH + 
               '.' + str(STARTYEAR) + '.' + str(ENDYEAR) + '.nc')
    iris.save(allmncubes,fileout, netcdf_format = "NETCDF3_CLASSIC", fill_value = 1.0E20)
  
    fileout = (FILEOUTSTART + EXPT + ATM_OCN.get(TYPE) + '.' + TYPE + 
               'sd' + MONTH + 
               '.' + str(STARTYEAR) + '.' + str(ENDYEAR) + '.nc')
    iris.save(allstcubes,fileout, netcdf_format = "NETCDF3_CLASSIC", fill_value = 1.0E20)

def get_pg_avg():
    """
    gets data and averages for pg files
    """

    varnames = get_all_fields()  # gets the fields in the files
    allmncubes = CubeList([])
    allstcubes = CubeList([])
  
    for i, name in enumerate(varnames):
        print(name)
        avgcube,stdevcube = get_average_for_variable_pg(name)
        allmncubes.append(avgcube)
        allstcubes.append(stdevcube)

    print(allmncubes)
    fileout = (FILEOUTSTART + EXPT + ATM_OCN.get(TYPE) + '.' + TYPE + 
               'clann' + '.' + str(STARTYEAR) + '.' + str(ENDYEAR) + '.nc')
    iris.save(allmncubes,fileout, netcdf_format = "NETCDF3_CLASSIC", fill_value = 1.0E20)
  
    fileout = (FILEOUTSTART + EXPT + ATM_OCN.get(TYPE) + '.' + TYPE + 
               'sdann' +  '.' + str(STARTYEAR) + '.' + str(ENDYEAR) + '.nc')
    iris.save(allstcubes,fileout, netcdf_format = "NETCDF3_CLASSIC", fill_value = 1.0E20)

def get_avg_for_year(year,variable):
    """
    gets the average for field: variable and year: year
    """
    monthsreq = ['ja','fb','mr','ar','my','jn','jl','ag','sp',
                     'ot','nv','dc']

    cubes = CubeList([])
    for monthuse in monthsreq:
        filename = (INFILE_START + str(year).zfill(4) + monthuse + '+.nc')
        cube = iris.load_cube(filename,variable)
        cube.coord('t').attributes = None
        cubes.append(cube)
    iris.util.equalise_attributes(cubes)
    yearcube = cubes.concatenate_cube()
    meanyear = yearcube.collapsed('t',iris.analysis.MEAN)
    meanyear_n = iris.util.new_axis(meanyear,scalar_coord = 't')
   
   
    return meanyear_n

def get_ann_average_for_variable(variable):
    """
    gets the average for this variable over all the files
    """


    allcubes = CubeList([])
    for year in range(STARTYEAR, ENDYEAR+1):
        cubeyravg = get_avg_for_year(year,variable)
        allcubes.append(cubeyravg)

    iris.util.equalise_attributes(allcubes)
    iris.util.unify_time_units(allcubes)
    #print(variable)
    #print(allcubes[0].coord('t').metadata)
    #print(allcubes[1].coord('t').metadata)
    #print(allcubes[2].coord('t').metadata)
    concatcube = allcubes.concatenate_cube(allcubes)
    #print(concatcube)
    meancube = concatcube.collapsed('t',iris.analysis.MEAN)
    meancube.coord('t').bounds = None
    stdevcube = concatcube.collapsed('t',iris.analysis.STD_DEV)
    stdevcube.coord('t').bounds = None
   

    meancubedata = meancube.data
    data2 = np.ma.where(meancubedata < 1.0E30, meancubedata, 1.0E20)
    meancube2 = meancube.copy(data=data2)

    sdcubedata = stdevcube.data
    data3 = np.ma.where(sdcubedata < 1.0E30, sdcubedata, 1.0E20)
    sdcube2 = stdevcube.copy(data=data3)

   
    return meancube2, sdcube2


def get_ann_avg():
    """
    gets data and averages
    """

    varnames = get_all_fields()  # gets the fields in the files
    allmncubes = CubeList([])
    allstcubes = CubeList([])
  
    for name in varnames:
        avgcube,stdevcube = get_ann_average_for_variable(name)
        allmncubes.append(avgcube)
        allstcubes.append(stdevcube)

    fileout = (FILEOUTSTART + EXPT + ATM_OCN.get(TYPE) + '.' + TYPE + 
               'cl' + MONTH + 
               '.' + str(STARTYEAR) + '.' + str(ENDYEAR) + '.nc')
    iris.save(allmncubes,fileout, netcdf_format = "NETCDF3_CLASSIC", fill_value=1.0E20)
  
    fileout = (FILEOUTSTART + EXPT + ATM_OCN.get(TYPE) + '.' + TYPE + 
               'sd' + MONTH + 
               '.' + str(STARTYEAR) + '.' + str(ENDYEAR) + '.nc')
    iris.save(allstcubes,fileout, netcdf_format = "NETCDF3_CLASSIC", fill_value = 1.0E20)

#=================================================================
# MAIN PROGRAM STARTS HERE

# for program to run
EXPT='xpdac'
STARTYEAR=4901
ENDYEAR=5000
TYPE = 'pg'
MONTHS = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','ann']
#MONTHS = ['jan','ann']
#MONTHS = ['ann']

# dictionaries
TYPEEXPT = {'pc' : 'pcpd','pd':'pcpd'}
ATM_OCN = {'pc' : 'a','pd':'a', 'pf' :'o', 'pg' :'o'}
MONTHIN = {'jan' : 'ja',   'feb':'fb',  'mar':'mr', 'apr':'ar',  'may':'my',
           'jun' : 'jn',   'jul':'jl',  'aug':'ag', 'sep':'sp',  'oct':'ot',
           'nov' : 'nv',   'dec':'dc'}

#program

#FILEOUTSTART = '/nfs/annie/earjcti/climatologies/' + EXPT + '/'
FILEOUTSTART = ''
INFILE_START = ('/nfs/lise/ee14s2r/work/um/' + EXPT + '/' + 
                        TYPEEXPT.get(TYPE,TYPE) + '/' + EXPT + 
                        ATM_OCN.get(TYPE) + 
                        '#' + TYPE + '00000')

if TYPE == 'pg':
    get_pg_avg()
else:
    for MONTH in MONTHS:
        if MONTH == 'ann':
            get_ann_avg()
        else:
            get_avgs()    
::::::::::::::
CEMAC/Xiaofang/Reduce_Antarctica_height.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
@author: earjcti

This program will create a netcdf file which contains the vegetation data for PlioMIP2
"""

import iris
import numpy as np


def overwrite_data(arr1, arr2):
    """
    overwrites the data in arr1 with arr2 where the following conditions are met
    1. arr1 is not a sea point (level 1 lt 2.0)
    2. arr1 is not ice (level 9/8 not equal 1)
    3. arr1 is not a partial lake (level 7/6 = 0)
    """
    
    nt, nz, ny, nx = np.shape(arr1)
    for j in range(0, ny):
        for i in range(0, nx):
            if arr1[0, 0, j, i] < 2.0: # not sea
                if arr1[0, 8, j, i] != 1.0: # not ice
                    if arr1[0, 6, j, i] == 0.0: # is not lake
                        vegdata = overwrite_nonlake(arr2[0, :, j, i])
                    else: # partial lake
                        vegdata = overwrite_lake(arr1[:, :, j, i], arr2[:, :, j, i])



    
def main():
    """
    1. loads data in from SJH file
    2. loads data in from my file
    3. overwrite SJH data with my data where appropriate
    4. write out to a new file
    """

    sjhcube = iris.load_cube(STEVEH_FILE, STEVEH_FIELD)
    sjhdata = sjhcube.data

    jctcube = iris.load_cube(PRISM3_DUMP, PRISM3_FIELD)
    jctdata = jctcube.data
    
    if np.shape(sjhdata) != np.shape(jctdata):
        print('shapes of data do not match')
        sys.exit(0)
        
    overwrite_data(sjhdata, jctdata)
    
    


"""
data required
"""

STEVEH_FILE = '/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrfrac.type_sjh.nc'
STEVEH_FIELD = 'FRACTIONS OF SURFACE TYPES'
PRISM3_DUMP = '/nfs/hera1/earjcti/um/xibol/netcdf/xibola@daz00c1.nc'
PRISM3_FIELD = 'TILE FRACTIONS (B.LAYER)'

main()::::::::::::::
COLLABORATORS/LAURIE/plot_orog.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_OROG
#PURPOSE
#    This program will plot the orography for the eocene simulation
# search for 'main program' to find end of functions
# Julia 13/09/2017



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
from mpl_toolkits.basemap import Basemap, shiftgrid, maskoceans


#functions are:
#  def plotdata
#  def annmean
#  def seasmean

# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,minval,maxval,valinc,V,uselog,cbarname):
    lons, lats = np.meshgrid(lon,lat)
    if fileno !=99:
        plt.subplot(2,2,fileno+1)



   # this is good for a tropical region
   # map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')
   # this is good for the globe
    map=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',resolution='c')
    x, y = map(lons, lats)
#    map.drawcoastlines()

    plotdata2=plotdata
    #plotdata=maskoceans(x,y,plotdata)
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu',extend='both')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='both')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu',extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                if uselog =='ra':
                    cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")
                else:
                    print(np.shape(plotdata))
                    cs = map.contourf(x,y,plotdata,V,extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")


    if fileno != 99:
        plt.title(titlename)
        cbar.set_label(cbarname,labelpad=-40)
    else:
        cbar.set_label(cbarname,labelpad=-70,size=20)
        cbar.ax.tick_params(labelsize=20)
        plt.title(titlename,loc='left',fontsize=20)
   

    plotdata=plotdata2

    #map.drawmapboundary

#end def plotdata



################################
# main program

# get orography
f=Dataset('/nfs/hera2/apps/metadata/experiments/tbomh/ancil/tbomh.qrparm.orog.nc','r')
lat = f.variables['latitude'][:]
lon = f.variables['longitude'][:]
height=f.variables['ht'][:]
height=np.squeeze(height)
f.close()


# get land sea mask

f=Dataset('/nfs/hera2/apps/metadata/experiments/tbomh/ancil/tbomh.qrparm.mask.nc','r')
latm = f.variables['latitude'][:]
lonm = f.variables['longitude'][:]
mask=f.variables['lsm'][:]
mask=np.squeeze(mask)
f.close()
# replace 1's with zeros and zeros with 1's
mask=(mask - 1.0)*1.0

masked_height=np.ma.masked_array(height,mask=mask)

# plot it
masked_height,lon = shiftgrid(180.,masked_height,lon,start=False)
plotdata(masked_height,99,lon,lat,'HadCM3 Eocene Orography',0,3000,250,0.0,'n','height (m)')
plt.show()




sys.exit(0)

####

::::::::::::::
COLLABORATORS/MANUEL/extract_monthly_averages_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Monday June 3rd 2019

#@author: earjcti
#
# Manuel has asked if I can extract some fields from Kanhu's experiments so 
# that he can do some averages on them.  He would like data from the 
# xkrax experiment which has time varying vegetation, greenhouse gases varying 
#
# Because he is comparing to GNIP he would like data from years 1957-2014 
# as these are the years where we have observations.  This is j57-k14 I think
#
#
########################################################
# other notes are

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
from iris.util import unify_time_units
import cf_units as unit
import sys

#####################################
def extract_fields(filestart,fileoutstart,startyear,endyear,varnamein):



    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    
    
    # loop over months
    print(varnamein)
    
    moncube=iris.cube.CubeList([])
    for mon in range(0,len(monthnames)):
    #for mon in range(9,12):
  
        allcubes=iris.cube.CubeList([])
        #loop over years
        for year in range(startyear,endyear+1):
            yearuse=year-1800
           
            if year < 1800:
                extrause='??'
            else:
                if year < 1900:
                    extrause='i'
                else:
                    if year < 2000:
                        extrause='j'
                        yearuse=yearuse-100
                    else:
                        if year < 2100:
                            extrause='k'
                            yearuse=yearuse-200
                   
           
            stringyear=np.str(yearuse).zfill(2)
      
            
            filename=filestart+extrause+stringyear+monthnames[mon]+'.nc'
            print(filename)
           
           
            cube=iris.load_cube(filename,varnamein)
            
            
          
            u = unit.Unit('months since 0800-12-01 00:00:00',
                                  calendar=unit.CALENDAR_360_DAY)
            cube.coord('t').units=u
           
            cube.coord('t').points=year-startyear+1
           
            if varnamein=='Stash code = 338':
               
                cube.coord('level-1').rename('zdim')
               
              
                cube16o= (cube.extract(iris.Constraint(zdim=1.))+
                          cube.extract(iris.Constraint(zdim=4.))+
                          cube.extract(iris.Constraint(zdim=7.))+
                          cube.extract(iris.Constraint(zdim=10.)))
                cube18o= (cube.extract(iris.Constraint(zdim=2.))+
                          cube.extract(iris.Constraint(zdim=5.))+
                          cube.extract(iris.Constraint(zdim=8.))+
                          cube.extract(iris.Constraint(zdim=11.)))
                cube=((cube18o / cube16o)-2005.2E-6)/2005.2E-9
                
                cube.rename('d18o')
                cube.units='unknown'
                
             
            
            if varnamein=='TOTAL PRECIPITATION RATE     KG/M2/S':
                cube=(cube.extract(iris.Constraint(surface=0.000000)))
                
            if varnamein=='TEMPERATURE AT 1.5M':
                cube=(cube.extract(iris.Constraint(ht=-1.000000)))
               
           
            allcubes.append(cube)
           
        
           
        #make sure the metadata on all cubes are the same
        equalise_attributes(allcubes)
        unify_time_units(allcubes)
        for i in range(1,len(allcubes)):
            allcubes[i].coord('t').attributes=allcubes[0].coord('t').attributes
        
        
        
      
        catcube=allcubes.concatenate_cube()
       
       
        nc,ny,nx=np.shape(catcube)
        if nc != endyear-startyear+1:
            print('the cube has not been concatenated correctly')
            print('we should have',endyear-startyear+1,'times')
            print('we have',nc,'times')
            sys.exit(0)
         
        # find mean across cube dimension
        tempcube = catcube.collapsed('t', iris.analysis.MEAN)
        meancube=iris.util.new_axis(tempcube, 't')
        meancube.coord('t').points=mon+1
        meancube.coord('t').bounds=None
       
       
        # append to cube containing all months
        moncube.append(meancube)
       
       
    # unifty attributes on cubes
    equalise_attributes(moncube)
    unify_time_units(moncube)
    for i in range(1,len(moncube)):
            moncube[i].coord('t').attributes=moncube[0].coord('t').attributes
   
    
    outcube=moncube.concatenate_cube()
    
    
    return(outcube)
  
    
   

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning

linuxwin='l'
            	
filestart={"l":'/nfs/hera1/earjcti/um/netcdf/xkrax_netcdf/xkraxa@pd',
           "w":'C:/Users/julia/OneDrive/WORK/DATA/TEMPORARY/xkraxa@pd'}

startyear=1958
endyear=2014

fileoutstart='output/'

fieldnames=['TOTAL PRECIPITATION RATE     KG/M2/S','TEMPERATURE AT 1.5M','Stash code = 338']
#fieldnames=['TOTAL PRECIPITATION RATE     KG/M2/S']

allcubes=[]

cube1=extract_fields(filestart.get(linuxwin),fileoutstart,startyear,endyear,fieldnames[0])
allcubes.append(cube1)
if len(fieldnames) >=2:
    cube2=extract_fields(filestart.get(linuxwin),fileoutstart,startyear,endyear,fieldnames[1])
    allcubes.append(cube2)
if len(fieldnames) >=3:
    cube3=extract_fields(filestart.get(linuxwin),fileoutstart,startyear,endyear,fieldnames[2])
    allcubes.append(cube3)
if len(fieldnames) >=4:
    print('you need to set up accessing more cubes')
    sys.exit(0)


fileout=fileoutstart+'data_xkrax.nc'        
iris.save(allcubes,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
  
#sys.exit(0)
::::::::::::::
COLLABORATORS/MANUEL/extract_monthly_averages.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Monday June 3rd 2019

#@author: earjcti
#
# Manuel has asked if I can extract some fields from Kanhu's experiments so 
# that he can do some averages on them.  He would like data from the 
# xkrax experiment which has time varying vegetation, greenhouse gases varying 
#
# Because he is comparing to GNIP he would like data from years 1957-2014 
# as these are the years where we have observations.  This is j57-k14 I think
#
#
########################################################
# other notes are

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
from iris.util import unify_time_units
import cf_units as unit
import sys

#####################################
def extract_fields(filestart,fileoutstart,startyear,endyear,varnamein):



    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    
    
    # loop over months
    print(varnamein)
    
    if varnamein=='Stash code = 338':
        moncube18o=iris.cube.CubeList([])
        moncube16o=iris.cube.CubeList([])
    else:
        moncube=iris.cube.CubeList([])
    for mon in range(0,len(monthnames)):
    #for mon in range(9,12):
  
        if varnamein=='Stash code = 338':
            allcubes18o=iris.cube.CubeList([])
            allcubes16o=iris.cube.CubeList([])
        else:   
            allcubes=iris.cube.CubeList([])
        #loop over years
        for year in range(startyear,endyear+1):
            yearuse=year-1800
           
            if year < 1800:
                extrause='??'
            else:
                if year < 1900:
                    extrause='i'
                else:
                    if year < 2000:
                        extrause='j'
                        yearuse=yearuse-100
                    else:
                        if year < 2100:
                            extrause='k'
                            yearuse=yearuse-200
                   
           
            stringyear=np.str(yearuse).zfill(2)
      
            
            filename=filestart+extrause+stringyear+monthnames[mon]+'.nc'
            print(filename)
           
           
            cube=iris.load_cube(filename,varnamein)
            
            
          
            u = unit.Unit('months since 0800-12-01 00:00:00',
                                  calendar=unit.CALENDAR_360_DAY)
            cube.coord('t').units=u
           
            cube.coord('t').points=year-startyear+1
           
            if varnamein=='Stash code = 338':
               
                cube.coord('level-1').rename('zdim')
               
              
                cube16o= (cube.extract(iris.Constraint(zdim=1.))+
                          cube.extract(iris.Constraint(zdim=4.))+
                          cube.extract(iris.Constraint(zdim=7.))+
                          cube.extract(iris.Constraint(zdim=10.)))
                cube18o= (cube.extract(iris.Constraint(zdim=2.))+
                          cube.extract(iris.Constraint(zdim=5.))+
                          cube.extract(iris.Constraint(zdim=8.))+
                          cube.extract(iris.Constraint(zdim=11.)))
                allcubes18o.append(cube18o)
                allcubes16o.append(cube16o)
                
             
            
            if varnamein=='TOTAL PRECIPITATION RATE     KG/M2/S':
                cube=(cube.extract(iris.Constraint(surface=0.000000)))
                allcubes.append(cube)
                
            if varnamein=='TEMPERATURE AT 1.5M':
                cube=(cube.extract(iris.Constraint(ht=-1.000000)))
                allcubes.append(cube)
               
           
           
           
        
           
        #make sure the metadata on all cubes are the same
        
        if varnamein=='Stash code = 338':
            # DO 18O
            equalise_attributes(allcubes18o)
            unify_time_units(allcubes18o)
            for i in range(1,len(allcubes18o)):
                allcubes18o[i].coord('t').attributes=allcubes18o[0].coord('t').attributes
        
        
            catcube18O=allcubes18o.concatenate_cube()       
            nc,ny,nx=np.shape(catcube18O)
            if nc != endyear-startyear+1:
                print('the cube 18O has not been concatenated correctly')
                print('we should have',endyear-startyear+1,'times')
                print('we have',nc,'times')
                sys.exit(0)
         
            # find mean across cube dimension
            tempcube18O = catcube18O.collapsed('t', iris.analysis.MEAN)
            meancube18O=iris.util.new_axis(tempcube18O, 't')
            meancube18O.coord('t').points=mon+1
            meancube18O.coord('t').bounds=None
       
            # append to cube containing all months
            moncube18o.append(meancube18O)
        
            # DO 16o
            equalise_attributes(allcubes16o)
            unify_time_units(allcubes16o)
            for i in range(1,len(allcubes16o)):
                allcubes16o[i].coord('t').attributes=allcubes16o[0].coord('t').attributes
        
            catcube16O=allcubes16o.concatenate_cube()       
            nc,ny,nx=np.shape(catcube16O)
            if nc != endyear-startyear+1:
                print('the cube 16O has not been concatenated correctly')
                print('we should have',endyear-startyear+1,'times')
                print('we have',nc,'times')
                sys.exit(0)
         
            # find mean across cube dimension
            tempcube16O = catcube16O.collapsed('t', iris.analysis.MEAN)
            meancube16O=iris.util.new_axis(tempcube16O, 't')
            meancube16O.coord('t').points=mon+1
            meancube16O.coord('t').bounds=None
       
            # append to cube containing all months
            moncube16o.append(meancube16O)
        
        
        else:   
            equalise_attributes(allcubes)
            unify_time_units(allcubes)
            for i in range(1,len(allcubes)):
                allcubes[i].coord('t').attributes=allcubes[0].coord('t').attributes
        
      
            catcube=allcubes.concatenate_cube()       
       
            nc,ny,nx=np.shape(catcube)
            if nc != endyear-startyear+1:
                print('the cube has not been concatenated correctly')
                print('we should have',endyear-startyear+1,'times')
                print('we have',nc,'times')
                sys.exit(0)
         
            # find mean across cube dimension
            tempcube = catcube.collapsed('t', iris.analysis.MEAN)
            meancube=iris.util.new_axis(tempcube, 't')
            meancube.coord('t').points=mon+1
            meancube.coord('t').bounds=None
       
       
            # append to cube containing all months
            moncube.append(meancube)
       
       
    # unifty attributes on cubes
    if varnamein=='Stash code = 338':
        equalise_attributes(moncube18o)
        unify_time_units(moncube18o)
        for i in range(1,len(moncube18o)):
                moncube18o[i].coord('t').attributes=moncube18o[0].coord('t').attributes    
        outcube18=moncube18o.concatenate_cube()
        
        equalise_attributes(moncube16o)
        unify_time_units(moncube16o)
        for i in range(1,len(moncube16o)):
                moncube16o[i].coord('t').attributes=moncube16o[0].coord('t').attributes    
        outcube16=moncube16o.concatenate_cube()
        
        outcube=((outcube18/outcube16)-2005.2E-6) / 2005.2E-9
            
        outcube.rename('d18o')
        outcube.units='unknown'
    else:
        equalise_attributes(moncube)
        unify_time_units(moncube)
        for i in range(1,len(moncube)):
                moncube[i].coord('t').attributes=moncube[0].coord('t').attributes    
        outcube=moncube.concatenate_cube()
    
    
    return(outcube)
  
    
   

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning

linuxwin='l'
            	
filestart={"l":'/nfs/hera1/earjcti/um/netcdf/xkrax_netcdf/xkraxa@pd',
           "w":'C:/Users/julia/OneDrive/WORK/DATA/TEMPORARY/xkraxa@pd'}

startyear=1958
endyear=2014
#endyear=1960

fileoutstart='output/'

fieldnames=['TOTAL PRECIPITATION RATE     KG/M2/S','TEMPERATURE AT 1.5M','Stash code = 338']
#fieldnames=['TOTAL PRECIPITATION RATE     KG/M2/S']

allcubes=[]

cube1=extract_fields(filestart.get(linuxwin),fileoutstart,startyear,endyear,fieldnames[0])
allcubes.append(cube1)
if len(fieldnames) >=2:
    cube2=extract_fields(filestart.get(linuxwin),fileoutstart,startyear,endyear,fieldnames[1])
    allcubes.append(cube2)
if len(fieldnames) >=3:
    cube3=extract_fields(filestart.get(linuxwin),fileoutstart,startyear,endyear,fieldnames[2])
    allcubes.append(cube3)
if len(fieldnames) >=4:
    print('you need to set up accessing more cubes')
    sys.exit(0)


fileout=fileoutstart+'data_xkrax.nc'        
iris.save(allcubes,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
  
#sys.exit(0)
::::::::::::::
COLLABORATORS/MANUEL/plot_monthly_averages_region.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Monday June 3rd 2019

#@author: earjcti
#
# Manuel has asked if I can extract some fields from Kanhu's experiments so 
# that he can do some averages on them.  He would like data from the 
# xkrax experiment which has time varying vegetation, greenhouse gases varying 
#
# Because he is comparing to GNIP he would like data from years 1957-2014 
# as these are the years where we have observations.  This is j57-k14 I think
#
#
########################################################
# other notes are

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
from iris.util import unify_time_units
import cf_units as unit
from mpl_toolkits.basemap import Basemap, shiftgrid, maskoceans
import sys



# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,
             minval,maxval,valinc,V,uselog,cbarname,lonmin,lonmax,latmin,latmax):
    lons, lats = np.meshgrid(lon,lat)
    if fileno !=99:        
        plt.subplot(1,2,fileno+1)



    map=Basemap(llcrnrlon=lonmin,urcrnrlon=lonmax,llcrnrlat=latmin,urcrnrlat=latmax,projection='cyl',resolution='c')
    x, y = map(lons, lats)
    map.drawcoastlines()
    
    # set up for drawing gridlines
    if lonmax-lonmin <= 60:
        londiff=10
    else:
        londiff=30
     
   
    if latmax-latmin <= 60:
        latdiff=10
    else:
        latdiff=30

    parallels=np.arange(-90,90,latdiff)
    meridians=np.arange(-180,360,londiff)

   
    map.drawparallels(parallels,labels=[False,True,False,False])
    map.drawmeridians(meridians,labels=[False,False,False,True])


    plotdata2=plotdata
    #plotdata=maskoceans(x,y,plotdata)
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu',extend='both')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='both')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu',extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                if uselog =='ra':
                    cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")
                else:
                    print(np.shape(plotdata))
                    if uselog=='nb':  # use bluescale
                        cmapuse='Blues'
                    else:
                        cmapuse='rainbow'
                    cs = map.contourf(x,y,plotdata,V,cmap=cmapuse,extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")


    if fileno != 99:
        plt.title(titlename)
        cbar.set_label(cbarname,labelpad=-40)
    else:
        cbar.set_label(cbarname,labelpad=-70,size=20)
        cbar.ax.tick_params(labelsize=20)
        plt.title(titlename,loc='left',fontsize=20)
   

    plotdata=plotdata2



#end def plotdata

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning

linuxwin='l'
            	
filein={"l":'output/data_xkrax.nc',
           "w":'C:/Users/julia/OneDrive/WORK/DATA/TEMPORARY/xkraxa@pd'}



fieldname='d18o'
cube=iris.load_cube(filein.get(linuxwin),fieldname)

if fieldname=='d18o': # also get precipitation
    fieldp='TOTAL PRECIPITATION RATE     KG/M2/S'
    cubeprecip=iris.load_cube(filein.get(linuxwin),fieldp)
    
print(cube.coord('t').points)    
datajune= cube.data[5,:,:]
datajul= cube.data[6,:,:]
dataaug=cube.data[7,:,:]

# find average without weighting
datajja=(datajune+datajul+dataaug)/3.0

lon=cube.coord('longitude').points
lat=cube.coord('latitude').points

print(datajja)

plotdata(datajja,0,lon,lat,'d18o no weighting',-25.,1.,1.,0,'n',
         'permille',270.,330.,-60.,20.)


# find and plot average with weighting

precipjune= cubeprecip.data[5,:,:]
precipjul= cubeprecip.data[6,:,:]
precipaug=cubeprecip.data[7,:,:]

# find average without weighting
datajja_w=(((datajune * precipjune)+(datajul*precipjul)+(dataaug*precipaug))
        /(precipjune+precipjul+precipaug))

plotdata(datajja_w,1,lon,lat,'d18o weighting',-25.,1.,1.,0,'n',
         'permille',270.,330.,-60.,20.)

#plotdata(datajja,99,lon,lat,'d18o no weighting',-25.,1.,1.,0,'n',
#         'permille',0.,360.,-90.,90.)
plt.show()

sys.exit(0)
::::::::::::::
COLLABORATORS/TAMARA/cloud_regiemes_CFMIP.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

We will use the joint pdf's creates by CFMIP to try and attempt to 
"""
import numpy as np
import iris
#from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt

# stuff for kmeans clustering
#from kneed import KneeLocator
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
#from sklearn.metrics import silhouette_score
#from sklearn.preprocessing import StandardScalar

import sys

  
def get_pdfcube():
    """
    gets the pdfcube
    """
    cubes = iris.load(FILEIN)
    for cube in cubes:
        if cube.var_name=='clisccp':
            jointpdf_cube = cube
     
    cloudtop_press = jointpdf_cube.coord('air_pressure').points
    thickname = 'atmosphere_optical_thickness_due_to_cloud'
    optical_depth = jointpdf_cube.coord(thickname).points
          
    return jointpdf_cube.data,cloudtop_press, optical_depth

def get_alt_albedo(joint_pdf_cube,alt_albedo_cube):
    """
    this will estimate the albedo from the jointpdf cube instead of the file
    """
    # albedos corresponding to the optical depths in the pdf boxes
    # from williams and webb 2008
    albedos = [0.028, 0.107, 0.232, 0.407, 0.626, 0.828, 0.950]
    ctp = joint_pdf_cube.coord('air_pressure').points
    tau = joint_pdf_cube.coord('atmosphere_optical_thickness_due_to_cloud').points
    # when calculating albedo it is unclear whether we should average over all points in the histogram or only over those where there is cloud.  
    # I have decided to only average over those where there is cloud, because it is cloud albedo not total albedo

    nz,ny,nx = joint_pdf_cube.data.shape # nz =ctp, ny =optical depth, nx = loc
    for i in range(0,nx): # over locs'
        joint_pdf = joint_pdf_cube.data[:,:,i]
        meanalbedo=0.0
        for j in range(0,nz): # over ctp
            for k in range(0,ny): # over optical depth
                if joint_pdf[j,k] > 0.0:
                    meanalbedo = (meanalbedo +
                                  (joint_pdf[j,k] * albedos[k]))
        alt_albedo_cube.data[i] = meanalbedo / np.sum(joint_pdf)
    
    return alt_albedo_cube

def get_pdfcube_tropics():
    """
    gets the pdfcube and extracts the tropical points only
    """
    cubes = iris.load(FILEIN)
    for cube in cubes:
        print(cube.var_name)
        if cube.var_name=='clisccp':
            jointpdf_cube = cube
        if cube.var_name == 'latitude':
            latitude_cube = cube
        if cube.var_name == 'pctisccp':
            meanctp_cube = cube
        if cube.var_name == 'albisccp':
            meanalbedo_cube = cube
        if cube.var_name == 'cltisccp':
            meantcc_cube = cube
        if cube.var_name == 'boxptopisccp':
            subcol_ctp_cube = cube
    
    subcol_ctp_cube.data.mask = np.where(subcol_ctp_cube.data < 0, 1.0, 0.0)
    
    subcol_ctp_mean_cube = subcol_ctp_cube.collapsed('subcolumn indices',iris.analysis.MEAN)
    
    alt_albedo_cube = meanalbedo_cube.copy()
    alt_albedo_cube = get_alt_albedo(jointpdf_cube,alt_albedo_cube)
    tropical_cubes = iris.cube.CubeList([])
    tropical_ctp  = iris.cube.CubeList([])
    tropical_albedo= iris.cube.CubeList([])
    tropical_tcc= iris.cube.CubeList([])
    tropical_tcc_alt = iris.cube.CubeList({})
    tropical_ctp_alt = iris.cube.CubeList({})
    tropical_albedo_alt = iris.cube.CubeList({})

    
    count=0
    for i, lat in enumerate(latitude_cube.data):
        if -20.0 < lat < 20.0:
            count=count+1
            tropical_cubes.append(jointpdf_cube[:,:,i])
            tropical_ctp.append(meanctp_cube[i])
            tropical_albedo.append(meanalbedo_cube[i])
            tropical_tcc.append(meantcc_cube[i])
            alternative_tcc = jointpdf_cube[:,:,i].collapsed(['air_pressure','atmosphere_optical_thickness_due_to_cloud'],iris.analysis.SUM)
            tropical_tcc_alt.append(alternative_tcc)
            tropical_ctp_alt.append(subcol_ctp_mean_cube[i])
            tropical_albedo_alt.append(alt_albedo_cube[i])

    jointpdf_cube = tropical_cubes.merge_cube()
    mean_ctp_cube = tropical_ctp.merge_cube()
    mean_ctp_cube.data = mean_ctp_cube.data / 100000.
    mean_tcc_cube = tropical_tcc.merge_cube()
    mean_tcc_cube.data = mean_tcc_cube.data / 100.
    mean_albedo_cube = tropical_albedo.merge_cube()
    # here we are getting an alternative tcc. Where we add up all the fractions
    # in the histogram.  The one that we took directly from the file
    # did not look correct.  The sum from the histogram looks better.  
    mean_tcc_alt_cube = tropical_tcc_alt.merge_cube()
    mean_tcc_alt_cube.data = mean_tcc_alt_cube.data / 100.
    mean_ctp_alt_cube = tropical_ctp_alt.merge_cube()
    mean_ctp_alt_cube.data = mean_ctp_alt_cube.data / 100000.
    mean_albedo_alt_cube = tropical_albedo_alt.merge_cube()
 
    

   
    mean_ctp_cube.data = np.where(mean_ctp_cube.data < 0, 
                                  np.nan, mean_ctp_cube.data)
    mean_tcc_cube.data = np.where(mean_tcc_cube.data < 0, 
                                  np.nan, mean_tcc_cube.data)
    mean_albedo_cube.data = np.where(mean_albedo_cube.data < 0, 
                                  np.nan, mean_albedo_cube.data)
    cloudtop_press = jointpdf_cube.coord('air_pressure').points
    thickname = 'atmosphere_optical_thickness_due_to_cloud'
    optical_depth = jointpdf_cube.coord(thickname).points
    dummy = [' ',' ',' ',' ',' ',' ',' ']
    
    #plt.plot(mean_tcc_cube.data)
    #plt.plot(mean_albedo_cube.data)
    #plt.plot(mean_ctp_cube.data)
    #plt.xlim(-5.0,100.0)
    #plt.vlines(0, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(10, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(20, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(30, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(40, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(50, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(60, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.vlines(70, ymin=0,ymax=1, linestyle='dotted',color='red')
    #plt.show()
    #sys.exit(0)
    fig = plt.figure(figsize=[11.4,11.4])
  
    for k in range(0, 90,10):
        pdf = jointpdf_cube.data[k,:,:]
        ax=plt.subplot(3,3,np.int(np.floor(k/10)+1))
        cs = ax.pcolormesh(pdf)
        if k==0 or k==30 or k==60 or k==150:
            plt.yticks([1,2,3,4,5,6,7], cloudtop_press/100., size=7)
            plt.ylabel('cloud top pressure')
        else:
            ax.set_yticklabels(dummy)
        if k>=60:
            plt.xticks([1,2,3,4,5,6,7], optical_depth, size=7,rotation=90)
            plt.xlabel('cloud optical depth')
        else:
            ax.set_xticklabels(dummy)
        plt.colorbar(cs)
        #plt.title(np.str(k) + ': ctp='+ "{:.2f}".format(mean_ctp_cube.data[k]),fontsize=7) 
        plt.title('alb='+"{:.2f}".format(mean_albedo_alt_cube.data[k]) + ': ctp='+ "{:.2f}".format(mean_ctp_alt_cube.data[k])+"\n" + 'tcc=' + "{:.2f}".format(mean_tcc_alt_cube.data[k]),fontsize=8)
    plt.subplots_adjust()
    plt.savefig('tropics_sample20.eps')
    plt.savefig('tropics_sample20.png')
    plt.close()

    return (jointpdf_cube.data,cloudtop_press, optical_depth,
            mean_ctp_alt_cube, mean_tcc_alt_cube, mean_albedo_alt_cube)
################################################################
def get_cloud_regiemes(joint_pdf_data, NK, cloud_regiemes, ctp, tau):
    """
    We are subsetting the joint_pdf_data into NK cloud regiemes
    We have some cloud_regiemes to start off with in cloud_regiemes
     
    For each pdf in joint_pdf_data
    1. decide which cloud regieme is nearest to this pdf
    2. add the pdf to this cloud regieme
    3. recalculate the cloud regieme with the addition of this pdf
    """
    fig = plt.figure(figsize=[11.4,11.4])
    dummy = [' ',' ',' ',' ',' ',' ',' ']
    nz, ny, nx = np.shape(joint_pdf_data)
    for k in range(0, 200,40):
        pdf = joint_pdf_data[:,:,k]
        ax=plt.subplot(4,5,k+1)
        cs = ax.pcolormesh(pdf)
        if k >=15:
            plt.xticks([1,2,3,4,5,6,7], tau, size=7, rotation=90)
        else:
            ax.set_xticklabels(dummy)
        if k==0 or k==5 or k==10 or k==15:
            plt.yticks([1,2,3,4,5,6,7], ctp, size=7)
        else:
            ax.set_yticklabels(dummy)
        plt.colorbar(cs)
        plt.title(np.str(k))
    plt.savefig('first20_histograms.eps')
    plt.close()


def test_sklearn(joint_pdf_data, tau, ctp):
    """
    testing stuff from the tutorial
    """
    nclusters=8
    nx, ny, nz = joint_pdf_data.shape
    joint_pdf_reshape = joint_pdf_data[:, :, :].reshape((nx * ny, nz))
    joint_pdf = np.transpose(joint_pdf_reshape)
    print(joint_pdf_reshape.shape)
    plt.subplot(2,1,1)
    for i in range(0,20):
        plt.plot(joint_pdf_reshape[:,i])

    kmeans = KMeans(init="random", n_clusters=nclusters, n_init=10)
    which_cluster=kmeans.fit_predict(joint_pdf)
    print(which_cluster[0:20])

    print(kmeans.cluster_centers_.shape)
    cluster_centers = kmeans.cluster_centers_.reshape((nclusters,nx, ny))
    plt.subplot(2,1,2)
    for i in range(0,nclusters):
        plt.plot(kmeans.cluster_centers_[i,:])
    plt.savefig('2d_clusters.eps')
    plt.close()
  
    fig = plt.figure(figsize=[11.4,11.4])
    print('cluster centers shape', cluster_centers.shape)

    for i in range(0,nclusters):
        ax = plt.subplot(3,3,i+1)
        cs = ax.pcolormesh(cluster_centers[i, :, :])
        plt.xticks([1,2,3,4,5,6,7], tau, size=7, rotation=90)
        plt.yticks([1,2,3,4,5,6,7], ctp, size=7)
        plt.colorbar(cs)
    plt.savefig('test_clusters.eps')
    plt.savefig('test_clusters.png')
    print('kmeans niter',kmeans.n_iter_)
    plt.close()
    # elbow method to see how many clusters
    #see=[]
    #for k in range(1,31):
    #    kmeans = KMeans(n_clusters=k)
    #    kmeans.fit(joint_pdf)
    #    see.append(kmeans.inertia_)
    #plt.plot(range(1,31),see)
    #plt.show()

       

def test_sklearn_tropics(joint_pdf_data, tau, ctp):
    """
    testing stuff from the tutorial
    """


    nclusters=9
    nx, ny, nz = joint_pdf_data.shape
    joint_pdf = joint_pdf_data[:, :, :].reshape((nx,nz * ny))
    print(joint_pdf.shape)
    
    scalar = StandardScaler()
    scale = scalar.fit_transform(joint_pdf)
    joint_pdf = scale
    print(joint_pdf_data)
    plt.subplot(2,1,1)
    for i in range(0,20):
        plt.plot(joint_pdf[i,:])

    kmeans = KMeans(init="k-means++", n_clusters=nclusters, n_init=10)
    which_cluster=kmeans.fit_predict(joint_pdf)
    print(which_cluster)
    total_in_cluster = np.zeros(nclusters)
    largest_cluster=0
    for i in range(0,nclusters):
        total_in_cluster[i] = np.sum((np.where(which_cluster == i, 1.0, 0.0)))
        if total_in_cluster[i] == np.max(total_in_cluster):
            largest_cluster=i

    print(kmeans.cluster_centers_.shape)
    cluster_centers = kmeans.cluster_centers_.reshape((nclusters,nz, ny))
    #plt.subplot(2,1,2)
    #for i in range(0,nclusters):
    #    plt.plot(kmeans.cluster_centers_[i,:])
    #plt.savefig('2d_clusters.eps')
    #plt.close()
  
    fig = plt.figure(figsize=[11.4,11.4])
    print('cluster centers shape', cluster_centers.shape)

    for i in range(0,nclusters):
        ax = plt.subplot(3,3,i+1)
        cs = ax.pcolormesh(cluster_centers[i, :, :])
        plt.xticks([1,2,3,4,5,6,7], tau, size=7, rotation=90)
        plt.yticks([1,2,3,4,5,6,7], ctp, size=7)
        plt.title("{:.1f}".format(total_in_cluster[i] *100./ np.sum(total_in_cluster)) + '%')
        plt.colorbar(cs)
    plt.savefig('test_clusters.eps')
    plt.savefig('test_clusters.png')
    print('kmeans niter',kmeans.n_iter_)
    print('largest cluster',largest_cluster)
    plt.close()
    # elbow method to see how many clusters
    #see=[]
    #for k in range(1,31):
    #    kmeans = KMeans(n_clusters=k)
    #    kmeans.fit(joint_pdf)
    #    see.append(kmeans.inertia_)
    #plt.plot(range(1,31),see)
    #plt.show()

 ##########################################################
def tropical_cloud_regiemes(CTP_data, TCC_data, albedo_data,
                            joint_pdf_data, ctpall, tauall):
    """
    estimates the cloud regieme for each tropical gridpoint
    """
    REGIEME_NAMES_TR = {0:"Shallow cumulus", 1:"Congestus",
                 2:"Thin cirrus", 3:"Stratocu./Cu. Transition",
                 4:"Anvil cirrus", 5:"Deep Convection",
                        6:"Stratocumulus", 7:"ClearSky"}
    REG_CHARS = {0:[0.261, 0.652, 0.314], 
                 1:[0.339, 0.483, 0.813],
                 2:[0.211, 0.356, 0.740],
                 3:[0.338, 0.784, 0.640],
                 4:[0.313, 0.327,0.944],
                 5:[0.532, 0.285,0.979],
                 6:[0.446, 0.722, 0.824]}

    regieme=np.zeros(len(CTP_data))
    min_dists=np.zeros(len(CTP_data))
    nz, ny, nx = np.shape(joint_pdf_data)
    regieme_pdf = np.zeros((8, ny, nx))
    regieme_count = np.zeros((8))
    dists = np.zeros(7)
    for i,ctp in enumerate(CTP_data):
        features = np.array([albedo_data[i], ctp, TCC_data[i]])
        if TCC_data[i] < 0.05:
            regieme[i]=7
            regieme_pdf[7,:,:] = joint_pdf_data[i,:,:]
            regieme_count[7] = regieme_count[7] + 1.0
        else:
            for reg in range(0,7):
                dists[reg] = np.linalg.norm(features - REG_CHARS.get(reg))
            regieme[i] = np.argmin(dists)
            regieme_pdf[int(regieme[i]), :, :] = joint_pdf_data[i,:,:]
            regieme_count[int(regieme[i])]=regieme_count[int(regieme[i])] + 1.0
            min_dists[i] = dists[np.argmin(dists)]
            if i==0 or i==10 or i==20 or i==30 or i==40 or i==50 or i==170:
                print(i,REGIEME_NAMES_TR.get(regieme[i]),dists,features)

    for reg in range(0,8):
        print(reg,np.sum(np.where(regieme==reg, 1.0, 0.0))*100./275.,regieme_count[reg]*100./275.)

    # create average regieme for each type and plot
    for reg in range(0,8):
        regieme_pdf[reg,:,:] = regieme_pdf[reg,:,:] / regieme_count[reg]
    
    fig = plt.figure(figsize=[11.4,11.4])
   
    for i in range(0,8):
        ax = plt.subplot(3,3,i+1)
        cs = ax.pcolormesh(regieme_pdf[i, :, :])
        plt.xticks([1,2,3,4,5,6,7], tauall, size=7, rotation=90)
        plt.yticks([1,2,3,4,5,6,7], ctpall, size=7)
        plt.title(REGIEME_NAMES_TR.get(i)+np.str(regieme_count[i]))
        plt.colorbar(cs)
    plt.savefig('williams_webb_tropical_clusters.eps')
    
    
########################################################
# main program

FILEIN = '/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/COLLABORATORS/TAMARA/CFMIP/COSPv2.0_juliatest/driver/data/outputs/UKMO/cosp2_output_um.nc'

# note joint pdf data(7, 7, 1236) dim3=loc/time, dim2=optical depth, dim1=ctp
#(joint_pdf_data, cloudtop_press, optical_depth) = get_pdfcube()

# subset the data into NK cloud_regiemes the start cloud regiemes will 
# be the first NK locations of joint_pdf_data
#NK=2
#start_cloud_regiemes = joint_pdf_data[0:NK, :, :]
#cloud_regiemes = get_cloud_regiemes(joint_pdf_data, NK, start_cloud_regiemes,
#                                    cloudtop_press,optical_depth)

print('julia0')
(joint_pdf_data, cloudtop_press, optical_depth, mean_ctp_cube, 
mean_tcc_cube, mean_albedo_cube) = get_pdfcube_tropics()

# attempt to put mean_ctp,mean_tcc and mean_albedo onto the cloud regieme
# averages
print('juli1')
cloud_regiemes = tropical_cloud_regiemes(mean_ctp_cube.data, mean_tcc_cube.data, mean_albedo_cube.data, joint_pdf_data, cloudtop_press, optical_depth)

print('julia2')
#plt.subplot(311)
#plt.plot(mean_ctp_cube.data)
#plt.subplot(312)
#plt.plot(mean_tcc_cube.data)
#plt.subplot(313)
#plt.plot(mean_albedo_cube.data)
#plt.show()

test_sklearn_tropics(joint_pdf_data, optical_depth, cloudtop_press)

::::::::::::::
COLLABORATORS/TAMARA/cloud_regiemes_IPSL.py
::::::::::::::
1#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

We will try and create some cloud regiemes from IPSL data 
"""
import numpy as np
import iris
#from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt

# stuff for kmeans clustering
#from kneed import KneeLocator
#from sklearn.datasets import make_blobs
#from sklearn.cluster import KMeans
#from sklearn.preprocessing import StandardScaler
#from sklearn.metrics import silhouette_score
#from sklearn.preprocessing import StandardScalar

import sys

  

def get_albedo_ctp(joint_pdf_cube,tcc_cube):
    """
    this will estimate the albedo from the jointpdf cube instead of the file
    """
    # albedos corresponding to the optical depths in the pdf boxes
    # from williams and webb 2008
    albedos = [0.028, 0.107, 0.232, 0.407, 0.626, 0.828, 0.950]
    ctp = joint_pdf_cube.coord('pressure2').points
    tau = joint_pdf_cube.coord('atmosphere_optical_thickness_due_to_cloud').points
    # when calculating albedo it is unclear whether we should average over all points in the histogram or only over those where there is cloud.  
    # I have decided to only average over those where there is cloud, because it is cloud albedo not total albedo
    nt, npress, ntau, ny,nx = joint_pdf_cube.shape # nt = times, npress=pressures, ntau =optical depth, ny = latitude, nx=lonagitude
    albedo_data = np.zeros((nt,ny,nx))
    ctp_data = np.zeros((nt,ny,nx))
    for t in range(0,5): # over times
        print('t is',t)
        for j in range(0,ny): # over lats'
            for i in range(0,nx): # over lons'
                temporary = joint_pdf_cube[t,:,:,j,i]
                datapdf=temporary.data
                meanalbedo=0.0
                avgctp =0.0
                divisor=0.0
                for ictp in range(0,npress): # over ctp
                    for itau in range(0,ntau): # over optical depth
                        if datapdf[ictp,itau] > 0.0:
                            meanalbedo = (meanalbedo +
                                        (datapdf[ictp,itau] * albedos[itau]))
                            avgctp = (avgctp + datapdf[ictp,itau] *ctp[ictp])
                            divisor = divisor + datapdf[ictp,itau]
                if divisor > 0:
                    albedo_data[t,j,i] = meanalbedo / divisor
                    ctp_data[t,j,i] = avgctp /divisor
                else:
                    albedo_data[t,j,i]=0
                    ctp_data[t,j,i]=0
    albedo_cube = tcc_cube.copy(data=albedo_data)
    ctp_cube = tcc_cube.copy(data=ctp_data)
   
    albedo_cube.long_name = 'albedo'
    ctp_cube.long_name = 'ctp'
    return albedo_cube, ctp_cube

def get_pdfcubes_region(tstart,tend):
    """
    gets the pdfcube and extracts the points that are required only
    input:  cube is the joint histograms for the globe
    """

    # this dictionary has latmin, latmax, lonmin, lonmax
    latlons = {"TWP" : [-20.0, 20.0, 130., 200.0]}

    # read in the file
    cube = iris.load_cube(FILENAME)
    cube.coord('atmosphere_optical_thickness_due_to_cloud').bounds=None
    iris.util.promote_aux_coord_to_dim_coord(cube,'atmosphere_optical_thickness_due_to_cloud')
    cube.attributes = None
    cube.cell_methods = None
    
    # extract the data for the region
    reglatlons = latlons.get(REGION)
    region_constraint = iris.Constraint(
        latitude = lambda cell: reglatlons[0] <= cell <= reglatlons[1], 
        longitude = lambda cell2: reglatlons[2] <=cell2 <=reglatlons[3]
    ) 
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points

    print('got region cube')
    region_pdf_cube_full = cube.extract(region_constraint)
    print(region_pdf_cube_full)
    region_pdf_cube = region_pdf_cube_full[tstart:tend, :,:,:,:]
    region_pdf_cube_full = 0
    cube =0.
    
    # get ctp, cloud tau, cloud albedo and cloud amount
    
    cloud_amount_cube = region_pdf_cube.collapsed(['pressure2','atmosphere_optical_thickness_due_to_cloud'],iris.analysis.SUM)
    cloud_amount_cube.long_name = 'tcc'
    print('got cloud amount cube')
    albedo_cube,ctp_cube = get_albedo_ctp(region_pdf_cube,cloud_amount_cube)

    fileout = '/nfs/hera1/earjcti/ISCCP/IPSL/'+REGIONNAMES.get(REGION)+'/tcc_alb_tcp_' + REGION + '_' + np.str(tstart) + '_' + np.str(tend) + '.nc'
    iris.save([region_pdf_cube,cloud_amount_cube, albedo_cube, ctp_cube],fileout,netcdf_format = "NETCDF3_CLASSIC",fill_value = -99999)
    sys.exit(0)

    
  
    for k in range(0, 200,10):
        pdf = jointpdf_cube.data[k,:,:]
        ax=plt.subplot(4,5,np.int(np.floor(k/10)+1))
        cs = ax.pcolormesh(pdf)
        if k==0 or k==50 or k==100 or k==150:
            plt.yticks([1,2,3,4,5,6,7], cloudtop_press/100., size=7)
        else:
            ax.set_yticklabels(dummy)
        if k>=150:
            plt.xticks([1,2,3,4,5,6,7], optical_depth, size=7,rotation=90)
        else:
            ax.set_xticklabels(dummy)
        plt.colorbar(cs)
        #plt.title(np.str(k) + ': ctp='+ "{:.2f}".format(mean_ctp_cube.data[k]),fontsize=7) 
        plt.title('alb='+"{:.2f}".format(mean_albedo_alt_cube.data[k]) + ': ctp='+ "{:.2f}".format(mean_ctp_alt_cube.data[k])+"\n" + 'tcc=' + "{:.2f}".format(mean_tcc_alt_cube.data[k]),fontsize=8)
    plt.savefig('tropics_sample20.eps')
    plt.close()

    return (jointpdf_cube.data,cloudtop_press, optical_depth,
            mean_ctp_alt_cube, mean_tcc_alt_cube, mean_albedo_alt_cube)
################################################################
def get_cloud_regiemes(joint_pdf_data, NK, cloud_regiemes, ctp, tau):
    """
    We are subsetting the joint_pdf_data into NK cloud regiemes
    We have some cloud_regiemes to start off with in cloud_regiemes
     
    For each pdf in joint_pdf_data
    1. decide which cloud regieme is nearest to this pdf
    2. add the pdf to this cloud regieme
    3. recalculate the cloud regieme with the addition of this pdf
    """
    fig = plt.figure(figsize=[11.4,11.4])
    dummy = [' ',' ',' ',' ',' ',' ',' ']
    nz, ny, nx = np.shape(joint_pdf_data)
    for k in range(0, 200,40):
        pdf = joint_pdf_data[:,:,k]
        ax=plt.subplot(4,5,k+1)
        cs = ax.pcolormesh(pdf)
        if k >=15:
            plt.xticks([1,2,3,4,5,6,7], tau, size=7, rotation=90)
        else:
            ax.set_xticklabels(dummy)
        if k==0 or k==5 or k==10 or k==15:
            plt.yticks([1,2,3,4,5,6,7], ctp, size=7)
        else:
            ax.set_yticklabels(dummy)
        plt.colorbar(cs)
        plt.title(np.str(k))
    plt.savefig('first20_histograms.eps')
    plt.close()


def test_sklearn(joint_pdf_data, tau, ctp):
    """
    testing stuff from the tutorial
    """
    nclusters=8
    nx, ny, nz = joint_pdf_data.shape
    joint_pdf_reshape = joint_pdf_data[:, :, :].reshape((nx * ny, nz))
    joint_pdf = np.transpose(joint_pdf_reshape)
    print(joint_pdf_reshape.shape)
    plt.subplot(2,1,1)
    for i in range(0,20):
        plt.plot(joint_pdf_reshape[:,i])

    kmeans = KMeans(init="random", n_clusters=nclusters, n_init=10)
    which_cluster=kmeans.fit_predict(joint_pdf)
    print(which_cluster[0:20])

    print(kmeans.cluster_centers_.shape)
    cluster_centers = kmeans.cluster_centers_.reshape((nclusters,nx, ny))
    plt.subplot(2,1,2)
    for i in range(0,nclusters):
        plt.plot(kmeans.cluster_centers_[i,:])
    plt.savefig('2d_clusters.eps')
    plt.close()
  
    fig = plt.figure(figsize=[11.4,11.4])
    print('cluster centers shape', cluster_centers.shape)

    for i in range(0,nclusters):
        ax = plt.subplot(3,3,i+1)
        cs = ax.pcolormesh(cluster_centers[i, :, :])
        plt.xticks([1,2,3,4,5,6,7], tau, size=7, rotation=90)
        plt.yticks([1,2,3,4,5,6,7], ctp, size=7)
        plt.colorbar(cs)
    plt.savefig('test_clusters.eps')
    print('kmeans niter',kmeans.n_iter_)
    plt.close()
    # elbow method to see how many clusters
    #see=[]
    #for k in range(1,31):
    #    kmeans = KMeans(n_clusters=k)
    #    kmeans.fit(joint_pdf)
    #    see.append(kmeans.inertia_)
    #plt.plot(range(1,31),see)
    #plt.show()

       

def test_sklearn_tropics(joint_pdf_data, tau, ctp):
    """
    testing stuff from the tutorial
    """


    nclusters=9
    nx, ny, nz = joint_pdf_data.shape
    joint_pdf = joint_pdf_data[:, :, :].reshape((nx,nz * ny))
    print(joint_pdf.shape)
    
    scalar = StandardScaler()
    scale = scalar.fit_transform(joint_pdf)
    joint_pdf = scale
    print(joint_pdf_data)
    plt.subplot(2,1,1)
    for i in range(0,20):
        plt.plot(joint_pdf[i,:])

    kmeans = KMeans(init="k-means++", n_clusters=nclusters, n_init=10)
    which_cluster=kmeans.fit_predict(joint_pdf)
    print(which_cluster)
    total_in_cluster = np.zeros(nclusters)
    largest_cluster=0
    for i in range(0,nclusters):
        total_in_cluster[i] = np.sum((np.where(which_cluster == i, 1.0, 0.0)))
        if total_in_cluster[i] == np.max(total_in_cluster):
            largest_cluster=i

    print(kmeans.cluster_centers_.shape)
    cluster_centers = kmeans.cluster_centers_.reshape((nclusters,nz, ny))
    #plt.subplot(2,1,2)
    #for i in range(0,nclusters):
    #    plt.plot(kmeans.cluster_centers_[i,:])
    #plt.savefig('2d_clusters.eps')
    #plt.close()
  
    fig = plt.figure(figsize=[11.4,11.4])
    print('cluster centers shape', cluster_centers.shape)

    for i in range(0,nclusters):
        ax = plt.subplot(3,3,i+1)
        cs = ax.pcolormesh(cluster_centers[i, :, :])
        plt.xticks([1,2,3,4,5,6,7], tau, size=7, rotation=90)
        plt.yticks([1,2,3,4,5,6,7], ctp, size=7)
        plt.title("{:.1f}".format(total_in_cluster[i] *100./ np.sum(total_in_cluster)) + '%')
        plt.colorbar(cs)
    plt.savefig('test_clusters.eps')
    print('kmeans niter',kmeans.n_iter_)
    print('largest cluster',largest_cluster)
    plt.close()
    # elbow method to see how many clusters
    #see=[]
    #for k in range(1,31):
    #    kmeans = KMeans(n_clusters=k)
    #    kmeans.fit(joint_pdf)
    #    see.append(kmeans.inertia_)
    #plt.plot(range(1,31),see)
    #plt.show()

 ##########################################################
def tropical_cloud_regiemes(CTP_data, TCC_data, albedo_data,
                            joint_pdf_data, ctpall, tauall):
    """
    estimates the cloud regieme for each tropical gridpoint
    """
    REGIEME_NAMES_TR = {0:"Shallow cumulus", 1:"Congestus",
                 2:"Thin cirrus", 3:"Stratocu./Cu. Transition",
                 4:"Anvil cirrus", 5:"Deep Convection",
                        6:"Stratocumulus", 7:"ClearSky"}
    REG_CHARS = {0:[0.261, 0.652, 0.314], 
                 1:[0.339, 0.483, 0.813],
                 2:[0.211, 0.356, 0.740],
                 3:[0.338, 0.784, 0.640],
                 4:[0.313, 0.327,0.944],
                 5:[0.532, 0.285,0.979],
                 6:[0.446, 0.722, 0.824]}

    regieme=np.zeros(len(CTP_data))
    min_dists=np.zeros(len(CTP_data))
    nz, ny, nx = np.shape(joint_pdf_data)
    regieme_pdf = np.zeros((8, ny, nx))
    regieme_count = np.zeros((8))
    dists = np.zeros(7)
    for i,ctp in enumerate(CTP_data):
        features = np.array([albedo_data[i], ctp, TCC_data[i]])
        if TCC_data[i] < 0.05:
            regieme[i]=7
            regieme_pdf[7,:,:] = joint_pdf_data[i,:,:]
            regieme_count[7] = regieme_count[7] + 1.0
        else:
            for reg in range(0,7):
                dists[reg] = np.linalg.norm(features - REG_CHARS.get(reg))
            regieme[i] = np.argmin(dists)
            regieme_pdf[int(regieme[i]), :, :] = joint_pdf_data[i,:,:]
            regieme_count[int(regieme[i])]=regieme_count[int(regieme[i])] + 1.0
            min_dists[i] = dists[np.argmin(dists)]
            if i==0 or i==10 or i==20 or i==30 or i==40 or i==50 or i==170:
                print(i,REGIEME_NAMES_TR.get(regieme[i]),dists,features)

    for reg in range(0,8):
        print(reg,np.sum(np.where(regieme==reg, 1.0, 0.0))*100./275.,regieme_count[reg]*100./275.)

    # create average regieme for each type and plot
    for reg in range(0,8):
        regieme_pdf[reg,:,:] = regieme_pdf[reg,:,:] / regieme_count[reg]
    
    fig = plt.figure(figsize=[11.4,11.4])
   
    for i in range(0,8):
        ax = plt.subplot(3,3,i+1)
        cs = ax.pcolormesh(regieme_pdf[i, :, :])
        plt.xticks([1,2,3,4,5,6,7], tauall, size=7, rotation=90)
        plt.yticks([1,2,3,4,5,6,7], ctpall, size=7)
        plt.title(REGIEME_NAMES_TR.get(i)+np.str(regieme_count[i]))
        plt.colorbar(cs)
    plt.savefig('williams_webb_tropical_clusters.eps')
    
    
########################################################
# main program

REGIONNAMES = {"TWP" : "Tropical_Western_Pacific"}

# latlons format[Latmin, latmax, lonmin, lonmax]
FILENAME = '/nfs/hera1/earjcti/ISCCP/clisccp_CFday_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr_20250101-20491231.nc'

REGION='TWP' # TWP Tropical Western Pacific

regionpdfcubes = get_pdfcubes_region(0,366)
::::::::::::::
COLLABORATORS/TAMARA/cloud_regiemes_test.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

We are trying to plot cloud regiemes based on Williams and Webb 2008.
This is an experimental program
1. plot mean cloud albedo, 
2. cloud top pressure
3. total cloud cover
"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys

  
def get_cloud_regieme_diagnostics():
    """
    for this test I think we need the following diagnostics
    1. total cloud cover - TRY TOTAL CLOUD AMOUNT - RANDOM OVERLAP
    2. cloud top pressure
    3. mean cloud albedo
    """
    tempcube = iris.util.squeeze(iris.load_cube(
            TESTFILE, 'TOTAL CLOUD AMOUNT - RANDOM OVERLAP'))
    TCCcube=tempcube.collapsed('t_1',iris.analysis.MEAN)

    tempcube = iris.util.squeeze(iris.load_cube(
            TESTFILE, 'INCOMING SW RAD FLUX (TOA): ALL TSS'))
    SW_IN_TOP_cube = tempcube.collapsed('t_1',iris.analysis.MEAN)
    #mask = np.where(SW_IN_TOP_cube.data == 0, 1.0, 0.0)
    #SW_IN_TOP_cube.data.mask = mask
   
    tempcube = iris.util.squeeze(iris.load_cube(
            TESTFILE, 'TOTAL DOWNWARD SURFACE SW FLUX'))
    SW_DOWN_SURF_cube = tempcube.collapsed('t',iris.analysis.MEAN)
    
    cloud_albedo_cube_data = 1.0 - (SW_DOWN_SURF_cube.data / SW_IN_TOP_cube.data)
    cloud_albedo_cube = SW_DOWN_SURF_cube.copy(data=cloud_albedo_cube_data)
    cloud_albedo_cube.long_name = 'albedo'   
    # mask where there is not much incoming sw radiation
    cloud_albedo_cube.data.mask = np.where(SW_IN_TOP_cube.data<20.0, 1.0, 0.0)
    

  
    tempcube = iris.util.squeeze(iris.load_cube(
            TESTFILE,'TOTAL CLOUD TOP HEIGHT (KFT)'))
    cloud_top_pressure_cube = tempcube.collapsed('t_1',iris.analysis.MEAN)
    #TCCcube.data.mask = mask
    #cloud_albedo_cube.data.mask = mask
    #cloud_top_pressure_cube.data.mask = mask

    # try and find alternative albedo as above but remove effects of surface albedo
    # I think this is (1.0 - downward solar surface / upwards surface cs)
    
    tempcube = iris.util.squeeze(iris.load_cube(
            TESTFILE, 'CLEAR-SKY (II) UPWARD SW FLUX (TOA)'))
    SW_UPCS_TOA_cube = tempcube.collapsed('t',iris.analysis.MEAN)

    alt_alb_cube_data = 1.0 - (SW_DOWN_SURF_cube.data / SW_UPCS_TOA_cube.data)
    alt_alb_cube = SW_DOWN_SURF_cube.copy(data = alt_alb_cube_data)
    alt_alb_cube.long_name = 'alternative albedo'
    alt_alb_cube.data = np.where(alt_alb_cube.data<0.0, 0.0, alt_alb_cube.data)
    alt_alb_cube.data = np.where(alt_alb_cube.data>1.0, 1.0, alt_alb_cube.data)
    
    plt.subplot(221)
    qplt.contourf(cloud_top_pressure_cube, levels=np.arange(10000, 90000, 10000), extend='both')
    plt.gca().coastlines()
    plt.subplot(222)
    qplt.contourf(TCCcube, levels=np.arange(0.20, 1.05, 0.05), extend='both')
    plt.gca().coastlines()
    plt.subplot(223)
    qplt.contourf(cloud_albedo_cube, levels=np.arange(0.20, 0.60, 0.05), extend='both')
    plt.gca().coastlines()
    plt.subplot(224)
    qplt.contourf(alt_alb_cube, levels=np.arange(0.2, 0.6, 0.05), extend='both')
    plt.gca().coastlines()
    #plt.show()
    

   
    return alt_alb_cube, cloud_top_pressure_cube, TCCcube,cloud_albedo_cube
                                 
  
##########################################################
def get_tropical_regiemes(albedo_cube, CTP_cube, TCC_cube,orig_alb_cld):
    """
    estimates the cloud regieme for each tropical gridpoint
    """
    REGIEME_NAMES_TR = {0:"Shallow cumulus", 1:"Congestus",
                 2:"Thin cirrus", 3:"Stratocu./Cu. Transition",
                 4:"Anvil cirrus", 5:"Deep Convection",
                 6:"Stratocumulus"}
    REGIEME_CHARACTERISTICS = {0:[0.261, 0.652, 0.314], 
                           1:[0.339, 0.483, 0.813],
                           2:[0.211, 0.356, 0.740],
                           3:[0.338, 0.784, 0.640],
                           4:[0.532, 0.285,0.979],
                           5:[0.446, 0.722, 0.824]}
   
    # upper and lower value of bin
    bin_lower = [0.,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0] 
    bin_upper = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1000000]
    binnamesp = []
    binnames = []
    for i, low in enumerate(bin_lower):
        binnamesp.append(np.str(np.int(low * 1000.)))
        binnames.append(np.str(np.int(low)))
    
   # binnames.append("{:.1f}".format(bin_upper[-1]))
    pressure_bins = np.zeros(11)
    TCC_bins = np.zeros(11)
    albedo_bins = np.zeros(11)
    alt_albedo_bins = np.zeros(11)

    for j, lat in enumerate(albedo_cube.coord('latitude').points):
        if -20 <= lat <= 20:
            for i, lon in enumerate(albedo_cube.coord('longitude').points):
                # get pressure
                pressure=CTP_cube.data[j,i] / 100000.
                cloud_frac = TCC_cube.data[j,i] 
                if cloud_frac == 0.0:
                    albedo=0.0
                else:
                    albedo = albedo_cube.data[j,i] / cloud_frac

                o_albedo = orig_alb_cld.data[j,i]
                print("{:.3f}".format(pressure))
                for b, low in enumerate(bin_lower):
                    if low < pressure < bin_upper[b]:
                        pressure_bins[b] = pressure_bins[b] + 1
                    if low < cloud_frac < bin_upper[b]:
                        TCC_bins[b] = TCC_bins[b] + 1
                    if low < o_albedo < bin_upper[b]:
                        alt_albedo_bins[b] = alt_albedo_bins[b] + 1
             

                #print(lon,lat,"{:.2f}".format(albedo),"{:.2f}".format(o_albedo),
#                      "{:.3f}".format(pressure),"{:.2f}".format(cloud_frac))
               
                                        

    # plot bar charts of where everything is
    plt.subplot(3,1,1)                   
    plt.bar(binnamesp,pressure_bins,align = 'edge')
    plt.xlabel('Pressure (hPa)')
    plt.ylabel('Number')
    plt.title('Tropical cloud top pressure')
  
    plt.subplot(3,1,2)                   
    plt.bar(binnames,TCC_bins,align = 'edge')
    plt.xlabel('Fraction')
    plt.ylabel('Number')
    plt.title('Cloud Fraction (tropics)')
    #plt.show()
########################################################
# main program

#regiemes are defined in Williams and web characteristics are 
# cloud albedo (frac), cloud top pressure (hpa) / 1000, total cloud cover (frac)
# tropics
# extratropics
REGIEME_NAMES_ET = {0: "Shallow cumulus", 1:"Congestus (TR)",
                 2:"Stratocu./Cu. Transition (TR)", 3: "cirrus",
                 4:"Stratocumulus", 5:"Frontal",
                 6:"Thin Cirrus"}
# extratropics - ice covered
REGIEME_NAMES_ETice = {0: "Shallow cumulus", 1:"Stratocumulus",
                 2:"Thick mid-level", 3: "Frontal",
                 4:"Thin mid-level", 5:"Thin Cirrus"}

MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

EXPT = 'tenvj'
#TESTFILE = '/nfs/hera1/earjcti/um/tenvo/pd/tenvoa@pdt99sp.nc'
TESTFILE = 'xozzaa@pap00dc.nc'
(albedo_cube, cloud_top_pressure_cube, TCCcube,orig_cloud_albedo) = get_cloud_regieme_diagnostics() 

iris.save([albedo_cube, cloud_top_pressure_cube, TCCcube,orig_cloud_albedo],'test.nc',
          fill_value = -999.999)


#topical_cloud_regiemes = get_tropical_regiemes(albedo_cube, cloud_top_pressure_cube, TCCcube,orig_cloud_albedo)


::::::::::::::
COLLABORATORS/TAMARA/extract_cloud_regiemes_single_site.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

We are trying to plot cloud regiemes based on Williams and Webb 2008.

We should have extracted:
1. cloud albedo
2. cloud top pressure
3. total cloud cover

use this to get cloud regiemes at a site
"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from numpy.linalg import norm  # used in calculating euclidian distance
import sys

def get_data():
    """
    extracts the data at the nearest gridbox
    """

    albedo_cube = iris.load_cube(FILEIN,'albedo')
    ctp_cube = iris.load_cube(FILEIN,'TOTAL CLOUD TOP HEIGHT (KFT)')
    tcc_cube = iris.load_cube(FILEIN,'TOTAL CLOUD AMOUNT - RANDOM OVERLAP')

    # find nearest longitude and latitude
    lonix = np.abs(albedo_cube.coord('longitude').points - SITELON).argmin()
    latix = np.abs(albedo_cube.coord('latitude').points - SITELAT).argmin()
   
    albedo = albedo_cube.data[latix,lonix]
    tcc = tcc_cube.data[latix,lonix]
    ctp = ctp_cube.data[latix,lonix]

    return albedo, tcc, ctp
##########################################################################
def get_ice_free_et_clouds(observations):
    """
    input observations are [albedo, total cloud cover, cloud top pressure]
    normalised on scale 0.1 
    gets the cloud types for an ice free extra tropics site
    """
    REGIEME_NAMES_ET = {0:"Shallow cumulus", 1:"Congestus",
                  2:"Stratocu./Cu. Transition", 3:"cirrus",
                  4:"Stratocumulus", 5: "Frontal", 6:"Thin Cirrus"}
                 
    REGIEME_CHARACTERISTICS = {0:np.array([0.286, 0.643, 0.473]), 
                           1:[0.457, 0.607, 0.932],
                           2:[0.375, 0.799, 0.802],
                           3:[0.325, 0.430, 0.914],
                           4:[0.438, 0.723,0.900],
                           5:[0.581, 0.393, 0.978],
                           6:[0.220, 0.389, 0.713] }

    norms = np.zeros(7)
    for cloud_type in range(0, 7):
        chars = REGIEME_CHARACTERISTICS.get(cloud_type)
        norms(cloud_type) = norm(observations - chars))
                                   
         



   
#########################################################################   
#SITENAME = 'Beaver Pond'
#SITELAT = 79.0   # 79N
#SITELON = 278.0  # 82W 278E

SITENAME = 'York'
SITELAT = 54.0
SITELON = 0.0
FILEIN = 'test.nc'

albedo, tcc, ctp = get_data()

cloud_types = get_ice_free_et_clouds(np.array([albedo,tcc,ctp/100000.]))

::::::::::::::
COLLABORATORS/TAMARA/plot_cloud_features.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

We are trying to plot cloud regiemes based on Williams and Webb 2008.

We should have extracted:
1. cloud albedo
2. cloud top pressure
3. total cloud cover

use this to get cloud regiemes at a site
"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from numpy.linalg import norm  # used in calculating euclidian distance
import sys

def get_data():
    """
    extracts the data at the nearest gridbox
    """

    ctp_cube = iris.load_cube(FILEIN,'TOTAL CLOUD TOP HEIGHT (KFT)')
    tcc_cube = iris.load_cube(FILEIN,'TOTAL CLOUD AMOUNT - RANDOM OVERLAP')

    # find nearest longitude and latitude
    lonix = np.abs(ctp_cube.coord('longitude').points - SITELON).argmin()
    latix = np.abs(ctp_cube.coord('latitude').points - SITELAT).argmin()
   
    tcc = iris.util.squeeze(tcc_cube[:, :, latix,lonix])
    ctp = iris.util.squeeze(ctp_cube.data[:, :, latix,lonix])

    # calculate albedo
    SW_IN_TOP_cube = iris.util.squeeze(iris.load_cube(
            FILEIN, 'INCOMING SW RAD FLUX (TOA): ALL TSS'))
   
    SW_DOWN_SURF_cube = iris.util.squeeze(iris.load_cube(
            FILEIN, 'TOTAL DOWNWARD SURFACE SW FLUX'))
    
    cloud_albedo_cube_data = 1.0 - (SW_DOWN_SURF_cube.data / SW_IN_TOP_cube.data)
    cloud_albedo_cube = SW_DOWN_SURF_cube.copy(data=cloud_albedo_cube_data)
    albedo = cloud_albedo_cube.data[:, latix, lonix]

    swintop = SW_IN_TOP_cube[:, latix, lonix]
    swdownsurf = SW_DOWN_SURF_cube[:, latix, lonix]

  
    return albedo, tcc, ctp, swintop, swdownsurf
##########################################################################
def plot_timeseries(albedo, tcc, ctp, swintop, swdownsurf):
    """
    plots the timeseries of the things we are using to diagnose cloud
    """
    tccdata = tcc.data
    time = np.arange(0, len(tccdata)) / 8.0
    plt.figure(figsize=(8.0, 12.0))
    plt.subplot(4,1,1)
    plt.plot(time,tccdata)
    plt.title(MONTHNAME + ' Total cloud cover')
    plt.ylabel('frac')
    
    plt.subplot(4,1,2)
    ctpdata = np.where(ctp.data > 1E10, np.nan, ctp.data)
    plt.plot(time,ctpdata/1000)
    plt.ylim(1000, 10)
    plt.title(MONTHNAME + ' Cloud Top Pressure')
    plt.xlabel('day')
    plt.ylabel('mb')
    plt.yscale('log')
   
    albedodata = albedo.data
    plt.subplot(4,1,3)
    plt.plot(time,swintop.data, color='red')
    plt.plot(time + 1.0/8.0,swdownsurf.data, color='blue')
    plt.plot(time,swdownsurf.data, color='cyan')
    #plt.plot(time,albedo.data)
    plt.title(MONTHNAME + ' radiation')
    #plt.ylim(0,1)
    plt.xlim(0,5)
    plt.ylabel('w/m2')
   
    albedodata = np.where(swintop.data < 50, np.nan, albedodata)
    plt.subplot(4,1,4)
    plt.plot(time,albedodata)
    plt.title(MONTHNAME + ' Albedo')
    plt.ylim(0,1)
    plt.ylabel('0-1')
   
    plt.subplots_adjust()
    plt.show()
    sys.exit(0)
##########################################################################
def get_ice_free_et_clouds(observations):
    """
    input observations are [albedo, total cloud cover, cloud top pressure]
    normalised on scale 0.1 
    gets the cloud types for an ice free extra tropics site
    """
    REGIEME_NAMES_ET = {0:"Shallow cumulus", 1:"Congestus",
                  2:"Stratocu./Cu. Transition", 3:"cirrus",
                  4:"Stratocumulus", 5: "Frontal", 6:"Thin Cirrus"}
                 
    REGIEME_CHARACTERISTICS = {0:np.array([0.286, 0.643, 0.473]), 
                           1:[0.457, 0.607, 0.932],
                           2:[0.375, 0.799, 0.802],
                           3:[0.325, 0.430, 0.914],
                           4:[0.438, 0.723,0.900],
                           5:[0.581, 0.393, 0.978],
                           6:[0.220, 0.389, 0.713] }

   # norms = np.zeros(7)
   # for cloud_type in range(0, 7):
   #     chars = REGIEME_CHARACTERISTICS.get(cloud_type)
   #     norms(cloud_type) = norm(observations - chars))
                                   
         



   
#########################################################################   
#SITENAME = 'Beaver Pond'
#SITELAT = 79.0   # 79N
#SITELON = 278.0  # 82W 278E

SITENAME = 'York'
SITELAT = 54.0
SITELON = 0.0
MONTHNAME = 'July' 
shortmonth = {'July' : 'jl'}
FILEIN = 'xozzaa@pap01' + shortmonth.get(MONTHNAME) +  '.nc'

albedo, tcc, ctp, swintop, swdownsurf = get_data()
plot_timeseries(albedo, tcc,ctp, swintop, swdownsurf)

cloud_types = get_ice_free_et_clouds(np.array([albedo,tcc,ctp/100000.]))

::::::::::::::
COLLABORATORS/TAMARA/plot_radiation_blocked.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

We are trying to see how much radiation the clouds block.

We are going to plot.
Downward surface radiation (clear sky)  / downward surface radiation.
"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from numpy.linalg import norm  # used in calculating euclidian distance
import sys

def get_data():
    """
    extracts the data at the nearest gridbox
    """

    swdowncube = iris.load_cube(FILEIN,'TOTAL DOWNWARD SURFACE SW FLUX')
    swdowncube_cs = iris.load_cube(FILEIN,'CLEAR-SKY (II) DOWN SURFACE SW FLUX')
    totcloud_cube = iris.load_cube(FILEIN,'TOTAL CLOUD AMOUNT - RANDOM OVERLAP')
   
    ratio_remaining = np.where(swdowncube.data > 20.0, 
                                swdowncube.data / swdowncube_cs.data, 
                              np.nan)
   
    ratio_blocked = np.where(swdowncube.data > 20.0, 
                               (1.0 - swdowncube.data / swdowncube_cs.data), 
                              np.nan)
    ratio_rem_cube = swdowncube.copy(data = ratio_remaining)
    ratio_rem_cube.long_name = 'ratio of sw radiation remaining after clouds'
    ratio_rem_cube.units = None
    ratio_rem_cube = iris.util.squeeze(ratio_rem_cube)
    ratio_blocked_cube = swdowncube.copy(data = ratio_blocked)
    ratio_blocked_cube.long_name = 'ratio of sw radiation blocked by clouds'
    ratio_blocked_cube = iris.util.squeeze(ratio_blocked_cube)
    ratio_blocked_cube.units = None

    totcloud_cube = iris.util.squeeze(totcloud_cube)


   
  
    return ratio_rem_cube, ratio_blocked_cube, totcloud_cube
##########################################################################
def plot_field(fieldcube, land_only, preind, totcloud_cube):
    """
    plots the cube 'fieldcube'
    """
    plt.figure(figsize=[11.0,6.0])
    if land_only == 'y':
       if preind == 'n':
            maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
       else:
            maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/e280/qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
       maskcube = iris.util.squeeze(maskcube)
       print(maskcube.data)
       print(fieldcube)
       fieldcube.data = np.ma.masked_where(maskcube.data == 0.0, fieldcube.data)
       totcloud_cube.data = np.ma.masked_where(maskcube.data == 0.0, totcloud_cube.data)
      
    plt.subplot(121)
    qplt.contourf(fieldcube, extend='max', levels = np.arange(0,0.55, 0.05))
    plt.gca().coastlines()
    plt.subplot(122)
    qplt.contourf(totcloud_cube, levels=np.arange(0, 1.1, 0.1))
    plt.gca().coastlines()
  
    plt.savefig('radiation_blocked_' + MONTHNAME + '.eps')


   
#########################################################################   
#SITENAME = 'Beaver Pond'
#SITELAT = 79.0   # 79N
#SITELON = 278.0  # 82W 278E

MONTHNAME = 'December' 
shortmonth = {'July' : 'jl', 'December' : 'dc','April':'ar'}
FILEIN = 'xozzaa@pdp00' + shortmonth.get(MONTHNAME) +  '.nc'
PRE_IND = 'y'

ratio_rem_cube, ratio_blocked_cube, totcloud_cube = get_data()
plot_field(ratio_blocked_cube,'y',PRE_IND, totcloud_cube)



::::::::::::::
COLLABORATORS/TAMARA/test.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 29.03.2022 by Julia

this is an experimental program to see what is in the MOSIS-CR***nc4 file
"""
import numpy as np
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys

  

TESTFILE = 'MODIS_CR.EqualAngle_daily.C61.V1.0.L3.2017.nc4'
cubes = iris.load(TESTFILE)
print(cubes)
::::::::::::::
COMMON/basic_plots/avg_fields.py
::::::::::::::
#NAME
#    average_fields.py
#PURPOSE 
#
#  This program will create netcdf files containing the average fields
#
#
# Julia 8.2.2017
# Julia 20.10.2018 ; included the ability to create database HadCM3 files


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")



class main():
    """
    this class will basically run the profram
    """
    def __init__(self, field):
        """
        input: field # a short field name
        modeltype: F=famous C=HadCM3, G=HadGEM
        
        gets data such as the list of the filenames we need
        and the longfieldname and also which type of file it is
        """
        
        longfield = {'temp1.5' : 'TEMPERATURE AT 1.5M',
                     'precip' : 'TOTAL PRECIPITATION RATE     KG/M2/S',
                     'cloud_cover' : 'TOTAL CLOUD AMOUNT - RANDOM OVERLAP',
                     'mslp' : 'PRESSURE AT MEAN SEA LEVEL',
                     'evapsea' : 'EVAPORATION FROM SEA (GBM)   KG/M2/S',
                     'seaiceconc' : 'AICE : ICE CONCENTRATION',
                     'oceansurftemp' : 'OCN TOP-LEVEL TEMPERATURE          K',
                     'surfsalinity': 'SALINITY (OCEAN)       (PSU-35)/1000',
                     'MLD' : 'MIXED LAYER DEPTH (OCEAN)          M',
                     'AMOC' : 'Meridional Overturning Stream Function (Atlantic)'
                     }
        
        atm_ocn_ind = {'temp1.5' : 'a',
                       'precip' : 'a',
                       'cloud_cover' : 'a',
                       'mslp' : 'a',
                       'evapsea' : 'a',
                       'seaiceconc' : 'o',
                       'oceansurftemp' :  'o',
                       'surfsalinity': 'o',
                       'MLD' : 'o',
                       'AMOC' : 'o'
                       }
        modelsep = {'F' : '#', 'C':'@'}
        fileletter = {'temp1.5' : 'pd',
                      'precip' : 'pd',  
                      'cloud_cover' : 'pd',
                      'mslp' : 'pd',
                      'evapsea' : 'pd',
                      'seaiceconc' : 'pf',
                      'oceansurftemp' : 'pf',
                      'surfsalinity': 'pf',
                      'MLD' : 'pf',
                      'AMOC' : 'pk2'}

        # centuary indicator for HadCM3
        centuaryind = {'0' : '0', '1':'1','2':'2','3' : '3',
                       '4' : '4', '5':'5','6':'6','7' : '7',
                       '8' : '8', '9':'9','10':'a','11' : 'b',
                       '12' : 'c', '13':'d','14':'e','15' : 'f',
                       '16' : 'g', '17':'h','18':'i','19' : 'j',
                       '20' : 'k', '21':'l','22':'m','23' : 'n',
                       '24' : 'o', '25':'p','26':'q','27' : 'r',
                       '28' : 's', '29':'t','30':'u','31' : 'v',
                       '32' : 'w', '33':'x','34':'y','35' : 'z'}
        
        if field == 'AMOC':
            fileloc = '/pk2/'
        else:
            fileloc = '/netcdf/'
        
        filestart = ('/nfs/hera1/earjcti/um/' + EXPT + fileloc 
                     + EXPT[0:5] + atm_ocn_ind.get(field)
                     + modelsep.get(MODELTYPE))
        
        if field == 'AMOC':
            filestart = filestart + 'pg'
        else:
            filestart = filestart + fileletter.get(field)
    
        
        self.fieldname = longfield.get(field)
        
        self.filenames = ([])
        print(STARTYEAR)
        for year in range(STARTYEAR, STARTYEAR + NYEARS):
            if MODELTYPE == 'F': # famous
                filenumber = str(year).zfill(9)
            if MODELTYPE == 'C': # hadcm3
                if year >=  1000:
                    cent = np.str(year)[0:2]
                    year2 = np.str(year)[2:4]
                else:
                    cent = np.str(year)[0:1]
                    year2 = np.str(year)[1:3]
                print(cent)
                filenumber = centuaryind.get(cent) + year2
                print(filenumber)
            self.filenames.append(filestart + filenumber)   
      
        self.filetype = fileletter.get(field)
        self.field = field
        
        
           
    def main_get_field(self):
        """
        this will get the longterm mean and interannual standard deviation 
        based on
        yearly averages from the files in self.filenames
        """
        
        cubelist_year = iris.cube.CubeList([])
        
        for i, filename in enumerate(self.filenames):
            cubeoneyear = self.main_get_oneyear(filename, i)
            cubelist_year.append(cubeoneyear)
        
        
      
        
        equalise_attributes(cubelist_year)
        iris.util.unify_time_units(cubelist_year)
        
        if self.field == 'AMOC':
             cube_allyears = cubelist_year.concatenate_cube()
             tvar = 'time'
        else:
            cube_allyears = cubelist_year.merge_cube()
            tvar = 't'
        
        print('cube all years',cube_allyears)
        cube_mean = cube_allyears.collapsed(tvar, iris.analysis.MEAN)
        cube_stdev = cube_allyears.collapsed(tvar, iris.analysis.STD_DEV)
            
        return cube_mean, cube_stdev
            
       
    def main_get_oneyear(self, filename, year):
        """
        read in one years supply of cubes
        average and return for adding to the cubelist
        """
        
        # fileending for famous and hadcm3 models
        fileend = {'F' : '+'}
        if self.filetype == 'pd' or self.filetype == 'pf':
            if AVG_SEAS == 'ann':
                cubes = iris.load(filename + '*', self.fieldname)

            if AVG_SEAS == 'djf':
                cubes = iris.cube.CubeList([])

                cube = iris.load_cube(filename + 'dc' + 
                                      fileend.get(MODELTYPE, '') + '.nc', 
                                      self.fieldname)
                cubes.append(cube)  

                cube = iris.load_cube(filename + 'ja' +
                                      fileend.get(MODELTYPE, '') + '.nc', 
                                      self.fieldname)
                cubes.append(cube)  

                cube = iris.load_cube(filename + 'fb' +
                                      fileend.get(MODELTYPE, '') + '.nc', 
                                      self.fieldname)  
                cubes.append(cube)

            if AVG_SEAS == 'jja':
                cubes = iris.cube.CubeList([])
                print(filename, fileend.get(MODELTYPE,''))
                cube = iris.load_cube(filename + 'jn' +
                                      fileend.get(MODELTYPE, '') + '.nc', 
                                      self.fieldname) 
                cubes.append(cube)  

                cube = iris.load_cube(filename + 'jl' +
                                      fileend.get(MODELTYPE, '') + '.nc', 
                                      self.fieldname)
                cubes.append(cube)  

                cube = iris.load_cube(filename + 'ag' +
                                      fileend.get(MODELTYPE, '') + '.nc', 
                                      self.fieldname)
                cubes.append(cube)


        
            if len(cubes) != NMONTHS:
                    print('wrong number of cubes')
                    sys.exit(0)
            else:
                equalise_attributes(cubes)
                cube1year = cubes.concatenate_cube()
                cube1year.coord('t').points = (
                    np.arange(year*NMONTHS + 0,year *NMONTHS +NMONTHS,1))
                cubeyear = cube1year.collapsed('t', iris.analysis.MEAN)

                  
        if self.filetype == 'pg' or self.filetype == 'pk2':
            cube = iris.load_cube(filename + '*', self.fieldname)
            cubeyear = cube
            cubeyear.coord('time').points = year 
            cubeyear.coord('time').attributes = None
            cubeyear.coord('time').units = 'year'
               
                
       
     
        return cubeyear
      
        
        
    
    
#=================================================================
# MAIN PROGRAM STARTS HERE

#expt='xkvje'
#startyear=2350
#nyears=50
#HadCM3='n'


MODELTYPE = 'C' # n=HadGEM, C=HadCM3, F=Famous
EXPT = 'xogzt'
STARTYEAR = 700
NYEARS=89
AVG_SEAS = 'ann' # ann, djf, jja

if AVG_SEAS == 'ann':
    NMONTHS = 12
if AVG_SEAS == 'djf' or AVG_SEAS== 'jja':
    NMONTHS =3

#FIELDS = ['temp1.5','precip','cloud_cover','mslp',
#          'evapsea','seaiceconc','oceansurftemp',
#          'surfsalinity','MLD']

#FIELDS = ['evapsea','seaiceconc','oceansurftemp',
#          'surfsalinity','MLD']

#FIELDS = ['AMOC']
FIELDS = ['temp1.5']

for i, field in enumerate(FIELDS):
    print(field)
    obj = main(field)
    meancube, stdevcube = obj.main_get_field()

    outfile = ('/nfs/hera1/earjcti/um/' + EXPT + '/average/' + EXPT + 
           '_' + field + '_' + AVG_SEAS +'_mean_' + np.str(STARTYEAR) 
               + '_' + np.str(STARTYEAR + NYEARS) + '.nc')
    iris.save(meancube, outfile, netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    outfile = ('/nfs/hera1/earjcti/um/' + EXPT + '/stdev/' + EXPT + 
           '_' + field + '_' + AVG_SEAS + '_stdev_'  + np.str(STARTYEAR) 
               + '_' + np.str(STARTYEAR + NYEARS)  + '.nc')
    iris.save(stdevcube, outfile, netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)
       

::::::::::::::
COMMON/basic_plots/difference_two_fields.py
::::::::::::::
#NAME
#    difference_two_fields.py
#PURPOSE 
#
#  This program will plot a difference of two fields to the screen

# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import iris
import iris.quickplot as qplt
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
 
    
#=================================================================
# MAIN PROGRAM STARTS HERE

field = '_p_1'
#field = '_Temperature'
expt1='xoorb'
expt2='xogzl'

file1 = '/nfs/hera1/earjcti/um/' + expt1 + '/database_averages/'+expt1+'_Annual_Average_a@pd'+field+'.nc'
file2 = '/nfs/hera1/earjcti/um/' + expt2 + '/database_averages/'+ expt2+ '_Annual_Average_a@pd'+field+'.nc'

if field == '_p_1':
    FIELDNAME = 'PSTAR AFTER TIMESTEP'
    levels = np.arange(-3000,3500,250)
if field == '_Temperature':
    FIELDNAME = 'TEMPERATURE AT 1.5M'
    levels = np.arange(-3.,3.2,0.2)


cube1 = iris.util.squeeze(iris.load_cube(file1,FIELDNAME))
cube1.long_name = expt1 + field
cube2 = iris.util.squeeze(iris.load_cube(file2,FIELDNAME))
cube2.long_name = expt2 + field

cubediff = cube2 - cube1
cubelist = [cube1,cube2]
qplt.contourf(cubediff, levels, extend='both',cmap='RdBu_r')
plt.gca().coastlines()
plt.title(FIELDNAME + ': xogzl - xoorb')
if field == '_p_1':
    plt.savefig('pressure_diff.png')
    plt.savefig('pressure_diff.pdf')
    iris.save(cubelist,'pressures.nc')
if field == '_Temperature':
    plt.savefig('temp_diff.png')
    plt.savefig('temp_diff.pdf')
    iris.save(cubelist,'temperatures.nc')
plt.close()

#



::::::::::::::
COMMON/basic_plots/jumaps.py
::::::::::::::
#
#  contour globe will plot data on a global map
#
def contourglobe(plotdata,subplotx,subploty,fileno,lon,lat,titlename,minval,maxval,valinc,V,uselog,cbarname,res):

    import numpy as np
    import matplotlib.pyplot as plt
    from mpl_toolkits.basemap import Basemap, shiftgrid



    lons, lats = np.meshgrid(lon,lat)
    plt.subplot(subplotx,subploty,fileno+1)

   # this is good for the globe
    map=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',resolution=res)
    #map.drawmapboundary(fill_color='aqua')
    map.drawmapboundary
    x, y = map(lons, lats)
    map.drawcoastlines() 
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.),extend="both")
        cbar = plt.colorbar(cs,orientation="horizontal",extend='max')
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu_r',extend="both")
            cbar = plt.colorbar(cs,orientation="horizontal",extend='max')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                cs = map.contourf(x,y,plotdata,V,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")

    plt.title(titlename)
    cbar.set_label(cbarname,labelpad=-40)
#end def contourglobe
::::::::::::::
COMMON/benchmarking/benchmarking.py
::::::::::::::
"""
#NAME
#    benchmarking.py
#PURPOSE 
#
#  This program will create a t-test to see whether the means from
# our climate runs are significantly different from Bristols
#
#NOTES
#  what is a t-test
#  1.  Our sample has 100 years so our data has 99degrees of freedom
#  2.  If our |t-value| >2  then our Null hypothesis (that the distributions are the same)
#                           is rejected at the 95% confidence level/
#  3.  So we are looking for |t-value| < 2
#  4.  t-value = (mean_expt - mean_climate) / s
#      I can't figure out whether s=standard dev or s=standard-dev / sqrt(n)

"""

# Import necessary libraries

import os
import numpy as np
import math
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
import iris.quickplot as qplt
import iris.plot as iplt
import sys
#from netCDF4 import Dataset, MFDataset

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

def round_to_n(x, n):
    """
    rounds x to n significant figures
    """
    if not x: return 0
    power = -int(math.floor(math.log10(abs(x)))) + (n - 1)
    factor = (10 ** power)
    return round(x * factor) / factor

def get_plot_limits(cube, ndiv, nsig):
    """
    gets the limit for the plot
    imput: a cube and the number of divisions on the plot
    nsig is the number of significant figures to round the difference to
    """

   
    valmin = round_to_n(np.min(cube.data), 2)
    valmax = round_to_n(np.max(cube.data), 2)
        
    valmax = np.min([valmin * -1.0, valmax])
    valmin = valmax * -1.0
    valdiff = round_to_n((valmax - valmin) / ndiv, nsig)
    
    # adjust valmax and valmin so that the difference is an exact multiple
    # of valdiff
    valmax = valdiff * (ndiv / 2.0)
    valmin = -1.0 * valmax
    
       
    return valmin, valmax, valdiff

def fix_mask(cube):
    """
    this is for if the valid min and valid max was wrong and the data has 
    ended up being masked everywhere 
    """
    #1. check if data is masked everywhere
   
    nmask = np.ma.count_masked(cube.data)
    nsize = cube.data.size
    
    if nmask > 0.9 * nsize:  # too much masking unmask everything
        cube.data.mask = False
        
    
        cube.attributes['valid_min'] = -100000.
        cube.attributes['valid_max'] = -100000.
        cube.attributes['fill_value'] = 2.0E20
        
    newcube = cube.copy()
    
    
    return newcube

class main():
    """
    this class will basically run the profram
    """
    def __init__(self, field):
        """
        input: field # a short field name
        
        gets data such as the list of the filenames we need
        and the longfieldname and also which type of file it is
        """
 
        if LINUX_WIN == 'w':
            startfname = "C:\\Users\\julia\\OneDrive\\WORK\\DATA\\"
            startout = "C:\\Users\\julia\\OneDrive\\WORK\\DATA\\"
        else:
            startout = "/nfs/hera1/earjcti/um/"
            if EXPT_TYPE == 'S':
                startfname = "/nfs/hera3/palaeo_share/PlioMIP2/processed/"
            else:
                startfname = "/nfs/hera1/earjcti/um/"
            
        longfield = {'temp1.5' : 'TEMPERATURE AT 1.5M',
                     'precip' : 'TOTAL PRECIPITATION RATE     KG/M2/S',
                     'precipmm' : 'TOTAL PRECIPITATION RATE MM/DAY',
                     'cloud_cover' : 'TOTAL CLOUD AMOUNT - RANDOM OVERLAP',
                     'mslp' : 'PRESSURE AT MEAN SEA LEVEL',
                     'mslp_hPa' : 'PRESSURE AT MEAN SEA LEVEL hPa',
                     'evapsea' : 'EVAPORATION FROM SEA (GBM)   KG/M2/S',
                     'seaiceconc' : 'AICE : ICE CONCENTRATION',
                     'icefrac' : 'SEA ICE FRACTION AFTER TIMESTEP',
                     'oceansurftemp' : 'OCN TOP-LEVEL TEMPERATURE          K',
                     'oceansurftempK' : 'OCN TOP-LEVEL TEMPERATURE K',
                     'surfsalinity': 'SALINITY (OCEAN)       (PSU-35)/1000',
                     'surfsalinitypsu': 'SALINITY (OCEAN) (PSU)',
                     'MLD' : 'MIXED LAYER DEPTH (OCEAN)          M',
                     'MLDm' : 'MIXED LAYER DEPTH (OCEAN) M',
                     'AMOC' : 'Meridional Overturning Stream Function (Atlantic)'
                    
                     }
        atm_ocn_ind = {'temp1.5' : 'a',
                       'precip' : 'a',
                       'cloud_cover' : 'a',
                       'mslp' : 'a',
                       'evapsea' : 'a',
                       'seaiceconc' : 'o',
                       'oceansurftemp' :  'o',
                       'surfsalinity': 'o',
                       'MLD' : 'o',
                       'AMOC' : 'o'
                       }
      
        fileletter = {'temp1.5' : 'pd',
                      'precip' : 'pd',  
                      'cloud_cover' : 'pd',
                      'mslp' : 'pd',
                      'evapsea' : 'pd',
                      'seaiceconc' : 'pf',
                      'oceansurftemp' : 'pf',
                      'surfsalinity': 'pf',
                      'MLD' : 'pf'}
        
        if EXPT_TYPE == 'S':
            self.meanfile = startfname + EXPT + '_Annual.nc'
            self.stdevfile = startfname + EXPT + '_Annual_std.nc'
        else:
            self.meanfile = (startfname + EXPT + '/average/' + EXPT + 
                         '_' + field + '_' + SEASON + '_mean_' + 
                         np.str(EXPT_STARTYEAR) + '_' +  
                         np.str(EXPT_STARTYEAR + NYEARS) + '.nc')
            self.stdevfile = (startfname + EXPT + '/stdev/' + EXPT + 
                          '_' + field + '_' + SEASON + '_stdev_' +
                          np.str(EXPT_STARTYEAR) + '_' +  
                          np.str(EXPT_STARTYEAR + NYEARS) + '.nc')
        print(self.meanfile)
       
        
        # get benchmarking data
        if CNTL_TYPE == 'B':
            self.meanbench = (startfname +  'Benchmarking/' + 
                              CNTL + atm_ocn_ind.get(field) + 
                              '.' +  fileletter.get(field) + 'cl' + 
                              SEASON + '.nc')
            self.sdbench = (startfname + 'Benchmarking/' + 
                              CNTL + atm_ocn_ind.get(field) + 
                              '.' +  fileletter.get(field) + 'sd' + 
                              SEASON + '.nc')
            
        elif EXPT_TYPE == 'S':
            self.meanbench = startfname + CNTL + '_Annual.nc'
            self.sdbench = startfname + CNTL + '_Annual_std.nc'
            
        else:
            self.meanbench = (startfname + CNTL + '/average/' + CNTL + 
                              '_' + field + '_' + SEASON +'_mean_' +
                               np.str(CNTL_STARTYEAR) + '_' +  
                               np.str(CNTL_STARTYEAR + NYEARS) + '.nc')
            self.sdbench = (startfname + CNTL + '/stdev/' + CNTL + 
                            '_' + field + '_' + SEASON +'_stdev_' +
                            np.str(CNTL_STARTYEAR) + '_' +  
                            np.str(CNTL_STARTYEAR + NYEARS) + '.nc')
                           
            
        self.fieldname = longfield.get(field)
        self.outstart = (startout + EXPT + '/benchmarking/' + EXPT + 
                         '_' + np.str(EXPT_STARTYEAR) + '_' +  
                         np.str(EXPT_STARTYEAR + NYEARS) + 
                         '_' + CNTL + '_' + np.str(CNTL_STARTYEAR) + 
                         '_' +  np.str(CNTL_STARTYEAR + NYEARS) + '_' +
                         field  + '_' + SEASON)
        self.field = field
        
      
           
    def main_benchmark_field(self):
        """
        this will compare the field with the benchmarked field and
        plot to a file
        """
        
        cube_exptmean = iris.load_cube(self.meanfile, self.fieldname)
        cube_exptsd = iris.load_cube(self.stdevfile, self.fieldname)
      
        if CNTL_TYPE == 'B':
            variable_constraint = iris.Constraint(cube_func=(lambda c: c.long_name == self.fieldname))
            cube_cntlmean = iris.load_cube(self.meanbench, variable_constraint)
            cube_temp = iris.load_cube(self.sdbench, variable_constraint)
            cube_cntlsd = fix_mask(cube_temp)
        else:
            cube_cntlmean = iris.load_cube(self.meanbench, self.fieldname)
            cube_cntlsd = iris.load_cube(self.sdbench, self.fieldname)
        
        # if steve#s salinity we just need two levels
        if self.field ==  'surfsalinitypsu':
            #cube_exptmean.coord('z2').var_name = 'depth'
            #cube_exptmean.coord('depth').standard_name = 'depth'
            #print(cube_exptmean.coords())
            cube_exptmean = cube_exptmean.extract(iris.Constraint(depth=0.0))
            print(cube_exptmean)
            sys.exit(0)
            cube_exptsd = cube_exptsd.extract(iris.Constraint(z2=0.0))
            cube_cntlmean = cube_cntlmean.extract(iris.Constraint(z2=0.0))
            cube_cntlsd = cube_cntlsd.extract(iris.Constraint(z2=0.0))
            
            
       
        # makes the exptmean, exptsd, cntlmean and cntlsd cubes all look the 
        # same
        cube_exptmean, cbarunits = self.mean_equalise_cubes(cube_exptmean)
        cube_cntlmean, cbarunits = self.mean_equalise_cubes(cube_cntlmean)
        cube_exptsd, cbarunits = self.mean_equalise_cubes(cube_exptsd)
        cube_cntlsd, cbarunits = self.mean_equalise_cubes(cube_cntlsd)
        
        print(cube_exptmean)
        print(cube_cntlmean)
        cube_anom = cube_exptmean - cube_cntlmean
        # to calculate the divisor in the t-statistic
        # you will have to look at how to do a t-test on the web for exlanation
        data1 = ((((cube_exptsd.data ** 2.0) * (NYEARS-1.0))
                          + ((cube_cntlsd.data ** 2.0) * (NYEARS-1.0)))
                     / (2.0*(NYEARS-1.0)))
        data2 = (np.sqrt(data1)) * (np.sqrt(2.0/NYEARS))
        cube_div = cube_anom.copy(data=data2)
        
       
        cube_tstat = cube_anom / cube_div
        print(cube_tstat.data)
       
      
        if self.field == 'AMOC':
            self.main_plot_AMOC(cube_anom, cbarunits, cube_tstat)
        else:
            self.main_plot_latlon(cube_anom, cbarunits, cube_tstat)
        # plot the anomaly
           
    def main_plot_AMOC(self, cube_anom, cbarunits, cube_tstat):
        """
        do a lat-depth plot of the field
        we are plotting the anomaly and the tvalue
        """
        
        ax = plt.subplot(1,2,1)
      
        anommin, anommax, anomdiff = get_plot_limits(cube_anom, 10, 1)
        cs = iplt.contourf(cube_anom, np.arange(anommin, anommax+anomdiff, anomdiff), 
                      cmap='RdYlBu_r', extend='both')
        cbar=plt.colorbar(cs,orientation="horizontal")
        plt.title(self.field + ' anomaly', fontsize=10)
        cbar.set_label('SV')
        
        # plot the tstatistic
        ax2 = plt.subplot(1, 2, 2)
        tmin, tmax, tdiff = get_plot_limits(cube_tstat, 10, 1)
        cs = iplt.contourf(cube_tstat, np.arange(tmin, tmax+tdiff, tdiff), cmap='RdYlBu_r', extend='both')
        cbar=plt.colorbar(cs,orientation="horizontal")
        
        iplt.contour(cube_tstat, [-np.inf, -2, 2, np.inf], hatches=[3*'\\\'', None, 3*'\\\''], colors='black')
        iplt.contourf(cube_tstat, [-np.inf, -2, 2, np.inf], hatches=[3*'\\\'', None, 3*'\\\''], colors='none')
        plt.title(self.field + ' t-value', fontsize=10)
        
        
        
        plt.tight_layout()
        print(self.outstart)
        plt.savefig(self.outstart + '.png')
        plt.savefig(self.outstart + '.pdf')
        #plt.show()
        plt.close()
        
       
      
        
    def main_plot_latlon(self, cube_anom, cbarunits, cube_tstat):
        """
        do a latlon plot of the field
        we are plotting the anomaly and the tvalue
        """

        ax = plt.subplot(1, 2, 1, projection = ccrs.PlateCarree())
        anommin, anommax, anomdiff = get_plot_limits(cube_anom, 10, 1)
        print(anommin, anommax, anomdiff)
        print(np.arange(anommin, anommax+anomdiff, anomdiff))
        
        cs = iplt.contourf(cube_anom, np.arange(anommin, anommax+anomdiff, anomdiff), 
                      cmap='RdYlBu_r', extend='both')
        cbar=plt.colorbar(cs,orientation="horizontal")
        if cube_anom.units == '':
            cbar.set_label(cbarunits)
        else:
            cbar.set_label(cube_anom.units)
        cbar.ax.tick_params(labelsize=8, labelrotation=60) 
        plt.title(self.fieldname + ' anomaly', fontsize=10)
        plt.gca().coastlines()
        #gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
        #gl.xlabels_top = False
        #gl.ylabels_left = False
        #gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
        #gl.xformatter = LONGITUDE_FORMATTER
        #gl.yformatter = LATITUDE_FORMATTER
        
        
        # plot the tstatistic
        ax2 = plt.subplot(1, 2, 2, projection = ccrs.PlateCarree())
        tmin, tmax, tdiff = get_plot_limits(cube_tstat, 10, 1)
        cs = iplt.contourf(cube_tstat, np.arange(tmin, tmax+tdiff, tdiff), cmap='RdYlBu_r', extend='both')
        cbar=plt.colorbar(cs,orientation="horizontal")
        cbar.ax.tick_params(labelsize=8, labelrotation=60) 
        iplt.contour(cube_tstat, [-np.inf, -2, 2, np.inf], hatches=[3*'\\\'', None, 3*'\\\''], colors='black')
        iplt.contourf(cube_tstat, [-np.inf, -2, 2, np.inf], hatches=[3*'\\\'', None, 3*'\\\''], colors='none')
        plt.title(self.fieldname + ' t-value', fontsize=10)
        plt.gca().coastlines()
        #gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
        #gl.xlabels_top = False
        #gl.ylabels_left = False
        #gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
        #gl.xformatter = LONGITUDE_FORMATTER
        #gl.yformatter = LATITUDE_FORMATTER
        
        plt.tight_layout()
    
        plt.savefig(self.outstart + '.png')
        plt.savefig(self.outstart + '.pdf')
        #plt.show()
        plt.close()
        
       
          
    def mean_equalise_cubes(self, cube):
        """
        makes the 2 cubes in the main program look the same for analysis
        they will look the same as cube_expt
        """    
        
        # check dimensions
        newcube = iris.util.squeeze(cube)
        
        
        # if cube is precipitation: change to mm/month
        if self.field == 'precip':
           
           newcube.long_name = 'Precipitation'
           newcube.name('precipitation_flux')
           newcube.title = 'Precipitation'
           self.fieldname = 'Precipitation'
           #newcube.units = 'kg m-2 s-1'
           #newcube.convert_units('mm day-1')
           newcube.data = newcube.data * 60. * 60. * 24.
           newcube.units = 'mm day-1'
           
        # check names etc
        cbarunits = ''
        if self.field == 'cloud_cover':
            self.fieldname = 'Cloud amount'
            newcube.units = ''
            cbarunits = 'fraction'
            
        if self.field == 'seaiceconc':
            newcube.units = ''
            cbarunits = 'fraction'
            
        if self.field == 'oceansurftemp':
            self.fieldname = 'Ocean Surface Temp'
        
        if self.field == 'surfsalinity':
            self.fieldname = 'Salinity'
            newcube.units = ''
            newcube.data = newcube.data * 1000.
            cbarunits = 'psu'
            
        if self.field == 'MLD':
            self.fieldname = 'Mixed Layer Depth'
            #cbarunits = 'm'
            
        if self.field == 'evapsea':
            self.fieldname = 'Evaporation from sea'
            newcube.data = newcube.data * 60. * 60. * 24.
            newcube.units = 'mm day-1'
           
           
        
 
               
        return newcube, cbarunits
       
    
#=================================================================
# MAIN PROGRAM STARTS HERE



LINUX_WIN='l'
NYEARS = 50
SEASON = 'djf'

# data from new experiemnt
MODELTYPE = 'F' # n=HadGEM, y=HadCM3, F=Famous

EXPT = 'xnnri_ARC4'
EXPT_STARTYEAR = 2300
#EXPT = 'Eoi400_ARC4_2450-2499'
EXPT_TYPE = 'A'#  A = average file like I made
               # B = Bristol file like Paul made
               # S = average file like Steve made

# data from good experiment
CNTL_TYPE = 'A' # A = average file like I made
               # B = Bristol file like Paul made
                 # S = like file steve made
CNTL = 'xnnrg_ARC4'
CNTL_STARTYEAR = 2300
#CNTL = 'Eoi400_2450-2499'
#CNTL = 'tcfze'
#CNTL_STARTYEAR = 0

FIELDS  = ['temp1.5','precip','cloud_cover','mslp',
          'seaiceconc','oceansurftemp',
          'surfsalinity','MLD', 'evapsea']

#FIELDS  = ['cloud_cover','mslp',
#          'seaiceconc','oceansurftemp',
#          'surfsalinity','MLD', 'evapsea']

#FIELDS_STEVE = ['temp1.5','precipmm','cloud_cover','mslp_hPa',
#          'icefrac','oceansurftempK',
#          'surfsalinity','MLD', 'evapsea']

#FIELDS = ['AMOC']
#FIELDS = ['temp1.5']

for i, field in enumerate(FIELDS):
    print(field)
    obj = main(field)
    obj.main_benchmark_field() # benchmarks this field

    
::::::::::::::
COMMON/create_um_ancil/create_P4_enh_veg_pi_ice.py
::::::::::::::
#NAME
#    create_P4_enh_veg_pi_ice.py
#PURPOSE 
#
#  This program will create a netcdf file that can be inputted into xancil to create
#  a vegetation ancil file.
#  The file will have P4 vegetation types but E280 ice sheets.

# Import necessary libraries

import numpy as np
import sys
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
 
    
#=================================================================
# MAIN PROGRAM STARTS HERE

# read in the data
EOI400_vegfile = '/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_mb_qrfrac.type.nc'
E280_vegfile = '/nfs/hera1/earjcti/ancil/preind2/qrfrac.type.nc'

cubes = iris.load(E280_vegfile)

EOI400_cube_orig = iris.load_cube(EOI400_vegfile)
E280_cube_orig = iris.load_cube(E280_vegfile)

EOI400_cube = iris.util.squeeze(EOI400_cube_orig)
E280_cube = iris.util.squeeze(E280_cube_orig)

pi_icecube = E280_cube[0,:,:]  # ice is on level 1 because of how it was setup

EOI400_data = EOI400_cube.data
nlev,nlat,nlon = np.shape(EOI400_data)

# find a location where the PI_ice  is different to the Plio_ice
for j in range(0,nlat):
    for i in range(0,nlon):
        if EOI400_data[8,j,i] !=pi_icecube.data[j,i]:
            if pi_icecube.data[j,i] == 1.0:
                EOI400_data[8,j,i] = pi_icecube.data[j,i]
                EOI400_data[0:7,j,i] = 0.0
            else:
                print('to change',i,j,EOI400_data[8,j,i],pi_icecube.data[j,i])
                sys.exit(0)
        if EOI400_data[8,j,i] > 1.0E36:
            EOI400_data[:,j,i] = -999.999


# save new vegetation file
new_cube = EOI400_cube.copy(data = EOI400_data)
iris.save(new_cube,'qrfrac_P4_lsm_pi_ice.nc',fill_value=-999.999)

#



::::::::::::::
COMMON/haney_forcing/haney_forcing_programs.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Spyder Editor

Here we are going to store all the classes and programs for doing our 
Permanent El nino experiments.  

We need to create the reference temperature for Haney forcing
and also the weighting (the relaxation to the forcing at each gridbox)

Julia 7/12/2019
"""
import iris
import iris.quickplot as qplt
import numpy as np
import itertools
import sys

class haney_nc():
    """
    this class contains all the stuff needed to make the netcdf file 
    of the reference temperatures which the program will relax to
    """
    def __init__(self):
        """
        sets up the filenames and variables
        (note we will use ple as the mask as this is on a 2d grid,
        with nan over land points)
        """
        
        self.ORIG_HANEY_FILE = FILESTART + 'qrclim.Haney_elnino.nc'
        self.MASK_FILE = FILESTART + 'xogzi.ostart.p50c1.nc'
        self.MASK_VARNAME = 'PLE:PRECIP-EVAP INTO OCEAN KG/M2/S A'
        self.NEW_HANEY_FILE = FILESTART + 'qrclim.Haney_elnino_new.nc'
        
    def create_nc(self):
        """
        creates an haney forcing sst with the same Lsm as is in the maskfile

        Returns
        -------
        None.

        """
        
        # get a lsm where sea =20. land is masked
        lsm_cube = self.get_mask()
        lon_lsm = lsm_cube.coord('longitude').points
        ydim, xdim = lsm_cube.shape
        lsm12_init = (np.broadcast_to(lsm_cube.data,(12, 1, ydim, xdim)))
        lsm12_alt = lsm12_init - 20.
        
        
        # get original cubes
        orig_cubes = iris.load(self.ORIG_HANEY_FILE)
        ncubes=len(orig_cubes)
        
         # if we want to vary the temperature by latitude
        if MASKTYPE == 'latvar':
            lsm12 = self.get_mask_latvar(lsm12_init)
        elif MASKTYPE == 'LANINA':
            lsm12 = self.get_mask_lanina(lsm12_init, lon_lsm)
        else: # set to constant
            lsm12 = lsm12_init
            
        
        
        # overwrite all lsm cubes with data from lsm cubes
        # note there are 12 months
        cubelist = iris.cube.CubeList([])
        for i in range(0, ncubes):
            cube = orig_cubes[i]
            
            if i == 0:
                cubelist.append(cube.copy(data=lsm12))
            else:
                cubelist.append(cube.copy(data=lsm12_alt))
                newcube = cube.copy(data=lsm12_alt)
                print(newcube.data)
        
        outfile = FILESTART + OUTFILEEND
        iris.save(cubelist, outfile, netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)
       
        
    def get_mask(self):
        """
        get's a 2d cube of the land sea mask
        land is missing data indicator
        sea is set to 20degC

        Returns
        -------
        lsmcube

        """
        
        cube = iris.load_cube(self.MASK_FILE, self.MASK_VARNAME)
        
        # remove all dimensions except longitude and latitude
        for coord in cube.coords():        
            name=coord.var_name
            if name !='latitude' and name!='longitude':
                cubenew = cube.collapsed(name, iris.analysis.MEAN)
                cube = cubenew
               
              
        mask_data = (cube.data)*0. + 20. # set to 20 everywhere
        mask_data  = np.ma.masked_where(np.ma.getmask(cube.data), 
                                        mask_data)
        mask_data = np.ma.filled(mask_data, 1.0E20)
        
        mask_cube = cube.copy(data = mask_data)
       
        return mask_cube


    def get_mask_latvar(self, lsmin):
        """
         Parameters
        ----------
        lsmin : array x y z t (t=12)
            array containing the lsm with all non masked values set to 20degC
      

        Returns
        -------
        lsmout

        This function will use the lsmin 
        It will overwrite it with the zonally averaged temperatures from paul's 
        Haney forcing file
        """
        
        # get data and find zonal mean
        cube = iris.load_cube(self.ORIG_HANEY_FILE, 'REF. SEA SURF. TEMPERATURE  DEG.C  A')
        haney_zm_cube = cube.collapsed('longitude', iris.analysis.MEAN)
        haney_zm_data = haney_zm_cube.data
       
        
        # overwrite data with zonal means
        nt, nz, ny, nx = np.shape(lsmin)
        lsmout = np.ma.masked_all_like(lsmin)
        
        for t in range(0, nt):
            for k in range(0, nz):
                for j in range(0, ny):
                    for i in range(0,nx):
                        if lsmin[t, k, j, i] < 100.0:
                            lsmout[t, k, j, i] = haney_zm_data[t, k, j]
                      
            
       
        return lsmout
    
    def get_mask_lanina(self, lsmin, lonlsmin):
        """
        Parameters
        ----------
        lsmin : array x y z t (t=12)
            array containing the lsm with all non masked values set to 20degC
        lonlsmin ; array containing the longitudes of the lsmin

        Returns
        -------
        lsmout

        This function will use the lsmin as a template
        It will get then get the average gradient across the Pacific from each month
        and will multiply it by 1.5 to get a strong La Nina
        
        The lsmin will be overwritten with the average monthly temperature everywhere
        except across the Pacific where it will be overwritten with the strong la nina
        """
        monthnames = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
        
        # setup outfile to overwrite with lanina
        nt, nz, ny, nx = np.shape(lsmin)
        lsmout = np.ma.masked_all_like(lsmin)
        
        for monthno, month in enumerate(monthnames):
            # get average data
            cube = iris.load_cube('AVG_FIELDS/xoorb-sst-mean-' + month + '.nc', 'TEMPERATURE (OCEAN)  DEG.C')
            nlats = len(cube.coord('latitude').points)
            nlons = len(cube.coord('longitude').points)
            
            # get pacific basin (from the writenc class)
            if monthno == 0:
              obj = haney_txt()
              pacific_region = obj.get_pacific_basin(FILESTART+'/basin_hadcm3', nlats, nlons)
      
            #####################################
            # set up temperature in Pacific region
            # this will be the difference between the Pacific mean and the local mean multiplied by 1.5
        
            lanina = cube.data #currently a copy of the average data
           
            pac_temp = pacific_region * lanina # this is the temperatures in the pacific but zero elsewhere
            pac_temp[pac_temp == 0.] = np.nan #this is temperature in pacific but nan elsewhere
            for j in range(0,nlats):
                total_pac_lat = np.sum(pacific_region[j, :])
                if total_pac_lat > 0.:
                    avg_pac_lat = np.nanmean(pac_temp[j, :]) # average temperature at that latitude in pacific
            
                # if we are in Pacific replace temperature with 
                # 1.5 * the local difference from the zonalmean
                for i in range(0, nlons):
                    if pacific_region[j,i] == 1.0:
                        lanina[j, i] = avg_pac_lat + (1.5 * (lanina[j,i] - avg_pac_lat))
                
        
            
        
           
       
            lanina_lons = cube.coord('longitude').points
        
            for i in range(0,nx):
                # note the input array has more longitudes than the mean array (use modulus)
                i2 = np.where(lanina_lons == np.mod(lonlsmin[i],360.))
          
                for j in range(0, ny):
                
                    if lsmin[monthno, :, j, i] < 100.0:
                        lsmout[monthno, :, j, i] = lanina[j, i2]
                        
                      
            print('monthno is',monthno,lsmout[monthno,0,20,20])
            
       
        return lsmout
# end of class
  


class haney_txt():
    """
    this class contains all the stuff needed to make the txt file which 
    contains the strength of the relaxation for each gridbox
    """
    def __init__(self):
        """
        sets up the filenames and variables
        """
        
        self.ORIG_HANEY_NC = FILESTART + 'qrclim.Haney_elnino.nc'
        self.HANEY_FILE_TXT = FILESTART + 'haney_elnino.dat'
        self.MAX_NHANEY = 163.  # this is the haney forcing on the equator in W/m2
        
    def create_txt(self):
        """
        creates the text file and writes out

        Returns
        -------
        None.

        """
        #1. get latitudes
        cube = iris.load_cube(self.ORIG_HANEY_NC, 'REF. SEA SURF. TEMPERATURE  DEG.C  A')
        lats = cube.coord('latitude').points
        nlats = len(lats)
        nlons = len(cube.coord('longitude').points)
      
        # Haney restoring function 14 day timescale 40S-40N with
        # value set to that of the Western Pacific warm pool.  ie expand western Pacific warm pool across 
        # Pacific      
        #The coupling coefficient  has units of watts per square meter per kelvin. 
        #A timescale can be derived as  = cph/, where  is the density, 
        #cp is the specific heat of seawater, and h is a typical mixed layer depth. 
        # Weaver and Sarachik (1991) used a timescale of 25 days.
        
        # if timescale is 14 days what is k
        # k= density * cp * mixed layer depth / 14
        #  = 1022 (kg/m3) * 4011 (J / kg/ k) * 50 (m) / 1209600 secs
        #  = 170 (J)  / (m2 * K * seconds) or 170 W/m2/K
        # so assuming that the mixed layer depth is 50m the
        # restoring timescale of 14 days correspoinds to haney forcing
        # coefficient of 170W/m2/s
        
        outarr = np.zeros((nlats,nlons))
      
        for j, lat in enumerate(lats):
            if np.abs(lat) < 20.0:
                outarr[j, :] = 170.0
            elif np.abs(lat) < 40.0:
                #lat=40 forcing 0, lat=20 forcing 170
                # forcing changes linearly between
                outarr[j, :] = 170. * (1.0- ((np.abs(lat) - 20.)/20.))
            else:
                outarr[j, :] = 0.0
                
        # if mask region is PACIFIC set to zero in other regions
        if MASKREGION == 'Pacific':
            # note that this was adjusted by me so that the Indian ocean was not included
            pac_basin = self.get_pacific_basin(FILESTART+ '/basin_hadcm3', nlats, nlons)
            outarr = outarr * pac_basin # zeros away from pacific
          
        # check the mask
       
        #cubetoplot = cube.collapsed(['t','unspecified'], iris.analysis.MEAN)
        #cubeplot2 = cubetoplot.copy(data=cubetoplot.data * pac_basin)
        #qplt.contourf(cubeplot2)
        
            
        f=open(self.HANEY_FILE_TXT,'w+')
        #np.savetxt(f, np.transpose(lats[:]), fmt='%9.2f')
        juheader = ('Each column is a latitude from ' + np.str(lats[0])
                   + ' to ' + np.str(lats[-1]))
        np.savetxt(f, np.transpose(outarr), header=juheader, fmt='%9.1f') 
        f.close()

    def get_pacific_basin(self,filename, nlats, nlons):
        """

        Parameters
        ----------
        filename : the name of the basin indices file

        Returns
        -------
        a numpy array containing the basin indices (1-pacific, 0 = not pacific)

        """
        
        f1 = open(filename, "r")
      
        lines = f1.readlines()
        
        # lines 1-2 are title, line 3-146 are data, line 147 onwards metadata
        rowno = np.zeros(144, dtype=int)
        pacstart = np.zeros(144, dtype=int)
        pacend = np.zeros(144, dtype=int)
        for count, line in enumerate(lines):
            if count in range(3,147):
               linedata = line.split() # index0 is row
                                       # index5 is pacstart
                                       # index6 is pacend
               rowno[count-3] = np.int(linedata[0])
               pacstart[count-3] = np.int(linedata[5])
               pacend[count-3] = np.int(linedata[6])
               
        print(pacstart)
        
        region = np.zeros((nlats, nlons))
        
        for count, row in enumerate(rowno):
            colstart = pacstart[count]
            colend = pacend[count]
           
            for i in range(colstart, colend+1):
                if (row >= 40) and (row <= 104):
                    region[row-1, i] = 1.0
       
        return region
     

LINUX_WIN = 'l'
FILESTART_DICT = {
                   "w" : "C:\\Users\\julia\\OneDrive\\WORK\\DATA\\TEMPORARY\\",
                   "l" : "/nfs/hera1/earjcti/Xiaofang/HANEY/"
                 }

OUTFILE_DICT = {
                   "LANINA" : "Hadley_lanina_new.nc",
                   "latvar" : "Hadley_latconst_new.nc",
                   "fixed" : "Hadley_constant.nc"
                   
                 }

MASKTYPE = 'LANINA' # fixed; fixed at 20deg, latvar: varies with latitude only
                    # La Nina : we will increase the gradient across the Pacific (like a lanina)
MASKREGION = 'Pacific' # Pacific, None

FILESTART = FILESTART_DICT.get(LINUX_WIN)
OUTFILEEND = OUTFILE_DICT.get(MASKTYPE)


#set up and run the code to produce the Haney nc file
#obj = haney_nc()
#obj.create_nc()

#set up and run the code to produce the Haney forcing text file
#this file contains the relaxiation coefficient for each gridcell
obj = haney_txt()
obj.create_txt()
::::::::::::::
HadCM3/compare_pi_with_data/compare_with_cru.py
::::::::::::::
1#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia
This program will plot the difference between the model and the CRU data
The run we want to use is a preindustrial run
The run should have been preprocessed by CEMAC/PLIOMIP2/regrid_HCM3_50_year_avg.py
"""

import os 
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
from iris.cube import CubeList
import iris.quickplot as qplt
import iris.analysis.cartography
import iris.coord_categorisation



def diffmonmean():
    """
    difference between CRU and the experiment for each month mean
    """
    monmean_exp_cube = iris.load_cube(FILESTART + EXPT + 
                                       '/NearSurfaceTemperature/means/mean_month.nc')
    monmean_CRU_cube = iris.load_cube('/nfs/hera1/earjcti/regridded/CRUTEMP/E280.NearSurfaceTemperature.mean_month.nc')

    # regrid the CRU data to the same grid as experiment
    monmean_CRUr_cube = monmean_CRU_cube.regrid(monmean_exp_cube,iris.analysis.Linear())


    # find the difference
    monmean_exp_cube.attributes = None
    monmean_CRUr_cube.attributes = None
    monmean_exp_cube.remove_coord('year')
    monmean_CRUr_cube.remove_coord('year')
    monmean_exp_cube.convert_units('degC')
    monmean_CRUr_cube.units='degC'
    
    print(monmean_exp_cube)
    print(monmean_CRUr_cube)
    diffcube = monmean_exp_cube - monmean_CRUr_cube
   
    monthnames=['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nor','dec']
    fig = plt.figure(figsize=[15.0,15.0])
    for mon in range(0,12):
        fig.add_subplot(4,3,mon+1)
        V = np.arange(-5,5,1)
        qplt.contourf(diffcube[mon,:,:],levels=V,extend='both',cmap='RdBu_r')
        plt.gca().coastlines()
  
        squares = np.square(diffcube.data[mon,:,:])
        avgsquares = np.mean(squares)
        rmse = np.sqrt(avgsquares)
        mae = np.mean(diffcube.data[mon,:,:])

        plt.title(monthnames[mon] +  ':RMSE:' + str(np.around(rmse,2))+ 'degC mae:' + str(np.around(mae,2)) + 'degC unweighted')
        plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/NearSurfaceTemperature/' + EXPT + '-CRU_monmean.eps')
        plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/NearSurfaceTemperature/' + EXPT + '-CRU_monmean.png')

def diffannmean():
    """
    difference between CRU and the experiment in the annual mean
    """
    annmean_exp_cube = iris.load_cube(FILESTART + EXPT + 
                                       '/NearSurfaceTemperature/means/allmean.nc')
    annmean_CRU_cube = iris.load_cube('/nfs/hera1/earjcti/regridded/CRUTEMP/E280.NearSurfaceTemperature.allmean.nc')

    # regrid the CRU data to the same grid as experiment
    annmean_CRUr_cube = annmean_CRU_cube.regrid(annmean_exp_cube,iris.analysis.Linear())

    # find the difference
    annmean_exp_cube.convert_units('degC')
    annmean_CRUr_cube.units='degC'
    diffcube = annmean_exp_cube - annmean_CRUr_cube
    
    V = np.arange(-5,5,1)
    qplt.contourf(diffcube,levels=V,extend='both',cmap='RdBu_r')
    plt.gca().coastlines()
    
    squares = np.square(diffcube.data)
    avgsquares = np.mean(squares)
    rmse = np.sqrt(avgsquares)
    mae = np.mean(diffcube.data)

    plt.title(EXPT + ' - CRU, RMSE:' + str(np.around(rmse,2))+  'degC;  MAE:'+ str(np.around(mae,2)) + 'degC - unweighted ')
    plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/NearSurfaceTemperature/' + EXPT + '-CRU_annmean.eps')
    plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/NearSurfaceTemperature/' + EXPT + '-CRU_annmean.png')


##########################################################
# main program

EXPT = 'xpkma'
FILESTART = '/nfs/hera1/earjcti/um/'

diffannmean()
diffmonmean()



::::::::::::::
HadCM3/compare_pi_with_data/compare_with_erasstv5.py
::::::::::::::
1#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia
This program will plot the difference between the model and the NOAA ERSSTv5
The run we want to use is a preindustrial run
The run should have been preprocessed by CEMAC/PLIOMIP2/regrid_HCM3_50_year_avg.py
"""

import os 
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
from iris.cube import CubeList
import iris.quickplot as qplt
import iris.analysis.cartography
import iris.coord_categorisation



def diffmonmean():
    """
    difference between ERSSTv5 and the experiment for each month mean
    """
    monmean_exp_cube = iris.load_cube(FILESTART + EXPT + 
                                       '/SST/means/mean_month.nc')
    monmean_ERSST_cube = iris.load_cube('/nfs/hera1/earjcti/regridded/NOAAERSST5/E280.SST.mean_month.nc')

    # regrid the experiment cube onto the same grid as ersst
    monmean_expr_cube = monmean_exp_cube.regrid(monmean_ERSST_cube,iris.analysis.Linear())

    # find the difference
    monmean_expr_cube.attributes = None
    monmean_ERSST_cube.attributes = None
    monmean_expr_cube.remove_coord('year')
    monmean_ERSST_cube.remove_coord('year')
  
    print(monmean_expr_cube)
    print(monmean_ERSST_cube)
    diffcube = monmean_expr_cube - monmean_ERSST_cube
   
    monthnames=['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nor','dec']
    fig = plt.figure(figsize=[15.0,15.0])
    for mon in range(0,12):
        fig.add_subplot(4,3,mon+1)
        V = np.arange(-5,5,1)
        qplt.contourf(diffcube[mon,:,:],levels=V,extend='both',cmap='RdBu_r')
        plt.gca().coastlines()
   
    
        squares = np.square(diffcube.data[mon,:,:])
        avgsquares = np.mean(squares)
        rmse = np.sqrt(avgsquares)
        mae = np.mean(diffcube.data[mon,:,:])

        plt.title(monthnames[mon]+ ': RMSE:' + str(np.around(rmse,1))+ 'mae:' + str(np.around(mae,1))  + 'degC- unweighted')
        plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/SST/' + EXPT + '-ERSSTv5_monmean.eps')
        plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/SST/' + EXPT + '-ERSSTv5_monmean.png')

def diffannmean():
    """
    difference between ERSSTv5 and the experiment in the annual mean
    """
    annmean_exp_cube = iris.load_cube(FILESTART + EXPT + 
                                       '/SST/means/allmean.nc')
    annmean_ERSST_cube = iris.load_cube('/nfs/hera1/earjcti/regridded/NOAAERSST5/E280.SST.allmean.nc')

    # regrid the experiment cube onto the same grid as ersst
    annmean_expr_cube = annmean_exp_cube.regrid(annmean_ERSST_cube,iris.analysis.Linear())

    # find the difference
    diffcube = annmean_expr_cube - annmean_ERSST_cube
    
    V = np.arange(-5,5,1)
    qplt.contourf(diffcube,levels=V,extend='both',cmap='RdBu_r')
    plt.gca().coastlines()
    
    squares = np.square(diffcube.data)
    avgsquares = np.mean(squares)
    rmse = np.sqrt(avgsquares)
    mae = np.mean(diffcube.data)


    plt.title(EXPT + ' - ERSST, RMSE:' + str(np.around(rmse,2))+ ', mae=' + str(np.around(mae,1)) + 'degC - unweighted')
    plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/SST/' + EXPT + '-ERSSTv5_annmean.eps')
    plt.savefig('/nfs/hera1/earjcti/HadCM3_plots/SST/' + EXPT + '-ERSSTv5_annmean.png')


##########################################################
# main program

EXPT = 'xpkma'
FILESTART = '/nfs/hera1/earjcti/um/'

diffannmean()
diffmonmean()



::::::::::::::
HadCM3/compare_two_HadCM3/annmean_diff.py
::::::::::::::
1#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia
This program will plot the annual mean difference between two models
for a given field
Note the data must have been preprocessed by CEMAC/PLIOMIP2/regrid_HCM3_50_year_avg.py
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt
from iris.cube import CubeList
import iris.quickplot as qplt
import iris.analysis.cartography
import iris.coord_categorisation
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap,Normalize


class PiecewiseNorm(Normalize):
    def __init__(self, levels, clip=False):
        # the input levels
        self._levels = np.sort(levels)
        # corresponding normalized values between 0 and 1
        self._normed = np.linspace(0, 1, len(levels))
        Normalize.__init__(self, None, None, clip)

    def __call__(self, value, clip=None):
        # linearly interpolate to get the normalized value
        return np.ma.masked_array(np.interp(value, self._levels, self._normed))

def diff_two_experiments():
    """
    plots the difference in annual mean between experiment1 and experiment2
    """
    annmean_exp1_cube = iris.load_cube(FILESTART + EXPT1 + FILEEND)
    annmean_exp2_cube = iris.load_cube(FILESTART + EXPT2 + FILEEND)

    anom_cube = annmean_exp2_cube - annmean_exp1_cube
   
    if type == 'PliominPi':
        V = np.arange(-10.0,11.0, 1.0)
    else:
        V = [-30,-15.,-10., -5., -2., -1., -0.5, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0, 30.]
   
    #plt.subplot(2,1,1)
    
    if FIELDNAME == 'TotalPrecipitation':
        for i,vind in enumerate(V):
            V[i] = vind / 10.
        mycmap = cm.get_cmap('rainbow_r', len(V)+2)
    else:
        mycmap = cm.get_cmap('rainbow', len(V)+2)
    newcolors = mycmap(np.linspace(0,1,len(V)+2))
    white = ([1,1,1,1])
    print('julia',int(len(V)/2),int(len(V)/2+1))
    newcolors[int((len(V)/2)):int(len(V)/2+2),:] = white
    mycmap = ListedColormap(newcolors)

    cs=iplt.contourf(anom_cube, levels=V, extend='both', cmap=mycmap,
                  norm=PiecewiseNorm(V))
    cbar = plt.colorbar(cs,orientation='horizontal',ticks=V)
    if FIELDNAME == 'TotalPrecipitation':
        cbar.set_label('mm/day')
    plt.title(FIELDNAME + ':' + EXPT2 + '-' + EXPT1 + ' annmean')
    plt.gca().coastlines()

   # plt.subplot(2,1,2)
   # qplt.contourf(anom_cube, levels=np.arange(-3,3.5,0.5), extend='both', cmap='Rd#Bu_r')
   # plt.title(FIELDNAME + ':' + EXPT2 + '-' + EXPT1 + ' annmean')
   # plt.gca().coastlines()

    fileout = ('/nfs/hera1/earjcti/HadCM3_plots/' + FIELDNAME + '/' + EXPT2 + '-' + EXPT1 + '_annmean')
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()


def diff_two_anomalies():
    """
    plots the difference in annual mean between experiment1 and experiment2
    """
    expt1e = EXPT1[0:5]
    expt1c = EXPT1[6:11]
    expt2e = EXPT2[0:5]
    expt2c = EXPT2[6:11]
  
    annmean_exp1e_cube = iris.load_cube(FILESTART + expt1e + FILEEND)
    annmean_exp1c_cube = iris.load_cube(FILESTART + expt1c + FILEEND)
    annmean_exp2e_cube = iris.load_cube(FILESTART + expt2e + FILEEND)
    annmean_exp2c_cube = iris.load_cube(FILESTART + expt2c + FILEEND)

    anom_cube = ((annmean_exp2e_cube - annmean_exp2c_cube) -
                 (annmean_exp1e_cube - annmean_exp1c_cube))

    V = [-15.,-10., -5., -2. -1., -0.5, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0]
    qplt.contourf(anom_cube, levels=V, extend='both', cmap="rainbow",
                  norm=PiecewiseNorm(levels))
    plt.title(FIELDNAME + ':' + EXPT2 + '-' + EXPT1 + ' annmean')
    plt.gca().coastlines()

    fileout = ('/nfs/hera1/earjcti/HadCM3_plots/' + FIELDNAME + '/' + EXPT2 + '-' + EXPT1 + '_annmean')
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

##########################################################
# main program

EXPT1 = 'xozza'
EXPT2 = 'xpkma'
#type = 'PliominPi' # type is PliominPi, PiminPi, PliominPlio
type = 'PiminPi'

FIELDNAME = 'TotalPrecipitation'

FILESTART = '/nfs/hera1/earjcti/um/'
FILEEND = '/' + FIELDNAME + '/means/allmean.nc'

if len(EXPT1) > 10 and len(EXPT2) > 10:
    diff_two_anomalies()
else:
     diff_two_experiments()

::::::::::::::
HadCM3/compare_two_HadCM3/compare_vegetation_fractions.py
::::::::::::::
1#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia
This program will plot the triffid plots for each vegetation type.
It will also plot the difference between two fields
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt
from iris.cube import CubeList
import iris.quickplot as qplt
import iris.analysis.cartography
import iris.coord_categorisation
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap,Normalize


class PiecewiseNorm(Normalize):
    def __init__(self, levels, clip=False):
        # the input levels
        self._levels = np.sort(levels)
        # corresponding normalized values between 0 and 1
        self._normed = np.linspace(0, 1, len(levels))
        Normalize.__init__(self, None, None, clip)

    def __call__(self, value, clip=None):
        # linearly interpolate to get the normalized value
        return np.ma.masked_array(np.interp(value, self._levels, self._normed))

def get_veg(fileend):
    """
    gets the iris vegetation cube for the file
    """
    filestart = '/nfs/hera1/earjcti/um/'
    
    fileuse = filestart + fileend[0:5] + '/pi/' + fileend
    cube = iris.load_cube(fileuse,'TILE FRACTIONS (B.LAYER)')
   
    return iris.util.squeeze(cube)

def plot_veg(cube, anom_ind, fileoutend):
    """
    plots the vegetation fraction for cube.
    If anom_ind = 'n' then the cube is vegetation
    If anom_ind = 'y' then the cube is an anomaly
    """
   
    vegtype = ['Broadleaf tree','needleleaf tree','c3 grass','c4 grass','shrub','urban','water','soil','ice']
    if anom_ind == 'Y' or anom_ind == 'y':
        V = np.arange(-0.3, 0.35, 0.05)
        mycmap = cm.get_cmap('RdBu_r', len(V)+2)
        newcolors = mycmap(np.linspace(0,1,len(V)+2))
        white = ([1,1,1,1])
        print('julia',int(len(V)/2),int(len(V)/2+1))
        newcolors[int((len(V)/2)):int(len(V)/2+3),:] = white
        mycmap = ListedColormap(newcolors)
        extendreq='both'
    else:
        V = np.arange(0.0, 1.1, 0.1)
        mycmap = 'Greens'
        extendreq='neither'
   
    fig = plt.figure(figsize=[8.0, 8.0])
  
    count=0
    for i in range(0,9):
        if i in (0,1,2,3,4,7):
            count=count+1
            cubeveg = cube[i,:,:]
            plt.subplot(3,2,count)
            cs=iplt.contourf(cubeveg, levels=V, extend=extendreq, cmap=mycmap)
            plt.gca().coastlines()
            plt.title(vegtype[i])

    fig.subplots_adjust(bottom=0.2, top=0.9, left=0.1, right=0.9,wspace=0.05,hspace=0.05)
    cb_ax=fig.add_axes([0.1,0.1,0.8,0.05])
    cbar=fig.colorbar(cs,cax=cb_ax,orientation='horizontal')

    fileout = ('/nfs/hera1/earjcti/HadCM3_plots/vegetation/' + fileoutend)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()


def diff_two_anomalies():
    """
    plots the difference in annual mean between experiment1 and experiment2
    """
    expt1e = EXPT1[0:5]
    expt1c = EXPT1[6:11]
    expt2e = EXPT2[0:5]
    expt2c = EXPT2[6:11]
  
    annmean_exp1e_cube = iris.load_cube(FILESTART + expt1e + FILEEND)
    annmean_exp1c_cube = iris.load_cube(FILESTART + expt1c + FILEEND)
    annmean_exp2e_cube = iris.load_cube(FILESTART + expt2e + FILEEND)
    annmean_exp2c_cube = iris.load_cube(FILESTART + expt2c + FILEEND)

    anom_cube = ((annmean_exp2e_cube - annmean_exp2c_cube) -
                 (annmean_exp1e_cube - annmean_exp1c_cube))

    V = [-15.,-10., -5., -2. -1., -0.5, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0]
    qplt.contourf(anom_cube, levels=V, extend='both', cmap="rainbow",
                  norm=PiecewiseNorm(levels))
    plt.title(FIELDNAME + ':' + EXPT2 + '-' + EXPT1 + ' annmean')
    plt.gca().coastlines()

    fileout = ('/nfs/hera1/earjcti/HadCM3_plots/' + FIELDNAME + '/' + EXPT2 + '-' + EXPT1 + '_annmean')
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

##########################################################
# main program

FILE1 = 'xozzza@pip00ja.nc'
FILE2 = 'tenvoa@piu00ja.nc'
#FILE2 = 'xozzza@piq00ja.nc'
veg_cube1 = get_veg(FILE1)
veg_cube2 = get_veg(FILE2)
veg_diff = veg_cube2 - veg_cube1

plot_veg(veg_cube1,'n',FILE1)
plot_veg(veg_cube2,'n',FILE2)
plot_veg(veg_diff,'y',FILE2 + '-' + FILE1)
::::::::::::::
HadCM3/compare_two_HadCM3/seasmean_diff.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.quickplot as qplt
import iris.plot as iplt
from iris.cube import CubeList


def plotfig(fig, figno, cube, figtitle, anom_ind):
     """
     plots the subfigure
     """
     if anom_ind == "n":
         cmapname = "terrain_r"
         if FIELDNAME == "TotalPrecipitation":
             Vuse = np.arange(0,21,1)
         
     else:
         Vuse=V
         if FIELDNAME == "TotalPrecipitation":
             cmapname = "RdBu"
         else:
             cmapname = "RdBu_r"
   
     fig.add_subplot(figno[0],2,figno[1])
     cs = iplt.contourf(cube, levels=Vuse,extend='both', cmap=cmapname)
     plt.title(FIELDNAME + ':' + figtitle)
     plt.gca().coastlines()
     cbar = plt.colorbar(cs,orientation='horizontal')

     return


#=====================================================================
def diff_two_experiments():
    """
    plots the difference in seasonal mean between experiment1 and experiment2
    """
    monmean_exp1_cube = iris.load_cube(FILESTART + EXPT1 + FILEEND)
    monmean_exp2_cube = iris.load_cube(FILESTART + EXPT2 + FILEEND)
    if FIELDNAME == 'TotalPrecipitation':
        monmean_exp1_cube.units = 'mm day'
        monmean_exp2_cube.units = 'mm day'
     

  
    anom_data = monmean_exp2_cube.data - monmean_exp1_cube.data
    anom_cube = monmean_exp1_cube.copy(data=anom_data)
    print(anom_cube)
  
    djf_anom_cube = (anom_cube[0,:,:] + anom_cube[1,:,:] + anom_cube[11,:,:]) / 3.0
    mam_anom_cube = (anom_cube[2,:,:] + anom_cube[3,:,:] + anom_cube[4,:,:]) / 3.0
    jja_anom_cube = (anom_cube[5,:,:] + anom_cube[6,:,:] + anom_cube[7,:,:]) / 3.0
    son_anom_cube = (anom_cube[8,:,:] + anom_cube[9,:,:] + anom_cube[10,:,:]) / 3.0


    djf_e1_cube = (monmean_exp1_cube[0,:,:] + monmean_exp1_cube[1,:,:] + monmean_exp1_cube[11,:,:]) / 3.0
    mam_e1_cube = (monmean_exp1_cube[2,:,:] + monmean_exp1_cube[3,:,:] + monmean_exp1_cube[4,:,:]) / 3.0
    jja_e1_cube = (monmean_exp1_cube[5,:,:] + monmean_exp1_cube[6,:,:] + monmean_exp1_cube[7,:,:]) / 3.0
    son_e1_cube = (monmean_exp1_cube[8,:,:] + monmean_exp1_cube[9,:,:] + monmean_exp1_cube[10,:,:]) / 3.0

    if FIELDNAME == 'NearSurfaceTemperature':
        djf_e1_cube.convert_units('degC')
        mam_e1_cube.convert_units('degC')
        jja_e1_cube.convert_units('degC')
        son_e1_cube.convert_units('degC')
  
    fig = plt.figure(figsize=[7,10])

    # plot them all
    plotfig(fig,[4,1],djf_e1_cube, EXPT1 + ' djf mean','n')
    plotfig(fig,[4,2],djf_anom_cube, EXPT2 + '-' + EXPT1 + ' djf mean','y')
    plotfig(fig,[4,3],mam_e1_cube, EXPT1 + ' mam mean','n')
    plotfig(fig,[4,4],mam_anom_cube, EXPT2 + '-' + EXPT1 + ' mam mean','y')
    plotfig(fig,[4,5],jja_e1_cube, EXPT1 + ' jja mean','n')
    plotfig(fig,[4,6],jja_anom_cube, EXPT2 + '-' + EXPT1 + ' jja mean','y')
    plotfig(fig,[4,7],son_e1_cube, EXPT1 + ' son mean','n')
    plotfig(fig,[4,8],son_anom_cube, EXPT2 + '-' + EXPT1 + ' son mean','y')
   
    fileout = ('/nfs/hera1/earjcti/HadCM3_plots/' + FIELDNAME + '/' + EXPT2 + '-' + EXPT1 + '_seasmean')
    plt.tight_layout()
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()




#=====================================================================
def diff_two_anomalies():
    """
    plots the difference in seasonal mean between experiment1 and experiment2
    """

    expt1e = EXPT1[0:5]
    expt1c = EXPT1[6:11]
    expt2e = EXPT2[0:5]
    expt2c = EXPT2[6:11]
  
    monmean_exp1e_cube = iris.load_cube(FILESTART + expt1e + FILEEND)
    monmean_exp1c_cube = iris.load_cube(FILESTART + expt1c + FILEEND)
    monmean_exp2e_cube = iris.load_cube(FILESTART + expt2e + FILEEND)
    monmean_exp2c_cube = iris.load_cube(FILESTART + expt2c + FILEEND)

    anom_data = ((monmean_exp2e_cube.data - monmean_exp2c_cube.data) - 
                 (monmean_exp1e_cube.data - monmean_exp1c_cube.data))

    anom_cube = monmean_exp1e_cube.copy(data=anom_data)

    djf_anom_cube = (anom_cube[0,:,:] + anom_cube[1,:,:] + anom_cube[11,:,:]) / 3.0
    mam_anom_cube = (anom_cube[2,:,:] + anom_cube[3,:,:] + anom_cube[4,:,:]) / 3.0
    jja_anom_cube = (anom_cube[5,:,:] + anom_cube[6,:,:] + anom_cube[7,:,:]) / 3.0
    son_anom_cube = (anom_cube[8,:,:] + anom_cube[9,:,:] + anom_cube[10,:,:]) / 3.0

    
    fig = plt.figure(figsize=[7,5])

    # plot them all
    plotfig(fig,[2,1],djf_anom_cube, EXPT2 + '-' + EXPT1 + ' djf','y')
    plotfig(fig,[2,2],mam_anom_cube, ' mam anom','y')
    plotfig(fig,[2,3],jja_anom_cube, ' jja anom','y')
    plotfig(fig,[2,4],son_anom_cube, ' son anom','y')
    
    fileout = ('/nfs/hera1/earjcti/HadCM3_plots/' + FIELDNAME + '/' + EXPT2 + '-' + EXPT1 + '_seasmean')
    plt.tight_layout()
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()


#
##########################################################
# main program

EXPT1 = 'xozzb-xozza'
EXPT2 = 'xpkmc-xpkma'
#type = 'PliominPi' # type is PliominPi, PiminPi, PliominPlio
type = 'PliominPi'

FIELDNAME = 'TotalPrecipitation'

FILESTART = '/nfs/hera1/earjcti/um/'
FILEEND = '/' + FIELDNAME + '/means/mean_month.nc'


if FIELDNAME == 'TotalPrecipitation':
    if type == 'PliominPi':
        V = np.arange(-3.0,3.5, 0.5)
    else:
        V = np.arange(-3.0, 3.5, 0.5)

if FIELDNAME == 'NearSurfaceTemperature':
    if type == 'PliominPi':
        V = np.arange(-10.0,11.0, 1.0)
    else:
        V = np.arange(-3.0, 3.5, 0.5)


if len(EXPT1) > 10 and len(EXPT2) > 10:
    diff_two_anomalies()
else:
    diff_two_experiments()

::::::::::::::
HadCM3/compare_two_HadCM3/statistical_significance_of_changes.py
::::::::::::::


okay so I was going to copy this from our large_scale_features paper.  But have decided that the benchmarking program works better
::::::::::::::
JASMIN_PROGS/Amazon/plot_d18o.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_D18O
#PURPOSE 
#    PLOT D18O FROM LOUISE/MAX TIMESLICE EXPERIMENTS MULTIPLE FILES
#
#
# Julia 20.09.2016


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,cbartitle,minval,maxval,diffval):
    lons, lats = np.meshgrid(lon,lat)
    plt.subplot(2,2,fileno+1)
   # map=Basemap(projection='robin',resolution='l')

   # this may be a good region for middle east
    map=Basemap(llcrnrlon=-90.0,urcrnrlon=0.0,llcrnrlat=-50.0,urcrnrlat=0.0,projection='cyl',resolution='c')   

    # this may be a good region for most of the globe
   # map=Basemap(llcrnrlon=0,llcrnrlat=-80,urcrnrlon=360,urcrnrlat=80,projection='mill')

    
   # map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #V=np.arange(np.amin(plotdata),np.amax(plotdata),np.amin(plotdata)/10)
    V=np.arange(minval,maxval,diffval)
   # cs = map.contourf(x,y,plotdata,V)
    cs=map.contourf(x,y,plotdata,V,extend='both')
    plt.title(titlename)
  
    cbar=map.colorbar(cs,location='bottom',pad="5%")
    cbar.ax.tick_params(labelsize=6)
    cbar.ax.set_title(cbartitle)

def getKey(item):
    return item[0]

def plottimeseries(d18o,dD,timeperiod):




#   plot my data and overplot Nizars data
    plt.subplot(3,1,1)
    plt.plot(timeperiod,d18o)
    plt.xlabel('ka')
    plt.ylabel('permil')
    plt.title('d18o_p over paraiso')

    plt.subplot(3,1,2)
    plt.plot(timeperiod,13.65+(6.53*np.asarray(d18o)))
    plt.xlabel('ka')
    plt.ylabel('permil')
    plt.title('13.65-(6.53*d18o) over paraiso')


    plt.subplot(3,1,3)
    plt.plot(timeperiod,dD)
    plt.xlabel('ka')
    plt.ylabel('permil')
    plt.title('dD over paraiso')

    #plt.subplot(4,1,4)
    #dxs=dD-(8.0*np.asarray(d18o))
    #plt.plot(timeperiod,dxs)
    #plt.xlabel('ka')
    #plt.ylabel('permil')
    #plt.title('dxs over paraiso')

    plt.tight_layout(pad=0.4,w_pad=0.5,h_pad=1.0)




#=====================================================
def extract_single_site_data(expt,filenames,expttime):


    # 1.  Set up details.  Gridbox required and filename

    #Paraiso
    lonreq=303.75
    latreq=-5.0

    #Tigre perdido
    #lonreq=281.25
    #latreq=-5.0

    dirname='/home/users/jctindall/mholloway/'+expt+'/pcpd/'
    os.chdir(dirname)


    #2. Set up filename and extract stash code 338
    #   print d18o and dD for that file


    f=MFDataset(filenames)
    f.dimensions
    f.variables

    lon = f.variables['longitude_1'][:]
    lat = f.variables['latitude_1'][:]
    times = f.variables['t'][:]
    xsize=len(lon)
    ysize=len(lat)
    tsize=len(times)
    dD=f.variables['dD'][:]
    dD=np.squeeze(dD)
    d18o=f.variables['dO18'][:]
    d18o=np.squeeze(d18o)
    h2o=f.variables['h2o'][:]
    h2o=np.squeeze(h2o)
    
    
    f.close()

    # we need to get annual average by weighting by precipitation amount

    dD_weight=dD*h2o
    dD_weightsum=np.sum(dD_weight,axis=0)
    d18o_weight=d18o*h2o
    d18o_weightsum=np.sum(d18o_weight,axis=0)
    tot_h2o=np.sum(h2o,axis=0)
    dD_weightavg=dD_weightsum/tot_h2o
    d18o_weightavg=d18o_weightsum/tot_h2o
    #print("shape tpt h2o",tot_h2o.shape)
    #print("shape mean dDweight",dD_weightsum.shape)


    # extract Paraiso d18o and dD and total annual precipitation

    ix1=(lon == lonreq)
    ix2=(lat ==latreq)
   
    temp=d18o_weightavg[ix2]
    single_pt_d18o=np.mean(temp[:,ix1])
    temp=dD_weightavg[ix2]
    single_pt_dD=np.mean(temp[:,ix1])
    temp=tot_h2o[ix2]
    single_pt_h2o=np.mean(temp[:,ix1])
    
    

    cbartitle='mm'
    titlename='avg'

    lontemp=lon
    d18o_weightavg,lon=shiftgrid(180,d18o_weightavg,lon,start=False)
    lon=lontemp
    tot_h2o,lon=shiftgrid(180,tot_h2o,lon,start=False)
    lon=lontemp
    dD_weightavg,lon=shiftgrid(180,dD_weightavg,lon,start=False)
   
    plotdata(tot_h2o*60.*60.*24.*30/tsize,0,lon,lat,titlename,cbartitle,0,200,10)

    

    #weighted dD
    titlename='dD'
    cbartitle='permil '
    plotdata(dD_weightavg,1,lon,lat,titlename,cbartitle,-100,200,20)

    #weighted d18o
    titlename='d18o'
    cbartitle='permil '
    plotdata(d18o_weightavg,2,lon,lat,titlename,cbartitle,-12,0,1)

    #weighted dxs
    titlename='dxs'
    cbartitle='permil '
    dxs=dD_weightavg - (8.0 * d18o_weightavg)
    plotdata(dxs,3,lon,lat,titlename,cbartitle,0,100,5)

    plt.tight_layout(pad=0.4,w_pad=0.5,h_pad=1.0)

    fileout='/home/users/jctindall/plots/python/Amazon/plot_d18o/'+expt+'_'+expttime+'_d18o_spatial.eps' 
    plt.savefig(fileout, bbox_inches='tight')  

    fileout='/home/users/jctindall/plots/python/Amazon/plot_d18o/'+expt+'_'+expttime+'_d18o_spatial.png' 
    plt.savefig(fileout, bbox_inches='tight')  

  
    plt.close()
   

    retdata=[single_pt_d18o,single_pt_dD,single_pt_h2o]

    return retdata


#=================================================================
# MAIN PROGRAM STARTS HERE


# xluba 0ka
retdata=extract_single_site_data('xluba','iso_xlubaa@pcr[3-6]*.nc','0ka')
alldata=[['xluba',0.0,retdata]]

# xlubb 1ka
retdata=extract_single_site_data('xlubb','iso_xlubba@pcr[3-6]*.nc','1ka')
alldata.append(['xlubb',-1.0,retdata])

# xlubc 2ka
retdata=extract_single_site_data('xlubc','iso_xlubca@pcr[3-6]*.nc','2ka')
alldata.append(['xlubc',-2.0,retdata])

# xlubd 3ka
retdata=extract_single_site_data('xlubd','iso_xlubda@pcr[3-6]*.nc','3ka')
alldata.append(['xlubd',-3.0,retdata])

# xlube 4ka
retdata=extract_single_site_data('xlube','iso_xlubea@pcr[3-6]*.nc','4ka')
alldata.append(['xlube',-4.0,retdata])

# xlubf 5ka
retdata=extract_single_site_data('xlubf','iso_xlubfa@pcr[3-6]*.nc','5ka')
alldata.append(['xlubf',-5.0,retdata])

# xlubg 6ka
retdata=extract_single_site_data('xlubg','iso_xlubga@pcr[3-6]*.nc','6ka')
alldata.append(['xlubg',-6.0,retdata])

# xlubh 7ka
retdata=extract_single_site_data('xlubh','iso_xlubha@pcr[3-6]*.nc','7ka')
alldata.append(['xlubh',-7.0,retdata])

# xlubi 8ka
retdata=extract_single_site_data('xlubi','iso_xlubia@pcr[3-6]*.nc','8ka')
alldata.append(['xlubi',-8.0,retdata])

# xlubj 9ka
retdata=extract_single_site_data('xlubj','iso_xlubja@pcr[3-6]*.nc','9ka')
alldata.append(['xlubj',-9.0,retdata])

# xlubk 10ka
retdata=extract_single_site_data('xlubk','iso_xlubka@pcr[3-6]*.nc','10ka')
alldata.append(['xlubk',-10.0,retdata])

# xlubl 11ka
retdata=extract_single_site_data('xlubl','iso_xlubla@pcr[3-6]*.nc','11ka')
alldata.append(['xlubl',-11.0,retdata])

# xlubm 12ka
retdata=extract_single_site_data('xlubm','iso_xlubma@pcr[3-6]*.nc','12ka')
alldata.append(['xlubm',-12.0,retdata])

# xlubn 13ka
retdata=extract_single_site_data('xlubn','iso_xlubna@pcr[3-6]*.nc','13ka')
alldata.append(['xlubn',-13.0,retdata])

# xlubo 14ka
retdata=extract_single_site_data('xlubo','iso_xluboa@pcr[3-6]*.nc','14ka')
alldata.append(['xlubo',-14.0,retdata])

# xlubp 15ka
retdata=extract_single_site_data('xlubp','iso_xlubpa@pcr[0-2]*.nc','15ka')
alldata.append(['xlubp',-15.0,retdata])

# xlubq 16ka
retdata=extract_single_site_data('xlubq','iso_xlubqa@pcr[0-2]*.nc','16ka')
alldata.append(['xlubq',-16.0,retdata])

# xlubr 17ka
retdata=extract_single_site_data('xlubr','iso_xlubra@pcr[0-2]*.nc','17ka')
alldata.append(['xlubr',-17.0,retdata])

# xlubs 18ka
retdata=extract_single_site_data('xlubs','iso_xlubsa@pcr[2-5]*.nc','18ka')
alldata.append(['xlubs',-18.0,retdata])

# xlubt 19ka
retdata=extract_single_site_data('xlubt','iso_xlubta@pcr[2-5]*.nc','19ka')
alldata.append(['xlubt',-19.0,retdata])

# xlubu 20ka
retdata=extract_single_site_data('xlubu','iso_xlubua@pcr[2-5]*.nc','20ka')
alldata.append(['xlubu',-20.0,retdata])

# xlubv 21ka
retdata=extract_single_site_data('xlubv','iso_xlubva@pcr[2-5]*.nc','21ka')
alldata.append(['xlubv',-21.0,retdata])



# extract d18o

datalen=len(alldata)
d18o=[alldata[i][2][0] for i in range(0,datalen)]
dD=[alldata[i][2][1] for i in range(0,datalen)]
timeperiod=[alldata[i][1] for i in range(0,datalen)]


plottimeseries(d18o,dD,timeperiod)
fileout='/home/users/jctindall/plots/python/Amazon/plot_d18o/Paraiso_d18o_timeseries.eps' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/home/users/jctindall/plots/python/Amazon/plot_d18o/Paraiso_d18o_timeseries.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()
::::::::::::::
JASMIN_PROGS/Middleeast/Nizar_plot_d18o.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_D18O
#PURPOSE 
#    PLOT D18O FROM LOUISE/MAX TIMESLICE EXPERIMENTS MULTIPLE FILES
#
#
# Julia 20.09.2016


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,cbartitle,minval,maxval,diffval):
    lons, lats = np.meshgrid(lon,lat)
    plt.subplot(2,2,fileno+1)
   # map=Basemap(projection='robin',resolution='l')

   # this may be a good region for middle east
    map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')   

    # this may be a good region for most of the globe
   # map=Basemap(llcrnrlon=0,llcrnrlat=-80,urcrnrlon=360,urcrnrlat=80,projection='mill')

    
   # map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #V=np.arange(np.amin(plotdata),np.amax(plotdata),np.amin(plotdata)/10)
    V=np.arange(minval,maxval,diffval)
   # cs = map.contourf(x,y,plotdata,V)
    cs=map.contourf(x,y,plotdata,V,extend='both')
    plt.title(titlename)
  
    cbar=map.colorbar(cs,location='bottom',pad="5%")
    cbar.ax.tick_params(labelsize=6)
    cbar.ax.set_title(cbartitle)

def getKey(item):
    return item[0]

def plottimeseries(d18o,dD,timeperiod):

    Nizardata=[[-2.4,-6.42,-33.3],
               [-8,-5.94,-33.2],
               [-21.9,-7.94,-52.23],
               [-16.2,-5.16,-26.5],
               [-18.6,-5.74,-30.9],
               [-11.2,-5.98,-31.2],
               ]
    Nizardata=sorted(Nizardata,key=getKey)
    print(Nizardata[0][0])
    datalen=len(Nizardata)
    Nzd18o=[Nizardata[i][1] for i in range(0,datalen)]
    NzdD=[Nizardata[i][2] for i in range(0,datalen)]
    Nztime=[Nizardata[i][0] for i in range(0,datalen)]

    print('time is',Nztime)
    print('d18o is',Nzd18o)



#   plot my data and overplot Nizars data
    plt.subplot(3,1,1)
    plt.plot(timeperiod,d18o)
    plt.plot(Nztime,Nzd18o)
    plt.xlabel('ka')
    plt.ylabel('permil')
    plt.title('d18o_p over Jordan')

    plt.subplot(3,1,2)
    plt.plot(timeperiod,13.65+(6.53*np.asarray(d18o)))
    plt.plot(Nztime,NzdD)
    plt.xlabel('ka')
    plt.ylabel('permil')
    plt.title('13.65-(6.53*d18o) over Jordan')


    plt.subplot(3,1,3)
    plt.plot(timeperiod,dD)
    plt.plot(Nztime,NzdD)
    plt.xlabel('ka')
    plt.ylabel('permil')
    plt.title('dD over Jordan')

    #plt.subplot(4,1,4)
    #dxs=dD-(8.0*np.asarray(d18o))
    #plt.plot(timeperiod,dxs)
    #plt.xlabel('ka')
    #plt.ylabel('permil')
    #plt.title('dxs over Jordan')

    plt.tight_layout(pad=0.4,w_pad=0.5,h_pad=1.0)




#=====================================================
def extract_single_site_data(expt,filenames,expttime):


    # 1.  Set up details.  Gridbox required and filename

    #Jordan
    lonreq=37.5
    latreq=30.0

    dirname='/home/users/jctindall/mholloway/'+expt+'/pcpd/'
    os.chdir(dirname)


    #2. Set up filename and extract stash code 338
    #   print d18o and dD for that file


    f=MFDataset(filenames)
    f.dimensions
    f.variables

    lon = f.variables['longitude_1'][:]
    lat = f.variables['latitude_1'][:]
    times = f.variables['t'][:]
    xsize=len(lon)
    ysize=len(lat)
    tsize=len(times)
    dD=f.variables['dD'][:]
    dD=np.squeeze(dD)
    d18o=f.variables['dO18'][:]
    d18o=np.squeeze(d18o)
    h2o=f.variables['h2o'][:]
    h2o=np.squeeze(h2o)
    
    
    f.close()

    # we need to get annual average by weighting by precipitation amount

    dD_weight=dD*h2o
    dD_weightsum=np.sum(dD_weight,axis=0)
    d18o_weight=d18o*h2o
    d18o_weightsum=np.sum(d18o_weight,axis=0)
    tot_h2o=np.sum(h2o,axis=0)
    dD_weightavg=dD_weightsum/tot_h2o
    d18o_weightavg=d18o_weightsum/tot_h2o
    #print("shape tpt h2o",tot_h2o.shape)
    #print("shape mean dDweight",dD_weightsum.shape)


    # extract Jordan d18o and dD and total annual precipitation

    ix1=(lon == lonreq)
    ix2=(lat ==latreq)

    temp=d18o_weightavg[ix2]
    single_pt_d18o=np.mean(temp[:,ix1])
    temp=dD_weightavg[ix2]
    single_pt_dD=np.mean(temp[:,ix1])
    temp=tot_h2o[ix2]
    single_pt_h2o=np.mean(temp[:,ix1])
    
    

    cbartitle='mm'
    titlename='avg'
    plotdata(tot_h2o*60.*60.*24.*30/tsize,0,lon,lat,titlename,cbartitle,0,100,5)

    

    #weighted dD
    titlename='dD'
    cbartitle='permil '
    plotdata(dD_weightavg,1,lon,lat,titlename,cbartitle,-100,200,20)

    #weighted d18o
    titlename='d18o'
    cbartitle='permil '
    plotdata(d18o_weightavg,2,lon,lat,titlename,cbartitle,-20,10,2)

    #weighted dxs
    titlename='dxs'
    cbartitle='permil '
    dxs=dD_weightavg - (8.0 * d18o_weightavg)
    plotdata(dxs,3,lon,lat,titlename,cbartitle,0,100,5)

    plt.tight_layout(pad=0.4,w_pad=0.5,h_pad=1.0)

    fileout='/home/users/jctindall/plots/python/Middleeast/plot_d18o/'+expt+'_'+expttime+'_d18o_spatial.eps' 
    plt.savefig(fileout, bbox_inches='tight')  

    fileout='/home/users/jctindall/plots/python/Middleeast/plot_d18o/'+expt+'_'+expttime+'_d18o_spatial.png' 
    plt.savefig(fileout, bbox_inches='tight')  
  
    plt.close()



    retdata=[single_pt_d18o,single_pt_dD,single_pt_h2o]

    return retdata


#=================================================================
# MAIN PROGRAM STARTS HERE


# xluba 0ka
retdata=extract_single_site_data('xluba','iso_xlubaa@pcr[3-6]*.nc','0ka')
alldata=[['xluba',0.0,retdata]]

# xlubb 1ka
retdata=extract_single_site_data('xlubb','iso_xlubba@pcr[3-6]*.nc','1ka')
alldata.append(['xlubb',-1.0,retdata])

# xlubc 2ka
retdata=extract_single_site_data('xlubc','iso_xlubca@pcr[3-6]*.nc','2ka')
alldata.append(['xlubc',-2.0,retdata])

# xlubd 3ka
retdata=extract_single_site_data('xlubd','iso_xlubda@pcr[3-6]*.nc','3ka')
alldata.append(['xlubd',-3.0,retdata])

# xlube 4ka
retdata=extract_single_site_data('xlube','iso_xlubea@pcr[3-6]*.nc','4ka')
alldata.append(['xlube',-4.0,retdata])

# xlubf 5ka
retdata=extract_single_site_data('xlubf','iso_xlubfa@pcr[3-6]*.nc','5ka')
alldata.append(['xlubf',-5.0,retdata])

# xlubg 6ka
retdata=extract_single_site_data('xlubg','iso_xlubga@pcr[3-6]*.nc','6ka')
alldata.append(['xlubg',-6.0,retdata])

# xlubh 7ka
retdata=extract_single_site_data('xlubh','iso_xlubha@pcr[3-6]*.nc','7ka')
alldata.append(['xlubh',-7.0,retdata])

# xlubi 8ka
retdata=extract_single_site_data('xlubi','iso_xlubia@pcr[3-6]*.nc','8ka')
alldata.append(['xlubi',-8.0,retdata])

# xlubj 9ka
retdata=extract_single_site_data('xlubj','iso_xlubja@pcr[3-6]*.nc','9ka')
alldata.append(['xlubj',-9.0,retdata])

# xlubk 10ka
retdata=extract_single_site_data('xlubk','iso_xlubka@pcr[3-6]*.nc','10ka')
alldata.append(['xlubk',-10.0,retdata])

# xlubl 11ka
retdata=extract_single_site_data('xlubl','iso_xlubla@pcr[3-6]*.nc','11ka')
alldata.append(['xlubl',-11.0,retdata])

# xlubm 12ka
retdata=extract_single_site_data('xlubm','iso_xlubma@pcr[3-6]*.nc','12ka')
alldata.append(['xlubm',-12.0,retdata])

# xlubn 13ka
retdata=extract_single_site_data('xlubn','iso_xlubna@pcr[3-6]*.nc','13ka')
alldata.append(['xlubn',-13.0,retdata])

# xlubo 14ka
retdata=extract_single_site_data('xlubo','iso_xluboa@pcr[3-6]*.nc','14ka')
alldata.append(['xlubo',-14.0,retdata])

# xlubp 15ka
retdata=extract_single_site_data('xlubp','iso_xlubpa@pcr[0-2]*.nc','15ka')
alldata.append(['xlubp',-15.0,retdata])

# xlubq 16ka
retdata=extract_single_site_data('xlubq','iso_xlubqa@pcr[0-2]*.nc','16ka')
alldata.append(['xlubq',-16.0,retdata])

# xlubr 17ka
retdata=extract_single_site_data('xlubr','iso_xlubra@pcr[0-2]*.nc','17ka')
alldata.append(['xlubr',-17.0,retdata])

# xlubs 18ka
retdata=extract_single_site_data('xlubs','iso_xlubsa@pcr[2-5]*.nc','18ka')
alldata.append(['xlubs',-18.0,retdata])

# xlubt 19ka
retdata=extract_single_site_data('xlubt','iso_xlubta@pcr[2-5]*.nc','19ka')
alldata.append(['xlubt',-19.0,retdata])

# xlubu 20ka
retdata=extract_single_site_data('xlubu','iso_xlubua@pcr[2-5]*.nc','20ka')
alldata.append(['xlubu',-20.0,retdata])

# xlubv 21ka
retdata=extract_single_site_data('xlubv','iso_xlubva@pcr[2-5]*.nc','21ka')
alldata.append(['xlubv',-21.0,retdata])



# extract d18o

datalen=len(alldata)
d18o=[alldata[i][2][0] for i in range(0,datalen)]
dD=[alldata[i][2][1] for i in range(0,datalen)]
timeperiod=[alldata[i][1] for i in range(0,datalen)]


plottimeseries(d18o,dD,timeperiod)
fileout='/home/users/jctindall/plots/python/Middleeast/plot_d18o/Jordan_d18o_timeseries.eps' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/home/users/jctindall/plots/python/Middleeast/plot_d18o/Jordan_d18o_timeseries.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()
::::::::::::::
JASMIN_PROGS/Middleeast/Nizar_plot_diags.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    Nizar_plot_diags
#PURPOSE 
#    PLOT Diagnostics from Louise/Max timeslice experiments for Nizar
#
#
# Julia 31.1.2017


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,cbartitle,minval,maxval,diffval):
    lons, lats = np.meshgrid(lon,lat)
    plt.subplot(2,2,fileno+1)
   # map=Basemap(projection='robin',resolution='l')

   # this may be a good region for middle east
    map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')   

    # this may be a good region for most of the globe
   # map=Basemap(llcrnrlon=0,llcrnrlat=-80,urcrnrlon=360,urcrnrlat=80,projection='mill')

    
   # map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #V=np.arange(np.amin(plotdata),np.amax(plotdata),np.amin(plotdata)/10)
    V=np.arange(minval,maxval,diffval)
   # cs = map.contourf(x,y,plotdata,V)
    cs=map.contourf(x,y,plotdata,V,extend='both')
    plt.title(titlename)
  
    cbar=map.colorbar(cs,location='bottom',pad="5%")
    cbar.ax.tick_params(labelsize=6)
    cbar.ax.set_title(cbartitle)

def getKey(item):
    return item[0]

def plottimeseries(field,timeperiod,fieldname,axisname):



#   plot my data and overplot Nizars data
    plt.subplot(3,1,1)
    print(np.shape(timeperiod),timeperiod)
    print(np.shape(field),field)
    plt.plot(timeperiod,field)
    plt.xlabel('ka')
    plt.ylabel(axisname)
    plt.title(fieldname+' over Jordan')


    plt.tight_layout(pad=0.4,w_pad=0.5,h_pad=1.0)




#=====================================================
def extract_single_site_data(expt,filenames,expttime,fieldreq,lonname,latname):

    if fieldreq=='convP':
        extract338='convP'
        fieldreq='QCL'

    if fieldreq=='lsP':
        extract338='lsP'
        fieldreq='QCL'
    

    # 1.  Set up details.  Gridbox required and filename

    #Jordan
    lonreq=37.5
    latreq=30.0

    dirname='/home/users/jctindall/umoutput/BAS_timeslices/'+expt+'/'
    os.chdir(dirname)


    #2. Set up filename and extract the field we want


    print(filenames)
    f=MFDataset(filenames)
    f.dimensions
    f.variables

    lon = f.variables[lonname][:]
    lat = f.variables[latname][:]
    times = f.variables['t'][:]
    xsize=len(lon)
    ysize=len(lat)
    tsize=len(times)
    fieldval=f.variables[fieldreq][:]
    fieldval=np.squeeze(fieldval)




    if fieldreq=='QCL':
        if extract338=='convP':
            fieldval2=fieldval
            fieldval=fieldval2[:,9,:,:]+fieldval2[:,6,:,:]
            fieldval=fieldval * 60. * 60. * 24. * 30.
            fieldval2=0
            print(np.shape(fieldval))
        if extract338=='lsP':
            fieldval2=fieldval
            fieldval=fieldval2[:,0,:,:]+fieldval2[:,3,:,:]
            fieldval=fieldval * 60. * 60. * 24. * 30.
            fieldval2=0
            print(np.shape(fieldval))

    nt,ny,nx=np.shape(fieldval)

#############temporary code for testing


# extract Jordan value

#    ix1=(lon == lonreq)
#    ix2=(lat ==latreq)
    
#    temp=fieldval[:,ix2]
#    single_pt_avg=temp[:,:,ix1]
    
    #print('single point average is',single_pt_avg,expt)
#    if fieldreq=='precip':
#        single_pt_avg=single_pt_avg * 60. * 60. * 24. * 30.
#    datasize=len(single_pt_avg)
#    print(datasize)
#    for i in range(0,datasize):
#        print(fieldreq,i,single_pt_avg[i])
#    #print('mm month',single_pt_avg)
#    print('mm year',np.sum(single_pt_avg))

#############end temporary code for testing



    # check that nt is a multiple of 120 (so that we get a full year)

    print('j1')
    checkval=nt%12  # remainder after dividing by 120
    if checkval != 0:
        print('error there are some files missingin ',expt)
        print('you will not be able to get an accurate annual average')
        sys.exit()

    
    f.close()

    # we need to get annual average by weighting by precipitation amount

    avgfield=np.mean(fieldval,axis=0)
    print('j2')
    


    # extract Jordan value

    ix1=(lon == lonreq)
    ix2=(lat ==latreq)

    temp=avgfield[ix2]
    single_pt_avg=temp[:,ix1]
    
    print('single point average is',single_pt_avg,expt)
    

    #cbartitle='mm'
    #titlename='avg'
    #plotdata(tot_h2o*60.*60.*24.*30/tsize,0,lon,lat,titlename,cbartitle,0,100,5)

    

    retdata=[single_pt_avg]

    return retdata


#=================================================================
# MAIN PROGRAM STARTS xluba



# HERE 0ka
retdata=extract_single_site_data('xluba','xlubaa@pcr[6-9]*.nc','0ka','convP','longitude_1','latitude_1')
alldata=[['xluba',0.0,retdata]]
print(retdata)

# xlubb 1ka
retdata=extract_single_site_data('xlubb','xlubba@pcr[3-6]*.nc','1ka','convP','longitude_1','latitude_1')
alldata.append(['xlubb',-1.0,retdata])
print(retdata)

# xlubc 2ka
retdata=extract_single_site_data('xlubc','xlubca@pcr[3-6]*.nc','2ka','convP','longitude_1','latitude_1')
alldata.append(['xlubc',-2.0,retdata])

# xlubd 3ka
retdata=extract_single_site_data('xlubd','xlubda@pcr[3-6]*.nc','3ka','convP','longitude_1','latitude_1')
alldata.append(['xlubd',-3.0,retdata])


# xlube 4ka
retdata=extract_single_site_data('xlube','xlubea@pcr[3-6]*.nc','4ka','convP','longitude_1','latitude_1')
alldata.append(['xlube',-4.0,retdata])

# xlubf 5ka
retdata=extract_single_site_data('xlubf','xlubfa@pcr[3-6]*.nc','5ka','convP','longitude_1','latitude_1')
alldata.append(['xlubf',-5.0,retdata])

# xlubg 6ka
retdata=extract_single_site_data('xlubg','xlubga@pcr[3-6]*.nc','6ka','convP','longitude_1','latitude_1')
alldata.append(['xlubg',-6.0,retdata])

# xlubh 7ka
retdata=extract_single_site_data('xlubh','xlubha@pcr[3-6]*.nc','7ka','convP','longitude_1','latitude_1')
alldata.append(['xlubh',-7.0,retdata])

# xlubi 8ka
retdata=extract_single_site_data('xlubi','xlubia@pcr[6-9]*.nc','8ka','convP','longitude_1','latitude_1')
alldata.append(['xlubi',-8.0,retdata])

# xlubj 9ka
retdata=extract_single_site_data('xlubj','xlubja@pcr[3-6]*.nc','9ka','convP','longitude_1','latitude_1')
alldata.append(['xlubj',-9.0,retdata])

# xlubk 10ka
retdata=extract_single_site_data('xlubk','xlubka@pcr[3-6]*.nc','10ka','convP','longitude_1','latitude_1')
alldata.append(['xlubk',-10.0,retdata])

# xlubl 11ka
retdata=extract_single_site_data('xlubl','xlubla@pcr[3-6]*.nc','11ka','convP','longitude_1','latitude_1')
alldata.append(['xlubl',-11.0,retdata])

# xlubm 12ka
retdata=extract_single_site_data('xlubm','xlubma@pcr[3-6]*.nc','12ka','convP','longitude_1','latitude_1')
alldata.append(['xlubm',-12.0,retdata])

# xlubn 13ka
retdata=extract_single_site_data('xlubn','xlubna@pcr[3-6]*.nc','13ka','convP','longitude_1','latitude_1')
alldata.append(['xlubn',-13.0,retdata])

# xlubo 14ka
retdata=extract_single_site_data('xlubo','xluboa@pcr[3-6]*.nc','14ka','convP','longitude_1','latitude_1')
alldata.append(['xlubo',-14.0,retdata])

# xlubp 15ka
retdata=extract_single_site_data('xlubp','xlubpa@pcr[0-2]*.nc','15ka','convP','longitude_1','latitude_1')
alldata.append(['xlubp',-15.0,retdata])

# xlubq 16ka
retdata=extract_single_site_data('xlubq','xlubqa@pcr[0-2]*.nc','16ka','convP','longitude_1','latitude_1')
alldata.append(['xlubq',-16.0,retdata])

# xlubr 17ka
retdata=extract_single_site_data('xlubr','xlubra@pcr[6-9]*.nc','17ka','convP','longitude_1','latitude_1')
alldata.append(['xlubr',-17.0,retdata])

# xlubs 18ka
retdata=extract_single_site_data('xlubs','xlubsa@pcr[2-5]*.nc','18ka','convP','longitude_1','latitude_1')
alldata.append(['xlubs',-18.0,retdata])

# xlubt 19ka
retdata=extract_single_site_data('xlubt','xlubta@pcr[2-5]*.nc','19ka','convP','longitude_1','latitude_1')
alldata.append(['xlubt',-19.0,retdata])

# xlubu 20ka
retdata=extract_single_site_data('xlubu','xlubua@pcr[2-5]*.nc','20ka','convP','longitude_1','latitude_1')
alldata.append(['xlubu',-20.0,retdata])

# xlubv 21ka
retdata=extract_single_site_data('xlubv','xlubva@pcr[2-5]*.nc','21ka','convP','longitude_1','latitude_1')
alldata.append(['xlubv',-21.0,retdata])



# extract d18o

datalen=len(alldata)
convP=[alldata[i][2] for i in range(0,datalen)]
timeperiod=[alldata[i][1] for i in range(0,datalen)]


#TdegC=np.asarray(lsP)-273.15
convP=np.squeeze(convP)
print('convP',convP)
print('timeperiod',timeperiod)
plottimeseries(convP,timeperiod,'total convP','mm/month')
fileout='/home/users/jctindall/plots/python/Middleeast/Nizar_plot_diags/Jordan_convP_timeseries.eps' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/home/users/jctindall/plots/python/Middleeast/Nizar_plot_diags/Jordan_convP_timeseries.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()
::::::::::::::
JASMIN_PROGS/NorthAtl/d18o_longitudinal_gradient.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    D18o longitudinal gradient
#PURPOSE 
#    
#    Jonathan Holmes wants to see if the longitudinal gradient of d18o in the
#    model agrees with the data.
#    This will be done for the 9-11ka - 7-8ka period
#
#    He actually wants the longitudinal gradient just at 
#    the sites that he has
#
#
#
# Julia 30.09.20181


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid
from scipy import stats


# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,cbartitle,minval,maxval,diffval):
    lons, lats = np.meshgrid(lon,lat)
    if fileno <= 90:
        plt.subplot(2,2,fileno+1)
   # map=Basemap(projection='robin',resolution='l')

   # this may be a good region for the North Atlantic region
    map=Basemap(llcrnrlon=-20.0,urcrnrlon=40.0,llcrnrlat=30.0,urcrnrlat=70.0,projection='cyl',resolution='c')   

    # this may be a good region for most of the globe
    #map=Basemap(llcrnrlon=0,llcrnrlat=-80,urcrnrlon=360,urcrnrlat=80,projection='mill')

    
   # map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #V=np.arange(np.amin(plotdata),np.amax(plotdata),np.amin(plotdata)/10)
    V=np.arange(minval,maxval,diffval)
   # cs = map.contourf(x,y,plotdata,V)
    cs=map.contourf(x,y,plotdata,V,extend='both',cmap='RdBu_r')
    #cs=map.contourf(x,y,plotdata,vmin=minval,vmax=maxval)
    plt.title(titlename)
  
    cbar=map.colorbar(cs,location='bottom',pad="5%")
    cbar.ax.tick_params(labelsize=10)
    cbar.ax.set_title(cbartitle)


def oplotdata(anom_lakes_d18o,datalons,datalats,minval,maxval,diffval,lake_spel):
# plot filled circles of the data over the map
    print(anom_lakes_d18o)
    V=np.arange(minval,maxval,diffval)
    print(minval,maxval)
    if lake_spel=='l': # lake
        plt.scatter(datalons,datalats,color='black',marker='o',s=110)
        plt.scatter(datalons,datalats,vmin=minval,vmax=maxval,c=anom_lakes_d18o,marker='o',s=70,cmap='RdBu_r')
    if lake_spel=='s': # speleothem
        plt.scatter(datalons,datalats,color='black',marker='^',s=110)
        plt.scatter(datalons,datalats,vmin=minval,vmax=maxval,c=anom_lakes_d18o,marker='^',s=70,cmap='RdBu_r')
    #plt.colorbar()
    


def getKey(item):
    return item[0]



#=====================================================
def extract_data(expt,filenames,expttime):


    # 1.  Set up details.  Gridbox required and filename


    dirname='/home/users/jctindall/mholloway/'+expt+'/pcpd/'
    os.chdir(dirname)


    #2. Set up filename and extract stash code 338
    #   print d18o and dD for that file


    f=MFDataset(filenames)
    f.dimensions
    f.variables

    lon = f.variables['longitude_1'][:]
    lat = f.variables['latitude_1'][:]
    times = f.variables['t'][:]
    xsize=len(lon)
    ysize=len(lat)
    tsize=len(times)
    dD=f.variables['dD'][:]
    dD=np.squeeze(dD)
    d18o=f.variables['dO18'][:]
    d18o=np.squeeze(d18o)
    h2o=f.variables['h2o'][:]
    h2o=np.squeeze(h2o)
    
    
    f.close()

    # we need to get annual average by weighting by precipitation amount

    dD_weight=dD*h2o
    dD_weightsum=np.sum(dD_weight,axis=0)
    d18o_weight=d18o*h2o
    d18o_weightsum=np.sum(d18o_weight,axis=0)
    tot_h2o=np.sum(h2o,axis=0)
    dD_weightavg=dD_weightsum/tot_h2o
    d18o_weightavg=d18o_weightsum/tot_h2o
    #print("shape tpt h2o",tot_h2o.shape)
    #print("shape mean dDweight",dD_weightsum.shape)

   # titlename=expttime
   # cbartitle='mm'
    lontemp=lon
    tot_h2o,lon=shiftgrid(180.,tot_h2o,lon,start=False,cyclic=360)
   # plotdata(tot_h2o*60.*60.*24.*30/tsize,0,lon,lat,titlename,cbartitle,0,150,10)
   # cbartitle='permil'
    lon=lontemp
    d18o_weightavg,lon=shiftgrid(180.,d18o_weightavg,lon,start=False,cyclic=360)
#    plotdata(d18o_weightavg,2,lon,lat,titlename,cbartitle,-40,10,5)
 #   plt.show()

    retdata=[d18o_weightavg,tot_h2o/tsize,lon,lat]

    return retdata





#=====================================================
def extract_data_seas(expt,filestart,expttime,monthnames):

    # 1.  Set up details.  Gridbox required and filename

    nmonths=len(monthnames)
    seasname=''  # get seasonname by using first letter of each month
    for mon in monthnames:
        seasname=seasname+mon[0]

    dirname='/home/users/jctindall/umoutput/BAS_timeslices/'+expt
    os.chdir(dirname)


    #2. Set up filename and extract stash code 338
    #   print d18o and dD for that file


    for monthno in range (0,nmonths):
        filenames=filestart+monthnames[monthno]+'.nc'
        print(dirname+filenames)
        f=MFDataset(filenames)
        f.dimensions
        f.variables

        lon = f.variables['longitude_1'][:]
        lat = f.variables['latitude_1'][:]
        times = f.variables['t'][:]
        xsize=len(lon)
        ysize=len(lat)
        tsize=len(times)
        print(filenames,tsize)

        alldata=f.variables['QCL'][:]
        h2o=alldata[:,0,:,:]+alldata[:,3,:,:]+alldata[:,6,:,:]+alldata[:,9,:,:]
        tot18o=alldata[:,1,:,:]+alldata[:,4,:,:]+alldata[:,7,:,:]+alldata[:,10,:,:]
    
        if monthno ==0: 
            # we need to get annual average by weighting by precipitation amount
            all_tot18o=tot18o
            all_h2o=h2o
            tsizesave=tsize
        else:
            if tsizesave != tsize:
                print('you have not got the same number of files for each month')
                print(tsizesave,tsize)

                sys.exit()

            all_tot18o=all_tot18o+tot18o
            all_h2o=all_h2o+h2o
    
        f.close()


    tot18o_avg=np.sum(all_tot18o,axis=0)  
    toth2o_avg=np.sum(all_h2o,axis=0)

    lontemp=lon
    toth2o_avg,lon=shiftgrid(180.,toth2o_avg,lon,start=False,cyclic=360)
    lon=lontemp
    tot18o_avg,lon=shiftgrid(180.,tot18o_avg,lon,start=False,cyclic=360)
    d18o_avg=((tot18o_avg/toth2o_avg)-2005.2E-6)/2005.2E-9


    retdata=[d18o_avg,toth2o_avg/tsize,lon,lat]

    return retdata

#====================================================================
def get_Jonathan_data(filename):
    
    f1=open(filename,'r')
    #discard titleline
    textline=f1.readline()

    datalons=[]
    datalats=[]
    data_5_7ka=[]
    data_9_11ka=[]

    for line in f1:
        linesplit=line.split(',') # the data in the file is split by comma
        datalats.append(np.float(linesplit[2]))
        datalons.append(np.float(linesplit[3]))
        data_5_7ka.append(np.float(linesplit[5]))
        data_9_11ka.append(np.float(linesplit[7]))

   
    retdata=[datalons,datalats,data_5_7ka,data_9_11ka]
    return retdata

#======================================================
def full_lon_plot(lon,lat,d18o_anom):
# this will plot the longitudinal gradient averaged over
# a range of latitudes just from the model

# make reduced array
    latmin=40.
    latmax=60.
    count=0
    for i in range(0,len(lon)):
        if lon[i] >= -10. and lon[i] <= 30.:
            count=count+1
    lonredu=np.zeros(count)
    count=0
    for j in range(0,len(lat)):
        if lat[j] >= latmin and lat[j] <= latmax:
            count=count+1
    latredu=np.zeros(count)

    d18oanom_redu=np.zeros((len(latredu),len(lonredu)))
    loncount=0
    for i in range(0,len(lon)):
        if lon[i] >= -10. and lon[i] <= 30.:
            lonredu[loncount]=lon[i]
            latcount=0
            for j in range(0,len(lat)):
                if lat[j] >= latmin and lat[j] <= latmax:
                    if loncount==0:
                        latredu[latcount]=lat[j]
                    d18oanom_redu[latcount,loncount]=d18o_anom[j,i]
                    latcount=latcount+1
            loncount=loncount+1

    meand18oanom=np.mean(d18oanom_redu,axis=0)
    maxd18oanom=np.max(d18oanom_redu,axis=0)
    mind18oanom=np.min(d18oanom_redu,axis=0)

    plt.subplot(211)
    plt.plot(lonredu,meand18oanom,label='meand18o')
    plt.xlabel('longitude')
    plt.ylabel('permille')
    plt.title('longitudinal average'+str(latmin)+'N-'+str(latmax)+'N')
    plt.legend()

    plt.subplot(212)
    plt.plot(lonredu,maxd18oanom,label='maximum d18o')
    plt.plot(lonredu,mind18oanom,label='minimum d18o')
    plt.xlabel('longitude')
    plt.ylabel('permille')
    plt.legend()

   
    fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_average_d18o.png' 
    plt.savefig(fileout, bbox_inches='tight')  
    plt.close()


#end def full_lon_plot

#########################################################
def site_lon_plot(lon,lat,d18oanom,lakedatalons,lakedatalats,lakedata_anom,speldatalons,speldatalats,speldata_anom):

    # mark lake outliers (on Jonathans plot these have 
    # longitude > 15 and d18o > 1)

    nlakes=len(lakedatalons)
    outliers=np.zeros(nlakes)
    for i in range(0,nlakes):
        if lakedatalons[i] > 15. and lakedata_anom[i] > 1.0:
            outliers[i]=1

        if lakedata_anom[i] < -1.5:
            outliers[i]=1
            print('found outlier')

    # temporary change so there are no outliers
    print('j2',lakedatalons)
    outliers[:]=0

    # set up model data at lake locations
    d18omod_lakeloc=np.zeros(nlakes)
    d18omod_lake_outliers=np.zeros(nlakes)
    d18omod_lake_nooutliers=np.zeros(nlakes)
    lonmod_l=np.zeros(nlakes)
    latmod_l=np.zeros(nlakes)

    # get nearest longitude and latitude
   
    lonss=np.zeros(nlakes)
    latss=np.zeros(nlakes)
    for lake in range(0,nlakes):
        lonss[lake]=(np.abs(lon-lakedatalons[lake])).argmin()
  
    
    for lake in range(0,nlakes):
        latss[lake]=(np.abs(lat-lakedatalats[lake])).argmin()
  
    lonss=lonss.astype(int) # change from float to int
    latss=latss.astype(int) # change from float to int
   
     
    for lake in range(0,nlakes):
        lonmod_l[lake]=lon[lonss[lake]]
        latmod_l[lake]=lat[latss[lake]]
        d18omod_lakeloc[lake]=d18oanom[latss[lake],lonss[lake]]

        if outliers[lake]==1:
            d18omod_lake_outliers[lake]=d18omod_lakeloc[lake]
            d18omod_lake_nooutliers[lake]=np.nan
        else:
            d18omod_lake_outliers[lake]=np.nan
            d18omod_lake_nooutliers[lake]=d18omod_lakeloc[lake]

    
       

    # set up model data at spel locations
    nspels=len(speldatalons)
    d18omod_spelloc=np.zeros(nspels)
    lonmod_s=np.zeros(nspels)
    latmod_s=np.zeros(nspels)

    # get nearest longitude and latitude
   
    lonss=np.zeros(nspels)
    latss=np.zeros(nspels)
    for spel in range(0,nspels):
        lonss[spel]=(np.abs(lon-speldatalons[spel])).argmin()
  
    
    for spel in range(0,nspels):
        latss[spel]=(np.abs(lat-speldatalats[spel])).argmin()
  
    lonss=lonss.astype(int) # change from float to int
    latss=latss.astype(int) # change from float to int
   
     
    for spel in range(0,nspels):
        lonmod_s[spel]=lon[lonss[spel]]
        latmod_s[spel]=lat[latss[spel]]
        d18omod_spelloc[spel]=d18oanom[latss[spel],lonss[spel]]


    # linear regression for speleothem
    slope_s,intercept_s,r_value,p_value,std_err=stats.linregress(lonmod_s,d18omod_spelloc)
    # linear regression for lake
    finiteYmask=np.isfinite(d18omod_lake_nooutliers)
    print('j1',lonmod_l)
    print('j1',d18omod_lake_nooutliers)
    slope_l,intercept_l,r_value,p_value,std_err=stats.linregress(lonmod_l[finiteYmask],d18omod_lake_nooutliers[finiteYmask])
      


    #plt.plot(lonmod_l,d18omod_lakeloc,'b^',label='lake')
    plt.plot(lonmod_l,d18omod_lake_outliers,'g^',markersize=10,label='lake outliers')
    plt.plot(lonmod_l,d18omod_lake_nooutliers,'bo',label='lake')
    plt.plot(lonmod_l,intercept_l+slope_l * lonmod_l,'b')
   
    plt.plot(lonmod_s,d18omod_spelloc,'ro',label='speleothem')
    plt.plot(lonmod_s,intercept_s+slope_s * lonmod_s,'r')
    plt.legend()
    plt.xlabel('longitude')
    plt.ylabel('permille')
    plt.title('linear regression at sites of data')
    fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_linregress_at_sites.png' 
    plt.savefig(fileout, bbox_inches='tight')  
    fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_linregress_at_sites.eps' 
    plt.savefig(fileout, bbox_inches='tight')  



    # print out lat, lon and d18odata and d18omodel to a text file
    filewrite='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_data_model.txt'
    f2=open(filewrite,'w+')
    f2.write('speleothem\n')
    f2.write('lon   lat   data(d18oanom)  model(d18oanom)\n')
    for spel in range(0,nspels):
       f2.write(str(speldatalons[spel])+' '+str(speldatalats[spel])+' '+str(round(speldata_anom[spel],2))+' '+str(round(d18omod_spelloc[spel],2))+'\n')
 
    f2.write('lakes \n')
    f2.write('lon   lat   data(d18oanom)  model(d18oanom)\n')
    for lake in range(0,nlakes):
        f2.write(str(lakedatalons[lake])+' '+str(lakedatalats[lake])+' '+str(round(lakedata_anom[lake],2))+' '+str(round(d18omod_lakeloc[lake],2))+'\n')
 
    f2.close()
#end def site_lon_plot


#=================================================================
# MAIN PROGRAM STARTS HERE

seasname='jfmamjjasond'
#seasname='son'
if seasname =='djf':
    monthnames=['dc','ja','fb']
if seasname =='jja':
    monthnames=['jn','jl','ag']
if seasname =='mam':
    monthnames=['mr','ar','my']
if seasname =='son':
    monthnames=['sp','ot','nv']
if seasname =='jfmamjjasond':
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']

#retdata=extract_data('xlubh','iso_xlubha@pcr[3-6]*','7ka')


# xlubh 7ka
retdata=extract_data_seas('xlubh','xlubha@pcr[3-6]*','7ka',monthnames)
d18o_7ka=retdata[0]+0.04 # constant is ice volume correction
h2o_7ka=retdata[1]
lon=retdata[2]
lat=retdata[3]

# xlubg 6ka
retdata=extract_data_seas('xlubg','xlubga@pcr[3-6]*','6ka',monthnames)
d18o_6ka=retdata[0]+0.04
h2o_6ka=retdata[1]
lon=retdata[2]
lat=retdata[3]

# xlubf 5ka
retdata=extract_data_seas('xlubf','xlubfa@pcr[3-6]*','7ka',monthnames)
d18o_5ka=retdata[0]+0.04
h2o_5ka=retdata[1]
lon=retdata[2]
lat=retdata[3]




# xlubj 9ka
retdata=extract_data_seas('xlubj','xlubja@pcr[3-6]*','9ka',monthnames)
print('julia',retdata[0])
d18o_9ka=retdata[0]+0.25 # constant is ice volume correction
h2o_9ka=retdata[1]


# xlubk 10ka
retdata=extract_data_seas('xlubk','xlubka@pcr[3-6]*','10ka',monthnames)
d18o_10ka=retdata[0]+0.33
h2o_10ka=retdata[1]


# xlubl 11ka
retdata=extract_data_seas('xlubl','xlubla@pcr[3-6]*','11ka',monthnames)
d18o_11ka=retdata[0]+0.42
h2o_11ka=retdata[1]


# get average 7ka d18o - weighted by amount

d18o_5_7ka=((d18o_7ka * h2o_7ka) +  (d18o_6ka * h2o_6ka)+  (d18o_5ka * h2o_5ka))/(h2o_7ka+h2o_6ka+h2o_5ka)
d18o_9_11ka=((d18o_9ka * h2o_9ka) +  (d18o_10ka * h2o_10ka) + (d18o_11ka * h2o_11ka))/(h2o_9ka+h2o_10ka+h2o_11ka)

d18o_anom=d18o_9_11ka - d18o_5_7ka

print(np.shape(d18o_anom))


# do a full longitudinal plot at all model gridboxes
# averaged over a range  

full_lon_plot(lon,lat,d18o_anom)



# get Jonathans data
#filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/lakes_data.csv'
filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/Jonathan_new_data.csv'
lakedata=get_Jonathan_data(filename)
lakedatalons=lakedata[0]
lakedatalats=lakedata[1]
lakedata_5_7ka=lakedata[2]
lakedata_9_11ka=lakedata[3]

filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/speleothem_data.csv'
speleodata=get_Jonathan_data(filename)
speleodatalons=speleodata[0]
speleodatalats=speleodata[1]
speleodata_5_7ka=speleodata[2]
speleodata_9_11ka=speleodata[3]


lakedata_anom=np.asarray(lakedata_9_11ka) - np.asarray(lakedata_5_7ka)

speleodata_anom=np.asarray(speleodata_9_11ka) - np.asarray(speleodata_5_7ka)


# do a plot at the sites
site_lon_plot(lon,lat,d18o_anom,lakedatalons,lakedatalats,lakedata_anom,speleodatalons,speleodatalats,speleodata_anom)



h2o_5_7ka=(h2o_7ka+h2o_6ka+h2o_5ka)/3.0
h2o_9_11ka=(h2o_9ka+h2o_10ka+h2o_11ka)/3.0


plotdata(d18o_9_11ka,0,lon,lat,'9-11ka d18o','permille',-12,-6,0.5)
plotdata(h2o_9_11ka*60.*60.*24.*30.,1,lon,lat,'9-11ka h2o','mm/month',0,100,10)

anom_h2o=h2o_9_11ka - h2o_5_7ka
titlename='precip 9-11ka - 5-7ka'
cbartitle='mm/month'
plotdata(anom_h2o*60.*60.*24.*30,3,lon,lat,titlename,cbartitle,-10,10,1)

print(np.shape(d18o_5_7ka), np.shape(d18o_9_11ka))

anom_d18o=d18o_9_11ka - d18o_5_7ka

print(np.shape(lakedata_5_7ka), np.shape(lakedata_9_11ka))
anom_lakes_d18o=np.asarray(lakedata_9_11ka) - np.asarray(lakedata_5_7ka)
anom_speleo_d18o=np.asarray(speleodata_9_11ka) - np.asarray(speleodata_5_7ka)

titlename='d18o 9-11ka - 5-7ka '+seasname
cbartitle='permille'
#plotdata(anom_d18o,2,lon,lat,titlename,cbartitle,-6,2,0.5)
plotdata(anom_d18o,2,lon,lat,titlename,cbartitle,-2,1,0.2)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()

plotdata(anom_d18o,99,lon,lat,titlename,cbartitle,-2.0,2.0,0.25)
oplotdata(anom_lakes_d18o,lakedatalons,lakedatalats,-2.0,2.0,0.25,'l')
oplotdata(anom_speleo_d18o,speleodatalons,speleodatalats,-2.0,2.0,0.25,'s')

print('lake',lakedatalons)
print('speleo',speleodatalons)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_d18o_data'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_d18o_data'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()



sys.exit()
::::::::::::::
JASMIN_PROGS/NorthAtl/extract_monthly_averages_old.py
::::::::::::::
#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-

#Created on Monday June 3rd 2019

#@author: earjcti
#
# Jonathan has asked if I can extract some fields from Max's experiments so 
# Matt Jones can run these through his programs
#
# they would like monthly data averaged over 9-11ka and 5-7ka
#
# update:  30.10.2019 Would now also like 1000 year timesices
#
#
########################################################
# other notes are

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
from iris.util import unify_time_units
import cf_units as unit
import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
    
#####################################
def extract_fields(filestart, varnamein, exptlist, offset):


    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    filetype='a@pd'

   
    if varnamein=='Stash code = 338' or  varnamein=='Stash code = 321':    
        filetype='a@pc'
    
    # loop over months
    print(varnamein)
    
    moncube=iris.cube.CubeList([])
    for mon in range(0,len(monthnames)):
    #for mon in range(0,2):
        print(monthnames[mon])
        allcubes=iris.cube.CubeList([])
        if varnamein=='Stash code = 338':
            all16ocubes=iris.cube.CubeList([])
            all18ocubes=iris.cube.CubeList([])

        #loop over years
        for expt in range(0,len(exptlist)):
            for year in range(STARTYEAR, ENDYEAR + 1):
                
                print(filestart,exptlist[expt])
                print(filetype, EXTRA, year, monthnames[mon])
                filename=(filestart + exptlist[expt] + 
                          '/' + exptlist[expt] + filetype + EXTRA + 
                          np.str(year) + monthnames[mon] + '.nc')
                

               
                cube=iris.load_cube(filename,varnamein)          
                u = unit.Unit('months since 0800-12-01 00:00:00',
                                  calendar=unit.CALENDAR_360_DAY)
                cube.coord('t').units=u
                cube.coord('t').points=((year - STARTYEAR + 1)+
                            expt*(ENDYEAR - STARTYEAR + 1))
           
                if varnamein=='Stash code = 338':
               
                    cube.coord('level-1').rename('zdim')
                    cube16o= (cube.extract(iris.Constraint(zdim=1.))+
                          cube.extract(iris.Constraint(zdim=4.))+
                          cube.extract(iris.Constraint(zdim=7.))+
                          cube.extract(iris.Constraint(zdim=10.)))
                    cube18o= (cube.extract(iris.Constraint(zdim=2.))+
                          cube.extract(iris.Constraint(zdim=5.))+
                          cube.extract(iris.Constraint(zdim=8.))+
                          cube.extract(iris.Constraint(zdim=11.)))

                    all16ocubes.append(cube16o)
                    all18ocubes.append(cube18o)


             
            
                if (varnamein=='TOTAL PRECIPITATION RATE     KG/M2/S' or
                   varnamein=='EVAPORATION FROM SEA (GBM)   KG/M2/S' or    
                   varnamein=='EVAP FROM SOIL SURF -AMOUNT KG/M2/TS' or
                   varnamein=='TRANSPIRATION RATE           KG/M2/S'):
                    cube=(cube.extract(iris.Constraint(surface=0.000000)))
                
                if (varnamein=='TEMPERATURE AT 1.5M' or 
                    varnamein=='RELATIVE HUMIDITY AT 1.5M'):
                    cube=(cube.extract(iris.Constraint(ht=-1.000000)))

                if (varnamein=='EVAP FROM CANOPY - AMOUNT   KG/M2/TS'):
                     cube=(cube.extract(iris.Constraint(level275=-1.000000)))

                if varnamein!='Stash code = 338':
                    allcubes.append(cube)


        ##################################
        # process monthly average for non d18op field

        if varnamein!='Stash code = 338':
           #make sure the metadata on all cubes are the same
           #print(allcubes)
            equalise_attributes(allcubes)
            unify_time_units(allcubes)
            for i in range(1,len(allcubes)):
                allcubes[i].coord('t').attributes=(
                    allcubes[0].coord('t').attributes)

            catcube=allcubes.concatenate_cube()
            print('concat cube is',np.shape(catcube))
       
            nc,ny,nx=np.shape(catcube)
            if nc != (ENDYEAR - STARTYEAR + 1)*(len(exptlist)):
                print('the cube has not been concatenated correctly')
                print('we should have', ENDYEAR - STARTYEAR + 1,'times')
                print('we have',nc,'times')
                sys.exit(0)
         
            # find mean across cube dimension
            tempcube = catcube.collapsed('t', iris.analysis.MEAN)
            meancube=iris.util.new_axis(tempcube, 't')
            meancube.coord('t').points=mon+1
            meancube.coord('t').bounds=None

        else: # do for d18op
           # sort out 16o cube
            equalise_attributes(all16ocubes)
            unify_time_units(all16ocubes)
            for i in range(1,len(all16ocubes)):
                all16ocubes[i].coord('t').attributes=(
                    all16ocubes[0].coord('t').attributes)

            catcube16o=all16ocubes.concatenate_cube()
       
            nc,ny,nx=np.shape(catcube16o)
            if nc != (ENDYEAR - STARTYEAR + 1)*(len(exptlist)):
                print('the 16o cube has not been concatenated correctly')
                print('we should have', ENDYEAR - STARTYEAR + 1,'times')
                print('we have',nc,'times')
                sys.exit(0)
            # sort out 18o cube
            equalise_attributes(all18ocubes)
            unify_time_units(all18ocubes)
            for i in range(1,len(all18ocubes)):
                all18ocubes[i].coord('t').attributes=(
                    all18ocubes[0].coord('t').attributes)

            catcube18o=all18ocubes.concatenate_cube()
       
            nc,ny,nx=np.shape(catcube18o)
            if nc != (ENDYEAR - STARTYEAR + 1)*(len(exptlist)):
                print('the 18o cube has not been concatenated correctly')
                print('we should have', ENDYEAR - STARTYEAR + 1,'times')
                print('we have',nc,'times')
                sys.exit(0)
         
            # find mean across cube dimension
            tempcube16o = catcube16o.collapsed('t', iris.analysis.MEAN)
            tempcube18o = catcube18o.collapsed('t', iris.analysis.MEAN)

            tempcube=((tempcube18o / tempcube16o)-2005.2E-6)/2005.2E-9
            # add average offset
            tempcube.data=tempcube.data+offset
            # replace nan with 0
            tempcube.data=np.where(
                tempcube16o.data==0.,0.0,tempcube.data) 
            tempcube.rename('d18o')
            tempcube.units='unknown'
                   
            meancube=iris.util.new_axis(tempcube, 't')
            meancube.coord('t').points=mon+1
            meancube.coord('t').bounds=None
        # end of stash code 338 check loop

       
       
        # append to cube containing all months
        moncube.append(meancube)
        

    print('end of months')       
    # unifty attributes on cubes
    equalise_attributes(moncube)
    unify_time_units(moncube)
    for i in range(1,len(moncube)):
            moncube[i].coord('t').attributes=moncube[0].coord('t').attributes
   
    
    outcube=moncube.concatenate_cube()
    


    if varnamein=='TOTAL PRECIPITATION RATE     KG/M2/S': 
        outcube.data=outcube.data * 60.*60.*24.
        outcube.long_name='TOTAL PRECIPITATION RATE    MM/DAY'
        outcube.units='mm/day'
            
    if varnamein=='EVAPORATION FROM SEA (GBM)   KG/M2/S':  
        outcube.data=outcube.data * 60.*60.*24.
        outcube.long_name='EVAPORATION FROM SEA    MM/DAY'
        outcube.units='mm/day'  
    if varnamein=='EVAP FROM SOIL SURF -AMOUNT KG/M2/TS': 
        outcube.data=outcube.data * 60.*60.*24. / 1800.
        outcube.long_name='EVAP FROM SOIL SURF   MM/DAY'
        outcube.units='mm/day'
    if varnamein=='TRANSPIRATION RATE           KG/M2/S': 
        outcube.data=outcube.data * 60.*60.*24.
        outcube.long_name='TRANSPIRATION RATE    MM/DAY'
        outcube.units='mm/day'
    if varnamein=='EVAP FROM CANOPY - AMOUNT   KG/M2/TS':
        outcube.data=outcube.data * 60.*60.*24. / 1800.
        outcube.long_name='EVAP FROM CANPOPY    MM/DAY'
        outcube.units='mm/day'
    
    return(outcube)
  
    
def extract_all_avg(filestart, exptnames, offset):
    """
    this will provide the loop that will extract the fieldnames one by one
    input:  filestart,  a list of experiments
    returns:  a cube containing monthly averaged data for each of the fields
    """
        
    cubelist = iris.cube.CubeList([])

    for field in range(0, len(FIELDNAMES)):
        cube = extract_fields(filestart, FIELDNAMES[field], exptnames, offset)
        cubelist.append(cube)
            
    
    return cubelist

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning

LINUXWIN = 'j' # l-linux, w=windows, j jasmin
            	
FILESTART = {"l":'/nfs/hera1/earjcti/um/netcdf/xkrax_netcdf/xkraxa@pd',
             "w":'C:/Users/julia/OneDrive/WORK/DATA/TEMPORARY/',
             "j":'/home/users/jctindall/umoutput/BAS_timeslices/'}


STARTYEAR=20
if LINUXWIN == 'w':
    ENDYEAR = 31
else:
    ENDYEAR=50


EXTRA='v'
EXPT_11_9=['xlubl','xlubk','xlubj']
EXPT_7_5=['xlubf','xlubg','xlubh']

EXPTNAME = {"0ka" : "xluba",
            "1ka" : "xlubb",
            "2ka" : "xlubc",
            "3ka" : "xlubd",
            "4ka" : "xlube",
            "5ka" : "xlubf",
            "6ka" : "xlubg",
            "7ka" : "xlubh",
            "8ka" : "xlubi",
            "9ka" : "xlubj",
            "10ka" : "xlubk",
            "11ka" : "xlubl",
            "12ka" : "xlubm",
            "13ka" : "xlubn",
            "14ka" : "xlubo",
            "15ka" : "xlubp",
            "16ka" : "xlubq",
            "17ka" : "xlubr",
            "18ka" : "xlubs",
            "19ka" : "xlubt",
            "20ka" : "xlubu",
            "21ka" : "xlubv"
            }


# based on Lambeck et al 2014 table S3.  assuming 140m of sea level drop equates to
# 1permil of ocean d18o (because their LGM sea level was 140m).
D18O_OFFSET = {"0ka" : 0.0,
               "1ka" : 0.0,
               "2ka" : 0.0,
               "3ka" : 0.01,
               "4ka" : 0.01,
               "5ka" : 0.02,
               "6ka" : 0.02,
               "7ka" : 0.04,
               "8ka" : 0.09,
               "9ka" : 0.19,
               "10ka" : 0.29,
               "11ka" : 0.39,
               "12ka" : 0.46,
               "13ka" : 0.5,
               "14ka" : 0.55,
               "15ka" : 0.7,
               "16ka" : 0.77,
               "17ka" : 0.82,
               "18ka" : 0.86,
               "19ka" : 0.9,
               "20ka" : 0.96,
               "21ka" : 1.0
               }

FILEOUTSTART='modeloutput/'

FIELDNAMES=['TOTAL PRECIPITATION RATE     KG/M2/S',
            'TEMPERATURE AT 1.5M',
            'Stash code = 338',
            'EVAPORATION FROM SEA (GBM)   KG/M2/S',
            'RELATIVE HUMIDITY AT 1.5M',
            'EVAP FROM SOIL SURF -AMOUNT KG/M2/TS',
            'EVAP FROM CANOPY - AMOUNT   KG/M2/TS',
            'TRANSPIRATION RATE           KG/M2/S']
#FIELDNAMES=['Stash code = 338']

########################################

# do expt 11-9ka

#allcubes = extract_all_avg(FILESTART.get(LINUXWIN), EXPT_11_9, 0.29)


#fileout = FILEOUTSTART + 'data_11-9ka.nc'        
#iris.save(allcubes, fileout, netcdf_format='NETCDF3_CLASSIC', fill_value=2.0E20)

##########################################
# do individual years

PERIODS = ['21ka']
for time in range(0, len(PERIODS)):
    exptlist = [(EXPTNAME.get(PERIODS[time]))]
    offset = (D18O_OFFSET.get(PERIODS[time]))
    timecubes = extract_all_avg(FILESTART.get(LINUXWIN), exptlist, offset)
    print('timecubes is',timecubes)
    fileout=FILEOUTSTART + 'data_' + PERIODS[time] + '.nc'        
    iris.save(timecubes, fileout, netcdf_format='NETCDF3_CLASSIC', fill_value=2.0E20)

    

  
#sys.exit(0)
::::::::::::::
JASMIN_PROGS/NorthAtl/extract_monthly_averages.py
::::::::::::::
#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-

#Created on Monday June 3rd 2019

#@author: earjcti
#
# Jonathan has asked if I can extract some fields from Max's experiments so 
# Matt Jones can run these through his programs
#
# they would like monthly data averaged over 9-11ka and 5-7ka
#
# update:  30.10.2019 Would now also like 1000 year timesices
# update: March 2020  Would also like to extract winds
#
#
########################################################
# other notes are

import os
import numpy as np
#import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
from iris.util import unify_time_units
import cf_units as unit
import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
  

######################################  
def find_year_mean(cubes, monthnumber):
    """
    find the mean over the year
    input a list of cubes, and the month they represent
    output the mean over time
    """
    equalise_attributes(cubes)
    unify_time_units(cubes)
    for i in range(1,len(cubes)):
        cubes[i].coord('t').attributes=(cubes[0].coord('t').attributes)
        catcube=cubes.concatenate_cube()
        
        nc,ny,nx=np.shape(catcube)
        if nc != (ENDYEAR - STARTYEAR + 1)*(len(exptlist)):
           print('the cube has not been concatenated correctly')
           print('we should have', ENDYEAR - STARTYEAR + 1,'times')
           print('we have',nc,'times')
           sys.exit(0)
         
       # find mean across cube dimension
        tempcube = catcube.collapsed('t', iris.analysis.MEAN)
        meancube=iris.util.new_axis(tempcube, 't')
        meancube.coord('t').points=monthnumber+1
        meancube.coord('t').bounds=None

    return meancube

#####################################
def extract_winds(filestart, fieldname, exptnames):
    """
    extract the wind fields ont he correct level
    """
    
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    filetype='a@pc'
    uvtype = fieldname[0:1]
    level = fieldname[1:4]
    level_flt = np.float(level)
   
    moncubes=iris.cube.CubeList([])
    for i, month in enumerate(monthnames):
        print(month)
        allcubes=iris.cube.CubeList([])

        #loop over years
        for j, expt in enumerate(exptlist):
            for year in range(STARTYEAR, ENDYEAR + 1):
                
                filename=(filestart + expt + '/' + expt + filetype + EXTRA + 
                          np.str(year) + month + '.nc')
                cube=iris.load_cube(filename, (uvtype + 
                                    ' COMPNT OF WIND ON PRESSURE LEVELS') )         
                u = unit.Unit('months since 0800-12-01 00:00:00',
                                  calendar=unit.CALENDAR_360_DAY)
                cube.coord('t').units=u
                cube.coord('t').points=((year - STARTYEAR + 1)+
                            j*(ENDYEAR - STARTYEAR + 1))
           
               
                # extract required level
                cube_level = cube.extract(iris.Constraint(p=level_flt))
                allcubes.append(cube_level)
        # find the mean of the year cube and append to the monthcube
        meancube = find_year_mean(allcubes, i)
        moncubes.append(meancube)

    # average month cubes
    unify_time_units(moncubes)
    for i in range(1,len(moncubes)):
            moncubes[i].coord('t').attributes = (
                moncubes[0].coord('t').attributes)
   
    
    outcube=moncubes.concatenate_cube()
    outcube.long_name=uvtype + ' COMPNT OF WIND AT ' + level + 'hPa'
    
        
    return outcube
    
   
    
def extract_fields(filestart, varnamein, exptlist, offset):


    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    filetype='a@pd'

   
    if (varnamein=='Stash code = 338' or  varnamein=='Stash code = 321'):    
        filetype='a@pc'
    
    # loop over months
    print(varnamein)
    
    moncube=iris.cube.CubeList([])
    for mon in range(0,len(monthnames)):
    #for mon in range(0,2):
        print(monthnames[mon])
        allcubes=iris.cube.CubeList([])
        if varnamein=='Stash code = 338':
            all16ocubes=iris.cube.CubeList([])
            all18ocubes=iris.cube.CubeList([])

        #loop over years
        for expt in range(0,len(exptlist)):
            for year in range(STARTYEAR, ENDYEAR + 1):
                
                print(filestart,exptlist[expt])
                print(filetype, EXTRA, year, monthnames[mon])
                filename=(filestart + exptlist[expt] + 
                          '/' + exptlist[expt] + filetype + EXTRA + 
                          np.str(year) + monthnames[mon] + '.nc')
                

                print(filename, varnamein)
                cube=iris.load_cube(filename,varnamein)          
                u = unit.Unit('months since 0800-12-01 00:00:00',
                                  calendar=unit.CALENDAR_360_DAY)
                cube.coord('t').units=u
                cube.coord('t').points=((year - STARTYEAR + 1)+
                            expt*(ENDYEAR - STARTYEAR + 1))
           
                if varnamein=='Stash code = 338':
               
                    cube.coord('level-1').rename('zdim')
                    cube16o= (cube.extract(iris.Constraint(zdim=1.))+
                          cube.extract(iris.Constraint(zdim=4.))+
                          cube.extract(iris.Constraint(zdim=7.))+
                          cube.extract(iris.Constraint(zdim=10.)))
                    cube18o= (cube.extract(iris.Constraint(zdim=2.))+
                          cube.extract(iris.Constraint(zdim=5.))+
                          cube.extract(iris.Constraint(zdim=8.))+
                          cube.extract(iris.Constraint(zdim=11.)))

                    all16ocubes.append(cube16o)
                    all18ocubes.append(cube18o)


             
            
                if (varnamein=='TOTAL PRECIPITATION RATE     KG/M2/S' or
                   varnamein=='EVAPORATION FROM SEA (GBM)   KG/M2/S' or    
                   varnamein=='TRANSPIRATION RATE           KG/M2/S' or
                   varnamein=='SUBLIM. FROM SURFACE (GBM)  KG/M2/TS'):
                    cube=(cube.extract(iris.Constraint(surface=0.000000)))
                
                if (varnamein=='TEMPERATURE AT 1.5M' or 
                    varnamein=='RELATIVE HUMIDITY AT 1.5M'):
                    cube=(cube.extract(iris.Constraint(ht=-1.000000)))

                if (varnamein=='EVAP FROM CANOPY - AMOUNT   KG/M2/TS'):
                     cube=(cube.extract(iris.Constraint(level275=-1.000000)))

                if varnamein!='Stash code = 338':
                    allcubes.append(cube)


        ##################################
        # process monthly average for non d18op field

        if varnamein!='Stash code = 338':
           meancube = find_year_mean(allcubes, mon)

        else: # do for d18op
           # sort out 16o cube
            equalise_attributes(all16ocubes)
            unify_time_units(all16ocubes)
            for i in range(1,len(all16ocubes)):
                all16ocubes[i].coord('t').attributes=(
                    all16ocubes[0].coord('t').attributes)

            catcube16o=all16ocubes.concatenate_cube()
       
            nc,ny,nx=np.shape(catcube16o)
            if nc != (ENDYEAR - STARTYEAR + 1)*(len(exptlist)):
                print('the 16o cube has not been concatenated correctly')
                print('we should have', ENDYEAR - STARTYEAR + 1,'times')
                print('we have',nc,'times')
                sys.exit(0)
            # sort out 18o cube
            equalise_attributes(all18ocubes)
            unify_time_units(all18ocubes)
            for i in range(1,len(all18ocubes)):
                all18ocubes[i].coord('t').attributes=(
                    all18ocubes[0].coord('t').attributes)

            catcube18o=all18ocubes.concatenate_cube()
       
            nc,ny,nx=np.shape(catcube18o)
            if nc != (ENDYEAR - STARTYEAR + 1)*(len(exptlist)):
                print('the 18o cube has not been concatenated correctly')
                print('we should have', ENDYEAR - STARTYEAR + 1,'times')
                print('we have',nc,'times')
                sys.exit(0)
         
            # find mean across cube dimension
            tempcube16o = catcube16o.collapsed('t', iris.analysis.MEAN)
            tempcube18o = catcube18o.collapsed('t', iris.analysis.MEAN)

            tempcube=((tempcube18o / tempcube16o)-2005.2E-6)/2005.2E-9
            # add average offset
            tempcube.data=tempcube.data+offset
            # replace nan with 0
            tempcube.data=np.where(
                tempcube16o.data==0.,0.0,tempcube.data) 
            tempcube.rename('d18o')
            tempcube.units='unknown'
                   
            meancube=iris.util.new_axis(tempcube, 't')
            meancube.coord('t').points=mon+1
            meancube.coord('t').bounds=None
        # end of stash code 338 check loop

       
       
        # append to cube containing all months
        moncube.append(meancube)
        

    print('end of months')       
    # unifty attributes on cubes
    equalise_attributes(moncube)
    unify_time_units(moncube)
    for i in range(1,len(moncube)):
            moncube[i].coord('t').attributes=moncube[0].coord('t').attributes
   
    
    outcube=moncube.concatenate_cube()
    


    if varnamein=='TOTAL PRECIPITATION RATE     KG/M2/S': 
        outcube.data=outcube.data * 60.*60.*24.
        outcube.long_name='TOTAL PRECIPITATION RATE    MM/DAY'
        outcube.units='mm/day'
            
    if varnamein=='EVAPORATION FROM SEA (GBM)   KG/M2/S':  
        outcube.data=outcube.data * 60.*60.*24.
        outcube.long_name='EVAPORATION FROM SEA    MM/DAY'
        outcube.units='mm/day'  
    if varnamein=='EVAP FROM SOIL SURF -AMOUNT KG/M2/TS': 
        outcube.data=outcube.data * 60.*60.*24. / 1800.
        outcube.long_name='EVAP FROM SOIL SURF   MM/DAY'
        outcube.units='mm/day'
    if varnamein=='TRANSPIRATION RATE           KG/M2/S': 
        outcube.data=outcube.data * 60.*60.*24.
        outcube.long_name='TRANSPIRATION RATE    MM/DAY'
        outcube.units='mm/day'
    if varnamein=='EVAP FROM CANOPY - AMOUNT   KG/M2/TS':
        outcube.data=outcube.data * 60.*60.*24. / 1800.
        outcube.long_name='EVAP FROM CANPOPY    MM/DAY'
        outcube.units='mm/day'
    if varnamein=='SUBLIM. FROM SURFACE (GBM) KG/M2/TS':
        outcube.data=outcube.data * 60.*60.*24. / 1800.
        outcube.long_name='SUBLIM FROM SURF    MM/DAY'
        outcube.units='mm/day'
            
    
    return(outcube)
  
    
def extract_all_avg(filestart, exptnames, offset):
    """
    this will provide the loop that will extract the fieldnames one by one
    input:  filestart,  a list of experiments
    returns:  a cube containing monthly averaged data for each of the fields
    """
        
    cubelist = iris.cube.CubeList([])
    cubelist_winds = iris.cube.CubeList([])

    for fieldno, fieldname in enumerate(FIELDNAMES):
        if fieldname in ['U200', 'V200', 'U850', 'V850']:
            cube = extract_winds(filestart, fieldname, exptnames)
            cubelist_winds.append(cube)
        else:
            cube = extract_fields(filestart, fieldname, exptnames, offset)
            cubelist.append(cube)
            
    
    return cubelist, cubelist_winds

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning

LINUXWIN = 'w' # l-linux, w=windows, j jasmin
            	
FILESTART = {"l":'/nfs/hera1/earjcti/um/netcdf/xkrax_netcdf/xkraxa@pd',
             "w":'C:/Users/julia/OneDrive/WORK/DATA/BAS_timeslices/',
             "j":'/home/users/jctindall/umoutput/BAS_timeslices/'}


STARTYEAR=20
if LINUXWIN == 'w':
    ENDYEAR = 50
else:
    ENDYEAR=50


EXTRA='v'
EXPT_11_9=['xlubl','xlubk','xlubj']
EXPT_7_5=['xlubf','xlubg','xlubh']

EXPTNAME = {"0ka" : "xluba",
            "1ka" : "xlubb",
            "2ka" : "xlubc",
            "3ka" : "xlubd",
            "4ka" : "xlube",
            "5ka" : "xlubf",
            "6ka" : "xlubg",
            "7ka" : "xlubh",
            "8ka" : "xlubi",
            "9ka" : "xlubj",
            "10ka" : "xlubk",
            "11ka" : "xlubl",
            "12ka" : "xlubm",
            "13ka" : "xlubn",
            "14ka" : "xlubo",
            "15ka" : "xlubp",
            "16ka" : "xlubq",
            "17ka" : "xlubr",
            "18ka" : "xlubs",
            "19ka" : "xlubt",
            "20ka" : "xlubu",
            "21ka" : "xlubv"
            }


# based on Lambeck et al 2014 table S3.  assuming 140m of sea level drop equates to
# 1permil of ocean d18o (because their LGM sea level was 140m).
D18O_OFFSET = {"0ka" : 0.0,
               "1ka" : 0.0,
               "2ka" : 0.0,
               "3ka" : 0.01,
               "4ka" : 0.01,
               "5ka" : 0.02,
               "6ka" : 0.02,
               "7ka" : 0.04,
               "8ka" : 0.09,
               "9ka" : 0.19,
               "10ka" : 0.29,
               "11ka" : 0.39,
               "12ka" : 0.46,
               "13ka" : 0.5,
               "14ka" : 0.55,
               "15ka" : 0.7,
               "16ka" : 0.77,
               "17ka" : 0.82,
               "18ka" : 0.86,
               "19ka" : 0.9,
               "20ka" : 0.96,
               "21ka" : 1.0
               }

FILEOUTSTART='modeloutput/'

FIELDNAMES=['TOTAL PRECIPITATION RATE     KG/M2/S',
            'TEMPERATURE AT 1.5M',
            'Stash code = 338',
            'EVAPORATION FROM SEA (GBM)   KG/M2/S',
            'RELATIVE HUMIDITY AT 1.5M',
            'SUBLIM. FROM SURFACE (GBM)  KG/M2/TS',
            'EVAP FROM CANOPY - AMOUNT   KG/M2/TS',
            'TRANSPIRATION RATE           KG/M2/S']
#FIELDNAMES=['U850','V850','U200','V200']
#FIELDNAMES = ['SUBLIM. FROM SURFACE (GBM)  KG/M2/TS']
#FIELDNAMES = ['EVAP FROM CANOPY - AMOUNT   KG/M2/TS']

########################################

# do expt 11-9ka

#allcubes = extract_all_avg(FILESTART.get(LINUXWIN), EXPT_11_9, 0.29)


#fileout = FILEOUTSTART + 'data_11-9ka.nc'        
#iris.save(allcubes, fileout, netcdf_format='NETCDF3_CLASSIC', fill_value=2.0E20)

##########################################
# do individual years

#ERIODS = ['11ka','10ka','9ka','8ka','7ka','6ka']
PERIODS = ['8ka','7ka','6ka']
#PERIODS = ['11ka']
for time in range(0, len(PERIODS)):
    exptlist = [(EXPTNAME.get(PERIODS[time]))]
    offset = (D18O_OFFSET.get(PERIODS[time]))
    timecubes, windcubes = (
        extract_all_avg(FILESTART.get(LINUXWIN), exptlist, offset))
    
    if len(timecubes) > 0:
        fileout=FILEOUTSTART + 'data_' + PERIODS[time] + '.nc'   
        print(fileout)
        iris.save(timecubes, fileout, netcdf_format='NETCDF3_CLASSIC',
                  fill_value=2.0E20)
    if len(windcubes) > 0:
        fileout=FILEOUTSTART + 'winds_' + PERIODS[time] + '.nc'        
        iris.save(windcubes, fileout, netcdf_format='NETCDF3_CLASSIC',
                  fill_value=2.0E20)


    

  
#sys.exit(0)
::::::::::::::
JASMIN_PROGS/NorthAtl/plot_d18o.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_D18O
#PURPOSE 
#    PLOT D18O FROM LOUISE/MAX TIMESLICE EXPERIMENTS MULTIPLE FILES
#
#    We are using this as to plot the D18o across the North Atlantic 
#    sector for Jonathan holmes
#    Initially we want to plot 9-11ka and compare this with 7-8ka
#
# Julia 23.01.2017


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,cbartitle,minval,maxval,diffval):
    lons, lats = np.meshgrid(lon,lat)
    if fileno <= 90:
        plt.subplot(2,2,fileno+1)
   # map=Basemap(projection='robin',resolution='l')

   # this may be a good region for the North Atlantic region
    map=Basemap(llcrnrlon=-20.0,urcrnrlon=40.0,llcrnrlat=30.0,urcrnrlat=70.0,projection='cyl',resolution='c')   

    # this may be a good region for most of the globe
    #map=Basemap(llcrnrlon=0,llcrnrlat=-80,urcrnrlon=360,urcrnrlat=80,projection='mill')

    
   # map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #V=np.arange(np.amin(plotdata),np.amax(plotdata),np.amin(plotdata)/10)
    V=np.arange(minval,maxval,diffval)
   # cs = map.contourf(x,y,plotdata,V)
    cs=map.contourf(x,y,plotdata,V,extend='both',cmap='RdBu_r')
    #cs=map.contourf(x,y,plotdata,vmin=minval,vmax=maxval)
    plt.title(titlename)
  
    cbar=map.colorbar(cs,location='bottom',pad="5%")
    cbar.ax.tick_params(labelsize=10)
    cbar.ax.set_title(cbartitle)


def oplotdata(anom_lakes_d18o,datalons,datalats,minval,maxval,diffval,lake_spel):
# plot filled circles of the data over the map
    print(anom_lakes_d18o)
    V=np.arange(minval,maxval,diffval)
    print(minval,maxval)
    if lake_spel=='l': # lake
        plt.scatter(datalons,datalats,color='black',marker='o',s=110)
        plt.scatter(datalons,datalats,vmin=minval,vmax=maxval,c=anom_lakes_d18o,marker='o',s=70,cmap='RdBu_r')
    if lake_spel=='s': # speleothem
        plt.scatter(datalons,datalats,color='black',marker='^',s=110)
        plt.scatter(datalons,datalats,vmin=minval,vmax=maxval,c=anom_lakes_d18o,marker='^',s=70,cmap='RdBu_r')
    #plt.colorbar()
    


def getKey(item):
    return item[0]



#=====================================================
def extract_data(expt,filenames,expttime):


    # 1.  Set up details.  Gridbox required and filename


    dirname='/home/users/jctindall/mholloway/'+expt+'/pcpd/'
    os.chdir(dirname)


    #2. Set up filename and extract stash code 338
    #   print d18o and dD for that file


    f=MFDataset(filenames)
    f.dimensions
    f.variables

    lon = f.variables['longitude_1'][:]
    lat = f.variables['latitude_1'][:]
    times = f.variables['t'][:]
    xsize=len(lon)
    ysize=len(lat)
    tsize=len(times)
    dD=f.variables['dD'][:]
    dD=np.squeeze(dD)
    d18o=f.variables['dO18'][:]
    d18o=np.squeeze(d18o)
    h2o=f.variables['h2o'][:]
    h2o=np.squeeze(h2o)
    
    
    f.close()

    # we need to get annual average by weighting by precipitation amount

    dD_weight=dD*h2o
    dD_weightsum=np.sum(dD_weight,axis=0)
    d18o_weight=d18o*h2o
    d18o_weightsum=np.sum(d18o_weight,axis=0)
    tot_h2o=np.sum(h2o,axis=0)
    dD_weightavg=dD_weightsum/tot_h2o
    d18o_weightavg=d18o_weightsum/tot_h2o
    #print("shape tpt h2o",tot_h2o.shape)
    #print("shape mean dDweight",dD_weightsum.shape)

   # titlename=expttime
   # cbartitle='mm'
    lontemp=lon
    tot_h2o,lon=shiftgrid(180.,tot_h2o,lon,start=False,cyclic=360)
   # plotdata(tot_h2o*60.*60.*24.*30/tsize,0,lon,lat,titlename,cbartitle,0,150,10)
   # cbartitle='permil'
    lon=lontemp
    d18o_weightavg,lon=shiftgrid(180.,d18o_weightavg,lon,start=False,cyclic=360)
#    plotdata(d18o_weightavg,2,lon,lat,titlename,cbartitle,-40,10,5)
 #   plt.show()

    retdata=[d18o_weightavg,tot_h2o/tsize,lon,lat]

    return retdata





#=====================================================
def extract_data_seas(expt,filestart,expttime,monthnames):

    # 1.  Set up details.  Gridbox required and filename

    nmonths=len(monthnames)
    seasname=''  # get seasonname by using first letter of each month
    for mon in monthnames:
        seasname=seasname+mon[0]

    dirname='/home/users/jctindall/umoutput/BAS_timeslices/'+expt
    os.chdir(dirname)


    #2. Set up filename and extract stash code 338
    #   print d18o and dD for that file


    for monthno in range (0,nmonths):
        filenames=filestart+monthnames[monthno]+'.nc'
        print(dirname+filenames)
        f=MFDataset(filenames)
        f.dimensions
        f.variables

        lon = f.variables['longitude_1'][:]
        lat = f.variables['latitude_1'][:]
        times = f.variables['t'][:]
        xsize=len(lon)
        ysize=len(lat)
        tsize=len(times)
        print(filenames,tsize)

        alldata=f.variables['QCL'][:]
        h2o=alldata[:,0,:,:]+alldata[:,3,:,:]+alldata[:,6,:,:]+alldata[:,9,:,:]
        tot18o=alldata[:,1,:,:]+alldata[:,4,:,:]+alldata[:,7,:,:]+alldata[:,10,:,:]
    
        if monthno ==0: 
            # we need to get annual average by weighting by precipitation amount
            all_tot18o=tot18o
            all_h2o=h2o
            tsizesave=tsize
        else:
            if tsizesave != tsize:
                print('you have not got the same number of files for each month')
                print(tsizesave,tsize)

                sys.exit()

            all_tot18o=all_tot18o+tot18o
            all_h2o=all_h2o+h2o
    
        f.close()


    tot18o_avg=np.sum(all_tot18o,axis=0)  
    toth2o_avg=np.sum(all_h2o,axis=0)

    lontemp=lon
    toth2o_avg,lon=shiftgrid(180.,toth2o_avg,lon,start=False,cyclic=360)
    lon=lontemp
    tot18o_avg,lon=shiftgrid(180.,tot18o_avg,lon,start=False,cyclic=360)
    d18o_avg=((tot18o_avg/toth2o_avg)-2005.2E-6)/2005.2E-9


    retdata=[d18o_avg,toth2o_avg/tsize,lon,lat]

    return retdata

#====================================================================
def get_Jonathan_data(filename):
    
    f1=open(filename,'r')
    #discard titleline
    textline=f1.readline()

    datalons=[]
    datalats=[]
    data_5_7ka=[]
    data_9_11ka=[]

    for line in f1:
        linesplit=line.split(',') # the data in the file is split by comma
        datalats.append(np.float(linesplit[2]))
        datalons.append(np.float(linesplit[3]))
        data_5_7ka.append(np.float(linesplit[5]))
        data_9_11ka.append(np.float(linesplit[7]))

   
    retdata=[datalons,datalats,data_5_7ka,data_9_11ka]
    return retdata

#=================================================================
# MAIN PROGRAM STARTS HERE

seasname='jfmamjjasond'
#seasname='son'
if seasname =='djf':
    monthnames=['dc','ja','fb']
if seasname =='jja':
    monthnames=['jn','jl','ag']
if seasname =='mam':
    monthnames=['mr','ar','my']
if seasname =='son':
    monthnames=['sp','ot','nv']
if seasname =='jfmamjjasond':
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']

#retdata=extract_data('xlubh','iso_xlubha@pcr[3-6]*','7ka')


# xlubh 7ka
retdata=extract_data_seas('xlubh','xlubha@pcr[3-6]*','7ka',monthnames)
d18o_7ka=retdata[0]+0.04 # constant is ice volume correction
h2o_7ka=retdata[1]
lon=retdata[2]
lat=retdata[3]

# xlubg 6ka
retdata=extract_data_seas('xlubg','xlubga@pcr[3-6]*','6ka',monthnames)
d18o_6ka=retdata[0]+0.04
h2o_6ka=retdata[1]
lon=retdata[2]
lat=retdata[3]

# xlubf 5ka
retdata=extract_data_seas('xlubf','xlubfa@pcr[3-6]*','7ka',monthnames)
d18o_5ka=retdata[0]+0.04
h2o_5ka=retdata[1]
lon=retdata[2]
lat=retdata[3]




# xlubj 9ka
retdata=extract_data_seas('xlubj','xlubja@pcr[3-6]*','9ka',monthnames)
print('julia',retdata[0])
d18o_9ka=retdata[0]+0.25 # constant is ice volume correction
h2o_9ka=retdata[1]


# xlubk 10ka
retdata=extract_data_seas('xlubk','xlubka@pcr[3-6]*','10ka',monthnames)
d18o_10ka=retdata[0]+0.33
h2o_10ka=retdata[1]


# xlubl 11ka
retdata=extract_data_seas('xlubl','xlubla@pcr[3-6]*','11ka',monthnames)
d18o_11ka=retdata[0]+0.42
h2o_11ka=retdata[1]


# get average 7ka d18o - weighted by amount

d18o_5_7ka=((d18o_7ka * h2o_7ka) +  (d18o_6ka * h2o_6ka)+  (d18o_5ka * h2o_5ka))/(h2o_7ka+h2o_6ka+h2o_5ka)
d18o_9_11ka=((d18o_9ka * h2o_9ka) +  (d18o_10ka * h2o_10ka) + (d18o_11ka * h2o_11ka))/(h2o_9ka+h2o_10ka+h2o_11ka)

print(np.shape(d18o_5_7ka), np.shape(d18o_9_11ka))


# get Jonathans data
filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/Jonathan_new_data.csv'
lakedata=get_Jonathan_data(filename)
lakedatalons=lakedata[0]
lakedatalats=lakedata[1]
lakedata_5_7ka=lakedata[2]
lakedata_9_11ka=lakedata[3]

filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/speleothem_data.csv'
speleodata=get_Jonathan_data(filename)
speleodatalons=speleodata[0]
speleodatalats=speleodata[1]
speleodata_5_7ka=speleodata[2]
speleodata_9_11ka=speleodata[3]


h2o_5_7ka=(h2o_7ka+h2o_6ka+h2o_5ka)/3.0
h2o_9_11ka=(h2o_9ka+h2o_10ka+h2o_11ka)/3.0


plotdata(d18o_9_11ka,0,lon,lat,'9-11ka d18o','permille',-12,-6,0.5)
plotdata(h2o_9_11ka*60.*60.*24.*30.,1,lon,lat,'9-11ka h2o','mm/month',0,100,10)

anom_h2o=h2o_9_11ka - h2o_5_7ka
titlename='precip 9-11ka - 5-7ka'
cbartitle='mm/month'
plotdata(anom_h2o*60.*60.*24.*30,3,lon,lat,titlename,cbartitle,-10,10,1)

print(np.shape(d18o_5_7ka), np.shape(d18o_9_11ka))

anom_d18o=d18o_9_11ka - d18o_5_7ka

print(np.shape(lakedata_5_7ka), np.shape(lakedata_9_11ka))
anom_lakes_d18o=np.asarray(lakedata_9_11ka) - np.asarray(lakedata_5_7ka)
anom_speleo_d18o=np.asarray(speleodata_9_11ka) - np.asarray(speleodata_5_7ka)

titlename='d18o 9-11ka - 5-7ka '+seasname
cbartitle='permille'
#plotdata(anom_d18o,2,lon,lat,titlename,cbartitle,-6,2,0.5)
plotdata(anom_d18o,2,lon,lat,titlename,cbartitle,-2,1,0.2)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()

plotdata(anom_d18o,99,lon,lat,titlename,cbartitle,-2.0,2.0,0.25)
oplotdata(anom_lakes_d18o,lakedatalons,lakedatalats,-2.0,2.0,0.25,'l')
oplotdata(anom_speleo_d18o,speleodatalons,speleodatalats,-2.0,2.0,0.25,'s')

print('lake',lakedatalons)
print('speleo',speleodatalons)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_d18o_data'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_d18o/North_atl_9-11ka_5-7ka_d18o_data'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()



sys.exit()
::::::::::::::
JASMIN_PROGS/NorthAtl/plot_d18osw.py
::::::::::::::
#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-

#This program will plot the fields for Jonathan Holmes from the 
#averaged data (data_6ka - data_11ka)
#
#We will do a map plot in the vicinity of Ireland
#We will plot the absolute value at 6ka, and an anomaly for all the other
#slices
#
#The fields we intend to plot are JJA:
#    d18op, SAT, precip amount, circulation patterns, SST, d18osw
#Created on Sat Mar  7 13:58:35 2020
#
#@author: julia

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt
import iris.quickplot as qplt

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
#os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid

def get_allcubes(field):
    """
    gets the data from all the timeslices and puts them in a list of cubes
    returns the list of cubes
    """
    allcubes = iris.cube.CubeList([])
    
    for i, slice in enumerate(SLICES):
        cube = get_data(slice, field)
        if i == 0:
            # we will plot the raw data
            allcubes.append(cube)
        else:
            # we will plot the anomaly from the first timeslice
            anom_data = cube.data - allcubes[0].data
            newcube = cube.copy(data=anom_data)
            allcubes.append(newcube)

    return allcubes

def get_abs_plotvals():
    """
    sets values for plot range for if we are doing a non_anomaly plot
    """
    
    if FIELDREQ == 'd18o':
        vmin = -10.0
        vmax = 1.0
        vdiff =1.0 
        
    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        vmin = 0.0
        vmax = 35.0
        vdiff = 5.0
    print(FIELDREQ)
   
    print(FIELDREQ)
    if (FIELDREQ == 'TOTAL PRECIPITATION RATE    MM/DAY'
    or FIELDREQ == 'evap'):
        vmin=0.0
        vmax=3.0
        vdiff=0.1
   
        
    return vmin, vmax, vdiff

def get_anom_plotvals():
    """
    sets values for plot range for if we are doing ananomaly plot
    """
    if FIELDREQ == 'd18o':
        vmin = -10
        vmax = 11
        vdiff = 1

    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        vmin = -3.0
        vmax = 3.2
        vdiff = 0.2
        
    if (FIELDREQ == 'TOTAL PRECIPITATION RATE    MM/DAY'
    or FIELDREQ == 'evap'):
        vmin=-10
        vmax=11
        vdiff=1.0
        
    return vmin, vmax, vdiff

def jja_weight_d18o_by_precip_amt(d18ocube, filename):
    """
    if we are doing d18o we need to weight by precipitation amount to calculate
    the three month average
    """
    # get precipitation amount
    allprecipcube = iris.load_cube(filename, 'TOTAL PRECIPITATION RATE    MM/DAY')
  
    # get jja fields
    precip_jja_cube = allprecipcube[5:8, :, :]
    d18o_jja_cube = d18ocube[5:8, :, :]
    
    # weight d18o by precipitaion amount and average
    newdata = ((precip_jja_cube.data * d18o_jja_cube.data) / 
               np.mean(precip_jja_cube.data, axis=0))
    newcube = precip_jja_cube.copy(data=newdata)
    weighted_cube = newcube.collapsed('t', iris.analysis.MEAN)
    
    
    return weighted_cube

def jja_avg(cube, filename):
    """
    simple 3 month average over jja of the cube
    """
    #
    # get jja fields
    jja_cube = cube[5:8, :, :]
    
    # weight d18o by precipitaion amount and average
    avg_cube = jja_cube.collapsed('t', iris.analysis.MEAN)
    
    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        avg_cube.data = avg_cube.data - 273.15
    
    return avg_cube

def sum_evap(filename_):
    """
    will add up all the fluxes that make evaporation and returns
    total evaporation within a cube
    The fluxes are:
      evaporation from canopy
      evaporation from sea
      transpiration
      sublim from surface
    """

    varnames_mm = ["EVAPORATION FROM SEA    MM/DAY",
                "TRANSPIRATION RATE    MM/DAY",
                "EVAP FROM CANPOPY    MM/DAY",
                ]
    
    varnames_ts = ["SUBLIM. FROM SURFACE (GBM)  KG/M2/TS"]
    
    
    for i, var in enumerate(varnames_mm):
        print(filename_, var)
        cube = iris.load_cube(filename_,var)
        cube.data = np.where(np.isnan(cube.data), 0, cube.data)
        cube.data = np.where(cube.data > 1.0E10, 0, cube.data)
        #cube = simplify_cube(cube)
        if i == 0:
            cubetot = cube
        else:
            cubetot = cubetot + cube
        print(var,cube.data)
            
        
    for i, var in enumerate(varnames_ts):
        cube = iris.load_cube(filename_,var)
        cube.data = np.where(np.isnan(cube.data), 0, cube.data)
        cube.data = np.where(cube.data > 1.0E10, 0, cube.data)
        #cube = simplify_cube(cube)
        cube.data = cube.data * 48.0
        cube.units = 'mm/day'
        cubetot = cubetot + cube
     
   
    return cubetot

def get_data(timeslice, field):
    """
    will get the data for the given timeslice and field
    and return in a cube
    """
    filename = FILESTART + timeslice + '.nc'
    print(filename, field)
    if field == 'evap':
        fieldcube = sum_evap(filename)
    else:
        print(filename)
        print(field)
        fieldcube = iris.load_cube(filename, field)
    
    if field == 'd18o':
        newcube = jja_weight_d18o_by_precip_amt(fieldcube, filename)
    else:
        newcube = jja_avg(fieldcube, filename)
        
    return newcube
        
def plot_data(cubelist):
    """
    plots all the cubes : one from each timeslice
    """
  
    for i, cube in enumerate(cubelist):
        print(i)
        plt.subplot(2, 3, i+1)
    
        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
                      llcrnrlat=45.0, urcrnrlat=70.0,
                      projection='cyl', resolution='l')

        latitudes = cube.coord('latitude').points
        longitudes = cube.coord('longitude').points
        
        data_shift, lons_shift = shiftgrid(
                180, cube.data, longitudes, start=False)
        
        lons, lats = np.meshgrid(lons_shift, latitudes)
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)
        
        plt.rcParams['text.latex.preamble']=[r"\usepackage{wasysym}"]
        if i==0:
            cmap_j = 'rainbow'
            vmin, vmax, vdiff = get_abs_plotvals()
            titlename = SLICES[i]
            cs = map.contourf(x, y, data_shift, 
                          levels =np.arange(vmin,vmax,vdiff),
                          cmap = cmap_j, extend='both')
            plt.title(titlename,fontsize=10)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.ax.tick_params(labelsize='small')
            cbar.set_label(u'\u2030',#horizontalalignment='left',
                           fontsize=8,labelpad=-30)

       
        else:
            cmap_j = 'RdBu_r'
            vmin, vmax, vdiff = get_anom_plotvals()
            titlename = SLICES[i] + '-' + SLICES[0]
            cs = map.contourf(x, y, data_shift * 10., 
                          levels =np.arange(vmin,vmax,vdiff),
                          cmap = cmap_j, extend='both')
            plt.title(titlename,fontsize=10)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.ax.tick_params(labelsize='small')
            cbar.set_label('x 10' + u'\u2030', 
                           #horizontalalignment='left',
                           fontsize=8,labelpad=-30)



      
        
    plt.savefig(FILEOUTSTART + '.png')
    plt.savefig(FILEOUTSTART + '.eps')
    plt.savefig(FILEOUTSTART + '.pdf')
    plt.close()


def plot_winds(u_cubes, v_cubes, lev):
    """
    plots all the cubes : one from each timeslice
    """

      
    #fig = plt.figure(figsize=[12.8, 9.6])
    fig = plt.figure(figsize=[50, 40],dpi=200)
    for i, ucube in enumerate(u_cubes):
        vcube = v_cubes[i]
        
        subplotno = np.int('23' + np.str(i+1))
        ax = fig.add_subplot(subplotno)
        print(subplotno)
        
        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
                      llcrnrlat=30.0, urcrnrlat=70.0,
                      projection='cyl', resolution='l')
        
        #map = Basemap(projection='cyl', resolution='l')
       

        latitudes = ucube.coord('latitude').points
        longitudes = ucube.coord('longitude').points
        
        u_shift, lons_shift = shiftgrid(
                180, ucube.data, longitudes, start=False)

        v_shift, lons_shift = shiftgrid(
                180, vcube.data, longitudes, start=False)
        
        lons, lats = np.meshgrid(lons_shift, latitudes)
        x, y = map(lons, lats)
        map.drawcoastlines()
       

        if i == 0: 
           titlename = SLICES[i]
           scalesize = 200
        else:
           scalesize = 50
           titlename = SLICES[i]  + '-' + SLICES[0]
        if lev == '850':
            scalesize = scalesize/3.0
            
        n=1 # plot every nth arrow
        Q = map.quiver(x[::n, ::n], y[::n, ::n], 
                       u_shift[::n, ::n], v_shift[::n, ::n], scale=scalesize,
                       headwidth=7, headlength=9)
        plt.title(titlename, fontsize=30, loc='left')
        qk = ax.quiverkey(Q, 0.7, 1.05, 10, ' 1 m/s', labelpos='E',
                          fontproperties={'size':30})
       
       
    
    plt.savefig(FILEOUTSTART + '.png')
    plt.savefig(FILEOUTSTART + '.pdf')
    plt.close()


def main():
    """
    Control: 1. See whether we are plotting winds
             2. read in data 
             3. setup data for plot
             4. plot : currenaly can only do a 6 panel figure
    """

    if FIELDREQ[0:2] == 'UV': # plotting winds
        winds = 'y'
        level = FIELDREQ[2:5]
        field1 = FIELDREQ[0:1] + ' COMPNT OF WIND AT ' + level + 'hPa'
        field2 = FIELDREQ[1:2] + ' COMPNT OF WIND AT ' + level + 'hPa'
        cubelist_u = get_allcubes(field1)            
        cubelist_v = get_allcubes(field2)            
        plot_winds(cubelist_u, cubelist_v, level)
    else:
        winds = 'n'
        field1 = FIELDREQ
        cubelist = get_allcubes(field1)            
        plot_data(cubelist)


############################################################
print('julia start')
SLICES = ['11ka', '10ka', '9ka', '8ka', '7ka', '6ka']
FIELDREQ = 'd18osw'
                    # 'UV200' 'UV850'
                    # 'TOTAL PRECIPITATION RATE    MM/DAY'
                    #'TEMPERATURE AT 1.5M'   
                    # evap
                    # d18o
LINUX_WIN = 'l'
NSLICES = len(SLICES)

shortfield = {'TOTAL PRECIPITATION RATE    MM/DAY' : 'precip',
              'TEMPERATURE AT 1.5M' : 'temp',
              'd18o' : 'd18o_p',
              'UV850' : 'winds_at_850hPa',
              'UV200' : 'winds_at_200hPa',
              'evap' : 'evap',
              'd18osw': 'd18o_sw'}
              
if FIELDREQ[0:2] == 'UV':
    filetype = 'winds_'
else:
    filetype = 'data_'

#if LINUX_WIN == 'w':
#   FILESTART = ('C:\\Users\\julia\\OneDrive\\WORK\\COLLABORATORS\\'
#                + 'JONATHAN_HOLMES\\data_slices\\' + filetype)
#   FILEOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\COLLABORATORS\\'
#                + 'JONATHAN_HOLMES\\plots\\' + FIELDREQ)
#else:
FILESTART = 'modeloutput/' + filetype                
FILEOUTSTART = 'modeloutput/plots/' + shortfield.get(FIELDREQ)

main()

::::::::::::::
JASMIN_PROGS/NorthAtl/plot_diagnostics_near_ireland_old.py
::::::::::::::
#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-

#This program will plot the fields for Jonathan Holmes from the 
#averaged data (data_6ka - data_11ka)
#
#We will do a map plot in the vicinity of Ireland
#We will plot the absolute value at 6ka, and an anomaly for all the other
#slices
#
#The fields we intend to plot are JJA:
#    d18op, SAT, precip amount, circulation patterns, SST, d18osw
#Created on Sat Mar  7 13:58:35 2020
#
#@author: julia

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt
import iris.quickplot as qplt

os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
#os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid

def get_allcubes(field):
    """
    gets the data from all the timeslices and puts them in a list of cubes
    returns the list of cubes
    """
    allcubes = iris.cube.CubeList([])
    
    for i, slice in enumerate(SLICES):
        cube = get_data(slice, field)
        if i == 0:
            # we will plot the raw data
            allcubes.append(cube)
        else:
            # we will plot the anomaly from the first timeslice
            anom_data = cube.data - allcubes[0].data
            newcube = cube.copy(data=anom_data)
            allcubes.append(newcube)

    return allcubes

def get_abs_plotvals():
    """
    sets values for plot range for if we are doing a non_anomaly plot
    """
    
    if FIELDREQ == 'd18o':
        vmin = -15.0
        vmax = 1.0
        vdiff =1.0 
        
    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        vmin = 0.0
        vmax = 35.0
        vdiff = 5.0
    print(FIELDREQ)
   
    print(FIELDREQ)
    if (FIELDREQ == 'TOTAL PRECIPITATION RATE    MM/DAY'
    or FIELDREQ == 'evap'):
        vmin=0.0
        vmax=3.0
        vdiff=0.1
   
        
    return vmin, vmax, vdiff

def get_anom_plotvals():
    """
    sets values for plot range for if we are doing ananomaly plot
    """
    if FIELDREQ == 'd18o':
        vmin = -1.0
        vmax = 1.1
        vdiff = 0.1

    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        vmin = -3.0
        vmax = 3.2
        vdiff = 0.2
        
    if (FIELDREQ == 'TOTAL PRECIPITATION RATE    MM/DAY'
    or FIELDREQ == 'evap'):
        vmin=-1.0
        vmax=1.0
        vdiff=0.1
        
    return vmin, vmax, vdiff

def jja_weight_d18o_by_precip_amt(d18ocube, filename):
    """
    if we are doing d18o we need to weight by precipitation amount to calculate
    the three month average
    """
    # get precipitation amount
    allprecipcube = iris.load_cube(filename, 'TOTAL PRECIPITATION RATE    MM/DAY')
  
    # get jja fields
    precip_jja_cube = allprecipcube[5:8, :, :]
    d18o_jja_cube = d18ocube[5:8, :, :]
    
    # weight d18o by precipitaion amount and average
    newdata = ((precip_jja_cube.data * d18o_jja_cube.data) / 
               np.mean(precip_jja_cube.data, axis=0))
    newcube = precip_jja_cube.copy(data=newdata)
    weighted_cube = newcube.collapsed('t', iris.analysis.MEAN)
    
    
    return weighted_cube

def jja_avg(cube, filename):
    """
    simple 3 month average over jja of the cube
    """
    #
    # get jja fields
    jja_cube = cube[5:8, :, :]
    
    # weight d18o by precipitaion amount and average
    avg_cube = jja_cube.collapsed('t', iris.analysis.MEAN)
    
    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        avg_cube.data = avg_cube.data - 273.15
    
    return avg_cube

def sum_evap(filename_):
    """
    will add up all the fluxes that make evaporation and returns
    total evaporation within a cube
    The fluxes are:
      evaporation from canopy
      evaporation from sea
      transpiration
      sublim from surface
    """

    varnames_mm = ["EVAPORATION FROM SEA    MM/DAY",
                "TRANSPIRATION RATE    MM/DAY",
                "EVAP FROM CANPOPY    MM/DAY",
                ]
    
    varnames_ts = ["SUBLIM. FROM SURFACE (GBM)  KG/M2/TS"]
    
    
    for i, var in enumerate(varnames_mm):
        print(filename_, var)
        cube = iris.load_cube(filename_,var)
        cube.data = np.where(np.isnan(cube.data), 0, cube.data)
        cube.data = np.where(cube.data > 1.0E10, 0, cube.data)
        #cube = simplify_cube(cube)
        if i == 0:
            cubetot = cube
        else:
            cubetot = cubetot + cube
        print(var,cube.data)
            
        
    for i, var in enumerate(varnames_ts):
        cube = iris.load_cube(filename_,var)
        cube.data = np.where(np.isnan(cube.data), 0, cube.data)
        cube.data = np.where(cube.data > 1.0E10, 0, cube.data)
        #cube = simplify_cube(cube)
        cube.data = cube.data * 48.0
        cube.units = 'mm/day'
        cubetot = cubetot + cube
     
   
    return cubetot

def get_data(timeslice, field):
    """
    will get the data for the given timeslice and field
    and return in a cube
    """
    filename = FILESTART + timeslice + '.nc'
    print(filename, field)
    if field == 'evap':
        fieldcube = sum_evap(filename)
    else:
        fieldcube = iris.load_cube(filename, field)
    
    if field == 'd18o':
        newcube = jja_weight_d18o_by_precip_amt(fieldcube, filename)
    else:
        newcube = jja_avg(fieldcube, filename)
        
    return newcube
        
def plot_data(cubelist):
    """
    plots all the cubes : one from each timeslice
    """
  
    for i, cube in enumerate(cubelist):
        print(i)
        plt.subplot(2, 3, i+1)
    
        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
                      llcrnrlat=30.0, urcrnrlat=70.0,
                      projection='cyl', resolution='l')

        latitudes = cube.coord('latitude').points
        longitudes = cube.coord('longitude').points
        
        data_shift, lons_shift = shiftgrid(
                180, cube.data, longitudes, start=False)
        
        lons, lats = np.meshgrid(lons_shift, latitudes)
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)
        
        
        if i==0:
            cmap_j = 'rainbow'
            vmin, vmax, vdiff = get_abs_plotvals()
            titlename = SLICES[i]
        else:
            cmap_j = 'RdBu_r'
            vmin, vmax, vdiff = get_anom_plotvals()
            titlename = SLICES[i] + '-' + SLICES[0]

        cs = map.contourf(x, y, data_shift, 
                          levels =np.arange(vmin,vmax,vdiff),
                          cmap = cmap_j, extend='both')
        plt.title(titlename)
        cbar = plt.colorbar(cs, orientation='vertical')
  
    plt.savefig(FILEOUTSTART + '.png')
    plt.savefig(FILEOUTSTART + '.pdf')
    plt.close()


def plot_winds(u_cubes, v_cubes, lev):
    """
    plots all the cubes : one from each timeslice
    """

      
    #fig = plt.figure(figsize=[12.8, 9.6])
    fig = plt.figure(figsize=[25, 20])
    for i, ucube in enumerate(u_cubes):
        vcube = v_cubes[i]
        
        subplotno = np.int('23' + np.str(i+1))
        ax = fig.add_subplot(subplotno)
        print(subplotno)
        
        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
                      llcrnrlat=30.0, urcrnrlat=70.0,
                      projection='cyl', resolution='l')
        
        #map = Basemap(projection='cyl', resolution='l')
       

        latitudes = ucube.coord('latitude').points
        longitudes = ucube.coord('longitude').points
        
        u_shift, lons_shift = shiftgrid(
                180, ucube.data, longitudes, start=False)

        v_shift, lons_shift = shiftgrid(
                180, vcube.data, longitudes, start=False)
        
        lons, lats = np.meshgrid(lons_shift, latitudes)
        x, y = map(lons, lats)
        map.drawcoastlines()
       

        if i == 0: 
           titlename = SLICES[i]
           scalesize = 200
        else:
           scalesize = 50
           titlename = SLICES[i]  + '-' + SLICES[0]
        if lev == '850':
            scalesize = scalesize/3.0
            
        n=1 # plot every nth arrow
        Q = map.quiver(x[::n, ::n], y[::n, ::n], 
                       u_shift[::n, ::n], v_shift[::n, ::n], scale=scalesize,
                       headwidth=7, headlength=9)
        plt.title(titlename, fontsize=30, loc='left')
        qk = ax.quiverkey(Q, 0.7, 1.05, 10, ' 1 m/s', labelpos='E',
                          fontproperties={'size':30})
       
       
    
    plt.savefig(FILEOUTSTART + '.png')
    plt.savefig(FILEOUTSTART + '.pdf')
    plt.close()


def main():
    """
    Control: 1. See whether we are plotting winds
             2. read in data 
             3. setup data for plot
             4. plot : currenaly can only do a 6 panel figure
    """

    if FIELDREQ[0:2] == 'UV': # plotting winds
        winds = 'y'
        level = FIELDREQ[2:5]
        field1 = FIELDREQ[0:1] + ' COMPNT OF WIND AT ' + level + 'hPa'
        field2 = FIELDREQ[1:2] + ' COMPNT OF WIND AT ' + level + 'hPa'
        cubelist_u = get_allcubes(field1)            
        cubelist_v = get_allcubes(field2)            
        plot_winds(cubelist_u, cubelist_v, level)
    else:
        winds = 'n'
        field1 = FIELDREQ
        cubelist = get_allcubes(field1)            
        plot_data(cubelist)


############################################################
print('julia start')
SLICES = ['6ka', '7ka', '8ka', '9ka', '10ka', '11ka']
FIELDREQ = 'evap'
                    # 'UV200' 'UV850'
                    # 'TOTAL PRECIPITATION RATE    MM/DAY'
                    #'TEMPERATURE AT 1.5M'   
                    # evap
                    # d18o
LINUX_WIN = 'w'
NSLICES = len(SLICES)

shortfield = {'TOTAL PRECIPITATION RATE    MM/DAY' : 'precip',
              'TEMPERATURE AT 1.5M' : 'temp',
              'd18o' : 'd18o_p',
              'UV850' : 'winds_at_850hPa',
              'UV200' : 'winds_at_200hPa',
              'evap' : 'evap'}
              
if FIELDREQ[0:2] == 'UV':
    filetype = 'winds_'
else:
    filetype = 'data_'

#if LINUX_WIN == 'w':
#   FILESTART = ('C:\\Users\\julia\\OneDrive\\WORK\\COLLABORATORS\\'
#                + 'JONATHAN_HOLMES\\data_slices\\' + filetype)
#   FILEOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\COLLABORATORS\\'
#                + 'JONATHAN_HOLMES\\plots\\' + FIELDREQ)
#else:
FILESTART = 'modeloutput/' + filetype                
FILEOUTSTART = 'modeloutput/plots/' + shortfield.get(FIELDREQ)

main()

::::::::::::::
JASMIN_PROGS/NorthAtl/plot_diagnostics_near_ireland.py
::::::::::::::
#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-

#This program will plot the fields for Jonathan Holmes from the 
#averaged data (data_6ka - data_11ka)
#
#We will do a map plot in the vicinity of Ireland
#We will plot the absolute value at 6ka, and an anomaly for all the other
#slices
#
#The fields we intend to plot are JJA:
#    d18op, SAT, precip amount, circulation patterns, SST, d18osw
#Created on Sat Mar  7 13:58:35 2020
#
#@author: julia

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt
import iris.quickplot as qplt

os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
#os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid

def get_allcubes(field):
    """
    gets the data from all the timeslices and puts them in a list of cubes
    returns the list of cubes
    """
    allcubes = iris.cube.CubeList([])
    
    for i, slice in enumerate(SLICES):
        cube = get_data(slice, field)
        if i == 0:
            # we will plot the raw data
            allcubes.append(cube)
        else:
            # we will plot the anomaly from the first timeslice
            anom_data = cube.data - allcubes[0].data
            newcube = cube.copy(data=anom_data)
            allcubes.append(newcube)

    return allcubes

def get_abs_plotvals():
    """
    sets values for plot range for if we are doing a non_anomaly plot
    """
    
    if FIELDREQ == 'd18o':
        vmin = -10.0
        vmax = 1.0
        vdiff =1.0 
        
    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        vmin = 0.0
        vmax = 35.0
        vdiff = 5.0
    print(FIELDREQ)
   
    print(FIELDREQ)
    if (FIELDREQ == 'TOTAL PRECIPITATION RATE    MM/DAY'
    or FIELDREQ == 'evap'):
        vmin=0.0
        vmax=3.0
        vdiff=0.1
   
        
    return vmin, vmax, vdiff

def get_anom_plotvals():
    """
    sets values for plot range for if we are doing ananomaly plot
    """
    if FIELDREQ == 'd18o':
        vmin = -10
        vmax = 11
        vdiff = 1

    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        vmin = -3.0
        vmax = 3.2
        vdiff = 0.2
        
    if (FIELDREQ == 'TOTAL PRECIPITATION RATE    MM/DAY'
    or FIELDREQ == 'evap'):
        vmin=-10
        vmax=11
        vdiff=1.0
        
    return vmin, vmax, vdiff

def jja_weight_d18o_by_precip_amt(d18ocube, filename):
    """
    if we are doing d18o we need to weight by precipitation amount to calculate
    the three month average
    """
    # get precipitation amount
    allprecipcube = iris.load_cube(filename, 'TOTAL PRECIPITATION RATE    MM/DAY')
  
    # get jja fields
    precip_jja_cube = allprecipcube[5:8, :, :]
    d18o_jja_cube = d18ocube[5:8, :, :]
    
    # weight d18o by precipitaion amount and average
    newdata = ((precip_jja_cube.data * d18o_jja_cube.data) / 
               np.mean(precip_jja_cube.data, axis=0))
    newcube = precip_jja_cube.copy(data=newdata)
    weighted_cube = newcube.collapsed('t', iris.analysis.MEAN)
    
    
    return weighted_cube

def jja_avg(cube, filename):
    """
    simple 3 month average over jja of the cube
    """
    #
    # get jja fields
    jja_cube = cube[5:8, :, :]
    
    # weight d18o by precipitaion amount and average
    avg_cube = jja_cube.collapsed('t', iris.analysis.MEAN)
    
    if FIELDREQ == 'TEMPERATURE AT 1.5M':
        avg_cube.data = avg_cube.data - 273.15
    
    return avg_cube

def sum_evap(filename_):
    """
    will add up all the fluxes that make evaporation and returns
    total evaporation within a cube
    The fluxes are:
      evaporation from canopy
      evaporation from sea
      transpiration
      sublim from surface
    """

    varnames_mm = ["EVAPORATION FROM SEA    MM/DAY",
                "TRANSPIRATION RATE    MM/DAY",
                "EVAP FROM CANPOPY    MM/DAY",
                ]
    
    varnames_ts = ["SUBLIM. FROM SURFACE (GBM)  KG/M2/TS"]
    
    
    for i, var in enumerate(varnames_mm):
        print(filename_, var)
        cube = iris.load_cube(filename_,var)
        cube.data = np.where(np.isnan(cube.data), 0, cube.data)
        cube.data = np.where(cube.data > 1.0E10, 0, cube.data)
        #cube = simplify_cube(cube)
        if i == 0:
            cubetot = cube
        else:
            cubetot = cubetot + cube
        print(var,cube.data)
            
        
    for i, var in enumerate(varnames_ts):
        cube = iris.load_cube(filename_,var)
        cube.data = np.where(np.isnan(cube.data), 0, cube.data)
        cube.data = np.where(cube.data > 1.0E10, 0, cube.data)
        #cube = simplify_cube(cube)
        cube.data = cube.data * 48.0
        cube.units = 'mm/day'
        cubetot = cubetot + cube
     
   
    return cubetot

def get_data(timeslice, field):
    """
    will get the data for the given timeslice and field
    and return in a cube
    """
    filename = FILESTART + timeslice + '.nc'
    print(filename, field)
    if field == 'evap':
        fieldcube = sum_evap(filename)
    else:
        fieldcube = iris.load_cube(filename, field)
    
    if field == 'd18o':
        newcube = jja_weight_d18o_by_precip_amt(fieldcube, filename)
    else:
        newcube = jja_avg(fieldcube, filename)
        
    return newcube
        
def plot_data(cubelist):
    """
    plots all the cubes : one from each timeslice
    """
  
    for i, cube in enumerate(cubelist):
        print(i)
        plt.subplot(2, 3, i+1)
    
        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
                      llcrnrlat=45.0, urcrnrlat=70.0,
                      projection='cyl', resolution='l')

        latitudes = cube.coord('latitude').points
        longitudes = cube.coord('longitude').points
        
        data_shift, lons_shift = shiftgrid(
                180, cube.data, longitudes, start=False)
        
        lons, lats = np.meshgrid(lons_shift, latitudes)
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)
        
        plt.rcParams['text.latex.preamble']=[r"\usepackage{wasysym}"]
        if i==0:
            cmap_j = 'rainbow'
            vmin, vmax, vdiff = get_abs_plotvals()
            titlename = SLICES[i]
            cs = map.contourf(x, y, data_shift, 
                          levels =np.arange(vmin,vmax,vdiff),
                          cmap = cmap_j, extend='both')
            plt.title(titlename,fontsize=10)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.ax.tick_params(labelsize='small')
            cbar.set_label(u'\u2030',#horizontalalignment='left',
                           fontsize=8,labelpad=-30)

       
        else:
            cmap_j = 'RdBu_r'
            vmin, vmax, vdiff = get_anom_plotvals()
            titlename = SLICES[i] + '-' + SLICES[0]
            cs = map.contourf(x, y, data_shift * 10., 
                          levels =np.arange(vmin,vmax,vdiff),
                          cmap = cmap_j, extend='both')
            plt.title(titlename,fontsize=10)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.ax.tick_params(labelsize='small')
            cbar.set_label('x 10' + u'\u2030', 
                           #horizontalalignment='left',
                           fontsize=8,labelpad=-30)



      
        
    plt.savefig(FILEOUTSTART + '.png')
    plt.savefig(FILEOUTSTART + '.eps')
    plt.savefig(FILEOUTSTART + '.pdf')
    plt.close()


def plot_winds(u_cubes, v_cubes, lev):
    """
    plots all the cubes : one from each timeslice
    """

      
    #fig = plt.figure(figsize=[12.8, 9.6])
    fig = plt.figure(figsize=[50, 40],dpi=200)
    for i, ucube in enumerate(u_cubes):
        vcube = v_cubes[i]
        
        subplotno = np.int('23' + np.str(i+1))
        ax = fig.add_subplot(subplotno)
        print(subplotno)
        
        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
                      llcrnrlat=30.0, urcrnrlat=70.0,
                      projection='cyl', resolution='l')
        
        #map = Basemap(projection='cyl', resolution='l')
       

        latitudes = ucube.coord('latitude').points
        longitudes = ucube.coord('longitude').points
        
        u_shift, lons_shift = shiftgrid(
                180, ucube.data, longitudes, start=False)

        v_shift, lons_shift = shiftgrid(
                180, vcube.data, longitudes, start=False)
        
        lons, lats = np.meshgrid(lons_shift, latitudes)
        x, y = map(lons, lats)
        map.drawcoastlines()
       

        if i == 0: 
           titlename = SLICES[i]
           scalesize = 200
        else:
           scalesize = 50
           titlename = SLICES[i]  + '-' + SLICES[0]
        if lev == '850':
            scalesize = scalesize/3.0
            
        n=1 # plot every nth arrow
        Q = map.quiver(x[::n, ::n], y[::n, ::n], 
                       u_shift[::n, ::n], v_shift[::n, ::n], scale=scalesize,
                       headwidth=7, headlength=9)
        plt.title(titlename, fontsize=30, loc='left')
        qk = ax.quiverkey(Q, 0.7, 1.05, 10, ' 1 m/s', labelpos='E',
                          fontproperties={'size':30})
       
       
    
    plt.savefig(FILEOUTSTART + '.png')
    plt.savefig(FILEOUTSTART + '.pdf')
    plt.close()


def main():
    """
    Control: 1. See whether we are plotting winds
             2. read in data 
             3. setup data for plot
             4. plot : currenaly can only do a 6 panel figure
    """

    if FIELDREQ[0:2] == 'UV': # plotting winds
        winds = 'y'
        level = FIELDREQ[2:5]
        field1 = FIELDREQ[0:1] + ' COMPNT OF WIND AT ' + level + 'hPa'
        field2 = FIELDREQ[1:2] + ' COMPNT OF WIND AT ' + level + 'hPa'
        cubelist_u = get_allcubes(field1)            
        cubelist_v = get_allcubes(field2)            
        plot_winds(cubelist_u, cubelist_v, level)
    else:
        winds = 'n'
        field1 = FIELDREQ
        cubelist = get_allcubes(field1)            
        plot_data(cubelist)


############################################################
print('julia start')
SLICES = ['11ka', '10ka', '9ka', '8ka', '7ka', '6ka']
FIELDREQ = 'd18o'
                    # 'UV200' 'UV850'
                    # 'TOTAL PRECIPITATION RATE    MM/DAY'
                    #'TEMPERATURE AT 1.5M'   
                    # evap
                    # d18o
LINUX_WIN = 'w'
NSLICES = len(SLICES)

shortfield = {'TOTAL PRECIPITATION RATE    MM/DAY' : 'precip',
              'TEMPERATURE AT 1.5M' : 'temp',
              'd18o' : 'd18o_p',
              'UV850' : 'winds_at_850hPa',
              'UV200' : 'winds_at_200hPa',
              'evap' : 'evap'}
              
if FIELDREQ[0:2] == 'UV':
    filetype = 'winds_'
else:
    filetype = 'data_'

#if LINUX_WIN == 'w':
#   FILESTART = ('C:\\Users\\julia\\OneDrive\\WORK\\COLLABORATORS\\'
#                + 'JONATHAN_HOLMES\\data_slices\\' + filetype)
#   FILEOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\COLLABORATORS\\'
#                + 'JONATHAN_HOLMES\\plots\\' + FIELDREQ)
#else:
FILESTART = 'modeloutput/' + filetype                
FILEOUTSTART = 'modeloutput/plots/' + shortfield.get(FIELDREQ)

main()

::::::::::::::
JASMIN_PROGS/NorthAtl/plot_temperature.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_temperature
#PURPOSE 
#    PLOT temperature FROM LOUISE/MAX TIMESLICE EXPERIMENTS MULTIPLE FILES
#
#    We are using this as to plot the temperature across the North Atlantic 
#    sector for Jonathan holmes
#    Initially we want to plot 9-11ka and compare this with 7-8ka
#
# Julia 07.04.2019 (This was updated from plot_d18o)


# Import necessary libraries

import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from netCDF4 import Dataset, MFDataset
from mpl_toolkits.basemap import Basemap,maskoceans, shiftgrid



# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,cbartitle,minval,maxval,diffval):
    lons, lats = np.meshgrid(lon,lat)
    if fileno <= 90:
        plt.subplot(2,2,fileno+1)
   # map=Basemap(projection='robin',resolution='l')

   # this may be a good region for the North Atlantic region
    map=Basemap(llcrnrlon=-20.0,urcrnrlon=40.0,llcrnrlat=30.0,urcrnrlat=70.0,projection='cyl',resolution='c')   

    # this may be a good region for most of the globe
    #map=Basemap(llcrnrlon=0,llcrnrlat=-80,urcrnrlon=360,urcrnrlat=80,projection='mill')

    
   # map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #V=np.arange(np.amin(plotdata),np.amax(plotdata),np.amin(plotdata)/10)
    V=np.arange(minval,maxval,diffval)
   # cs = map.contourf(x,y,plotdata,V)
    cs=map.contourf(x,y,plotdata,V,extend='both',cmap='RdBu_r')
    #cs=map.contourf(x,y,plotdata,vmin=minval,vmax=maxval)
    plt.title(titlename)
  
    cbar=map.colorbar(cs,location='bottom',pad="5%")
    cbar.ax.tick_params(labelsize=10)
    cbar.ax.set_title(cbartitle)


def oplotdata(anom_lakes_temp,datalons,datalats,minval,maxval,diffval,lake_spel):
# plot filled circles of the data over the map
    print(anom_lakes_temp)
    V=np.arange(minval,maxval,diffval)
    print(minval,maxval)
    if lake_spel=='l': # lake
        plt.scatter(datalons,datalats,color='black',marker='o',s=110)
        plt.scatter(datalons,datalats,vmin=minval,vmax=maxval,c=anom_lakes_temp,marker='o',s=70,cmap='RdBu_r')
    if lake_spel=='s': # speleothem
        plt.scatter(datalons,datalats,color='black',marker='^',s=110)
        plt.scatter(datalons,datalats,vmin=minval,vmax=maxval,c=anom_lakes_temp,marker='^',s=70,cmap='RdBu_r')
    #plt.colorbar()
    


def getKey(item):
    return item[0]





#=====================================================
def extract_data_seas(expt,filestart,expttime,monthnames):

    # 1.  Set up details.  Gridbox required and filename

 
    nmonths=len(monthnames)
    seasname=''  # get seasonname by using first letter of each month
    for mon in monthnames:
        seasname=seasname+mon[0]

    dirname='/home/users/jctindall/umoutput/BAS_timeslices/'+expt
    os.chdir(dirname)
 

    #2. Set up filename and extract stash code 338
    #   print temp and dD for that file

    count=0
    for monthno in range (0,nmonths):
        print(monthno)
        filenames=filestart+monthnames[monthno]+'.nc'
        print(dirname+filenames)
        f=MFDataset(filenames)
        f.dimensions
        f.variables

        lon = f.variables['longitude'][:]
        lat = f.variables['latitude'][:]
        times = f.variables['t'][:]
        xsize=len(lon)
        ysize=len(lat)
        tsize=len(times)
        print(filenames,tsize)

        tempdata=f.variables['temp_1'][:]
        h2o_data=f.variables['precip'][:]

       
        
        
        if monthno ==0: 
            # we need to get annual average by weighting by precipitation amount
            all_temp=(tempdata*h2o_data) # precipitation weighted temperature
            all_h2o=h2o_data
            count=count+1
            tsizesave=tsize
        else:
            if tsizesave != tsize:
                print('you have not got the same number of files for each month')
                print(tsizesave,tsize)

                sys.exit()

            all_temp=all_temp+(tempdata*h2o_data)
            all_h2o=all_h2o+h2o_data
            count=count+1
    
            print('j1',monthno,h2o_data[0,0,20,20],all_h2o[0,0,20,20]*60.*60.*24/count,count)
        f.close()

    
    all_temp=all_temp/all_h2o     # weight by precipitation amount
    temp=np.mean(all_temp,axis=0) # climatological average
    h2o=np.mean(all_h2o/count,axis=0)
    temp=np.squeeze(temp)
    h2o=np.squeeze(h2o)

    
    print(np.shape(temp))

    lontemp=lon
    temp,lon=shiftgrid(180.,temp,lon,start=False,cyclic=360)
    lon=lontemp
    h2o,lon=shiftgrid(180.,h2o,lon,start=False,cyclic=360)


    retdata=[temp,h2o,lon,lat]

    return retdata

#====================================================================
def get_Jonathan_data(filename):
    
    f1=open(filename,'r')
    #discard titleline
    textline=f1.readline()

    datalons=[]
    datalats=[]
    data_5_7ka=[]
    data_9_11ka=[]

    for line in f1:
        linesplit=line.split(',') # the data in the file is split by comma
        datalats.append(np.float(linesplit[2]))
        datalons.append(np.float(linesplit[3]))
        data_5_7ka.append(np.float(linesplit[5]))
        data_9_11ka.append(np.float(linesplit[7]))

   
    retdata=[datalons,datalats,data_5_7ka,data_9_11ka]
    return retdata

#=================================================================
# MAIN PROGRAM STARTS HERE

print('start of program')
seasname='jfmamjjasond'
#seasname='son'
if seasname =='djf':
    monthnames=['dc','ja','fb']
if seasname =='jja':
    monthnames=['jn','jl','ag']
if seasname =='mam':
    monthnames=['mr','ar','my']
if seasname =='son':
    monthnames=['sp','ot','nv']
if seasname =='jfmamjjasond':
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']

#retdata=extract_data('xlubh','iso_xlubha@pcr[3-6]*','7ka')


# xlubh 7ka
print('about to extract')
retdata=extract_data_seas('xlubh','xlubha@pdr[3-6]*','7ka',monthnames)
temp_7ka=retdata[0]
h2o_7ka=retdata[1]
lon=retdata[2]
lat=retdata[3]

print('got 7ka')

# xlubg 6ka
retdata=extract_data_seas('xlubg','xlubga@pdr[3-6]*','6ka',monthnames)
temp_6ka=retdata[0]
h2o_6ka=retdata[1]
lon=retdata[2]
lat=retdata[3]
print('got 6ka')


# xlubf 5ka
retdata=extract_data_seas('xlubf','xlubfa@pdr[3-6]*','7ka',monthnames)
temp_5ka=retdata[0]
h2o_5ka=retdata[1]
lon=retdata[2]
lat=retdata[3]
print('got 5ka')




# xlubj 9ka
retdata=extract_data_seas('xlubj','xlubja@pdr[3-6]*','9ka',monthnames)
print('julia',retdata[0])
temp_9ka=retdata[0]
h2o_9ka=retdata[1]


# xlubk 10ka
retdata=extract_data_seas('xlubk','xlubka@pdr[3-6]*','10ka',monthnames)
temp_10ka=retdata[0]
h2o_10ka=retdata[1]


# xlubl 11ka
retdata=extract_data_seas('xlubl','xlubla@pdr[3-6]*','11ka',monthnames)
temp_11ka=retdata[0]
h2o_11ka=retdata[1]


# get average temp - weighted by amount

temp_5_7ka_wt=((temp_7ka * h2o_7ka) +  (temp_6ka * h2o_6ka)+  (temp_5ka * h2o_5ka))/(h2o_7ka+h2o_6ka+h2o_5ka)
temp_9_11ka_wt=((temp_9ka * h2o_9ka) +  (temp_10ka * h2o_10ka) + (temp_11ka * h2o_11ka))/(h2o_9ka+h2o_10ka+h2o_11ka)

h2o_5_7ka=(h2o_5ka+h2o_6ka+h2o_7ka)*60.*60.*24. / 3.0
h2o_9_11ka=(h2o_9ka+h2o_10ka+h2o_11ka)*60.*60.*24. / 3.0

# plot
titlename='temp anomaly (9-11ka - 5-7ka)'
cbartitle='deg C'
print(temp_9_11ka_wt-temp_5_7ka_wt)
plotdata((temp_9_11ka_wt-temp_5_7ka_wt),0,lon,lat,titlename,cbartitle,-5.0,5.5,0.5)

titlename='h2o  (9-11ka)'
cbartitle='mm/day'
print(h2o_9_11ka-h2o_5_7ka)
plotdata((h2o_9_11ka),3,lon,lat,titlename,cbartitle,0.0,5.0,0.5)

titlename='h2o anom (9-11ka - 5-7ka)'
cbartitle='mm/day'
print(h2o_9_11ka-h2o_5_7ka)
plotdata((h2o_9_11ka-h2o_5_7ka),1,lon,lat,titlename,cbartitle,-0.5,0.6,0.1)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_temp/North_atl_9-11ka_5-7ka_'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight') 
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_temp/North_atl_9-11ka_5-7ka_'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')   
sys.exit(0)

# end here unless you want to overplot data
print(np.shape(temp_5_7ka), np.shape(temp_9_11ka))


# get non weighted temperature
temp_5_7ka=(temp_7ka+temp_6ka+temp_5ka)/3.
temp_9_11ka=(temp_9ka+temp_10ka+temp_11ka)/3.

# get Jonathans data
filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/Jonathan_new_data.csv'
lakedata=get_Jonathan_data(filename)
lakedatalons=lakedata[0]
lakedatalats=lakedata[1]
lakedata_5_7ka=lakedata[2]
lakedata_9_11ka=lakedata[3]

filename='/home/users/jctindall/programs/PYTHON/NorthAtl/data/speleothem_data.csv'
speleodata=get_Jonathan_data(filename)
speleodatalons=speleodata[0]
speleodatalats=speleodata[1]
speleodata_5_7ka=speleodata[2]
speleodata_9_11ka=speleodata[3]


h2o_5_7ka=(h2o_7ka+h2o_6ka+h2o_5ka)/3.0
h2o_9_11ka=(h2o_9ka+h2o_10ka+h2o_11ka)/3.0


#plotdata(temp_9_11ka,0,lon,lat,'9-11ka temp','degC',-12,-6,0.5)
#plotdata(h2o_9_11ka*60.*60.*24.*30.,1,lon,lat,'9-11ka h2o','mm/month',0,100,10)

#anom_h2o=h2o_9_11ka - h2o_5_7ka
#titlename='precip 9-11ka - 5-7ka'
#cbartitle='mm/month'
#plotdata(anom_h2o*60.*60.*24.*30,3,lon,lat,titlename,cbartitle,-10,10,1)

#print(np.shape(temp_5_7ka), np.shape(temp_9_11ka))

anom_temp=temp_9_11ka - temp_5_7ka
plt.show()
sys.exit(0)


print(np.shape(lakedata_5_7ka), np.shape(lakedata_9_11ka))
anom_lakes_temp=np.asarray(lakedata_9_11ka) - np.asarray(lakedata_5_7ka)
anom_speleo_temp=np.asarray(speleodata_9_11ka) - np.asarray(speleodata_5_7ka)

titlename='temp 9-11ka - 5-7ka '+seasname
cbartitle='permille'
#plotdata(anom_temp,2,lon,lat,titlename,cbartitle,-6,2,0.5)
plotdata(anom_temp,2,lon,lat,titlename,cbartitle,-2,1,0.2)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_temp/North_atl_9-11ka_5-7ka_'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_temp/North_atl_9-11ka_5-7ka_'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()

plotdata(anom_temp,99,lon,lat,titlename,cbartitle,-2.0,2.0,0.25)
oplotdata(anom_lakes_temp,lakedatalons,lakedatalats,-2.0,2.0,0.25,'l')
oplotdata(anom_speleo_temp,speleodatalons,speleodatalats,-2.0,2.0,0.25,'s')

print('lake',lakedatalons)
print('speleo',speleodatalons)


fileout='/home/users/jctindall/plots/python/NorthAtl/plot_temp/North_atl_9-11ka_5-7ka_temp_data'+seasname+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/home/users/jctindall/plots/python/NorthAtl/plot_temp/North_atl_9-11ka_5-7ka_temp _data'+seasname+'.png' 
plt.savefig(fileout, bbox_inches='tight')  
plt.close()



sys.exit()
::::::::::::::
MITgcm/BOMMS/plot_ocn_currents.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_OCN_CURRENTS
#PURPOSE
#    This program will plot the ocean currents for Steve Hunters BOMMS output
#
# search for 'main program' to find end of functions
# Julia 04/11/2020


import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
from mpl_toolkits.basemap import Basemap, shiftgrid
import iris
import iris.plot as iplt

sys.path.append('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/COMMON')
from jumaps import contourglobe


#functions are:
#  def plotquiver

# from the program on jasmin.
 #fig = plt.figure(figsize=[12.8, 9.6])
#    fig = plt.figure(figsize=[25, 20])
#    for i, ucube in enumerate(u_cubes):
#        vcube = v_cubes[i]
#        
#        subplotno = np.int('43' + np.str(i+1))
#        ax = fig.add_subplot(subplotno)
#        
#        map = Basemap(llcrnrlon=-20.0, urcrnrlon=20.0,
#                      llcrnrlat=30.0, urcrnrlat=70.0,
#                      projection='cyl', resolution='l')
#
#        latitudes = ucube.coord('latitude').points
#        longitudes = ucube.coord('longitude').points
#        
#        u_shift, lons_shift = shiftgrid(
#                180, ucube.data, longitudes, start=False)##
#
#        v_shift, lons_shift = shiftgrid(
#                180, vcube.data, longitudes, start=False)
        
#        lons, lats = np.meshgrid(lons_shift, latitudes)
#        x, y = map(lons, lats)
#        map.drawcoastlines()
       

#        if i == 0: 
#           titlename = SLICES[i]
#           scalesize = 200
#        else:
#           scalesize = 50
#           titlename = SLICES[i]  + '-' + SLICES[0]
#        if lev == '850':
#            scalesize = scalesize/3.0
            
#        n=1 # plot every nth arrow
#        Q = map.quiver(x[::n, ::n], y[::n, ::n], 
#                       u_shift[::n, ::n], v_shift[::n, ::n], scale=scalesize,
#                       headwidth=7, headlength=9)
#        plt.title(titlename, fontsize=30, loc='left')
#        qk = ax.quiverkey(Q, 0.7, 1.05, 10, ' 1 m/s', labelpos='E',
#                          fontproperties={'size':30})
       
       

# functions start here
def plotquiver(udata,vdata,lon,lat,titlename, fig, i, maskcube):


    lons, lats = np.meshgrid(lon,lat)
    subplotno = np.int('23' + np.str(i))
    ax = fig.add_subplot(subplotno)
      
    map=Basemap(projection='npstere', resolution='c', lon_0=0, 
                boundinglat=20.0,round=True,lat_0=90)
  
    x, y = map(lons, lats)
   
    n=19 # plot every nth arrow
    scalesize=2
    print(maskcube.shape)
    print(np.shape(udata))
    sys.exit(0)
    map.contourf(x, y, maskcube.data)
    
   # Q = map.quiver(x[::n, ::n], y[::n, ::n], 
   #                udata[::n, ::n], vdata[::n, ::n], scale=scalesize,
   #                headwidth=7, headlength=9)
   # plt.title(titlename, fontsize=30, loc='left')
   # map.drawparallels(np.arange(0., 80., 20.))
   # qk = ax.quiverkey(Q, 0.7, 1.05, 0.1, ' 1 cm/s', labelpos='E',
   #                       fontproperties={'size':30})
    
#end def plotquiver





#enddef zonalmean_ann_height
def plot_UV(Ucube_top, Vcube_top):
    """
    extracts U and V for each month and plots
    """

    x = Ucube_top.coord('longitude').points
    y = Ucube_top.coord('latitude').points
    monthnames = ['Jan','Feb','Mar','Apr','May','Jun']
    fig = plt.figure(figsize=[25, 20])
    maskcube = iris.load_cube(MASK, 'dem_0.5x0.5')
    print(maskcube)
    print(maskcube.transpose)
    maskcube2 = maskcube.transpose
    print(maskcube2)
    sys.exit(0)

    for month in range(1,7):
        Ucube = Ucube_top.extract(iris.Constraint(month=month))
        print(Ucube)
        Vcube = Vcube_top.extract(iris.Constraint(month=month))
         
        u = Ucube.data
        v = Vcube.data
        
       
        plotquiver(u, v, x, y, monthnames[month-1], fig, month, maskcube)

    fileout = ('/nfs/hera1/earjcti/MITgcm/processed_results/pyplots/' 
               + MAP + '_UV0_lev_' + np.str(LEVEL) + '.eps') 
    plt.savefig(fileout)
        
        


def main():
    """
    main loop
    """

    Ucube_all = iris.load_cube(FILENAME, 'U')
    Vcube_all = iris.load_cube(FILENAME, 'V')
   
    Ucube_top = Ucube_all.extract(iris.Constraint(depth=-5.0))
    Vcube_top = Vcube_all.extract(iris.Constraint(depth=-5.0))

    plot_UV(Ucube_top, Vcube_top)
    
################################
# main program

MAP = 'Map28a4x'
if MAP == 'Map28a4x':
    MASK = '/nfs/hera1/earjcti/MITgcm/Matlab/Map28a_EK_Barremian_125_DEM.nc'
LEVEL=-5.0
#LEVEL=-45.0 #
#LEVEL=-154.47001

FILENAME = ('/nfs/hera1/earjcti/MITgcm/processed_results/' + MAP
           + '_ARC4_48PE_50L_2kyr_UV_Monthly_720360.nc')
main()

::::::::::::::
MITgcm/TESTING/MITGCM_plotinput.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Spyder Editor

This will attempt to plot the input from a MITGCM global ocean cube sphere simulation
"""

import xarray as xr
import numpy as np
import xgcm
import xmitgcm
#import intake
from matplotlib import pyplot as plt
import cartopy.crs as ccrs


import netCDF4
import matplotlib as mpl
import MITgcmutils as mit
from MITgcmutils import cs as mitcs
import iris
import iris.plot as iplt
import sys
import os
from iris.experimental.equalise_cubes import equalise_attributes

plt.rcParams['figure.figsize'] = (10,6)
#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap



if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

def plot_input_Tsurf():
    """
    read in all the faces and plot to map
    """
    
    
    raw = np.fromfile(FILESTART + 
                      'input/lev_surfT_cs_12m.bin', dtype='>f')
    print('surfT SIZE',np.shape(raw))
    surfT = np.reshape(raw, (12, 32, 192, 2))
    print(surfT[0,:,:,0])

    # read in all of the coordinates
    os.chdir(FILESTART + 'run')
    
    ntiles = 12
    for tile in range(1,ntiles+1):
        XCname = 'XC.0' + np.str(tile).zfill(2) + '.001'
        YCname = 'YC.0' + np.str(tile).zfill(2) + '.001'

        XC = mit.rdmds(XCname)
        YC = mit.rdmds(YCname)

        if tile == 1:
            nx,ny = np.shape(XC)
            allXC = np.ma.zeros((ntiles, nx, ny))
            allYC = np.ma.zeros((ntiles, nx, ny))
            allTsurf = np.ma.zeros((ntiles, nx, ny))

        allXC[tile-1, :, :] = XC
        allYC[tile-1, :, :] = YC

    
    allXCma = np.ma.masked_where(allXC==0.0, allXC)
    allYCma = np.ma.masked_where(allYC==0.0, allYC)
    for tile in range(0,ntiles):
        print(np.shape(surfT))
        print(np.shape(allTsurf))
        print(np.shape(allXC))
        print(tile)

        allTsurf[tile,:,:] = np.ma.masked_where(allXC[tile,:,:]==0.0, surfT[6,:,:,0]) 
    Tsurf1ma_2 = np.ma.masked_where(allXC[0,:,:]==0.0, surfT[6,:,:,0])     

    print('j1',np.shape(allTsurf[0,:,:]))
    print('j2',np.shape(Tsurf1ma_2))
   
    XC = mit.rdmds('XC')
    YC = mit.rdmds('YC')
   

    #plt.plot(XC[0,:])
    #for i in range(0,12):
    #    plt.plot(allXCma[i,0,:])
    #plt.plot(YC[0,:])
    #plt.show()
    #sys.exit(0)
    # plot surfT on a partial grid
    #print(surfT[0,:,:])

    cs=plt.contourf(surfT[0,:,:,0])

    vals = np.arange(-5000,5000,100)
#    for i in range(0,ntiles):
#        cs = plt.contourf(allXCma[i,:,:], allYCma[i,:,:], allTsurf[i,:,:], 
#                          levels=vals, latlon=True, extend='both')
         
    cbar = plt.colorbar(cs)
    plt.show()
    sys.exit(0)
    
def plot_input_faces():
    """ 
    trying to plot input faces
    """


    raw = np.fromfile(FILESTART + 
                      'input/grid_cs32.face001.bin', dtype='>f')
    raw2 = np.reshape(raw, (18, 33, 33,2))
    cs = plt.contourf(raw2[1,:,:,0])
    cbar = plt.colorbar(cs)
    plt.show()
    sys.exit(0)
    print('FACE2 SIZE',np.shape(raw))
    
    #plt.plot(raw)
    #plt.show()
    #sys.exit(0)
    
    print('raw',raw[0:100])
    print('raw2',raw2[0:100])
    print('diff',raw2[0:100] - raw[0:100])

    os.chdir(FILESTART + 'run')
    XCname = 'XC.001.001'
    XC = mit.rdmds(XCname)
    print(XC)
    print(np.shape(XC))
    print(np.min(XC), np.max(XC))
    minxc=np.min(XC)
    maxxc=np.max(XC)
    #for i, val in enumerate(raw):
        #if minxc < val < maxxc:
            #if (val in XC):
            #    print(i, val)
            #if val == minxc:
            #    print('foundmin',i)
            #    sys.exit(0)
    plt.plot(raw)
    plt.ylim(minxc,maxxc)
    plt.xlim(minxc,100)
    plt.show()
            
    sys.exit(0)

#    XCsize = 
#xC yC dxF dyF rA xG yG dxV dyU rAz dxC dyC rAw rAs dxG dyG AngleCS AngleSN

#guess XC, YC dxf, dyf, rA anglecs, anglesn=32*32 *7 = 7168 (halo = 
#guess Xg, YG dxV, dyU, raZ= 33*17*2 *5 = 5610
#dxc raw dxg = 33*16*2*3 = 3168
#dyC ras dyg= 32*17*2*3 = 3264
#total = 19210 (should be 19602) extra = 392
# 39204 / 18(number of fields = 2178 / 33 = 66
# how about reshaping as (18, 33, 33, 2)


    raw = np.fromfile(FILESTART + 
                      'input/grid_cs32.face003.bin', dtype='>f')
    print('FACE3 SIZE',np.shape(raw))

    raw = np.fromfile(FILESTART + 
                      'input/grid_cs32.face004.bin', dtype='>f')
    print('FACE4 SIZE',np.shape(raw))

    raw = np.fromfile(FILESTART + 
                      'input/grid_cs32.face005.bin', dtype='>f')
    print('FACE5 SIZE',np.shape(raw))


    raw = np.fromfile(FILESTART + 
                      'input/grid_cs32.face006.bin', dtype='>f')
    print('FACE6 SIZE',np.shape(raw))

    

def plot_input_bathy():

    # get bathymetry file
    raw = np.fromfile(FILESTART + 
                      'input/bathy_cs32.bin', dtype='>f')
    print(np.shape(raw))
    bathy = np.reshape(raw, (32, 6,32,2))


    # get face001
    raw = np.fromfile(FILESTART + 'input/grid_cs32.face001.bin')
    #raw = np.fromfile(FILESTART + 'input/proj_cs32_2uEvN.bin')
    #proj = np.reshape(raw, (32,6,32,2))
    #print(proj)
    print(np.shape(raw))
    plt.plot(raw)
    plt.show()
    sys.exit(0)
    
    ax1=plt.subplot(3,2,1)
    cs=ax1.contourf(bathy[:, 0,:,0])
    cbar = plt.colorbar(cs, extend='both')

    ax2=plt.subplot(3,2,2)
    cs2=ax2.contourf(bathy[:, 1,:,0])
    cbar = plt.colorbar(cs2, extend='both')
   
    ax3=plt.subplot(3,2,3)
    cs3=ax3.contourf(bathy[:, 2,:,0])
    cbar = plt.colorbar(cs3, extend='both')

    ax4=plt.subplot(3,2,4)
    cs4=ax4.contourf(bathy[:, 3,:,0])
    cbar = plt.colorbar(cs4, extend='both')
    
    ax5=plt.subplot(3,2,5)
    cs5=ax5.contourf(bathy[:, 4,:,0])
    cbar = plt.colorbar(cs5, extend='both')

    ax6=plt.subplot(3,2,6)
    cs6=ax6.contourf(bathy[:, 5,:,0])
    cbar = plt.colorbar(cs6, extend='both')
    plt.show()
    
    #print(np.shape(bathy))
    #print(bathy)
    #lsm = np.where(bathy < 0.0, 1.0, 0.0)

   


linux_win = 'L'

if linux_win == 'L':
    FILESTART = '/nfs/hera1/earjcti/MITgcm/mysetups/global_ocean.cs32x15/'
else:
    FILESTART = "C:\\Users\\julia\\OneDrive\\WORK\\DATA\\MITgcm\\data"
NFACES=6
    
#plot_input_Tsurf()
plot_input_faces()
#plot_input_bathy()

::::::::::::::
MITgcm/TESTING/MITGCM_plotoutput.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

import xarray as xr
import numpy as np
import xgcm
#import xmitgcm
#import intake
from matplotlib import pyplot as plt
import cartopy.crs as ccrs


import netCDF4
import matplotlib as mpl
import MITgcmutils as mit
from MITgcmutils import cs as mitcs
import iris
import iris.plot as iplt
import sys
import os
from iris.experimental.equalise_cubes import equalise_attributes

plt.rcParams['figure.figsize'] = (10,6)
#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap



if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")


def try_use_xgcm():
    """
    currently works for plotting a single file
    """
    
    # plots a single tile
    fname = (FILESTART + "grid.t001.nc")
    
    dsg = xr.open_dataset(fname)
    print(dsg.coords)
    print(dsg.exch2_nNeighbours)
    print(dsg.XC)

   
    print('check dims',dsg.X)
    grid = xgcm.Grid(dsg, coords={'X': {'center': 'X'}, 
                              'Y': {'center': 'Y'}, 
                              'Z': {'center': 'Z'}}, periodic=['X','Y'])

    print('grid is',grid)
    

    fname = (FILESTART + "dynDiag.0000072000.t001.nc")
    
    dsf = xr.open_dataset(fname)
    ds1 =  xr.DataArray(dsf.THETA[0,0,:,:],  dims=['y', 'x'],
                    coords = {'lat': (('y', 'x'), dsg.YC),
                           'lon': (('y', 'x'), dsg.XC)})
    # 

    #ax = plt.subplot(projection=ccrs.PlateCarree())
    #ds1.plot.contourf('lon','lat', ax=ax)
    #ax.coastlines(); ax.gridlines(draw_labels=True);
    
    # open all the datasets and add face as a dimsion
    
    for i in range(1,5):
        dsg = xr.open_dataset(FILESTART + "grid.t00"+np.str(i)+".nc")
        dsf = xr.open_dataset(FILESTART + "dynDiag.0000072000.t00"+np.str(i) + ".nc")
    
    
        ds =  xr.DataArray(dsf.THETA[0,0,:,:],  dims=['y', 'x'],
                    coords = {'lat': (('y', 'x'), dsg.YC),
                           'lon': (('y', 'x'), dsg.XC)})
                                                      
        expanded  = ds.expand_dims('Face')
       
       
        print(dsg.exch2_myFace)
        print(expanded)
        
    sys.exit(0)
        
    
    
    # this is trying to open lots of datasets
    
    dsg2= xr.open_mfdataset("C:\\Users\\julia\\OneDrive\\WORK\\DATA\\" +
        "MITgcm\\grid.t*[1-4].nc", concat_dim = 'Face')
    print('dsg2',dsg2)
    sys.exit(0)
    
    
    dsf2 = xr.open_mfdataset("C:\\Users\\julia\\OneDrive\\WORK\\DATA\\" +
                 "MITgcm\\dynDiag.0000072000.t0*[1-4].nc", concat_dim = 'Face')
    
    #dsf2.THETA[2,0,0,:,:].plot.contourf()
    
    ds2 =  xr.DataArray(dsf2.THETA[:, 0, 0, :, :],  dims=['Face','y', 'x'],
                    coords = {'lat': (('Face','y', 'x'), dsg2.YC),
                           'lon': (('Face','y', 'x'), dsg2.XC)})
    
    
    ax1 = plt.subplot(2, 2, 1, projection=ccrs.PlateCarree())
    ds2[1,:,:].plot.contourf('lon','lat', ax=ax1)
    ax1.coastlines(); ax1.gridlines(draw_labels=True)
    
    ax2 = plt.subplot(2, 2, 2, projection=ccrs.PlateCarree())
    ds2[2,:,:].plot.contourf('lon','lat', ax=ax2)
    ax2.coastlines(); ax2.gridlines(draw_labels=True)
    
    ax3 = plt.subplot(2, 2, 3, projection=ccrs.PlateCarree())
    ds2[3,:,:].plot.contourf('lon','lat', ax=ax3)
    ax3.coastlines(); ax3.gridlines(draw_labels=True)
    
    ax4 = plt.subplot(2, 2, 4, projection=ccrs.PlateCarree())
    ds2[4,:,:].plot.contourf('lon','lat', ax=ax4)
    ax4.coastlines(); ax4.gridlines(draw_labels=True)
 
def try_plot_netcdf_iris():
    """
    This seems to work okay but the coordinate system is very hard to merge
    the cubes
    """

   
    
    level=0
    # try and plot this seems to be working
    fig = plt.figure();
    mp = Basemap(projection='moll',lon_0 = 0.,
             resolution = 'l')
    for gridno in range(1,13):
        filechar = np.str(gridno).zfill(2)
        grid = (FILESTART + "/grid.t0" + filechar + ".nc")

        XCcube = iris.load_cube(grid,'XC')
        YCcube = iris.load_cube(grid,'YC')
        depthcube = iris.load_cube(grid,'Depth')
        
        fname = (FILESTART + "/dynDiag.0000072000.t0" + filechar + ".nc")
        
   
        THETAcube = iris.load_cube(fname,'THETA')
        temp=THETAcube.data[0, level, :, :]
       
        depthdata=depthcube.data
        thetadata = np.ma.masked_where(depthdata==0., temp)
        
        if gridno == 11:
            
            newdata = np.where(XCcube.data < 0., XCcube.data + 360., XCcube.data)
            newdata2 = np.where(XCcube.data > 0., XCcube.data + 360., XCcube.data)
           
            XCcube.data = newdata
            XCcube11 = XCcube.copy(data=newdata2)
            
        
        
        valmin=-5.0
        valmax=35.
        valdiff=1.0
        mp.contourf(XCcube.data,YCcube.data, thetadata, 
                     levels = np.arange(valmin,valmax,valdiff), latlon=True)
        if gridno == 11:
            mp.contourf(XCcube11.data,YCcube.data, thetadata, 
                     levels = np.arange(valmin,valmax,valdiff), latlon=True)
        
    plt.colorbar()
    plt.show()


    
def try_plot_glue():
    """
    try and plot the data that has been glued together
    DOes not work very well because everything has been glued together badly
    It does not work either if we plot things in iris
    """
    

    gridfile = (FILESTART + "grid.nc")
    
    
    file2read = netCDF4.Dataset(gridfile, 'r')
    
    xc = file2read.variables['XC'][:, :]
    xg = file2read.variables['XG'][:, :]
    yc = file2read.variables['YC'][:, :]
    yg = file2read.variables['YG'][:, :]
    zc = file2read.variables['RC'][:]
    zf = file2read.variables['RF'][:]

    print(xc)
    print(np.shape(xc))
    print(np.shape(yc))
    #print(yc)
    
    mp = Basemap(projection='moll',lon_0 = 180.,
             resolution = 'l')

    plt.clf()
    print(mit)
    h = mitcs.pcol(xc, yc, xc, projection = mp)
    #mp.fillcontinents(color = 'grey')
    #mp.drawmapboundary()
    mp.drawmeridians(np.arange(0, 360, 30))
    mp.drawparallels(np.arange(-90, 90, 30))
    
  
    plt.show()
    plt.close()
    sys.exit(0)

        # for derivatives and integrals
   

    fname2 = ("C:\\Users\\julia\\OneDrive\\WORK\\DATA\\" +
        "MITgcm\\dynDiag.0000072000.t*nc")
        
    
    nc = mit.mnc_files(fname2, fpatt=fname)
    print(nc)
    XC=nc.variables['XC'][:]
    YC=nc.variables['YC'][:]
    nc2 = mit.mnc_files(fname2)
    THETA = nc2.variables['THETA'][:]
        
    print('j')
    print('j1',nc)
    print('j2',XC)
    print('j3',np.shape(XC))
    print('j4',np.shape(THETA[0,0,:,:]))

    nc.close()
    nc2.close()

def try_plot_xmitgcm():
    """
    try and plot with xmitgcm
    """
    fgrid = ("C:\\Users\\julia\\OneDrive\\WORK\\DATA\\" +
        "MITgcm\\grid")
    
    cs32_extra_metadata = xmitgcm.utils.get_extra_metadata(domain='cs', nx=32)

    # Then we read the grid from the input files
    # cs is cube sphere
    grid = xmitgcm.utils.get_grid_from_input(fgrid + 'grid_cs32.face001',
                                         geometry='cs',
                                         extra_metadata=cs32_extra_metadata)
    print(grid)

    fdata = ("C:\\Users\\julia\\OneDrive\\WORK\\DATA\\" +
        "MITgcm\\data")
    
    print('here')

    ds = xmitgcm.open_mdsdataset(fdata, fgrid, geometry='cs',
                             prefix=['Eta'],
                             iters='all',
                             delta_t=3600,
                             ref_date='1979-01-01 00:00:00',
                             swap_dims = False,
                             llc_method = 'smallchunks')
    print('julia')
    print(ds)
#

linux_win = 'L'

if linux_win == 'L':
    FILESTART = '/nfs/hera1/earjcti/MITgcm/mysetups/global_ocean.cs32x15/run/mnc_out_0002/'
else:
    FILESTART = "C:\\Users\\julia\\OneDrive\\WORK\\DATA\\MITgcm\\data"
    
try_plot_netcdf_iris() # works sort of
#try_plot_glue()
#try_plot_xmitgcm() # would be okay for llc but cube-sphere cs geometry not supported yet
#try_use_xgcm()
::::::::::::::
MITgcm/TESTING/test.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jan 30 09:57:27 2020
testing mitgcm

@author: earjcti
"""

import os
from MITgcmutils import mds
import matplotlib.pyplot as plt
import numpy as np



def plot_output():
    XC = mds.rdmds('XC')
    YC = mds.rdmds('YC')
    Tend = mds.rdmds('T', 3600)
    Tmid = mds.rdmds('T', 3420)
    print(XC)
    print(np.shape(XC))
    print(np.shape(Tend))
    print(Tend[0,:,:])
    plt.contourf(XC, YC, Tend[0,:,:], cmap='RdBu_r')
    plt.colorbar()
    plt.show()
    
def plot_input():
     #MITgcm likes its binary in big endian, float ('>f') or double ('>d')
    
    raw = np.fromfile('input/bathymetry.bin', dtype='>f')
    print(np.shape(raw))
    bathy = np.reshape(raw, (40,90))
    
    print(np.shape(bathy))
    print(bathy)
    lsm = np.where(bathy < 0.0, 1.0, 0.0)

    os.chdir('run')
    XC = mds.rdmds('XC')
    YC = mds.rdmds('YC')
        

    cs = plt.contourf(XC, YC, lsm)
    cbar = plt.colorbar(cs)
    plt.show()

    # Temp has 12 time records, 15 vertical levels, 40 in lat, 90 in lon
    #temp = np.reshape(raw, (12,15,40,90))
    
    
os.chdir('/nfs/hera1/earjcti/MITgcm/verification/tutorial_global_oce_latlon/')
plot_input()
::::::::::::::
PLIOMIP3/boundary_conditions/EOI400_basic_bc.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created May 2022 by Julia

This program will produce the boundary conditions for the 
PlioMIP3_basic_Eoi400 experiment.

The topo will be as for P4 but will be modern over
Canadian Archeapelego
Bering Straight
Those shelves near Australia

"""

import numpy as np
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import matplotlib.pyplot as plt
import netCDF4
import sys


def get_files(fileend, pi_fieldname, plio_fieldname):
    """
    gets the pliocene and the preindustrial files
    """

    filestart = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/'     
    pi_filename = 'Modern_std/Modern_std/Modern_std_' + fileend + '.nc'
    plio_filename = 'Plio_enh/Plio_enh/Plio_enh_' + fileend + '.nc'

    pi_cube = iris.load_cube(filestart + pi_filename,pi_fieldname)
    plio_cube = iris.load_cube(filestart + plio_filename,plio_fieldname)

    return plio_cube, pi_cube

def change_bering_strait(plio_cube, pi_cube):
    """
    inputs: pi and pliocene_core topography
    output: pliocene topography with bering straight changed back to pi

    """

    lats = plio_cube.coord('latitude').points
    lons = plio_cube.coord('longitude').points
    newpliodata = np.zeros((len(lats),len(lons)))
   
    plio_data = plio_cube.data
    pi_data = pi_cube.data

    count=0
    for j, lat in enumerate(lats):
        for i, lon in enumerate(lons):
            if (50 < lat < 80 and lon < -150
                and (plio_data[j,i] > 0 and pi_data[j,i] < 0.0)):
                count=count+1
                newpliodata[j,i] = pi_data[j,i]
            else:
                newpliodata[j,i] = plio_data[j,i]    
    

    new_plio_cube = plio_cube.copy(data = newpliodata)

    plt.subplot(3,3,1)
    plio_reg_cube = plio_cube.extract(iris.Constraint(latitude = lambda cell: 50 < cell < 80, longitude=lambda cell: cell < -150))
    V = np.arange(-5, 1, 6)
    cs = iplt.pcolormesh(plio_reg_cube, vmin=-5, vmax=5, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('plio1')
    

    plt.subplot(3,3,2)
    pi_reg_cube = pi_cube.extract(iris.Constraint(latitude = lambda cell: 50 < cell < 80, longitude=lambda cell: cell < -150))
    V = np.arange(-5, 6, 1)  
    cs = iplt.pcolormesh(pi_reg_cube, vmin=-5, vmax=5, cmap='RdBu_r')
    plt.title('pi1')
 

    plt.subplot(3,3,3)
    newplio_reg_cube = new_plio_cube.extract(iris.Constraint(latitude = lambda cell: 50 < cell < 80, longitude=lambda cell: cell < -150))
    V = np.arange(-5, 6, 1)  
    cs = iplt.pcolormesh(newplio_reg_cube, vmin=-5, vmax=5, cmap='RdBu_r')
    plt.title('newplio')
 

    plt.subplot(3,3,4)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(plio_reg_cube, vmin=-500, vmax=500, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('plio2')
    

    plt.subplot(3,3,5)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(pi_reg_cube, vmin=-500, vmax=500, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('pi2')
   
    plt.subplot(3,3,6)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(newplio_reg_cube, vmin=-500, vmax=500,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio')
   
    plt.subplot(3,3,7)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(plio_reg_cube - pi_reg_cube, vmin=-500, vmax=500,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('orig diff')
   
   
    plt.subplot(3,3,8)
    V = np.arange(-50, 60, 10)
    cs = iplt.pcolormesh(newplio_reg_cube - plio_reg_cube,  vmin=-500, vmax=500,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio - old plio')
  

    plt.subplot(3,3,9)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(newplio_reg_cube - pi_reg_cube,  vmin=-500, vmax=500,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio - pi')
    
    return new_plio_cube


def change_canadian_arctic(plio_cube, pi_cube):
    """
    inputs: pi and pliocene topography
    output: pliocene topography with CAA changed back to pi

    """

    lats = plio_cube.coord('latitude').points
    lons = plio_cube.coord('longitude').points
    newpliodata = np.zeros((len(lats),len(lons)))
   
    plio_data = plio_cube.data
    pi_data = pi_cube.data

    count=0
    for j, lat in enumerate(lats):
        for i, lon in enumerate(lons):
            if (lat > 75 and  -75 < lon < -60
                and (plio_data[j,i] > 0.0 and pi_data[j,i] < 150.0)):
                count=count+1
                newpliodata[j,i] = min([pi_data[j,i], -10.0])
            else:
                newpliodata[j,i] = plio_data[j,i]  
           # if lat == 80.5 or lon ==-65.5:
           #     newpliodata[j,i] = 10000.
            if lat == 80.5 and (lon ==-65.5 or lon ==-64.5):
                newpliodata[j,i] = -10.0
            
     
  
    new_plio_cube = plio_cube.copy(data = newpliodata)

    plt.subplot(3,3,1)
    plio_reg_cube = plio_cube.extract(iris.Constraint(latitude = lambda cell: 60 < cell < 90, longitude=lambda cell: -130 < cell < -50))
    V = np.arange(-5, 6, 1)
    cs = iplt.pcolormesh(plio_reg_cube, vmin=-5, vmax=5, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('plio1')
    

    plt.subplot(3,3,2)
    pi_reg_cube = pi_cube.extract(iris.Constraint(latitude = lambda cell: 60 < cell < 90, longitude=lambda cell: -130 < cell < -50))
    V = np.arange(-5, 6, 1)  
    cs = iplt.pcolormesh(pi_reg_cube,  vmin=-5, vmax=5, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('pi1')
 

    plt.subplot(3,3,3)
    newplio_reg_cube = new_plio_cube.extract(iris.Constraint(latitude = lambda cell: 60 < cell < 90, longitude=lambda cell: -130 < cell < -50))
    V = np.arange(-5, 6, 1)  
    cs = iplt.pcolormesh(newplio_reg_cube,  vmin=-5, vmax=5, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio')
 

    plt.subplot(3,3,4)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(plio_reg_cube,  vmin=-500, vmax=500, cmap='RdBu_r') 
    plt.gca().coastlines()
    plt.title('plio2')
    

    plt.subplot(3,3,5)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(pi_reg_cube,  vmin=-500, vmax=500, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('pi2')
   
    plt.subplot(3,3,6)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(newplio_reg_cube,  vmin=-500, vmax=500, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio')
   
    plt.subplot(3,3,7)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(plio_reg_cube - pi_reg_cube,  vmin=-500, vmax=500, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('orig diff')
   
   
    plt.subplot(3,3,8)
    V = np.arange(-50, 60, 10)
    cs = iplt.pcolormesh(newplio_reg_cube - plio_reg_cube,  vmin=-5, vmax=5, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio - old plio')
  

    plt.subplot(3,3,9)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(newplio_reg_cube - pi_reg_cube,  vmin=-500, vmax=500, cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title('newplio - pi')
    
    plt.show()
    sys.exit(0)
    return new_plio_cube


def change_australian_shelves(plio_cube, pi_cube):
    """
    inputs: pi and pliocene topography
    output: pliocene topography with shelves near Australia changed back to pi

    """

    lats = plio_cube.coord('latitude').points
    lons = plio_cube.coord('longitude').points
    newpliodata = np.zeros((len(lats),len(lons)))
   
    plio_data = plio_cube.data
    pi_data = pi_cube.data

    count=0
    for j, lat in enumerate(lats):
        for i, lon in enumerate(lons):
            if (-30 < lat < 30 and  90 < lon < 180
                and (plio_data[j,i] > 0 and pi_data[j,i] < 0.0)):
                count=count+1
                newpliodata[j,i] = pi_data[j,i]
            else:
                newpliodata[j,i] = plio_data[j,i]    
    

    new_plio_cube = plio_cube.copy(data = newpliodata)

    plt.subplot(3,3,1)
    plio_reg_cube = plio_cube.extract(iris.Constraint(latitude = lambda cell: -30 < cell < 30, longitude=lambda cell: 90 < cell < 180))
    V = np.arange(-5, 6, 1)
    cs = iplt.pcolormesh(plio_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('plio1')
    

    plt.subplot(3,3,2)
    pi_reg_cube = pi_cube.extract(iris.Constraint(latitude = lambda cell: -30 < cell < 30, longitude=lambda cell: 90 < cell < 180))
    V = np.arange(-5, 6, 1)  
    cs = iplt.pcolormesh(pi_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
   
    plt.title('pi1')
 

    plt.subplot(3,3,3)
    newplio_reg_cube = new_plio_cube.extract(iris.Constraint(latitude = lambda cell: -30 < cell < 30, longitude=lambda cell: 90 < cell < 180))
    V = np.arange(-5, 6, 1)  
    cs = iplt.pcolormesh(newplio_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('newplio')
 

    plt.subplot(3,3,4)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(plio_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('plio2')
    

    plt.subplot(3,3,5)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(pi_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('pi2')
   
    plt.subplot(3,3,6)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(newplio_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('newplio')
   
    plt.subplot(3,3,7)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(plio_reg_cube - pi_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('orig diff')
   
   
    plt.subplot(3,3,8)
    V = np.arange(-50, 60, 10)
    cs = iplt.pcolormesh(newplio_reg_cube - plio_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('newplio - old plio')
  

    plt.subplot(3,3,9)
    V = np.arange(-500, 600, 100)
    cs = iplt.pcolormesh(newplio_reg_cube - pi_reg_cube, levels=V, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title('newplio - pi')
    

    return new_plio_cube


###############################################
# main program


# get Pliocene and preindustrial files

pliocore_topo_cube, pi_topo_cube = get_files('topo_v1.0',
                                             'etopo1_topo','p4_topo')


# change bering strait 
new_plio_topo = change_bering_strait(pliocore_topo_cube, pi_topo_cube)

# change Canadian arctic archipelego
new_plio_topo2 = change_canadian_arctic(new_plio_topo, pi_topo_cube)

# change Australasian shelves
new_plio_topo = change_australian_shelves(new_plio_topo2, pi_topo_cube)

# plot new land sea mask
plt.subplot(1,1,1)
iplt.contour(pliocore_topo_cube, levels=(0.0))
plt.show()
::::::::::::::
PlioMIP_new/energybal/plot_energybal_for_lauren.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_ENERGYBAL
#PURPOSE
#    This program will plot the energy balance for the pliocene simulations
#
# search for 'main program' to find end of functions
# Julia 11/1/2018



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
import iris
import iris.plot as iplt
import iris.quickplot as qplt
from mpl_toolkits.basemap import Basemap, shiftgrid


#functions are:
#  def global_enbal
#  def seasmean


def global_enbal(exptid):
    """
    this looks to be a check that the temperature contribution from all the different sources is equal to the modelled temperature
    """

    def mean_data(filename):
        """
        gets the mean data from the file"
        """
        
        f = open(filename,"r")
        lines = f.readlines()
        avgs = lines[2]
        mean, sd = avgs.split(',')
       
        return mean

    #===================================================================
    # read in data from  average temperature files produced for Dan

    T_mean = np.float(mean_data(FILESTART + '/' + exptid + '.NearSurfaceTemperature.data.txt'))

    # get upward and downward sw radiation at the top of the atmosphere
    # remember alpha = sw_up_toa / sw_down_toa (rsut / rsdt)
    # incoming sw

    rsut_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rsut.data.txt'))
    rsdt_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rsdt.data.txt'))
    rsut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsut.allmean.nc')
    rsdt_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsdt.allmean.nc')


    # get terms for effctive longwave emissivity e = lw_up_toa / lw_up_surf
    # = rlut / rlus
    
    rlut_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rlut.data.txt'))
    rlut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlut.allmean.nc')

    if MODELNAME == 'HadCM3':
        # lw upward surface = lw_down_surf - netdown_surf I think.  
        rlds_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rlds.data.txt'))
        rlds_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlds.allmean.nc')
        flns_mean = np.float(mean_data(FILESTART + '/' + exptid + '.flns.data.txt'))
        flns_cube = iris.load_cube(FILESTART + '/' + exptid + '.flns.allmean.nc')
        rlus_mean = rlds_mean - flns_mean
        rlus_cube = rlds_cube - flns_cube
    else:
        rlus_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rlus.data.txt'))
        rlus_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlus.allmean.nc')

    # ====================================
    # get alpha and epsilon

    mean_alpha=rsut_mean/rsdt_mean
    alpha_cube = rsut_cube / rsdt_cube

    mean_epsilon=rlut_mean/rlus_mean
    epsilon_cube = rlut_cube / rlus_cube

  
    #============================================
    # calculate terms in equation
    So=1367 # solar constant
    sigma=5.67 * (10.0 ** (-8.))

    print('alphas',mean_alpha)
    print('epsilon',mean_epsilon)

    #t4=So / 4.0 * (1.0-alpha) / (epsilon * sigma)
    #t=t4 ** (1./4.)

    # calculate t_mean using average values of globe
    t4_mean=(So / 4.0) * (1.0-mean_alpha) / (mean_epsilon * sigma)
    t_mean_formula=t4_mean ** (1./4.)

    # calculate t_mean at every point of the globe and average afterwards
    t4_data=(So / 4.0) * (1.0-alpha_cube.data) / (epsilon_cube.data * sigma)
    t_mean_data=t4_data ** (1./4.)
    t_mean_cube = alpha_cube.copy(data=t_mean_data)

    t_mean_cube.coord('latitude').guess_bounds()
    t_mean_cube.coord('longitude').guess_bounds()
   
    grid_areas = iris.analysis.cartography.area_weights(t_mean_cube)
    t_mean_avg_cube = t_mean_cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)
    
    
    print('globvals',So/4.0,mean_alpha,mean_epsilon,sigma)



    print('    ')
    print(exptid)
    print('=========')
    print('t obs=',T_mean,' degC')
    print('t mean formula=',t_mean_formula-273.15,' degC')
    print('t mean from average=',t_mean_avg_cube.data - 273.15)

   
#end def global_enbal

### LAUREN START
def split_gge(whatever you want your input_cube to be called):
   """
   Lauren this is the new section you need to write.
   1. Read in the Pliocene topography and check it is on the correct grid
   2. Read in the Preindustrial topography and check it is on the correct grid
   3. delta_T_topo=(topo_plio-topo_pi) * (-5.5) / 1000.
   4. delta_T_gge_only = input_cube - deltaT_topo   
   5. return deltaT topo and deltaT gge only to the calling routing
   6, (In calling program change what you are plotting to plot ghg and orog seperately). 
   """

   


####LAUREN END
################################################
def dh_zonal_enbal(exptid):

   
    #==============
    # read in data from  average temperature files produced for Dan

    T_cube = iris.load_cube(FILESTART + '/' + exptid + '.NearSurfaceTemperature.allmean.nc')
 
    # get upward and downward sw radiation at top of atmos
    rsut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsut.allmean.nc')
    rsdt_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsdt.allmean.nc')
  
    # get upward sw radiation at toa clear sky (downwards same as rsdt)
    rsutcs_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsutcs.allmean.nc')
   # outgoing lw  (toa) rlut and clear sky rlutcs
    rlutcs_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlutcs.allmean.nc') 
    rlut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlut.allmean.nc')


    # outgoing lw  (surface) rlus and clear sky rluscs
    if MODELNAME == 'HadCM3':
        # lw upward surface = lw_down_surf - netdown_surf I think.  
        rlds_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlds.allmean.nc')
        flns_cube = iris.load_cube(FILESTART + '/' + exptid + '.flns.allmean.nc')
        rlus_cube = rlds_cube - flns_cube
    else:
        rlus_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlus.allmean.nc')
    if MODELNAME == 'COSMOS':
        rlus_cube = rlus_cube * -1.0
        rsut_cube = rsut_cube * -1.0

    # rluscs = tested using rlus
    # test using -1 * rldscs
    #rluscs_cube = iris.load_cube(FILESTART + '/' + exptid + '.rluscs.allmean.nc')
    rluscs_cube = rlus_cube
    
    # ====================================
    # get the zonal average fields


    alpha_cube = rsut_cube / rsdt_cube
    alpha_cube_cs = rsutcs_cube / rsdt_cube
    epsilon_cube = rlut_cube / rlus_cube
    epsilon_cube_cs = rlutcs_cube / rluscs_cube


    sigma=5.67 * (10.0 ** (-8.))

    #============================================
    # calculate terms in equation
    H_cube=(-1.0) * ((rsdt_cube - rsut_cube) -(rlut_cube))
   
    components=[epsilon_cube, epsilon_cube_cs, alpha_cube, alpha_cube_cs, 
                H_cube, T_cube, rsdt_cube]
    return(components)

#end def dh_zonal_enbal




##########################
def main_dh_zonal(preind_expt,plio_expt):
    """
    performs the energy baland calculation as detailed in dan hills paper
    """
    # get all the fields needed for the energy balance
    components=dh_zonal_enbal(preind_expt)
    emis_pi_cube=components[0]
    emis_pi_cs_cube=components[1]
    alpha_pi_cube=components[2]
    alpha_pi_cs_cube=components[3]
    H_pi_cube=components[4]
    temp_pi_cube=components[5]
    sw_down_pi_cube=components[6]
    
    components=dh_zonal_enbal(plio_expt)
    emis_plio_cube = components[0]
    emis_plio_cs_cube = components[1]
    alpha_plio_cube = components[2]
    alpha_plio_cs_cube = components[3]
    H_plio_cube = components[4]
    temp_plio_cube = components[5]
    sw_down_plio_cube = components[6]
   
    sigma=5.67 * (10.0**-8)

    # do energy balance.  

    # greenhouse gas and topography
    t4 = ((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cs_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4 = ((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_pi_cs_cube.data * sigma)
    t_2 = t4 ** (1./4.)

    deltaT_gge_cube = alpha_plio_cube.copy(data = t_1 - t_2)
    deltaT_gge_lat_cube= deltaT_gge_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
   
    lat = deltaT_gge_lat_cube.coord('latitude').points

### LAUREN  : Here you want to split deltaT_gge_lat_cube into GHG and topography
###           You might have to rename things

    (deltaT_topo, deltaT_gge_only) = split_gge(deltaT_gge_lat_cube)


##### END LAUREN

    plt.plot(lat,deltaT_gge_lat_cube.data,label='GHG+ topography',color="blue")
  
    # cloud emisivity
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cs_cube.data * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_pi_cube.data * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_pi_cs_cube.data * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ce_cube = alpha_plio_cube.copy(data = (t_1-t_2) - (t_3 - t_4))
    deltaT_ce_lat_cube= deltaT_ce_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
   
    plt.plot(lat,deltaT_ce_lat_cube.data, 
             label='cloud emissivity',color="orange")

  
    # cloud albedo
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_pi_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_pi_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ca_cube = alpha_plio_cube.copy(data = (t_1-t_2) - (t_3 - t_4))
    deltaT_ca_lat_cube= deltaT_ca_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
   
    plt.plot(lat,deltaT_ca_lat_cube.data, label='cloud albedo',color="purple")
   
   
    # clear sky albedo
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_pi_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_2=t4 ** (1./4.)

    print('t1 components',sw_down_plio_cube.data[178,0],alpha_plio_cs_cube.data[178,0],H_plio_cube.data[178,0],emis_plio_cube.data[178,0])
    print('t2 components',sw_down_plio_cube.data[178,0],alpha_pi_cs_cube.data[178,0],H_plio_cube.data[178,0],emis_plio_cube.data[178,0])

    deltaT_csa_cube = alpha_plio_cube.copy(data = t_1-t_2)
    deltaT_csa_lat_cube= deltaT_csa_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)

    for i, latval in enumerate(deltaT_csa_lat_cube.coord('latitude').points):
        if latval == 88.5:
            print('j2',i,latval,deltaT_csa_lat_cube.data[i],t_1[i,0],t_2[i,0],t_1[i,0] - t_2[i,0])
    
  
    plt.plot(lat,deltaT_csa_lat_cube.data,label='clear sky albedo',color="chocolate",linestyle="dashdot")
   
    # heat transport

    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_pi_cube.data) / (emis_plio_cube.data * sigma)
    t_2=t4 ** (1./4.)

    deltaT_H_cube = alpha_plio_cube.copy(data=t_1-t_2)
    deltaT_H_lat_cube= deltaT_H_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
  
    plt.plot(lat,deltaT_H_lat_cube.data,label='heat transport',color="red")
   
    # compare with total temperature change
    deltaT_cube=temp_plio_cube-temp_pi_cube
    deltaT_lat_cube= deltaT_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
  
    plt.plot(lat,deltaT_lat_cube.data,label='actual temperature change')


    # sum of all energy balance terms
    deltaT_sum = (deltaT_gge_lat_cube.data +  deltaT_ce_lat_cube.data + 
                  deltaT_ca_lat_cube.data + deltaT_csa_lat_cube.data + 
                  deltaT_H_lat_cube.data)
    plt.plot(lat,deltaT_sum, color='black',linestyle='dotted',
             label='sum EB terms')


    for i,latind in enumerate(lat):
        if latind == 75.5:
            print('found totals',deltaT_sum[i], deltaT_lat_cube.data[i])
            print('comps',deltaT_gge_lat_cube.data[i],deltaT_ce_lat_cube.data[i],
                  deltaT_ca_lat_cube.data[i], deltaT_csa_lat_cube.data[i],
                  deltaT_H_lat_cube.data[i])
  #          sys.exit(0)
        
   

    plt.legend()
    plt.ylim(-8.0,15.0)
    mp.pyplot.axhline(y=0,xmin=-90,xmax=90,color='black')
    plt.title('Energy balance: '+ MODELNAME+ ' - Dan Hill')
    plt.xlabel('latitude')
    plt.ylabel('pliocene warming')
    
    filestart='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/energybal/DH_energybal_'+MODELNAME + '_' + EXPT + '_' + CNTL
    plt.savefig(filestart + '.eps', bbox_inches='tight')  
    plt.savefig(filestart + '.png', bbox_inches='tight')  

    plt.close()
    sys.exit(0)

# end of main part of Dan Hills energy balance





################################
# main program

# annual mean

MODELNAME = 'NorESM1-F'
FILESTART = '/nfs/hera1/earjcti/regridded/' + MODELNAME
CNTL = 'E280'
EXPT = 'E400'

# check to see if Dan Hills global energy balance equation is correct
#global_enbal(CNTL)
#global_enbal(EXPT)
#sys.exit(0)


# Dan Hills energy balance
main_dh_zonal(CNTL,EXPT)


::::::::::::::
PlioMIP_new/energybal/plot_energybal.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_ENERGYBAL
#PURPOSE
#    This program will plot the energy balance for the pliocene simulations
#
# search for 'main program' to find end of functions
# Julia 11/1/2018



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
import iris
import iris.plot as iplt
import iris.quickplot as qplt
from mpl_toolkits.basemap import Basemap, shiftgrid


#functions are:
#  def global_enbal
#  def seasmean


def global_enbal(exptid):
    """
    this looks to be a check that the temperature contribution from all the different sources is equal to the modelled temperature
    """

    def mean_data(filename):
        """
        gets the mean data from the file"
        """
        
        f = open(filename,"r")
        lines = f.readlines()
        avgs = lines[2]
        mean, sd = avgs.split(',')
       
        return mean

    #===================================================================
    # read in data from  average temperature files produced for Dan

    T_mean = np.float(mean_data(FILESTART + '/' + exptid + '.NearSurfaceTemperature.data.txt'))

    # get upward and downward sw radiation at the top of the atmosphere
    # remember alpha = sw_up_toa / sw_down_toa (rsut / rsdt)
    # incoming sw

    rsut_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rsut.data.txt'))
    rsdt_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rsdt.data.txt'))
    rsut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsut.allmean.nc')
    rsdt_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsdt.allmean.nc')


    # get terms for effctive longwave emissivity e = lw_up_toa / lw_up_surf
    # = rlut / rlus
    
    rlut_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rlut.data.txt'))
    rlut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlut.allmean.nc')

    if MODELNAME == 'HadCM3' or (MODELNAME == 'CESM2'):
        # lw upward surface = lw_down_surf - netdown_surf I think.  
        rlds_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rlds.data.txt'))
        rlds_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlds.allmean.nc')
        flns_mean = np.float(mean_data(FILESTART + '/' + exptid + '.flns.data.txt'))
        flns_cube = iris.load_cube(FILESTART + '/' + exptid + '.flns.allmean.nc')
        rlus_mean = rlds_mean - flns_mean
        rlus_cube = rlds_cube - flns_cube
    else:
        rlus_mean = np.float(mean_data(FILESTART + '/' + exptid + '.rlus.data.txt'))
        rlus_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlus.allmean.nc')

    # ====================================
    # get alpha and epsilon

    mean_alpha=rsut_mean/rsdt_mean
    alpha_cube = rsut_cube / rsdt_cube

    mean_epsilon=rlut_mean/rlus_mean
    epsilon_cube = rlut_cube / rlus_cube

  
    #============================================
    # calculate terms in equation
    So=1367 # solar constant
    sigma=5.67 * (10.0 ** (-8.))

    print('alphas',mean_alpha)
    print('epsilon',mean_epsilon)

    #t4=So / 4.0 * (1.0-alpha) / (epsilon * sigma)
    #t=t4 ** (1./4.)

    # calculate t_mean using average values of globe
    t4_mean=(So / 4.0) * (1.0-mean_alpha) / (mean_epsilon * sigma)
    t_mean_formula=t4_mean ** (1./4.)

    # calculate t_mean at every point of the globe and average afterwards
    t4_data=(So / 4.0) * (1.0-alpha_cube.data) / (epsilon_cube.data * sigma)
    t_mean_data=t4_data ** (1./4.)
    t_mean_cube = alpha_cube.copy(data=t_mean_data)

    t_mean_cube.coord('latitude').guess_bounds()
    t_mean_cube.coord('longitude').guess_bounds()
   
    grid_areas = iris.analysis.cartography.area_weights(t_mean_cube)
    t_mean_avg_cube = t_mean_cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)
    
    
    print('globvals',So/4.0,mean_alpha,mean_epsilon,sigma)



    print('    ')
    print(exptid)
    print('=========')
    print('t obs=',T_mean,' degC')
    print('t mean formula=',t_mean_formula-273.15,' degC')
    print('t mean from average=',t_mean_avg_cube.data - 273.15)

   
#end def global_enbal

################################################
def dh_zonal_enbal(exptid):

   
    #==============
    # read in data from  average temperature files produced for Dan

    T_cube = iris.load_cube(FILESTART + '/' + exptid + '.NearSurfaceTemperature.allmean.nc')
 
    # get upward and downward sw radiation at top of atmos
    rsut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsut.allmean.nc')
    rsdt_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsdt.allmean.nc')
  
    # get upward sw radiation at toa clear sky (downwards same as rsdt)
    if (MODELNAME == 'NorESM1-F' or MODELNAME == 'NorESM-L' or 
        (MODELNAME == 'CESM2' and exptid == 'E400')):
        rsntcs_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsntcs.allmean.nc')
        rsnt_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsnt.allmean.nc')
        rsutcs_cube = rsdt_cube - rsntcs_cube
       
    else:
        rsutcs_cube = iris.load_cube(FILESTART + '/' + exptid + '.rsutcs.allmean.nc')
        
  
   # outgoing lw  (toa) rlut and clear sky rlutcs
    rlutcs_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlutcs.allmean.nc') 
    rlut_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlut.allmean.nc')


    # outgoing lw  (surface) rlus and clear sky rluscs
    if MODELNAME == 'HadCM3':
        # lw upward surface = lw_down_surf - netdown_surf I think.  
        rlds_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlds.allmean.nc')
        rlns_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlns.allmean.nc')
        rlus_cube = rlds_cube - rlns_cube
    else:
        rlus_cube = iris.load_cube(FILESTART + '/' + exptid + '.rlus.allmean.nc')
    if MODELNAME == 'COSMOS':
        rlus_cube = rlus_cube * -1.0
        rsut_cube = rsut_cube * -1.0

    # rluscs = tested using rlus
  
    rluscs_cube = rlus_cube
    
    # ====================================
    # get the zonal average fields


    alpha_cube = rsut_cube / rsdt_cube
    alpha_cube_cs = rsutcs_cube / rsdt_cube
    epsilon_cube = rlut_cube / rlus_cube
    epsilon_cube_cs = rlutcs_cube / rluscs_cube


    sigma=5.67 * (10.0 ** (-8.))

    #============================================
    # calculate terms in equation
    H_cube=(-1.0) * ((rsdt_cube - rsut_cube) -(rlut_cube))
   
    components=[epsilon_cube, epsilon_cube_cs, alpha_cube, alpha_cube_cs, 
                H_cube, T_cube, rsdt_cube, rlutcs_cube, rluscs_cube]
    return(components)

#end def dh_zonal_enbal


def rf_zonal_enbal(exptname,HadCM3):

    if HadCM3 == 'y':
        filestart='/nfs/hera1/earjcti/um/'+exptname+'/database_averages/'+exptname
    else:
        filestart='/nfs/hera1/earjcti/um/HadGEM_data/'+exptname+'/database_averages/'+exptname


    #==============
    # read in data from  average temperature files produced for Dan

    f=Dataset(filestart+'_Annual_Average_a@pd_Temperature.nc')
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]
    atemp=f.variables['temp'][:]
    atemp=np.squeeze(atemp)
    ny,nx=np.shape(atemp)

    # get upward and downward sw radiation
    # incoming sw
    f=Dataset(filestart+'_Annual_Average_a@pd_field200.nc')
    sw_down=f.variables['field200'][:]
    sw_down=np.squeeze(sw_down)

    # outgoing sw 
    f=Dataset(filestart+'_Annual_Average_a@pd_field201.nc')
    sw_up=f.variables['field201'][:]
    sw_up=np.squeeze(sw_up)
 
    # surface upwards sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field207_1.nc')
    sw_surf_cs_up=f.variables['field207_1'][:]
    sw_surf_cs_up=np.squeeze(sw_surf_cs_up)
 
    # surface downwards sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field208.nc')
    sw_surf_cs_down=f.variables['field208'][:]
    sw_surf_cs_down=np.squeeze(sw_surf_cs_down)
 
  # outgoing sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field207.nc')
    swcs_up=f.variables['field207'][:]
    swcs_up=np.squeeze(swcs_up)
 

   # outgoing lw  (toa)
    f=Dataset(filestart+'_Annual_Average_a@pd_olr.nc')
    lw_toa_up=f.variables['olr'][:]
    lw_toa_up=np.squeeze(lw_toa_up)
  
    # outgoing lw  (toa clear sky)
    f=Dataset(filestart+'_Annual_Average_a@pd_csolr.nc')
    lwcs_toa=f.variables['field207'][:]
    lwcs_toa=np.squeeze(lwcs_toa)
    

    # net downward longwave surface
    f=Dataset(filestart+'_Annual_Average_a@pd_longwave.nc')
    net_lwdown_surf=f.variables['longwave'][:]
    net_lwdown_surf=np.squeeze(net_lwdown_surf)

 
    # incoming lw  (surface)
    f=Dataset(filestart+'_Annual_Average_a@pd_ilr.nc')
    lw_surf_down=f.variables['ilr'][:]
    lw_surf_down=np.squeeze(lw_surf_down)


    # net downward shortwave flux surface
    f=Dataset(filestart+'_Annual_Average_a@pd_solar.nc')
    net_swdown_surf=f.variables['solar'][:]
    net_swdown_surf=np.squeeze(net_swdown_surf)

    # total downward shortwave flux surface
    f=Dataset(filestart+'_Annual_Average_a@pd_field203.nc')
    sw_surf_down=f.variables['field203'][:]
    sw_surf_down=np.squeeze(sw_surf_down)


    # total cloud amount random overlap
    f=Dataset(filestart+'_Annual_Average_a@pd_TotalCloud.nc')
    cloud_frac=f.variables['field30'][:]
    cloud_frac=np.squeeze(cloud_frac)


    lw_surf_up=lw_surf_down-net_lwdown_surf # upward lw rad is downlw - net downlw

    sw_surf_up=sw_surf_down - net_swdown_surf # upwards sw rad at surf


    # topography
    if exptname=='xkvje':
        orog_fname='/nfs/hera1/earjcti/um/HadGEM_ancils/qrparm.orog_new.nc'
    if exptname=='xkvjg':
        orog_fname='/nfs/hera2/apps/metadata/ancil/PRISM3_ALT/HadGEM2/HadGEM_pliocene_orog.nc'
    if exptname=='xibos':
        orog_fname='/nfs/hera2/apps/metadata/ancil/preind2/qrparm.orog.nc'
    if exptname=='xibot' or exptname=='xoorb' or exptname=='xoorf':
        orog_fname='/nfs/hera2/apps/metadata/ancil/PRISM3_ALT/qrparm.orog.nc'
    f=Dataset(orog_fname)
    orog=f.variables['ht'][:]
    orog=np.squeeze(orog)


    
    # ====================================
    # get gridded epsilon and alpha



    grid_epsilon=lw_toa_up/(lw_surf_up)
    grid_epsilon_cs=lwcs_toa / lw_surf_up


    # from Taylor 2007 
    # alpha is now surface albedo.  We denote planetary albedo:  A.
    # alpha=(1-c)*alpha_clr + (c*alpha_oc)
    # oc=overcast, clr=clear sky, c=cloud fraction

    grid_A=sw_up/sw_down
    grid_A_clr=swcs_up/sw_down
    grid_alpha_clr=sw_surf_cs_up / sw_surf_cs_down
    grid_alpha=sw_surf_up / sw_surf_down
    grid_alpha_oc=(grid_alpha - ((1-cloud_frac) * grid_alpha_clr)) / cloud_frac


    #===============================
    # get amount scattered (gamma) and 
    # amount not absorbed (mu)  (absorbtion =1-mu)
    # see Taylor 2007 equation 9 for the equations

    grid_mu=(sw_surf_down / sw_down) * (1.0 - grid_alpha) + grid_A
    grid_mu_clr=(sw_surf_cs_down / sw_down) * (1.0 - grid_alpha_clr)+grid_A_clr
    grid_mu_oc=(grid_mu - ((1-cloud_frac) * grid_mu_clr)) / cloud_frac
    grid_mu_cld=grid_mu_oc / grid_mu_clr

    grid_gamma=((grid_mu - (sw_surf_down / sw_down)) /
                (grid_mu - (grid_alpha * (sw_surf_down / sw_down))))
    grid_gamma_clr=((grid_mu_clr - (sw_surf_cs_down / sw_down)) /
                (grid_mu_clr - (grid_alpha_clr * (sw_surf_cs_down / sw_down))))
    grid_gamma_oc=(grid_gamma - ((1-cloud_frac) * grid_gamma_clr)) / cloud_frac
    grid_gamma_cld=1.0 - ((1.0 - grid_gamma_oc)/(1.0 - grid_gamma_clr))
                 

    # note these values of gamma, alpha and mu have been checked against
    # equations 7 and 8 of Taylor 2007.  Therefore if A is correct and
    # qs_hat_down is correct then gamma alpha and mu are also correct.  

  
    #===============================
    # get fields for use in energy balance calculation
 
    meantemp=np.average(atemp,axis=1)
    mean_sw_up=np.average(sw_up,axis=1)
    mean_sw_down=np.average(sw_down,axis=1)
    mean_sw_surf_cs_down=np.average(sw_surf_cs_down,axis=1)
    mean_sw_surf_down=np.average(sw_surf_down,axis=1)
    mean_lw_toa_up=np.average(lw_toa_up,axis=1)
    mean_lwcs_toa_up=np.average(lwcs_toa,axis=1)
    mean_lw_surf_down=np.average(lw_surf_down,axis=1)
    mean_net_lwdown_surf=np.average(net_lwdown_surf,axis=1)
    mean_alpha=np.average(grid_alpha,axis=1)
    mean_alpha_clr=np.average(grid_alpha_clr,axis=1)
    mean_alpha_oc=np.average(grid_alpha_oc,axis=1)
    mean_alpha=np.average(grid_alpha,axis=1)
    mean_epsilon=np.average(grid_epsilon,axis=1)
    mean_epsilon_cs=np.average(grid_epsilon_cs,axis=1)
    mean_lw_surf_up=np.average(lw_surf_up,axis=1)
    mean_cloud=np.average(cloud_frac,axis=1)
    mean_mu=np.average(grid_mu,axis=1)
    mean_mu_clr=np.average(grid_mu_clr,axis=1)
    mean_mu_oc=np.average(grid_mu_oc,axis=1)
    mean_mu_cld=np.average(grid_mu_cld,axis=1)
    mean_gamma=np.average(grid_gamma,axis=1)
    mean_gamma_clr=np.average(grid_gamma_clr,axis=1)
    mean_gamma_oc=np.average(grid_gamma_oc,axis=1)
    mean_gamma_cld=np.average(grid_gamma_cld,axis=1)
    mean_A=np.average(grid_A,axis=1)
    mean_orog=np.average(orog,axis=1)

    #plt.subplot(3,1,1)
    #plt.plot(lat,mean_alpha-0.1,label='mean_alpha-0.1')
    #plt.plot(lat,mean_alpha_clr,label='mean_alpha_clr')
    #plt.plot(lat,mean_alpha_oc,label='mean_alpha_oc')
    #plt.legend()
    #plt.title('different values')

    #plt.subplot(3,1,2)
    #plt.plot(lat,mean_mu,label='mean_mu')
    #plt.plot(lat,mean_mu_clr,label='mean_mu_clr')
    #plt.plot(lat,mean_mu_oc,label='mean_mu_oc')
    #plt.legend()


    #plt.subplot(3,1,3)
    #plt.plot(lat,mean_gamma,label='mean_gamma')
    #plt.plot(lat,mean_gamma_clr,label='mean_gamma_clr')
    #plt.plot(lat,mean_gamma_oc,label='mean_gamma_oc')  
    #plt.legend()

    #plt.show()
   


    sigma=5.67 * (10.0 ** (-8.))

    #============================================
    # calculate terms in equation
    H=(-1.0) * ((mean_sw_down - mean_sw_up) -(mean_lw_toa_up))
   
    

    t4=((mean_sw_down * (1.0-mean_alpha)) + H) / (mean_epsilon * sigma)
    t=t4 ** (1./4.)


    components=[mean_epsilon,mean_epsilon_cs,mean_alpha_clr,mean_alpha_oc,mean_mu_cld,mean_gamma_cld,mean_mu_clr,mean_gamma_clr,mean_cloud,H,meantemp,mean_sw_down,lat,mean_alpha,mean_mu,mean_gamma,mean_mu_oc,mean_gamma_oc,mean_A,mean_orog]
    return(components)

#end def rf_zonal_enbal


##########################
def main_dh_zonal(preind_expt,plio_expt):
    """
    performs the energy baland calculation as detailed in dan hills paper
    """
    # get all the fields needed for the energy balance
    components=dh_zonal_enbal(preind_expt)
    emis_pi_cube=components[0]
    emis_pi_cs_cube=components[1]
    alpha_pi_cube=components[2]
    alpha_pi_cs_cube=components[3]
    H_pi_cube=components[4]
    temp_pi_cube=components[5]
    sw_down_pi_cube=components[6]
    rlutcs_pi_cube = components[7]
    rluscs_pi_cube = components[8]
    
    components=dh_zonal_enbal(plio_expt)
    emis_plio_cube = components[0]
    emis_plio_cs_cube = components[1]
    alpha_plio_cube = components[2]
    alpha_plio_cs_cube = components[3]
    H_plio_cube = components[4]
    temp_plio_cube = components[5]
    sw_down_plio_cube = components[6]
    rlutcs_plio_cube = components[7]
    rluscs_plio_cube = components[8]
  
 
    sigma=5.67 * (10.0**-8)

    # do energy balance.  

    # greenhouse gas and topography
    t4 = ((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cs_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4 = ((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_pi_cs_cube.data * sigma)
    t_2 = t4 ** (1./4.)

    deltaT_gge_cube = alpha_plio_cube.copy(data = t_1 - t_2)
    deltaT_gge_lat_cube= deltaT_gge_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)

    emis_plio_cs_cube.long_name = 'pliocene cs emissivity'
    emis_pi_cs_cube.long_name = 'pi cs emissivity'
    emis_diff = emis_plio_cs_cube - emis_pi_cs_cube
    emis_diff.long_name = 'cs emissivity difference'
    emis_diff_lat = emis_diff.collapsed('longitude',iris.analysis.MEAN)
    rlutcs_diff = rlutcs_plio_cube - rlutcs_pi_cube
    rlutcs_diff_lat = rlutcs_diff.collapsed('longitude', iris.analysis.MEAN)
    rlutcs_diff_lat.long_name = 'rlutcs diff'
    rluscs_diff = rluscs_plio_cube - rluscs_pi_cube
    rluscs_diff_lat = rluscs_diff.collapsed('longitude', iris.analysis.MEAN)
    rluscs_diff_lat.long_name = 'rluscs diff'
    iris.save([emis_plio_cs_cube, emis_pi_cs_cube, emis_diff, emis_diff_lat, rlutcs_diff_lat, rluscs_diff_lat],MODELNAME + '_emis.nc')
   
    lat = deltaT_gge_lat_cube.coord('latitude').points
    plt.plot(lat,deltaT_gge_lat_cube.data,label='GHG+ topography',color="blue")
  
    # cloud emisivity
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cs_cube.data * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_pi_cube.data * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_pi_cs_cube.data * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ce_cube = alpha_plio_cube.copy(data = (t_1-t_2) - (t_3 - t_4))
    deltaT_ce_lat_cube= deltaT_ce_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
   
    plt.plot(lat,deltaT_ce_lat_cube.data, 
             label='cloud emissivity',color="orange")

  
    # cloud albedo
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_pi_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_pi_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ca_cube = alpha_plio_cube.copy(data = (t_1-t_2) - (t_3 - t_4))
    deltaT_ca_lat_cube= deltaT_ca_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
   
    plt.plot(lat,deltaT_ca_lat_cube.data, label='cloud albedo',color="purple")
   
   
    # clear sky albedo
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_pi_cs_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_2=t4 ** (1./4.)

    print('t1 components',sw_down_plio_cube.data[178,0],alpha_plio_cs_cube.data[178,0],H_plio_cube.data[178,0],emis_plio_cube.data[178,0])
    print('t2 components',sw_down_plio_cube.data[178,0],alpha_pi_cs_cube.data[178,0],H_plio_cube.data[178,0],emis_plio_cube.data[178,0])

    deltaT_csa_cube = alpha_plio_cube.copy(data = t_1-t_2)
    deltaT_csa_lat_cube= deltaT_csa_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)

    for i, latval in enumerate(deltaT_csa_lat_cube.coord('latitude').points):
        if latval == 88.5:
            print('j2',i,latval,deltaT_csa_lat_cube.data[i],t_1[i,0],t_2[i,0],t_1[i,0] - t_2[i,0])
    
  
    plt.plot(lat,deltaT_csa_lat_cube.data,label='clear sky albedo',color="chocolate",linestyle="dashdot")
   
    # heat transport

    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_plio_cube.data) / (emis_plio_cube.data * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio_cube.data * (1.0-alpha_plio_cube.data)) + H_pi_cube.data) / (emis_plio_cube.data * sigma)
    t_2=t4 ** (1./4.)

    deltaT_H_cube = alpha_plio_cube.copy(data=t_1-t_2)
    deltaT_H_lat_cube= deltaT_H_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
  
    plt.plot(lat,deltaT_H_lat_cube.data,label='heat transport',color="red")
   
    # compare with total temperature change
    deltaT_cube=temp_plio_cube-temp_pi_cube
    deltaT_lat_cube= deltaT_cube.collapsed(['longitude'],
                              iris.analysis.MEAN)
  
    plt.plot(lat,deltaT_lat_cube.data,label='actual temperature change')


    # sum of all energy balance terms
    deltaT_sum = (deltaT_gge_lat_cube.data +  deltaT_ce_lat_cube.data + 
                  deltaT_ca_lat_cube.data + deltaT_csa_lat_cube.data + 
                  deltaT_H_lat_cube.data)
    plt.plot(lat,deltaT_sum, color='black',linestyle='dotted',
             label='sum EB terms')
    #print(deltaT_gge_lat_cube.data)
    


    for i,latind in enumerate(lat):
        if latind == 75.5:
            print('found totals',deltaT_sum[i], deltaT_lat_cube.data[i])
            print('comps',deltaT_gge_lat_cube.data[i],deltaT_ce_lat_cube.data[i],
                  deltaT_ca_lat_cube.data[i], deltaT_csa_lat_cube.data[i],
                  deltaT_H_lat_cube.data[i])
  #          sys.exit(0)
        
   

    plt.legend()
    plt.ylim(-8.0,15.0)
    mp.pyplot.axhline(y=0,xmin=-90,xmax=90,color='black')
    plt.title('Energy balance: '+ MODELNAME+ ' - Dan Hill')
    plt.xlabel('latitude')
    plt.ylabel('pliocene warming')
    
    filestart='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/energybal/DH_energybal_'+MODELNAME + '_' + EXPT + '_' + CNTL
    plt.savefig(filestart + '.eps', bbox_inches='tight')  
    plt.savefig(filestart + '.png', bbox_inches='tight')  

    plt.close()
    sys.exit(0)

# end of main part of Dan Hills energy balance

def main_rf_energybal(modelname):
    components=rf_zonal_enbal(preind_expt,HadCM3)
    emis_pi=components[0]
    emis_pi_cs=components[1]
    alpha_pi_clr=components[2]
    alpha_pi_oc=components[3]
    mu_pi_cld=components[4]
    gamma_pi_cld=components[5]
    mu_pi_clr=components[6]
    gamma_pi_clr=components[7]
    cloud_pi=components[8]
    H_pi=components[9]
    temp_pi=components[10]
    sw_down_pi=components[11]
    lat=components[12]
    alpha_pi=components[13]
    mu_pi=components[14]
    gamma_pi=components[15]
    mu_pi_oc=components[16]
    gamma_pi_oc=components[17]
    pi_A=components[18]
    topo_pi=components[19]


    components=rf_zonal_enbal(pliop2_expt,HadCM3)
    emis_plio=components[0]
    emis_plio_cs=components[1]
    alpha_plio_clr=components[2]
    alpha_plio_oc=components[3]
    mu_plio_cld=components[4]
    gamma_plio_cld=components[5]
    mu_plio_clr=components[6]
    gamma_plio_clr=components[7]
    cloud_plio=components[8]
    H_plio=components[9]
    temp_plio=components[10]
    sw_down_plio=components[11]
    lat=components[12]
    alpha_plio=components[13]
    mu_plio=components[14]
    gamma_plio=components[15]
    mu_plio_oc=components[16]
    gamma_plio_oc=components[17]
    plio_A=components[18]
    topo_plio=components[19]


    delta_T_topo=(topo_plio-topo_pi) * (-5.5) / 1000.
    sigma=5.67 * (10.0**-8)

# print j corresponding to 75deg
    for j in range(0,len(lat)):
        if lat[j]==75:
            lat75=j
            print(j,lat[j])

#taylor 2007 equation 16 a-c and 7
# note the equation 16a is misleading.  I think it should be
#delta_A_alpha=(1-c)delta_A_alpha_clr + c*delta_A_alpha_oc 

# get the change in albedo due to alpha


# new use mean values
    mu=(mu_plio+mu_pi)/2.0
    gamma=(gamma_plio+gamma_pi)/2.0
    alpha=(alpha_plio+alpha_pi)/2.0
    cloud=(cloud_plio+cloud_pi)/2.0

    A_alpha_pi_clr=((mu * gamma) + ((mu * alpha_pi_clr * (1.0-gamma)**2.0)/ (1.0 - alpha_pi_clr * gamma)))
    A_alpha_plio_clr=((mu * gamma) + ((mu * alpha_plio_clr * (1.0-gamma)**2.0)/ (1.0 - alpha_plio_clr * gamma)))
    A_alpha_pi_oc=((mu * gamma) + ((mu * alpha_pi_oc * (1.0-gamma)**2.0)/ (1.0 - alpha_pi_oc * gamma)))
    A_alpha_plio_oc=((mu * gamma) + ((mu * alpha_plio_oc * (1.0-gamma)**2.0)/ (1.0 - alpha_plio_oc * gamma)))
    A_alpha_diff=(1.0-cloud)*(A_alpha_plio_clr - A_alpha_pi_clr)+cloud*(A_alpha_plio_oc - A_alpha_pi_oc)
    A_alpha_plio=(1.0-cloud)*A_alpha_plio_clr + cloud*A_alpha_plio_oc
    A_alpha_pi=(1.0-cloud)*A_alpha_pi_clr + cloud*A_alpha_pi_oc
# julia print out at 75N
    print('surface alpha at 75',A_alpha_diff[lat75],A_alpha_plio[lat75],A_alpha_pi[lat75])

# get the change in albedo due to clouds eqn 16b


    A_cld_pi_gamma=((mu * gamma_pi_cld) + ((mu * alpha * (1.0-gamma_pi_cld)**2.0)/ (1.0 - alpha * gamma_pi_cld)))
    A_cld_plio_gamma=((mu * gamma_plio_cld) + ((mu * alpha * (1.0-gamma_plio_cld)**2.0)/ (1.0 - alpha * gamma_plio_cld)))
    A_cld_pi_mu=((mu_pi_cld * gamma) + ((mu_pi_cld * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))
    A_cld_plio_mu=((mu_plio_cld * gamma) + ((mu_plio_cld * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))

# equation 15 A=(a-c)Aclr + c Aoc

    mu_clr=(mu_plio_clr + mu_pi_clr)/2.0
    gamma_clr=(gamma_plio_clr + gamma_pi_clr)/2.0
    alpha_clr=(alpha_plio_clr + alpha_pi_clr)/2.0
    mu_oc=(mu_plio_oc + mu_pi_oc)/2.0
    gamma_oc=(gamma_plio_oc + gamma_pi_oc)/2.0
    alpha_oc=(alpha_plio_oc + alpha_pi_oc)/2.0

    clravg=((mu_clr * gamma_clr) + ((mu_clr * alpha_clr * (1.0-gamma_clr)**2.0)/ (1.0 - alpha_clr * gamma_clr)))
    ocavg=((mu_oc * gamma_oc) + ((mu_oc * alpha_oc * (1.0-gamma_oc)**2.0)/ (1.0 - alpha_oc * gamma_oc)))
    A_deltaC=((cloud_pi-cloud_plio)*clravg)+((cloud_plio-cloud_pi)*ocavg)


    A_cld_diff=A_cld_plio_mu-A_cld_pi_mu + A_cld_plio_gamma-A_cld_pi_gamma + A_deltaC

# julia print out at 75N
    print('cloud A at 75',A_cld_diff[lat75])



# get the change in albedo due to clear skies


    A_clr_pi_mu=((mu_pi_clr * gamma) + ((mu_pi_clr * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))
    A_clr_plio_mu=((mu_plio_clr * gamma) + ((mu_plio_clr * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))
    A_clr_pi_gamma=((mu * gamma_pi_clr) + ((mu * alpha * (1.0-gamma_pi_clr)**2.0)/ (1.0 - alpha * gamma_pi_clr)))
    A_clr_plio_gamma=((mu * gamma_plio_clr) + ((mu * alpha * (1.0-gamma_plio_clr)**2.0)/ (1.0 - alpha * gamma_plio_clr)))
    A_clr_diff=(A_clr_plio_mu - A_clr_pi_mu)+(A_clr_plio_gamma - A_clr_pi_gamma)
    print('clr A at 75',A_clr_diff[lat75],A_clr_plio_mu[lat75],A_clr_pi_mu[lat75],A_clr_plio_gamma[lat75],A_clr_pi_gamma[lat75],cloud_pi[lat75],cloud_plio[lat75])


# get change in planetary albedo to check budgets

    A_diff=plio_A-pi_A


#plt.subplot(2,1,1)
    plt.plot(lat,A_alpha_diff,label='A_alpha')
    plt.plot(lat,A_cld_diff,label='A_cld')
    plt.plot(lat,A_clr_diff,label='A clr diff')
    plt.plot(lat,A_diff,label='total planetary Albedo diff')
    plt.plot(lat,A_alpha_diff+A_cld_diff+A_clr_diff,label='sum')
    plt.legend()
    plt.title('plio-pi A components')

    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_albedo'+pliop2_expt+'-'+preind_expt+'.eps' 
    plt.savefig(fileout, bbox_inches='tight')  

    plt.close()


#print values at 75N in plot
    print('A_alpha_diff',A_alpha_diff[lat75])
    print('A_cld_diff',A_cld_diff[lat75])
    print('A_clr_diff',A_clr_diff[lat75])
    print('A_diff',A_diff[lat75])
    print(' ')



# we are now decomposing but will also find the average value from 55N-90N
# create weighting array
    weightarr_Arctic=np.cos(np.deg2rad(lat))
    for i in range(0,len(lat)):
        if lat[i] < 55:
            weightarr_Arctic[i]=0.
    weightarr=np.cos(np.deg2rad(lat))

    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio_cs * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_pi_cs * sigma)
    t_2=t4 ** (1./4.)

    deltaT_gge=t_1-t_2 - delta_T_topo


#plt.subplot(2,1,2)
    fig=plt.figure()
    ax=plt.subplot(111)
    ax.plot(lat,deltaT_gge,label='Greenhouse gas emissivity',color="blue")
#plt.plot(lat,delta_T_topo,label='topography')
    print('Arctic T gge',np.average(deltaT_gge,weights=weightarr_Arctic))
    print('Arctic T topo',np.average(delta_T_topo,weights=weightarr_Arctic))


    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio_cs * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_pi * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_pi_cs * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ce=(t_1-t_2) - (t_3 - t_4)
    print('Arctic T cloud emisivity',np.average(deltaT_ce,weights=weightarr_Arctic))
    ax.plot(lat,deltaT_ce, label='cloud emissivity',color="orange")

# surface albedo

    t4=((sw_down_plio * (1.0-A_alpha_plio)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-A_alpha_pi)) + H_plio) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_surfalpha=(t_1-t_2) 
    ax.plot(lat,deltaT_surfalpha, label='surface albedo',color="chocolate",linestyle='dotted')
    print('Arctic T surface albedo',np.average(deltaT_surfalpha,weights=weightarr_Arctic))

# check alternate value for surface (this seems to work fine)
# we did this to check how we had done clear sky and cloud
    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-(plio_A-A_alpha_diff))) + H_plio) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_altsurf=t_1-t_2
#plt.plot(lat,deltaT_altsurf,label='alt surface albedo')
    print('Arctic T altsurf',np.average(deltaT_altsurf,weights=weightarr_Arctic))


# clear sky albedo

    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-(plio_A-A_clr_diff))) + H_plio) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_csalbedo=t_1-t_2
    ax.plot(lat,deltaT_csalbedo,label='clear sky albedo',color="chocolate",linestyle="dashed")
    print('Arctic T csalbedo',np.average(deltaT_csalbedo,weights=weightarr_Arctic))

#cloud albedo

    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-(plio_A-A_cld_diff))) + H_plio) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_cldalbedo=t_1-t_2
    ax.plot(lat,deltaT_cldalbedo,label='cloud albedo',color="purple")
    print('Arctic T cloud albedo',np.average(deltaT_cldalbedo,weights=weightarr_Arctic))



    t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-plio_A)) + H_pi) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_H=t_1-t_2
    ax.plot(lat,deltaT_H,label='heat transport',color="red")
    print('Arctic T Heat transport',np.average(deltaT_H,weights=weightarr_Arctic))

    deltaT=temp_plio-temp_pi
#plt.plot(lat,deltaT,label='actual temperature change')
    print('Arctic T total change',np.average(deltaT,weights=weightarr_Arctic))


    total_components=deltaT_gge+deltaT_ce+deltaT_surfalpha+deltaT_csalbedo+deltaT_cldalbedo+deltaT_H
#plt.plot(lat,total_components,label='total accountable')


    deltaT_cloud=deltaT_ce+deltaT_cldalbedo

    print('Mean T gge',np.average(deltaT_gge,weights=weightarr))
    print('Mean T topo',np.average(delta_T_topo,weights=weightarr))
    print('Mean T cloud emisivity',np.average(deltaT_ce,weights=weightarr))
    print('Mean T surface albedo',np.average(deltaT_surfalpha,weights=weightarr))
    print('Mean T csalbedo',np.average(deltaT_csalbedo,weights=weightarr))
    print('Mean T cloud albedo',np.average(deltaT_cldalbedo,weights=weightarr))
    print('Mean T Heat transport',np.average(deltaT_H,weights=weightarr))
    print('Mean T total change',np.average(deltaT,weights=weightarr))






    print('Mean cloud changes',np.average(deltaT_cloud,weights=weightarr))


    plt.ylim(-4.0,8.0)
    mp.pyplot.axhline(y=0,xmin=-90,xmax=90,color='black')
    if HadCM3 == 'y':
        plt.title('d) Energy Balance: HadCM3',loc='left',fontsize=18)
    else:
        plt.title('c) Energy Balance: HadGEM2',loc='left',fontsize=18)
    plt.xlabel('Latitude',fontsize=15)
    degC=u'\N{DEGREE SIGN}'+'C'
    plt.ylabel('Pliocene Warming ('+degC+')',fontsize=15)


    plt.legend(fontsize=15)
    plt.tick_params(axis='both',labelsize=15)
    box=ax.get_position()
    ax.set_position([box.x0,box.y0+box.height*0.1,box.width,box.height*0.9])
    ax.legend(loc='lower center',bbox_to_anchor=(0.5,-0.4),ncol=3)


    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_energybal'+pliop2_expt+'-'+preind_expt+'.eps' 
    plt.savefig(fileout, bbox_inches='tight')  
    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_energybal'+pliop2_expt+'-'+preind_expt+'.png' 
    plt.savefig(fileout, bbox_inches='tight')  

    plt.close()

    filetext='/home/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_energybal'+pliop2_expt+'-'+preind_expt+'.tex'
    f= open(filetext,"w+")
    f.write('latitude,Greenhouse gas emissivity,Cloud emissivity,surfacealbedo,'+
        'clear sky albedo,cloud albedo,heat_transport \n')
    for i in range(0,len(lat)):
        f.write(np.str(lat[i])+','+np.str(deltaT_gge[i])+','+
            np.str(deltaT_ce[i])+','+
            np.str(deltaT_surfalpha[i])+','+
            np.str(deltaT_csalbedo[i])+','+
            np.str(deltaT_cldalbedo[i])+','+np.str(deltaT_H[i])
            +'\n')
        f.close()






################################
# main program

# annual mean

MODELNAME = 'NorESM1-F'
FILESTART = '/nfs/hera1/earjcti/regridded/' + MODELNAME
CNTL = 'E280'
EXPT = 'EOI400'

# check to see if Dan Hills global energy balance equation is correct
#global_enbal(CNTL)
#global_enbal(EXPT)
#sys.exit(0)


# Dan Hills energy balance
main_dh_zonal(CNTL,EXPT)


# Ran Fengss energy balance

main_rf_energybal(modelname)
::::::::::::::
PlioMIP_new/energybal/regrid_fields_for_energybal_CESM2.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
  
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm_400(fieldname):
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename, fieldname)
    print(cubeall)
    

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    print('here', filename_)
    allcube = iris.load(filename_)
    print(allcube)
    print(fielduse_)
    
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name, fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'rlus':
           cube = (cube2 - cube1)
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       print('here',filename,fielduse)
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'
                    +exptnameout+'.'+
                    fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F' or modelname == 'NorESM-L'):
        cube = get_cesm12(exptnamein)
    else:
        cube100 = iris.load_cube(filename)

   
  
    ###########################################
   
    

    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    ann_avg_cube = regridded_cube.collapsed('time',iris.analysis.MEAN)
    ann_avg_cube.long_name = fieldnameout

    # write the cubes out to a file

  
    outfile = outstart+'allmean.nc'
    iris.save(ann_avg_cube, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS",
                     "rlut" : "FLUT",
                     "rsut" : "FSUTOA",
                     "rsdt" : ["FSNTOA","FSUTOA"],
                     "rsntcs" : "FSNTOAC",
                     "rlutcs" : "FLUTC",
                     "rlut" : "FLUT",
                     "rlus" : ["FLNS","FLDS"],
                     "rlds" : "FLDS",
                     "rlns" : "FLNS",
                     "rsnt" : "FSNTOA"
                     }


    CESM2_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS",
                     "rlut" : "Upwelling longwave flux at top of model"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "PMIP4-midPliocene-eoi400",
                    "E400" : "CMIP6-piControl.400",
                     "E280": "CMIP6-piControl",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "E400" : "_monthly_climatology.",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : "1100-11199.monclim",
                  "E400" : "monthly_climatology",
                  "Eoi400" : "1101-1200.monclim"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E400":"PlioMIP2-E400_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein, fieldnamein)
        filename = (filename+fielduse + '/MIROC4m_' + exptnamein + '_' + 
                    atm_ocn_ind.get(fieldnamein, "Amon") + '_' + fielduse + '.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein, fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein,"")+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein, fieldnamein)
        filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
      
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein, exptnamein)
        fielduse = fieldname.get(fieldnamein, fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        if fieldnamein == 'rlus':
            fields = CESM12_FIELDS.get(fieldnamein)
            filename = [('/nfs/b0164/Data/NorESM1-F/Energy balance/' +
                        modelname+'_'+
                        exptnamein+'_'+fields[0]+'_climo.nc')]
      
            filename.append('/nfs/b0164/Data/NorESM1-F/Energy balance/' +
                        modelname+'_'+
                        exptnamein+'_'+fields[1]+'_climo.nc')
      
            fielduse = fields
        else:
            fielduse = CESM12_FIELDS.get(fieldnamein)
            filename = ('/nfs/b0164/Data/NorESM1-F/Energy balance/' +modelname+'_'+
                 exptnamein+'_'+fielduse+'_climo.nc')
          
            

    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'rsdt' or fieldnamein == 'rlus':
            fields = CESM12_FIELDS.get(fieldnamein)
            filename = [('/nfs/b0164/Data/NCAR/Energybalance/b.e21.B1850.f09_g17.'
                    + CESM2_EXTRA.get(exptnamein) +  '_' + fields[0] + '_' + 
                    CESM2_TIME.get(exptnamein) + '.nc')]
            filename.append(('/nfs/b0164/Data/NCAR/Energybalance/b.e21.B1850.f09_g17.'
                    + CESM2_EXTRA.get(exptnamein) +  '_' + fields[1] + '_' + 
                    CESM2_TIME.get(exptnamein) + '.nc'))
            fielduse = fields
        else:
            print(exptnamein, CESM2_TIME.get(exptnamein))
            filename = ('/nfs/b0164/Data/NCAR/Energybalance/b.e21.B1850.f09_g17.'
                    + CESM2_EXTRA.get(exptnamein) +  '_' + 
                    CESM12_FIELDS.get(fieldnamein) + '_' + 
                    CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CESM2" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

#fieldnamein = ['tas']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['rlds']
#exptnamein = ['Eoi400','E280']
exptnamein = ['E280']
if linux_win  == 'l':
    filestart = '/nfs/b0164/Data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])
       
        print(retdata)
        fielduse = retdata[0]
        filename = retdata[1]
       
        fieldnameout = fieldname.get(fieldnamein[field], fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/energybal/regrid_fields_for_energybal.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
  
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm(fieldname, filenames):
    """
    here filenames contains the start of the appropriate files
    """
    #cubeall = iris.load_cube(filenames + '*', fieldname)
    cubeall = iris.load(filename + '*')
    equalise_attributes(cubeall)
    cube = cubeall.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    print(allcube)
    print(fielduse_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name, fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       print('here',filename,fielduse)
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'
                    +exptnameout+'.'+
                    fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F'):
        cube100 = get_noresm(fielduse,filename)
    else:
        cube100 = iris.load_cube(filename)

   
  
    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        if modelname == 'CESM2':
            print('may only have 50 years of data in file')
            sys.exit(0)
        else:
            cube = cube100
    else:
        if modelname == 'CESM2' and exptnamein == 'Eoi400':
            cube = cube100
        else:
            cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim

    

    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A' or modelname == 'HadGEM3'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-Utr' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS",
                     "rlut" : "FLUT"
                     }


    CESM2_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS",
                     "rlut" : "Upwelling longwave flux at top of model"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "PMIP4-midPliocene-eoi400",
                    "E400" : "CMIP6-piControl.400",
                     "E280": "CMIP6-piControl",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    NORESM_EXTRA = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_",
                    "E280" : "piControl_r1i1p1f1_gn_"}

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "E400" : "_monthly_climatology.",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : "1100-11199.monclim",
                  "E400" : "monthly_climatology",
                  "Eoi400" : "1101-1200.monclim"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E400":"PlioMIP2-E400_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein, fieldnamein)
        filename = (filename+fielduse + '/MIROC4m_' + exptnamein + '_' + 
                    atm_ocn_ind.get(fieldnamein, "Amon") + '_' + fielduse + '.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein, fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein,"")+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein, fieldnamein)
        filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
      
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein, exptnamein)
        fielduse = fieldname.get(fieldnamein, fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein,fieldnamein)
        print(fielduse)
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/'+modelname+'/'+
                 fielduse+'_Amon_NorESM1-F_' + NORESM_EXTRA.get(exptnamein) )
          
            

    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        print(fieldnamein, exptnamein)
        print(CESM2_TIME.get(exptnamein))
        filename = ('/nfs/b0164/Data/NCAR/Energybalance/b.e21.B1850.f09_g17.'
                    + CESM2_EXTRA.get(exptnamein) +  '_' + 
                    CESM12_FIELDS.get(fieldnamein) + '_' + 
                    CESM2_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "NorESM1-F" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

#fieldnamein = ['tas']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['rlus']
exptnamein = ['Eoi400','E280']
#exptnamein = ['E400']
if linux_win  == 'l':
    filestart = '/nfs/b0164/Data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])
       
        print(retdata)
        fielduse = retdata[0]
        filename = retdata[1]
       
        fieldnameout = fieldname.get(fieldnamein[field], fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/extremes/annual_means.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia
Used for comparing with diags 6-9
Plots the difference between the pliocene and the preindustrial mean temperature for each month
"""
import numpy as np
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import matplotlib.pyplot as plt
import sys




   
#########################################################
def plot_anomalies(cube, field_ind, plotname, title):
    """
    this is for plotting all the anomalies
    """ 

    cube.units = 'degC'
    if field_ind == 't':
        vals = np.arange(0,14,2)
        cs = qplt.contourf(cube, levels=vals, cmap='Reds', extend='both')
    if field_ind == 'p':
        vals = np.arange(-1.5,1.5,0.2)
        cs = qplt.contourf(cube, levels=vals, cmap='BrBG', extend='both')
  
    plt.gca().coastlines()
    plt.title(title) 
    
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/means/' + plotname)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()


def plot_textremes_toplevel():
    """
    for each month plots the difference between the Tmean for the pliocene and the preindustrial
    """

    # temperature
    filestart = '/nfs/hera1/earjcti/regridded/HadCM3/'
    plio_cube_nsat = iris.load_cube(filestart + 'EOI400.NearSurfaceTemperature.allmean.nc')
    pi_cube_nsat = iris.load_cube(filestart + 'E280.NearSurfaceTemperature.allmean.nc')

    print(plio_cube_nsat.data)
    print(pi_cube_nsat.data)
    
    anom_cube_nsat = plio_cube_nsat - pi_cube_nsat
    print(anom_cube_nsat.data)
    plotname = 'HadCM3_pliomip2_NSAT_annmean_' 

    plot_anomalies(anom_cube_nsat,'t', plotname,'Plio_PI: SAT')
  
    # precipitation
    filestart = '/nfs/hera1/earjcti/regridded/HadCM3/'
    plio_cube_prec = iris.load_cube(filestart + 'EOI400.TotalPrecipitation.allmean.nc')
    pi_cube_prec = iris.load_cube(filestart + 'E280.TotalPrecipitation.allmean.nc')

    
    anom_cube_prec = plio_cube_prec - pi_cube_prec
    plotname = 'HadCM3_pliomip2_prec_annmean_' 

    plot_anomalies(anom_cube_prec,'p', plotname,'Plio_PI: prec')
  
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

EXPTNAME = 'tenvj'
CNTLNAME = 'xozza'
plot_textremes_toplevel()
 

::::::::::::::
PlioMIP_new/extremes/ETCCDI_10_13.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will deal with indices 10-13 these are

10.  percentage of days when TN < 10th percentile for preindustrial (TN=daily minimum temperature)
11. percentage of days when TX < 10th percentile for preindustrial (TX=daily maximum temperature)
12. percentage of days when TN > 90th percentile
13. percentage of days when TX > 90th percentile

"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
from iris.coords import DimCoord
import matplotlib.pyplot as plt
import sys
from iris.experimental.equalise_cubes import equalise_attributes
from os.path import exists



#######################################################################
#  STEP 1 WRITE ALL THE PERCENTILES TO A FILE   
def get_months_days(day):
    """
    To find percentiles we find all the temperatres for the 100 years which 
    are within 2 days. 
    So for 15th May, we would include temperatures 
    from 13th 14th 15th 16th 17th May
    """

    # find central month
    monthreq = np.int(np.floor(day / 30))
    dayreq = day - monthreq * 30

    # find day before
    daym1 = dayreq -1 
    monthm1 = monthreq
    if daym1 < 0:
        monthm1 = monthreq - 1
        if monthm1 < 0: monthm1 = monthm1 + 12
        daym1 = daym1 + 30


    # find two days before
    daym2 = dayreq -2 
    monthm2 = monthreq
    if daym2 < 0:
        monthm2 = monthreq - 1
        if monthm2 < 0: monthm2 = monthm2 + 12
        daym2 = daym2 + 30
    

    # find day after
    dayp1 = dayreq +1 
    monthp1 = monthreq
    if dayp1 >= 30:
        monthp1 = monthreq + 1
        if monthp1 >= 12: monthp1 = monthp1 - 12
        dayp1 = dayp1 - 30

    # find two days after
    dayp2 = dayreq +2 
    monthp2 = monthreq
    if dayp2 >= 30:
        monthp2 = monthreq + 1
        if monthp2 >= 12: monthp2 = monthp2 - 12
        dayp2 = dayp2 - 30

    monthsreq = [monthm2, monthm1, monthreq, monthp1, monthp2]
    daysreq = [daym2, daym1, dayreq, dayp1, dayp2]
    return [monthsreq, daysreq]

def get_temperatures(monthreq,daysreq):
    """
    get the temperatures that are appropriate for finding the percentiles
    for the day of interest
    """
    monthnames = {0:'ja',1:'fb',2:'mr',3:'ar',4:'my',5:'jn',6:'jl',
                  7:'ag',8:'sp',9:'ot',10:'nv',11:'dc'}
    if MIN_MAX == 'min':
         # this is minimum temperature
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_1'))
    if MIN_MAX == 'max':
         # this is maximum temperature
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp'))
    if MIN_MAX == 'mean':
         # this is maximum temperature
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_2'))

    temperature_cubes = iris.cube.CubeList([])
  
    for i, month in enumerate(monthreq):
        day = daysreq[i]
        for year in range(0,100):
            filename = (FILESTART + np.str(year).zfill(2) 
                            + monthnames.get(month) + '.nc')
            if exists(filename):

                cubelist = iris.load(filename, constraints=variable_constraint)
                cube = cubelist[0]
                monthprev = month
        
                tempinit = cube[day, :, :, :]
                temperature = iris.util.squeeze(tempinit)
                temperature_cubes.append(temperature)
            
    equalise_attributes(temperature_cubes)
    print(temperature_cubes)
    temperatures = temperature_cubes.merge_cube()
    print(daysreq,'merged')
   
    return temperatures
    
def percentiles(nperc, temperatures_cube):
    """
    get the percentiles
    in nperc ; the percentiles we want
       temperatures cube;  the temperatures for that day for which we
                           need to find the percentiles, 
                           DIMENSIONS: t latitude, longitude
    """
    data = temperatures_cube.data
    # cube shape for putting the new data
    cube_shape = temperatures_cube.collapsed('t', iris.analysis.MEAN)  
    percentile_array = np.zeros((len(nperc), 73, 96))
    for lat in range(0, 73):
        for lon in range(0, 96):
            # find the sorted temperature
            temperature = np.sort(data[:, lat, lon])
            ntemp = len(temperature)
            for i, perc in enumerate(nperc):
                index = np.int(np.rint(perc * ntemp / 100))
                percentile_array[i, lat, lon] = temperature[index]
                
    # put the percentiles into the cube  
    percentile_cubelist = iris.cube.CubeList([])
    for i in range(0, len(nperc)):
        cube = cube_shape.copy(data=percentile_array[i, :, :])
        if MIN_MAX == 'min':
            cube.long_name = np.str(nperc[i]) + 'th percentile of minimum temperatures'
        if MIN_MAX == 'max':
            cube.long_name = np.str(nperc[i]) + 'th percentile of maximum temperatures'
        if MIN_MAX == 'mean':
            cube.long_name = np.str(nperc[i]) + 'th percentile of maximum temperatures'
        cube.var_name = np.str(nperc[i]) + 'th_percentile'
        percentile_cubelist.append(cube)

    return percentile_cubelist
    

def get_HadCM3_percentiles(expt, extra):
    """
    gets the diagnostics percentiles for writing to a file
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra
  
    # THIS PART OF THE PROGRAM WILL CALCULATE THE PERCENTILES FOR EACH DAY   
    for day in range(0, 360):
        # find days that we need to get temperature.  This is the day required
        # two days before and two days afterwards
        monthsreq, daysreq = get_months_days(day)

        print(monthsreq,daysreq)
        # get all the minimum/ maximum temperatures that fall on one of the 
        # correct days
        temperatures_cube = get_temperatures(monthsreq,daysreq)
        print(temperatures_cube)
        # we should have 500 temperatures so we can find the percentiles
        percentiles_cubelist = percentiles([5, 10, 90, 95],temperatures_cube) 
      
        # write the percentiles to a file for that day.  
        print(np.str(day))
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                    EXPT + '/diag_10_13/daily_percentiles/' + 
                    MIN_MAX + '_temperature_' + np.str(day) + '.nc')

        iris.save(percentiles_cubelist, filename, 
                  netcdf_format="NETCDF3_CLASSIC")
###########################################################################
#  STEP 2 
#  READ ALL THE PERCENTILES FROM THE FILE (OBTAINED IN STEP 1) AND
#  SEE HOW MANY DAYS ARE MORE EXTREME  
def get_month_temperatures(month):      
    """
    get the temperatures over 100 years for this month
    """
    monthnames = {0:'ja',1:'fb',2:'mr',3:'ar',4:'my',5:'jn',6:'jl',
                  7:'ag',8:'sp',9:'ot',10:'nv',11:'dc'}
    # orig code that didn't quite work
   # if MIN_MAX == 'min':
   #      # this is minimum temperature
   #     variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == '#temp_1'))
   # if MIN_MAX == 'max':
   #      # this is maximum temperature
   #     variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp'))
   # if MIN_MAX == 'mean':
   #      # this is maximum temperature
   #     variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == '#temp_2'))

    temperature_cubes = iris.cube.CubeList([])
  
    for year in range(0,NYEARS):
        filename = (FILESTART + np.str(year).zfill(2) 
                            + monthnames.get(month) + '.nc')
        #if exists(filename):
        #cubelist = iris.load(filename, constraints=variable_constraint)
        #cube = cubelist[0]
        # put year in ht field for concatenation

        # new code
        cubes=iris.load(filename)
        cubelist = iris.cube.CubeList([])
        max_temp = []
        for cube in cubes:
            if (cube.var_name == 'temp' or cube.var_name == 'temp_1' 
                or cube.var_name == 'temp_2'):
               try:
                   cube.coord('t_1').rename('t')
               except:
                   pass
               cubelist.append(cube)
               max_temp.append(np.max(cube.data))
        maxindex = np.argmax(max_temp)
        minindex = np.argmin(max_temp)
        indexes = np.argsort(max_temp)
        if MIN_MAX == 'min':
            cubereq = cubelist[indexes[0]]
        if MIN_MAX == 'max':
            cubereq = cubelist[indexes[2]]
        if MIN_MAX == 'mean':
            cubereq = cubelist[indexes[1]]
       
        cubereq.coord('ht').points = year
        cubereq.coord('t').points = np.arange(0,30,1)
        temperature_cubes.append(cubereq)
            

    equalise_attributes(temperature_cubes)
    print(temperature_cubes)
    temperatures = temperature_cubes.concatenate_cube()
    return temperatures

def percentiles_from_file(day):
    """
    reads in the percentiles that we have obtained in step 1
    and returns
    """
    maxminind = {'min': 'minimum', 'max':'maximum'}
   
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   CNTL + '/diag_10_13/daily_percentiles/' + 
                   MIN_MAX + '_temperature_' + np.str(day) + '.nc')
    perc5_cube = iris.load_cube(filename, '5th percentile of ' + 
                               maxminind.get(MIN_MAX) + ' temperatures')
    perc10_cube = iris.load_cube(filename, '10th percentile of ' + 
                               maxminind.get(MIN_MAX) + ' temperatures')
    perc90_cube = iris.load_cube(filename, '90th percentile of ' + 
                               maxminind.get(MIN_MAX) + ' temperatures')
    perc95_cube = iris.load_cube(filename, '95th percentile of ' + 
                               maxminind.get(MIN_MAX) + ' temperatures')

    return perc5_cube, perc10_cube, perc90_cube, perc95_cube

def nextr(temperatures_cube, percentile_cube, lt_gt, percentile, day):
    """
    here we find the number of temperatures that is more extreme than the
    percentile
    """      
    if lt_gt == 'less than':
        extreme_data = np.where(temperatures_cube.data <= percentile_cube.data, 
                                1.0, 0.0)
    if lt_gt == 'more than':
        extreme_data = np.where(temperatures_cube.data >= percentile_cube.data, 
                                1.0, 0.0)
    
    extreme_allyears = ((np.sum(extreme_data, axis=0) * 100)  /
                        np.size(extreme_data,axis=0))
    extreme_cube = percentile_cube.copy(data=extreme_allyears)
    name = ('pcent of years ' + lt_gt + ' ' + CNTL + 
            ' ' + np.str(percentile) + 'th percentile')
    extreme_cube.long_name = name
    extreme_cube.cell_methods = None
    extreme_cube.remove_coord('t')
    extreme_cube.attributes = None
    extreme_cube.var_name = 'pcent_extreme'
    time = DimCoord(day,standard_name = 'time', long_name = 't', 
                              var_name = None, units = 'day')
    print(extreme_cube)
    extreme_cube.add_aux_coord(time)
    ext3d_cube = iris.util.new_axis(extreme_cube, time)
   
    return ext3d_cube
       
def find_nextremes():
    """
    this will find the number of days which are more extreme than the
    percentiles (5th, 10th, 90th, 95th) in the control  (normally preindustrial)
    """  
    extreme5_cubelist = iris.cube.CubeList([])
    extreme10_cubelist =iris.cube.CubeList([])
    extreme90_cubelist = iris.cube.CubeList([])
    extreme95_cubelist = iris.cube.CubeList([])

    for month in range(0,12):
        month_temp_cube = get_month_temperatures(month)
        for day in range(0, 30):
            # get percentile cubes for day
            dayperc = (month *30) + day
            print('dayperc is',dayperc)
            (perc5_cube, perc10_cube, 
             perc90_cube, perc95_cube) = percentiles_from_file(dayperc)
      
             # get temperatures for this day in the year in expt
            Tcube = month_temp_cube[day, :, :, :]
           
            # find how many temperatures are more extreme than percentiles
            extreme5_cube = nextr(Tcube, perc5_cube, 'less than', 5, day)
            extreme10_cube = nextr(Tcube, perc10_cube, 'less than', 10, day)
            extreme90_cube = nextr(Tcube, perc90_cube, 'more than', 90, day)
            extreme95_cube = nextr(Tcube, perc95_cube, 'more than', 95, day)

            # put days in 0-360 format
            extreme5_cube.coord('time').points=[dayperc]
            extreme10_cube.coord('time').points=[dayperc]
            extreme90_cube.coord('time').points=[dayperc]
            extreme95_cube.coord('time').points=[dayperc]

            extreme5_cubelist.append(extreme5_cube)
            extreme10_cubelist.append(extreme10_cube)
            extreme90_cubelist.append(extreme90_cube)
            extreme95_cubelist.append(extreme95_cube)

    equalise_attributes(extreme5_cubelist)
    equalise_attributes(extreme10_cubelist)
    equalise_attributes(extreme90_cubelist)
    equalise_attributes(extreme95_cubelist)
   
  
    extreme5_year_cube = extreme5_cubelist.concatenate_cube()
    extreme10_year_cube = extreme10_cubelist.concatenate_cube()
    extreme95_year_cube = extreme95_cubelist.concatenate_cube()
    extreme90_year_cube = extreme90_cubelist.concatenate_cube()
   
    filename = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + EXPT + '/diag_10_13/percentage_more_extreme_than_T'+ MIN_MAX + '_' + CNTL + '_5th_percentile.nc'
    iris.save(extreme5_year_cube, filename ,netcdf_format="NETCDF3_CLASSIC")
  
    filename = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + EXPT + '/diag_10_13/percentage_more_extreme_than_T'+ MIN_MAX + '_'+  CNTL + '_10th_percentile.nc'
    iris.save(extreme10_year_cube, filename ,netcdf_format="NETCDF3_CLASSIC")
   
    filename = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + EXPT + '/diag_10_13/percentage_more_extreme_than_T'+ MIN_MAX + '_' + CNTL + '_90th_percentile.nc'
    iris.save(extreme90_year_cube, filename ,netcdf_format="NETCDF3_CLASSIC")
   
    filename = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + EXPT + '/diag_10_13/percentage_more_extreme_than_T'+ MIN_MAX + '_' + CNTL + '_95th_percentile.nc'
    iris.save(extreme95_year_cube, filename ,netcdf_format="NETCDF3_CLASSIC")
   
#########################################################
#STEP 3 plot information about extremes
def get_percentile(percentile):
    """
    gets the control temperature of percentile
    """
    maxminind = {'min': 'minimum', 'max':'maximum'}
   
    cubelist = iris.cube.CubeList([])
    for day in range(0, 360):
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                    CNTL + '/diag_10_13/daily_percentiles/' + 
                    MIN_MAX + '_temperature_' + np.str(day) + '.nc')
        perc_cube = iris.load_cube(filename, np.str(percentile) + 
                                    'th percentile of ' + 
                                    maxminind.get(MIN_MAX) + ' temperatures')
        perc_cube.coord('t').points=day
        perc_cube.coord('t').attributes=None
        perc_cube.coord('t').bounds=None
        cubelist.append(perc_cube)

    equalise_attributes(cubelist)
    percentiles_cube = cubelist.merge_cube()
    mean_percentile = percentiles_cube.collapsed('t',iris.analysis.MEAN)
    mean_percentile.convert_units('celsius')
   
    return mean_percentile

    
def plot_extremes_exceeded_annual(percentile, ocn_mask):
    """
    reads in the files showing when the extremes have been exceeded and find
    an annual averages them and plots (land only)
    """
    gtlt = {10 : ' < ', 5 : ' < ' ,90 : ' > ', 95  : ' > '}
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + EXPT 
                + '/diag_10_13/percentage_more_extreme_than_T' + MIN_MAX 
                + '_' + CNTL + '_' + np.str(percentile) 
                + 'th_percentile.nc')
    cube = iris.load_cube(filename)
   
    if ocn_mask == 'y':
        if EXPT == 'tenvj':
            filelsm = '/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrparm.mask.nc'
            cube = iris.load(filelsm)
            print(cube)
            exptlsmcube=iris.load_cube(filelsm,'LAND MASK (LOGICAL: LAND=TRUE)')
            cube_ann_avg.data.mask = (exptlsmcube.data - 1.0) * (-1.0)
    
    if percentile==10:
        valmin=0
        valmax=16
        valrange=2
    if percentile==90 or percentile==95:
        valmin=0
        valmax=55
        valrange=5

    cube_ann_avg.units = None
   # qplt.pcolormesh(cube_ann_avg,vmin=minval, vmax=maxval)
    qplt.contourf(cube_ann_avg,levels=np.arange(valmin,valmax,valrange),
                  extend='max')
    plt.gca().coastlines()
    plt.title('% days: T' + MIN_MAX + gtlt.get(percentile) + 
              TIME.get(CNTL) + ' ' + np.str(percentile)  + 
              ' percentile',fontsize=8)
   
def plot_temp(temp_cube, ocn_mask, title, vals):
    """
    plots a cube of temperatures 
    """   
  
    if ocn_mask == 'y':
        if EXPT == 'tenvj':
            filelsm = '/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrparm.mask.nc'
            cube = iris.load(filelsm)
            print(cube)
            exptlsmcube=iris.load_cube(filelsm,'LAND MASK (LOGICAL: LAND=TRUE)')
            temp_cube.data.mask = (exptlsmcube.data - 1.0) * (-1.0)
        
    qplt.contourf(temp_cube, levels=vals, extend='both')
    plt.gca().coastlines()
    plt.title(title,fontsize=8)
   
    
  
def plot_extremes(perc_low, perc_high):
    """
    we will do a 4 column plot.
    1. cntl 90th percentile
    2. average difference between highest and lowest percentiles
       (to get an idea of natural variability) 
percencentage of days in pliocene which exceed perc_high for pi
    3. difference between perc_high and perc low (to get an idea of 
       natural variability    
    4. percentage of days in pliocene which are lower than perc_low for pi
    """
    temp_percentile_low = get_percentile(perc_low)
    temp_percentile_high = get_percentile(perc_high)
    temp_perc_diff = temp_percentile_high - temp_percentile_low
    
    plt.subplot(2,2,1)
    title = ('ann_avg ' + np.str(perc_low) + 'percentile of T' + 
             MIN_MAX +  ' for' + TIME.get(CNTL))
    vals = np.arange(-45,45,15)
    plot_temp(temp_percentile_low,'y',title, vals )
    
    plt.subplot(2,2,2)
    title = (TIME.get(CNTL) + ' T' + MIN_MAX + ': ' + np.str(perc_high) + 'th -'
             + np.str(perc_low) + 'th percentile')
    vals = np.arange(0,21,3)
    plot_temp(temp_perc_diff,'y',title, vals)

    plt.subplot(2,2,3)
    plot_extremes_exceeded_annual(perc_low, 'y')
   
    plt.subplot(2,2,4)
    plot_extremes_exceeded_annual(perc_high, 'y')
    
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag10-13/' +
               EXPT + '_' + CNTL + np.str(perc_low) + '_' + np.str(perc_high) + 
               '_percentiles_annual_' + 'T' + MIN_MAX)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()
   
   
   
    
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'
MIN_MAX = 'max'
NYEARS = 100
TIME = {'xozza': 'PI', 'tenvj' : 'plio','xozzm':'E560'}
  

# this is for obtaining the percentiles used in climate change indices   
# STEP1
#EXPT= 'tenvj'
EXPT = 'xozzm'  # this is the experiments we are checking
CNTL = 'xozza'  # we are seeing how many of the days  in the experiment
                    # have temperatures more extreme than those in the control 
FILESTART = '/nfs/hera1/earjcti/um/'+EXPT+'/pb/'+EXPT+'a@pbw'
#get_HadCM3_percentiles(EXPT,'w')
#get_HadCM3_percentiles(CNTL,'o')

# STEP 2
# count the number of days with the temperature greater than all the percentiles
find_nextremes()

# STEP3
# plot information from the extremes.  Will need to use the files derived in step 2
plot_extremes(5,95)  # number in brackets is the percentile
::::::::::::::
PlioMIP_new/extremes/ETCCDI_14_15.py
::::::::::::::

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will deal with indices 14-15 these are warm spell and cold spell duration index

14 WSDI, Warm spell duration index: Annual count of days with at least 6 consecutive days when TX > 90th percentile

Let TXij be the daily maximum temperature on day i in period j and let TXin90 be the calendar day 90th percentile centred on a 5-day window for the base period 1961-1990. Then the number of days per period is summed where, in intervals of at least 6 consecutive days:

TXij > TXin90

15 CSDI, Cold spell duration index: Annual count of days with at least 6 consecutive days when TN < 10th percentile

Let TNij be the daily maximum temperature on day i in period j and let TNin10 be the calendar day 10th percentile centred on a 5-day window for the base period 1961-1990. Then the number of days per period is summed where, in intervals of at least 6 consecutive days:


"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
from iris.coords import DimCoord
import matplotlib.pyplot as plt
import sys
from iris.experimental.equalise_cubes import equalise_attributes
from os.path import exists


def percentiles_from_file():
    """
    reads in the percentiles that we have obtained in step 1
    and returns 12 cubes one for each month
    """
    if MIN_MAX == 'min':
        fieldname = '10th percentile of minimum temperatures'
    if MIN_MAX == 'max':
        fieldname = '90th percentile of maximum temperatures'
    
   
    perc_cube_allmonths = iris.cube.CubeList([])

    for month in range(0,12):
        cubelist = iris.cube.CubeList([])
        daystart = month * 30
        dayend = daystart + 30

        for day in range(daystart,dayend):
            filename = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                        CNTLNAME + '/diag_10_13/daily_percentiles/' + 
                        MIN_MAX + '_temperature_' + np.str(day) + '.nc')
            perc_cube = iris.load_cube(filename, fieldname)
            # stuff to make sure they can concatenate
            perc_cube.cell_methods = None
            perc_cube.remove_coord('t')
            perc_cube.attributes = None
            perc_cube.var_name = 'pcent_extreme'
            time = DimCoord(day,standard_name = 'time', long_name = 't', 
                              var_name = None, units = 'day')
            perc_cube.add_aux_coord(time)
            perc_cube_3d = iris.util.new_axis(perc_cube, time)

            cubelist.append(perc_cube_3d)

    
        percentile_month_cube = cubelist.concatenate_cube()
        perc_cube_allmonths.append(percentile_month_cube)

    return perc_cube_allmonths

       
    
def calculate_extreme_spell():
    """
    this function will calculate the number of days which belong to an
    extreme spell for each gridbox in the 100 year period
    """

    # read in percentiles file.  we need one of these per month
    # the cubelist will contain one cube per month
    perc_cubelist = percentiles_from_file()
    data_cube = perc_cubelist[0][0,:,:] # shape reqd for putting warm spell data
    warm = {'min' : 'cold', 'max' : 'warm'}
    warm_cold = {'min' : 'coldspell', 'max' : 'warmspell'}
   
    # get_temperature data and find where it is extreme
    if MIN_MAX == 'min':
        # this is minimum temperature
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_1'))
    if MIN_MAX == 'max':
        # this is maximum temperature
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp'))
    if MIN_MAX == 'mean':
         # this is maximum temperature
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_2'))

    # find where the data is extreme
    extreme_data = np.zeros((NYEARS * 12,30, 73, 96))
    filestart = ('/nfs/hera1/earjcti/um/' + EXPTNAME + '/pb/' + EXPTNAME
                 + 'a@pb' + EXTRA )
    
    for year in range(0,0 + NYEARS):
        print(year)
        cubelist_ann = iris.cube.CubeList([])
        for month in range(0,12):
            monix = (year * 12) + month
            percentile_cube = perc_cubelist[month]
            filename = (filestart + np.str(year).zfill(2) 
                        + MONTHNAMES.get(month) + '.nc')
            if exists(filename):
                print(filename)
            else:
                print('replacing',filename)
                filename = (filestart + np.str(year-1).zfill(2) 
                        + MONTHNAMES.get(month) + '.nc')
                print('with',filename)

            
            cubes = iris.load(filename, constraints=variable_constraint)
            cube = iris.util.squeeze(cubes[0])
            print(np.shape(cube.data))
            print(np.shape(percentile_cube.data))
         
            if MIN_MAX == 'min':
                extreme_data[monix,:,:] = np.where(cube.data 
                                                   <= percentile_cube.data, 
                                                   1.0, 0.0)
            if MIN_MAX == 'max':
                extreme_data[monix,:,:] = np.where(cube.data 
                                                   >= percentile_cube.data, 
                                                   1.0, 0.0)
         
    #use the extreme data array to find out how many wam/cold spells there
    #are 
    spell_array = np.zeros((73,96))
    for lat in range(0,73):
        print('lat is',lat)
        for lon in range(0,96):
            poss_warm_spell = 0.0
            for day in range(0,360*NYEARS):
                monix=np.int(np.floor(day/30))
                dayix = day - monix * 30
                # check extremes
                if extreme_data[monix,dayix,lat,lon] == 1.0:
                    poss_warm_spell = poss_warm_spell+1.0
                # if not an extreme reset start of warm spell
                if (extreme_data[monix,dayix,lat,lon]==0.0 
                    and poss_warm_spell > 0.0):
                    if poss_warm_spell >=6: 
                        spell_array[lat,lon] = (spell_array[lat,lon] + 
                                                poss_warm_spell)
                    poss_warm_spell=0.0
          
    spell_array = (spell_array *100.) / (360. * NYEARS)
    spell_cube = data_cube.copy(data=spell_array)
    spell_cube.units=None
    spell_cube.long_name = 'percentage of ' + warm.get(MIN_MAX) + ' spell days'
    outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   EXPTNAME + '/diag14_15/' + EXTRA + '_' + CNTLNAME + '_' + 
                   warm_cold.get(MIN_MAX) + 
                   '_diag14-15.nc') 
    iris.save(spell_cube, outfile, netcdf_format="NETCDF3_CLASSIC")
    qplt.contourf(spell_cube)
    plt.show()
#########################################################
def plot_nextreme_spell(ocn_mask):
    """
    plots the number of days in a warm spll or a cold spell
    """
    warm_cold = {'min' : 'coldspell', 'max' : 'warmspell'}
    period = {'tenvj':'mPWP', 'xozza':'PI', 'xozzb':'mPWP'}
   
    gtlt = {10 : ' < ', 5 : ' < ' ,90 : ' > ', 95  : ' > '}
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   EXPTNAME + '/diag14_15/' + EXTRA + '_' + CNTLNAME + '_' + 
                   warm_cold.get(MIN_MAX) + 
                   '_diag14-15.nc')

    cube = iris.load_cube(filename)
    print(cube)
 
    if ocn_mask == 'y':
        if EXPTNAME == 'tenvj':
            filelsm = '/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrparm.mask.nc'
            exptlsmcube=iris.load_cube(filelsm,'LAND MASK (LOGICAL: LAND=TRUE)')
            cube.data.mask = (exptlsmcube.data - 1.0) * (-1.0)
        if EXPTNAME == 'xozza':
            filelsm = '/nfs/hera1/earjcti/ancil/preind2/qrparm.mask.nc'
            exptlsmcube=iris.load_cube(filelsm,'LAND MASK (LOGICAL: LAND=TRUE)')
            cube.data.mask = (exptlsmcube.data - 1.0) * (-1.0)
    
   
  
    if MIN_MAX == 'max' and EXPTNAME == 'tenvj' and CNTLNAME == 'xozza':
        valmin=0.
        valmax=55.
        valdiff=5.
    else:
        valmin=0.
        valmax=6.
        valdiff=1.


    cube.units = '%'
   # qplt.pcolormesh(cube_ann_avg,vmin=minval, vmax=maxval)
    qplt.contourf(cube,levels=np.arange(valmin,valmax,valdiff),
                  extend='max')
    plt.gca().coastlines()
    plt.title(('percentage of ' + period.get(EXPTNAME) + ' days that are ' 
              + warm_cold.get(MIN_MAX) + ' days in '+ period.get(CNTLNAME)),
              fontsize=10)
    filestart = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/' +
                 'diag14_15/' + EXTRA + '_' + EXPTNAME + '_' + CNTLNAME + 
                 '_percentage_' + 
                   warm_cold.get(MIN_MAX))
    plt.savefig(filestart + '.eps')
    plt.savefig(filestart + '.png')
   
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'
MIN_MAX = 'max' # max is warm spell duration index
                # min is cold spell duration index

NYEARS = 100
MONTHNAMES = {0:'ja',1:'fb',2:'mr',3:'ar',4:'my',5:'jn',6:'jl',
                  7:'ag',8:'sp',9:'ot',10:'nv',11:'dc'}
  
EXPTNAME = 'xozzb'
CNTLNAME = 'xozza'
EXTRA='o'



nextreme_spell = calculate_extreme_spell()

plot_nextreme_spell('y')
::::::::::::::
PlioMIP_new/extremes/ETCCDI_1_4.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will write
indices 1-4 to a file.  These are:

1. FD: Number of frost days:  Annual count of days when TN (daily minimum temperature) < 0degC
2. SU: Number of summer days:  Annual count of days when TX (daily maimum temperature) > 25degC
3. ID: Icing days:  Annual count of days when TX (daily maximum temperature) < 0degC
4. TR, Number of tropical nights.  Annual count of days when TN (daily minimum temperature) > 20degC

"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys


def get_HCM3_year_data(filestart,year):
    """
    reads in the maximum  and minimum temperature for the year and puts it in 
    a single cube
    """
    maxTcubelist = iris.cube.CubeList([])
    minTcubelist = iris.cube.CubeList([])
    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    for month in months:
        filename = filestart + np.str(year).zfill(2) + month + '.nc'
        cubes=iris.load(filename)
        temperature_cubes = iris.cube.CubeList([])
        max_temp = []
        for cube in cubes:
            if (cube.var_name == 'temp' or cube.var_name == 'temp_1' 
                or cube.var_name == 'temp_2'):
               try:
                   cube.coord('t_1').rename('t')
               except:
                   pass
               temperature_cubes.append(cube)
               max_temp.append(np.max(cube.data))
     
        maxindex = np.argmax(max_temp)
        minindex = np.argmin(max_temp)
        indexes = np.argsort(max_temp)
        minTcube = temperature_cubes[indexes[0]]
        meanTcube = temperature_cubes[indexes[1]]
        maxTcube = temperature_cubes[indexes[2]]
    
        # check you have got maxT, meanT and minT in correct order        
        if np.max(maxTcube.data) < np.max(meanTcube.data):
            print('cubes not in right order')
            sys.exit(0)

        if np.max(meanTcube.data) < np.max(minTcube.data):
            print('cubes not in right order2')
            sys.exit(0)

        maxTcubelist.append(maxTcube-273.15)
        minTcubelist.append(minTcube-273.15)

    equalise_attributes(maxTcubelist)
    equalise_attributes(minTcubelist)
    maxTyearcube = maxTcubelist.concatenate_cube(maxTcubelist)
    minTyearcube = minTcubelist.concatenate_cube(minTcubelist)
  
    return maxTyearcube, minTyearcube
   

def get_HadCM3_diagnostics(expt, extra):
    """
    gets the diagnostics (frost days, summer days, icing days tropical nights
    from HadCM3)
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra
  
    for year in range(0, 99):
        yearuse = np.str(year).zfill(2)
        maxTcube, minTcube  = get_HCM3_year_data(filestart, year)

        # frost where daily min T is less than 0
        frost = np.where(minTcube.data < 0, 1.0, 0)
        frostcube = minTcube.copy(data = frost)
        frostdays = frostcube.collapsed('t',iris.analysis.SUM)
        frostdayscube = iris.util.squeeze(frostdays)
        frostdayscube.long_name = 'number of frost days'
        frostdayscube.units = None

        # summer where daily maximum T is greater than 25
        summer = np.where(maxTcube.data > 25, 1.0, 0)
        summercube = maxTcube.copy(data = summer)
        summerdays = summercube.collapsed('t',iris.analysis.SUM)
        summerdayscube = iris.util.squeeze(summerdays)
        summerdayscube.long_name = 'number of summer days'
        frostdayscube.units = None
        
        # Icing days: when daily maximum temperature < 0degC
        icing = np.where(maxTcube.data < 0, 1.0, 0)
        icingcube = maxTcube.copy(data = icing)
        icingdays = icingcube.collapsed('t',iris.analysis.SUM)
        icingdayscube = iris.util.squeeze(icingdays)
        icingdayscube.long_name = 'number of icing days'
        icingdayscube.units = None

        # Number of tropical nights where daily minimum temperature) > 20degC
        aladin = np.where(minTcube.data > 20., 1.0, 0)
        aladincube = minTcube.copy(data = aladin)
        aladin_nights = aladincube.collapsed('t',iris.analysis.SUM)
        aladin_nights_cube = iris.util.squeeze(aladin_nights)
        aladin_nights_cube.long_name = 'number of tropical nights'
        aladin_nights_cube.units = None


        cubelist = [frostdayscube, summerdayscube, icingdayscube,
                    aladin_nights_cube]

        outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   expt + '/diag1-4/' + extra + '_' + 
                   'diag1-4_' + np.str(year) + '.nc')
        iris.save(cubelist, outfile, netcdf_format="NETCDF3_CLASSIC")

        print('saved cube',outfile)
        
##########################################################
def read_data(expt,extra,startyear,endyear):
    """
    reads in the data for each year, finds the sum and returns
    """
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/'

    frostcubelist = iris.cube.CubeList([])
    summercubelist = iris.cube.CubeList([])
    icingcubelist = iris.cube.CubeList([])
    tropcubelist = iris.cube.CubeList([])
 
    for year in range(startyear,endyear):
        filename = (filestart + expt + '/diag1-4/' + extra + 
                    '_diag1-4_'+ np.str(year) + '.nc')

        frostcubelist.append(iris.load_cube(filename,'number of frost days'))
        summercubelist.append(iris.load_cube(filename,'number of summer days'))
        icingcubelist.append(iris.load_cube(filename,'number of icing days'))
        tropcubelist.append(iris.load_cube(filename,
                                           'number of tropical nights'))

    equalise_attributes(frostcubelist)
    allfrostcube = frostcubelist.merge_cube()
    meanfrostcube = allfrostcube.collapsed('t',iris.analysis.MEAN)
   
    equalise_attributes(summercubelist)
    allsummercube = summercubelist.merge_cube()
    meansummercube = allsummercube.collapsed('t',iris.analysis.MEAN)
    meansummercube.units = None
  
    equalise_attributes(icingcubelist)
    allicingcube = icingcubelist.merge_cube()
    meanicingcube = allicingcube.collapsed('t',iris.analysis.MEAN)
   
    equalise_attributes(tropcubelist)
    alltropcube = tropcubelist.merge_cube()
    meantropcube = alltropcube.collapsed('t',iris.analysis.MEAN)

    return (meanfrostcube, meansummercube, meanicingcube,meantropcube)
  
def plot_anom(cube,field,ocn_mask, expt, cntl, cube_cntl, cube_expt):
    """
    this will plot the anomaly cube between the pliocene and the pi
    """
    fieldname = {'frost' : 'number of frost days',
                 'summer': 'number of summer days',
                 'icing': 'number of icing days',
                 'tropical_nights' : 'number of tropical nights'}

    if ocn_mask == 'y':
       maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
      
       cube.data.mask = (maskcube.data - 1.0) * (-1.0)
       cube_cntl.data.mask = (maskcube.data - 1.0) * (-1.0)
       cube_expt.data.mask = (maskcube.data - 1.0) * (-1.0)
      
    plt.subplot(2,2,1)
    qplt.contourf(cube_cntl, levels=np.arange(0,300,20), extend='max')
    plt.gca().coastlines()
    plt.title(fieldname.get(field) + ':' +  TIME.get(cntl))

    plt.subplot(2,2,2)
    qplt.contourf(cube_expt, levels=np.arange(0,300,20), extend='max')
    plt.gca().coastlines()
    plt.title(fieldname.get(field) + ':' +  TIME.get(expt))

    plt.subplot(2,2,3)
    if field == 'frost' or field == 'icing':
        cmapname = 'Blues_r'
        vals = np.arange(-40,5,5)
    else:
        cmapname = 'Reds'
        vals = np.arange(0,65,5)
    qplt.contourf(cube,cmap=cmapname, levels=vals,extend='both')
    plt.gca().coastlines()
    plt.title(fieldname.get(field) + ': ' + TIME.get(expt) + '-' +TIME.get(cntl))
    plt.subplot(2,2,4)
    qplt.contourf((cube / cube_cntl) * 100.,cmap=cmapname, 
                  levels=vals,extend='both')
    plt.gca().coastlines()
    plt.title('percentage change: ' + TIME.get(expt) + '-' +TIME.get(cntl))


    plt.tight_layout()
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag1-4/' + 
               field + '_' + expt + '-' + cntl)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()
    
    
def plot_n_extremes(expt, exptextra, cntl, cntlextra, nyears):
    """
    plots the number of days that are: 1. frost days, 2. summer days,
    3. icing days, 4. tropical nights and how it changes between plio and cntl
    """
    (frost_expt_cube, summerday_expt_cube, 
    icing_expt_cube, tropnight_expt_cube) = read_data(expt,exptextra,0, nyears)

    (frost_cntl_cube, summerday_cntl_cube, 
    icing_cntl_cube, tropnight_cntl_cube) = read_data(cntl,cntlextra,0, nyears)

    frost_anom_cube = frost_expt_cube - frost_cntl_cube
    plot_anom(frost_anom_cube,'frost','y', expt,cntl,
              frost_cntl_cube, frost_expt_cube)
  
    summer_anom_cube = summerday_expt_cube - summerday_cntl_cube
    plot_anom(summer_anom_cube,'summer','y', expt,cntl,
              summerday_cntl_cube, summerday_expt_cube)
  
    icing_anom_cube = icing_expt_cube - icing_cntl_cube
    plot_anom(icing_anom_cube,'icing','y', expt,cntl,
              icing_cntl_cube, icing_expt_cube)
  
    trop_anom_cube = tropnight_expt_cube - tropnight_cntl_cube
    plot_anom(trop_anom_cube,'tropical_nights','y', expt,cntl,
              tropnight_cntl_cube, tropnight_expt_cube)
  
    
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

TIME = {'tenvj' : 'mPlio', 'xozza' : 'PI', 'xozzb' : 'Plio','tenvs':'E560',
        'xozzm' : 'E560'}
# this is for if you actually want to get the diagnostics.
# ie write them to the file /nfs/hera1/earjcti/PLIOMIP2/..ETCCDI....diags 1-4  
#if MODELNAME == 'HadCM3':
#    get_HadCM3_diagnostics('xozzm','w')

# plot map of number of days that are 'extreme' according to the ETCCDI 1-4
# criteria
# plot anomaly from plio and pi

plot_n_extremes('xozzm','w','xozza','o',99)  # tenvj plio, xozza control, xozzb control
 

::::::::::::::
PlioMIP_new/extremes/ETCCDI_16.py
::::::::::::::

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will deal with indices 16 this is monthly average daily temperature range

DTR, Daily temperature range: Monthly mean difference between TX and TN

Let TXij and TNij be the daily maximum and minimum temperature respectively on day i in month j. If I represents the number of days in j, then:

DTR j = SIGMA(TXij - TN) / 30.  

"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
from iris.coords import DimCoord
import matplotlib.pyplot as plt
import sys
from iris.experimental.equalise_cubes import equalise_attributes
from os.path import exists


def get_HCM3_year_data(filestart,year):
    """
    reads in the maximum  and minimum temperature for the year and puts it in 
    a single cube
    """
    DTRcubelist = iris.cube.CubeList([])
    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    for monthno, month in enumerate(months):
        filename = filestart + np.str(year).zfill(2) + month + '.nc'
        # load in data
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp'))
        cube = iris.load(filename, constraints=variable_constraint)
        maxTcube = cube[0]

        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_1'))
        cube = iris.load(filename, constraints=variable_constraint)
        minTcube = cube[0]

        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_2'))
        cube = iris.load(filename, constraints=variable_constraint)
        meanTcube = cube[0]

        # check you have got maxT, meanT and minT in correct order        
        if np.max(maxTcube.data) < np.max(meanTcube.data):
            print('cubes not in right order')
            sys.exit(0)

        if np.max(meanTcube.data) < np.max(minTcube.data):
            print('cubes not in right order2')
            sys.exit(0)

        dtrdailycube = maxTcube - minTcube
        dtrmonthcube = dtrdailycube.collapsed('t', iris.analysis.MEAN)
        dtrmonthcube.coord('t').points = monthno + 1
        DTRcubelist.append(dtrmonthcube)
       
    equalise_attributes(DTRcubelist)
    iris.util.unify_time_units(DTRcubelist)
    DTRcube = DTRcubelist.merge_cube()
    DTRcube.coord('ht').rename('year')
    DTRcube.coord('year').points = year
    DTRcube.coord('t').rename('month')
    DTRcube.coord('month').attributes=None
    DTRcube.coord('month').units=None
    DTRcube.coord('month').bounds=None
    DTRcube.cell_methods = None
   
    return DTRcube
   
def calculate_daily_temperature_range():
    """
    for each year and for each month calculates the average daily
    temperature range Tmax - Tmin
    """
    filestart = ('/nfs/hera1/earjcti/um/' + EXPTNAME + '/pb/' + 
                 EXPTNAME + 'a@pb' + EXTRA)
  
    DTR_year_cubes = iris.cube.CubeList([])
    for year in range(0, 100):
        yearuse = np.str(year).zfill(2)
        DTRcube  = get_HCM3_year_data(filestart, year)
        DTR_year_cubes.append(DTRcube)

    equalise_attributes(DTR_year_cubes)
    iris.util.unify_time_units(DTR_year_cubes)
   

    allDTR_cube = DTR_year_cubes.concatenate_cube()
    allDTR_cube.long_name = 'Diurnal Temperature Range'
    allDTR_cube.units='Celsius'

    fileout = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
               EXPTNAME + '/diag16/DTR.nc')
    iris.save(allDTR_cube,fileout)

#########################################################
def plot_daily_temperature_range(ocn_mask):
    """
    plots the daily temperature range in EXPTNAME and also the 
    difference between the experiment and the control
    """ 
    TIME = {'tenvj' : 'mPWP',
            'xozza' : 'PI'}
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/'

    # read in control data (normally pi)
    cube_cntl = iris.load_cube(filestart + CNTLNAME + '/diag16/DTR.nc')
    cube_cntl_avg = cube_cntl.collapsed('year',iris.analysis.MEAN)
    # read in experiment data (normally pliocene)
    cube_expt = iris.load_cube(filestart + EXPTNAME + '/diag16/DTR.nc')
    cube_expt_avg = cube_expt.collapsed('year',iris.analysis.MEAN)
    # read in masks in case they are needed
    maskplio=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
    maskpi=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/e280/qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
       
      
   
    for mon in range(1,13):
        cube_cntl_mon = cube_cntl_avg.extract(iris.Constraint(month=mon))
        cube_expt_mon = cube_expt_avg.extract(iris.Constraint(month=mon))
       
        if ocn_mask == 'y':
            if TIME.get(EXPTNAME) == 'mPWP':
                cube_expt_mon.data.mask = (maskplio.data - 1.0) * (-1.0)
            if TIME.get(CNTLNAME) == 'PI':
                cube_cntl_mon.data.mask = (maskpi.data - 1.0) * (-1.0)
      
        plt.subplot(2,2,1)
        qplt.contourf(cube_expt_mon, levels=np.arange(0,22,2), extend='both')
        plt.gca().coastlines()
        plt.title(MONTHNAMES.get(mon-1) + 
                  ': Tmax - Tmin:' +  TIME.get(EXPTNAME))

        plt.subplot(2,2,2)
        qplt.contourf(cube_cntl_mon, levels=np.arange(0,22,2), extend='both')
        plt.gca().coastlines()
        plt.title('Tmax - Tmin:' +  TIME.get(CNTLNAME))

        plt.subplot(2,2,3)
        print(cube_expt_mon)
        print(cube_cntl_mon)
        diffcube = cube_expt_mon - cube_cntl_mon
        qplt.contourf(diffcube,levels=np.arange(-3,3.5,0.5), extend='both',
                      cmap='RdBu_r')
        plt.gca().coastlines()
        plt.title('Tmax - Tmin:' + TIME.get(EXPTNAME) + '-'+ TIME.get(CNTLNAME))
        

        plt.tight_layout()
        fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/' + 
                    'diag16/' + EXPTNAME + '_' + CNTLNAME + '_' + 
                   MONTHNAMES.get(mon-1))
        plt.savefig(fileout + '.eps')
        plt.savefig(fileout + '.png')
        plt.close()
    
   
   
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'
NYEARS = 100
MONTHNAMES = {0:'January',1:'February',2:'March',3:'April',4:'May',5:'June',
              6:'July', 7:'August',8:'September',9:'October',10:'November',
              11:'December'}
  
######################################
#  step 1.  Calculate daily temperature range
EXPTNAME = 'xozzb'
EXTRA='o'

#calculate_daily_temperature_range() # and write to a file

#####################################
# step 2.  Plot daily temperature range
# can only do this after we have done step 1

EXPTNAME = 'xozzb'
CNTLNAME = 'xozza'
EXTRA = 'o'
plot_daily_temperature_range('y') # pass 'y' for mask ocean, 'n' for no ocn mask
::::::::::::::
PlioMIP_new/extremes/ETCCDI_17_18.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will write
indices 17 and 18 to a file:

17: Rx1day: Monthly maximum 1-day precipitation
Let RRij be the daily precipitation amount on day i in month j.  The maximum 1-day value for month j is:
Rx1dayj = max(RRij)

18: Rx5day: Monthly maximum consecutive 5-day precipitation
Let RRkj be the precipitation amount for the 5 day interval ending k in period j.  The maximum 5-day values for the period j are
Rx5dayj = max(RRkj)

"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys


def get_Hcm3_max(filename,month,year):
    """
    reads in all the precipitation fields
    figures out which precipitation field is the mean
    # diagnostic 17
    finds the maximum precipitation in the file
    converts to mm/day
    # diagnostic 18
    finds the 5 day precipitation
    finds the maximum of the 5 day precipitation
    sends back
    """

    # read in the precipitation fields and figure out which is the mean
    cubes = iris.load(filename)
    for cube in cubes:
        if cube.var_name == 'precip':
            precipcube = cube
            maxp = np.max(precipcube.data)
        if cube.var_name == 'precip_1':
            precip1cube = cube
            maxp1 = np.max(precip1cube.data)
        if cube.var_name == 'precip_2':
            precip2cube = cube
            maxp2 = np.max(precip2cube.data)

    maxps = np.max([maxp, maxp1, maxp2])
    minps = np.min([maxp, maxp1, maxp2])
    found = 'n'
    if minps < maxp < maxps:
        meanp = maxp
        meanprecipcube = precipcube
        found = 'y'

    if minps < maxp2 < maxps:
        if found == 'y':
            print('error')
            sys.exit(0)
        else:
            meanp = maxp2
            meanprecipcube = precip2cube
            found = 'y'

    if minps < maxp1 < maxps:
        if found == 'y':
            print('error')
            sys.exit(0)
        else:
            meanp = maxp1
            meanprecipcube = precip1cube
            found = 'y'


    # get diagnostic 17
    maxprecipcube = meanprecipcube.collapsed('t',iris.analysis.MAX)
    maxprecipcube = iris.util.squeeze(maxprecipcube)

    # convert to mm/day.  Also add a dimension for year and month
    yearcoord = iris.coords.DimCoord(2,long_name='year')
    maxprecipcube.add_aux_coord(yearcoord,data_dims=None)
    moncoord = iris.coords.DimCoord(3,long_name='month')
    maxprecipcube.add_aux_coord(moncoord,data_dims =None)
    maxprecipcube = iris.util.new_axis(maxprecipcube,'year')
    maxprecipcube = iris.util.new_axis(maxprecipcube,'month')
    maxprecipcube.coord('year').points = year
    maxprecipcube.coord('month').points = month
    maxprecipcube = maxprecipcube * 60. *60. *24.
    maxprecipcube.units = 'mm/day'
    maxprecipcube.remove_coord('t')
    maxprecipcube.remove_coord('surface')

    # get diagnostic 18
    # get 5 day precipipitation
    meandata = meanprecipcube.data
    precip5day = np.zeros(np.shape(meandata))
    print(np.shape(meanprecipcube.data))
    for i in range(0,25):
        precip5day[i,:,:,:] = (meandata[i,:,:,:] + meandata[i+1, :,:,:] + 
                               meandata[i+2,:,:,:] + meandata[i+3, :,:,:] + 
                               meandata[i+4, :,:,:])
    precip5cube = meanprecipcube.copy(data = precip5day)
    max5precipcube = precip5cube.collapsed('t',iris.analysis.MAX)
    max5precipcube = iris.util.squeeze(max5precipcube)
    max5precipcube.add_aux_coord(yearcoord, data_dims=None)
    max5precipcube.add_aux_coord(moncoord,data_dims=None)
    max5precipcube = iris.util.new_axis(max5precipcube,'year')
    max5precipcube = iris.util.new_axis(max5precipcube,'month')
    max5precipcube.coord('year').points = year
    max5precipcube.coord('month').points = month
    max5precipcube = max5precipcube * 60. * 60. * 24.
    max5precipcube.units = 'mm/5days'
    max5precipcube.remove_coord('t')
    max5precipcube.remove_coord('surface')
        
        

    return maxprecipcube, max5precipcube
   

def get_HadCM3_diagnostics(expt, extra):
    """
    gets the diagnostics (frost days, summer days, icing days tropical nights
    from HadCM3)
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra
    monthnames = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']

    cubes = iris.cube.CubeList([])
    cubes_5 = iris.cube.CubeList([])
    for year in range(0, 100):
        yearuse = np.str(year).zfill(2)
        for month in range(0,12):
            filename = ('/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 
                        'a@pb' + extra + yearuse + monthnames[month] + '.nc')
            (maxprecip_cube,
             max5precip_cube)= get_Hcm3_max(filename, month, year)
            cubes.append(maxprecip_cube)
            cubes_5.append(max5precip_cube)

    equalise_attributes(cubes)
    maxprecip = cubes.concatenate_cube()
    maxprecip.long_name = 'maximum precip for a day in this month'
    maxprecip.var_name = 'precip'
  

    equalise_attributes(cubes_5)
    max5precip = cubes_5.concatenate_cube()
    max5precip.long_name = 'maximum precip for a 5 day period in this month'
    maxprecip.var_name = 'precip'
    outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   expt + '/diag17_18/' + extra + '_' + 'diag17_18.nc')
    iris.save([maxprecip, max5precip], outfile, netcdf_format="NETCDF3_CLASSIC")

  
  
    
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

TIME = {'tenvj' : 'mPlio', 'xozza' : 'PI', 'xozzb' : 'Plio','tenvs':'E560'}
# this is for if you actually want to get the diagnostics.
# ie write them to the file /nfs/hera1/earjcti/PLIOMIP2/..ETCCDI....diags 17 
if MODELNAME == 'HadCM3':
    max1dayprecip = get_HadCM3_diagnostics('tenvj','o')

 

::::::::::::::
PlioMIP_new/extremes/ETCCDI_23_24.py
::::::::::::::

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 14.07.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will deal with indices 23-24 these are dry spell and wet spell duration index

23 CDD: Maximum length of dry spell, maximum number of consecutive days with RR< 1mm.
Let RRij be the daily predipitation amount on day i in preiod j.  Count the largest number of consecutive days where RRij < 1mm

(ETCCDI did not specify a period.  However I will use a period of 1 year.  However if the 31st december is a dry spell then we will just keep going.  We will not let a dry spell last for more than 180 days).  

23 WDD: Maximum length of wet spell, maximum number of consecutive days with RR> 1mm.
Let RRij be the daily predipitation amount on day i in preiod j.  Count the largest number of consecutive days where RRij > 1mm
This is also known as consecutive wet days.


"""
import numpy as np
import iris
import iris.quickplot as qplt
from iris.coords import DimCoord
import matplotlib.pyplot as plt
import sys
from os.path import exists


def just_testing():
    """
    just check if the sum of the daily precipitation matches the monthly precip
    """
    monthname = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    daily_cubes = iris.cube.CubeList([])
    monthly_cubes = iris.cube.CubeList([])
    for month in range(0,12):
        pointsstart =  month *30.
        points=np.arange(pointsstart, pointsstart+30, 1.0)
        day = iris.coords.DimCoord(points,
                             long_name = 't', attributes = None,
                             bounds=None, circular=False)
     
        cubes = iris.load('/nfs/hera1/earjcti/um/'+ EXPTNAME + '/pb/' + EXPTNAME + 'a@pb' + EXTRA + '02'+ monthname[month] + '.nc')
        if EXPTNAME == 'xozzm':
            varreq = 'precip'
        else:
            varreq = 'precip_2'
        for cube in cubes:
            if cube.var_name == varreq:
                try:
                    cube.coord('t_1'.rename('t'))
                except:
                    pass
                cube = cube * 24.* 60. * 60.
                cube.units='mm/day'
                cube.long_name = 'precip from daily data'
                iris.util.demote_dim_coord_to_aux_coord(cube, 't')
                cube.remove_coord('t')
                cube.add_dim_coord(day,0)
                cube = iris.util.squeeze(cube)
                daily_cubes.append(cube)
                
                
        cube = iris.load_cube('/nfs/hera1/earjcti/um/'+EXPTNAME+'/pd/' + EXPTNAME + 'a@pd'+ EXTRA + '02' + monthname[month] + '.nc','TOTAL PRECIPITATION RATE     KG/M2/S')
        cube = cube * 24.* 60. * 60.
        cube.units='mm/day'
        cube = iris.util.squeeze(cube)
        monthly_cubes.append(cube)

    iris.util.unify_time_units(daily_cubes)
    iris.util.equalise_attributes(daily_cubes)
    daily_cube = daily_cubes.concatenate_cube()

    print(monthly_cubes)
    iris.util.unify_time_units(monthly_cubes)
    iris.util.equalise_attributes(monthly_cubes)
    monthly_cube = monthly_cubes.merge_cube()
              
    daily_mean_cube = daily_cube.collapsed('t',iris.analysis.MEAN)
    daily_max_cube = daily_cube.collapsed('t',iris.analysis.MAX)
    daily_mean_cube = iris.util.squeeze(daily_mean_cube)
    daily_max_cube = iris.util.squeeze(daily_max_cube)
    monthly_mean_cube = monthly_cube.collapsed('t',iris.analysis.MEAN)
    monthly_mean_cube = iris.util.squeeze(monthly_mean_cube)
    monthly_mean_cube.long_name = 'precip from monthly data'
   
    fig = plt.figure(figsize=[11.0,8.0])
    plt.subplot(221)
    qplt.contourf(daily_mean_cube,levels=np.arange(0,20,2))
    plt.subplot(222)
    qplt.contourf(monthly_mean_cube,levels=np.arange(0,20,2))
    plt.subplot(223)
    qplt.contourf(monthly_mean_cube - daily_mean_cube,levels=np.arange(-1.0,2.0,0.2))
    plt.title('difference between daily and monthly')
    plt.subplot(224)
    qplt.contourf(daily_max_cube,levels=np.arange(0,110,10),extend='max')
    plt.title('maximum precip from daily data')

    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/test_' + EXPTNAME + '.eps')
    sys.exit(0)

def get_ann_precip(year):
    """
    gets the precipitation for a year
    """
    cubelist_precip = iris.cube.CubeList([])
      
    filestart = ('/nfs/hera1/earjcti/um/' + EXPTNAME + '/pb/' + 
                 EXPTNAME +  'a@pb' + EXTRA + np.str(year).zfill(2))
    for month in range(0,12):
        filename = filestart + MONTHNAMES.get(month) + '.nc'
        cubes = iris.load(filename)

        for cube in cubes:
            if cube.var_name == 'precip':
                precipcube = cube
                precipcube = iris.util.squeeze(precipcube)
                maxp = np.mean(precipcube.data)
            if cube.var_name == 'precip_1':
                precip1cube = cube
                precip1cube = iris.util.squeeze(precip1cube)
                maxp1 = np.mean(precip1cube.data)
            if cube.var_name == 'precip_2':
                precip2cube = cube
                precip2cube = iris.util.squeeze(precip2cube)
                maxp2 = np.mean(precip2cube.data)
        
       # it didn't like the time coordinate.  Set a new one
        try:
            tcoord = precip1cube.coord('t') 
            tcoordname = 't'
        except: 
            tcoord = precip1cube.coord('t_1')
            tcoordname = 't_1'

        pointsstart = (year * 360.) + (month *30.)
        points=np.arange(pointsstart, pointsstart+30, 1.0)
        day = iris.coords.DimCoord(points,
                             long_name = 't', 
                             attributes = tcoord.attributes,
                             bounds=None, circular=False)
       
        iris.util.demote_dim_coord_to_aux_coord(precip1cube, tcoordname)
        precip1cube.remove_coord(tcoordname)
        precip1cube.add_dim_coord(day,0)
        
        try:
            iris.util.demote_dim_coord_to_aux_coord(precipcube,'t')
            precipcube.remove_coord('t')
        except:
            iris.util.demote_dim_coord_to_aux_coord(precipcube,'t_1')
            precipcube.remove_coord('t_1')
        precipcube.add_dim_coord(day,0)

        try:
            iris.util.demote_dim_coord_to_aux_coord(precip2cube,'t')
            precip2cube.remove_coord('t')
        except:
            iris.util.demote_dim_coord_to_aux_coord(precip2cube,'t_1')
            precip2cube.remove_coord('t_1')
        precip2cube.add_dim_coord(day,0)
    

        maxps = np.max([maxp, maxp1, maxp2])
        minps = np.min([maxp, maxp1, maxp2])
        found = 'n'
        #print(maxp,maxp1,maxp2)
        #print(maxps,minps,maxp)
        
        if minps < maxp < maxps:
            meanp = maxp
            meanprecipcube = precipcube
            print('using precipcube')
            found = 'y'

        if minps < maxp2 < maxps:
            if found == 'y':
                print('error')
                sys.exit(0)
            else:
                meanp = maxp2
                meanprecipcube = precip2cube
                print('using precipcube2')
                found = 'y'

        if minps < maxp1 < maxps:
            if found == 'y':
                print('error')
                sys.exit(0)
            else:
                meanp = maxp1
                meanprecipcube = precip1cube
                print('using precip1cube')
                found = 'y'

        meanprecipcube = meanprecipcube * 60. * 60. * 24.
        print(month,meanprecipcube)
        meanprecipcube.units = 'mm/day'
        cubelist_precip.append(meanprecipcube)
        meanprecipcube=0
      

    iris.util.equalise_attributes(cubelist_precip)
    iris.util.unify_time_units(cubelist_precip)
    ann_precip_cube = cubelist_precip.concatenate_cube()
    ann_precip_cube = iris.util.squeeze(ann_precip_cube)
    return ann_precip_cube

def get_wet_spell(this_year_precip, next_year_precip):
    """
    this will calculate the wet spell as the number of days when
    the precip was > 1mm day
    """
    nt,ny,nx = np.shape(this_year_precip)
    nwet_array = np.zeros((ny,nx))
    ndry_array = np.zeros((ny,nx))
    for j in range(0,ny): 
        for i in range(0,nx):
            nwetdays=0
            count_wet=0
            ndrydays=0
            count_dry=0
            for t in range(0,nt):
                # get wet/dry info
                #print(this_year_precip[t,j,i])
                if this_year_precip[t,j,i] > 1.0: # move to max dry spell if req
                    count_wet = count_wet+1
                    
                    if count_dry > ndrydays:
                        ndrydays=count_dry
                    count_dry=0
                else: # move to maximum wetspell if appropriate and reset
                    nwetdays = np.max([count_wet, nwetdays])
                    count_wet = 0

                    count_dry=count_dry+1
                #print(t,count_wet,count_dry,this_year_precip[t,j,i])
            nwet_array[j,i] = nwetdays
            ndry_array[j,i] = ndrydays
            # if we have got to the end of the year and we are in
            # the middle of the wet spell add some from the next year
            
            if count_dry ==360:
                ndry_array[j,i]=360.
            if count_wet == 360:
                nwet_array[j,i]=360.
            if count_wet > 0 and nwetdays < 360:
                for t in range(0,179):
                    if next_year_precip[t,j,i] > 1.0:
                        count_wet = count_wet+1
                    else:
                        nwetdays = np.max([count_wet,nwetdays])
                        nwet_array[j,i] = np.min([360,nwetdays])
                        break
            if nwet_array[j,i] > 200:
                print('wet > 200 at',j,i,nwet_array[j,i])
            
            # if we have got to the end of the year and we are in
            # the middle of the wet spell add some from the next year
            
            if count_dry > 0 and ndrydays <360:
                for t in range(0,179):
                    if next_year_precip[t,j,i] < 1.0:
                        count_dry = count_dry+1
                    else:
                        ndrydays = np.max([count_dry,ndrydays])
                        ndry_array[j,i] = np.min([360,ndrydays])
                        break
    return nwet_array, ndry_array

def create_cube(datareqd,cube,long_name,year):
    """
    we are going to move the data to a cube.
    but then we are going to add a new time axis for concatenation
    """
    datacube = cube.copy(data=datareqd)
    datacube.long_name = long_name
    datacube.units='days'
    datacube.coord('surface').rename('year')
    datacube = iris.util.new_axis(datacube,scalar_coord='year')
    datacube.coord('year').points = [year]

    return datacube

def calculate_extreme_spell():
    """
    this function will calculate the number of days which belong to an
    extreme spell for each gridbox in the 100 year period
    """
    startyear=1
    endyear=99

    # get mean precipitation data in mm/day 
    # (collapse the time dimension to create a cube to store data)
    this_year_cube = get_ann_precip(startyear)
    lat_lon_cube = this_year_cube.collapsed('t',iris.analysis.MEAN)

    wet_spell_allcubes = iris.cube.CubeList([])
    dry_spell_allcubes = iris.cube.CubeList([])
   
    for year in range(startyear,endyear):
        print(year)
        next_year_cube = get_ann_precip(year+1)
        print('1')
        (wet_spell_data,
         dry_spell_data)= get_wet_spell(this_year_cube.data,next_year_cube.data)
        print('2')

        wet_spell_cube = create_cube(wet_spell_data,lat_lon_cube,'wet spell duration',year)
        dry_spell_cube = create_cube(dry_spell_data,lat_lon_cube,'dry spell duration',year)
        
        wet_spell_allcubes.append(wet_spell_cube)
        dry_spell_allcubes.append(dry_spell_cube)
        
    wet_spell_allyears_cube = wet_spell_allcubes.concatenate_cube()
    dry_spell_allyears_cube = dry_spell_allcubes.concatenate_cube()
    outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
               EXPTNAME + '/diag_23_24/'+ EXTRA+'_wet_spell_duration.nc')
    iris.save([wet_spell_allyears_cube, dry_spell_allyears_cube],
               outfile, netcdf_format="NETCDF3_CLASSIC")
   
def average_and_plot_wet_spell():
    """
    will average the wet spell over all the years in the file and plot
    """

    # read in cubes difference and remove lsm
    filein = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
               EXPTNAME + '/diag_23_24/'+ EXTRA+'_wet_spell_duration.nc')
    wetcube = iris.load_cube(filein,'wet spell duration')
    drycube = iris.load_cube(filein,'dry spell duration')

    wet_avg_expt_cube = wetcube.collapsed('year',iris.analysis.MEAN)
    dry_avg_expt_cube = drycube.collapsed('year',iris.analysis.MEAN)
    

    filein = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
               CNTLNAME + '/diag_23_24/'+ CNTLEXTRA+'_wet_spell_duration.nc')
    wetcube = iris.load_cube(filein,'wet spell duration')
    drycube = iris.load_cube(filein,'dry spell duration')

    wet_avg_cntl_cube = wetcube.collapsed('year',iris.analysis.MEAN)
    dry_avg_cntl_cube = drycube.collapsed('year',iris.analysis.MEAN)

    wet_anom_cube = wet_avg_expt_cube - wet_avg_cntl_cube
    dry_anom_cube = dry_avg_expt_cube - dry_avg_cntl_cube

    maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
    vals = np.arange(-30,35,5)
    wet_anom_cube.data.mask = (maskcube.data - 1.0) * (-1.0)
    dry_anom_cube.data.mask = (maskcube.data - 1.0) * (-1.0)

    cs = qplt.contourf(wet_anom_cube, levels=vals, cmap='BrBG', extend='both')
    plt.gca().coastlines()
    plt.title('wet spell duration Plio-Pi ')
   

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/wet_spell_' + EXPTNAME + '-' + CNTLNAME)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

    vals = np.arange(-50,60,10)
   
    cs = qplt.contourf(dry_anom_cube, levels=vals, cmap='BrBG_r', extend='both')
    plt.gca().coastlines()
    plt.title('dry spell duration Plio-Pi ')
   

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/dry_spell_' + EXPTNAME + '-' + CNTLNAME)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

    # plot pi values
    wet_avg_cntl_cube.data.mask = (maskcube.data - 1.0) * (-1.0)
    dry_avg_cntl_cube.data.mask = (maskcube.data - 1.0) * (-1.0)

    vals = np.arange(0,110,10)
   
    cs = qplt.contourf(dry_avg_cntl_cube, levels=vals, cmap='copper_r', extend='max')
    plt.gca().coastlines()
    plt.title('dry spell duration PI ')
   

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/dry_spell_' + CNTLNAME)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

    cs = qplt.contourf(wet_avg_cntl_cube, levels=vals, cmap='BuGn', extend='max')
    plt.gca().coastlines()
    plt.title('wet spell duration PI ')
   

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/wet_spell_' + CNTLNAME)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()


    # plot plio values
    wet_avg_expt_cube.data.mask = (maskcube.data - 1.0) * (-1.0)
    dry_avg_expt_cube.data.mask = (maskcube.data - 1.0) * (-1.0)

    vals = np.arange(0,110,10)
   
    cs = qplt.contourf(dry_avg_expt_cube, levels=vals, cmap='copper_r', extend='max')
    plt.gca().coastlines()
    plt.title('dry spell duration mPWP ')
   

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/dry_spell_' + EXPTNAME)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

    cs = qplt.contourf(wet_avg_expt_cube, levels=vals, cmap='BuGn', extend='max')
    plt.gca().coastlines()
    plt.title('wet spell duration mPWP ')
   

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/wet_spell_' + EXPTNAME)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

    
    #wet_leeds = wet_avg_cube.extract(iris.Constraint(longitude=0,latitude=52.5))
    #dry_leeds = dry_avg_cube.extract(iris.Constraint(longitude=0,latitude=52.5))

    #print('wet',wet_leeds.data)
    #print('dry',dry_leeds.data)

    

##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'
MIN_MAX = 'max' # max is warm spell duration index
                # min is cold spell duration index
print('start of program')
MONTHNAMES = {0:'ja',1:'fb',2:'mr',3:'ar',4:'my',5:'jn',6:'jl',
                  7:'ag',8:'sp',9:'ot',10:'nv',11:'dc'}
  
EXPTNAME = 'tenvj'
EXTRA='o'

# this will check if the average of the daily precipitation matches the
# monthly precipitation
#just_testing()

#nextreme_spell = calculate_extreme_spell()

CNTLNAME = 'xozza'
CNTLEXTRA = 'p'
average_and_plot_wet_spell()
::::::::::::::
PlioMIP_new/extremes/ETCCDI_23_24_very_wet_spell.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 14.07.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program is based on indices 23-24 these are dry spell and wet spell duration index

Here we are looking at extreme wet spells.  which is where spell where precip exceeds 4mm/day and 8mm/day

1.: Maximum length of very wet spell (4mm), maximum number of consecutive days with RR> 4mm.
Let RRij be the daily predipitation amount on day i in preiod j.  Count the largest number of consecutive days where RRij > 1mm
This is also known as consecutive wet days.

2.: Maximum length of extremely wet spell (8mm), maximum number of consecutive days with RR> 4mm.
Let RRij be the daily predipitation amount on day i in preiod j.  Count the largest number of consecutive days where RRij > 1mm


"""
import numpy as np
import iris
import iris.quickplot as qplt
from iris.coords import DimCoord
import matplotlib.pyplot as plt
import sys
from os.path import exists


def just_testing():
    """
    just check if the sum of the daily precipitation matches the monthly precip
    """
    monthname = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    daily_cubes = iris.cube.CubeList([])
    monthly_cubes = iris.cube.CubeList([])
    for month in range(0,12):
        pointsstart =  month *30.
        points=np.arange(pointsstart, pointsstart+30, 1.0)
        day = iris.coords.DimCoord(points,
                             long_name = 't', attributes = None,
                             bounds=None, circular=False)
     
        cubes = iris.load('/nfs/hera1/earjcti/um/'+ EXPTNAME + '/pb/' + EXPTNAME + 'a@pb' + EXTRA + '02'+ monthname[month] + '.nc')
        if EXPTNAME == 'xozzm':
            varreq = 'precip'
        else:
            varreq = 'precip_2'
        for cube in cubes:
            if cube.var_name == varreq:
                try:
                    cube.coord('t_1'.rename('t'))
                except:
                    pass
                cube = cube * 24.* 60. * 60.
                cube.units='mm/day'
                cube.long_name = 'precip from daily data'
                iris.util.demote_dim_coord_to_aux_coord(cube, 't')
                cube.remove_coord('t')
                cube.add_dim_coord(day,0)
                cube = iris.util.squeeze(cube)
                daily_cubes.append(cube)
                
                
        cube = iris.load_cube('/nfs/hera1/earjcti/um/'+EXPTNAME+'/pd/' + EXPTNAME + 'a@pd'+ EXTRA + '02' + monthname[month] + '.nc','TOTAL PRECIPITATION RATE     KG/M2/S')
        cube = cube * 24.* 60. * 60.
        cube.units='mm/day'
        cube = iris.util.squeeze(cube)
        monthly_cubes.append(cube)

    iris.util.unify_time_units(daily_cubes)
    iris.util.equalise_attributes(daily_cubes)
    daily_cube = daily_cubes.concatenate_cube()

    print(monthly_cubes)
    iris.util.unify_time_units(monthly_cubes)
    iris.util.equalise_attributes(monthly_cubes)
    monthly_cube = monthly_cubes.merge_cube()
              
    daily_mean_cube = daily_cube.collapsed('t',iris.analysis.MEAN)
    daily_max_cube = daily_cube.collapsed('t',iris.analysis.MAX)
    daily_mean_cube = iris.util.squeeze(daily_mean_cube)
    daily_max_cube = iris.util.squeeze(daily_max_cube)
    monthly_mean_cube = monthly_cube.collapsed('t',iris.analysis.MEAN)
    monthly_mean_cube = iris.util.squeeze(monthly_mean_cube)
    monthly_mean_cube.long_name = 'precip from monthly data'
   
    fig = plt.figure(figsize=[11.0,8.0])
    plt.subplot(221)
    qplt.contourf(daily_mean_cube,levels=np.arange(0,20,2))
    plt.subplot(222)
    qplt.contourf(monthly_mean_cube,levels=np.arange(0,20,2))
    plt.subplot(223)
    qplt.contourf(monthly_mean_cube - daily_mean_cube,levels=np.arange(-1.0,2.0,0.2))
    plt.title('difference between daily and monthly')
    plt.subplot(224)
    qplt.contourf(daily_max_cube,levels=np.arange(0,110,10),extend='max')
    plt.title('maximum precip from daily data')

    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag23_24/test_' + EXPTNAME + '.eps')
    sys.exit(0)

def get_ann_precip(year):
    """
    gets the precipitation for a year
    """
    cubelist_precip = iris.cube.CubeList([])
      
    filestart = ('/nfs/hera1/earjcti/um/' + EXPTNAME + '/pb/' + 
                 EXPTNAME +  'a@pb' + EXTRA + np.str(year).zfill(2))
    for month in range(0,12):
        filename = filestart + MONTHNAMES.get(month) + '.nc'
        cubes = iris.load(filename)

        for cube in cubes:
            if cube.var_name == 'precip':
                precipcube = cube
                precipcube = iris.util.squeeze(precipcube)
                maxp = np.mean(precipcube.data)
            if cube.var_name == 'precip_1':
                precip1cube = cube
                precip1cube = iris.util.squeeze(precip1cube)
                maxp1 = np.mean(precip1cube.data)
            if cube.var_name == 'precip_2':
                precip2cube = cube
                precip2cube = iris.util.squeeze(precip2cube)
                maxp2 = np.mean(precip2cube.data)
        
       # it didn't like the time coordinate.  Set a new one
        try:
            tcoord = precip1cube.coord('t') 
            tcoordname = 't'
        except: 
            tcoord = precip1cube.coord('t_1')
            tcoordname = 't_1'

        pointsstart = (year * 360.) + (month *30.)
        points=np.arange(pointsstart, pointsstart+30, 1.0)
        day = iris.coords.DimCoord(points,
                             long_name = 't', 
                             attributes = tcoord.attributes,
                             bounds=None, circular=False)
       
        iris.util.demote_dim_coord_to_aux_coord(precip1cube, tcoordname)
        precip1cube.remove_coord(tcoordname)
        precip1cube.add_dim_coord(day,0)
        
        try:
            iris.util.demote_dim_coord_to_aux_coord(precipcube,'t')
            precipcube.remove_coord('t')
        except:
            iris.util.demote_dim_coord_to_aux_coord(precipcube,'t_1')
            precipcube.remove_coord('t_1')
        precipcube.add_dim_coord(day,0)

        try:
            iris.util.demote_dim_coord_to_aux_coord(precip2cube,'t')
            precip2cube.remove_coord('t')
        except:
            iris.util.demote_dim_coord_to_aux_coord(precip2cube,'t_1')
            precip2cube.remove_coord('t_1')
        precip2cube.add_dim_coord(day,0)
    

        maxps = np.max([maxp, maxp1, maxp2])
        minps = np.min([maxp, maxp1, maxp2])
        found = 'n'
        #print(maxp,maxp1,maxp2)
        #print(maxps,minps,maxp)
        
        if minps < maxp < maxps:
            meanp = maxp
            meanprecipcube = precipcube
#            print('using precipcube')
            found = 'y'

        if minps < maxp2 < maxps:
            if found == 'y':
                print('error')
                sys.exit(0)
            else:
                meanp = maxp2
                meanprecipcube = precip2cube
#                print('using precipcube2')
                found = 'y'

        if minps < maxp1 < maxps:
            if found == 'y':
                print('error')
                sys.exit(0)
            else:
                meanp = maxp1
                meanprecipcube = precip1cube
#                print('using precip1cube')
                found = 'y'

        meanprecipcube = meanprecipcube * 60. * 60. * 24.
 #       print(month,meanprecipcube)
        meanprecipcube.units = 'mm/day'
        cubelist_precip.append(meanprecipcube)
        meanprecipcube=0
      

    iris.util.equalise_attributes(cubelist_precip)
    iris.util.unify_time_units(cubelist_precip)
    ann_precip_cube = cubelist_precip.concatenate_cube()
    ann_precip_cube = iris.util.squeeze(ann_precip_cube)
    return ann_precip_cube

def get_wet_spell(this_year_precip, next_year_precip):
    """
    this will calculate the wet spell as the number of days when
    the precip was > 1mm day
    """
    nt,ny,nx = np.shape(this_year_precip)
    nwet_array = np.zeros((ny,nx))
    nvery_wet_array = np.zeros((ny,nx))
    for j in range(0,ny): 
        for i in range(0,nx):
            nwetdays=0
            count_wet=0
            nvery_wetdays=0
            count_very_wet=0
            for t in range(0,nt):
                # get wet/very_wet info
                #print(this_year_precip[t,j,i])
                if this_year_precip[t,j,i] > 4.0:
                    count_wet = count_wet+1                    
                else: # move to maximum wetspell if appropriate and reset
                    nwetdays = np.max([count_wet, nwetdays])
                    count_wet = 0

                if this_year_precip[t,j,i] > 8.0:
                    count_very_wet = count_very_wet+1                    
                else: # move to maximum wetspell if appropriate and reset
                    nvery_wetdays = np.max([count_wet, nwetdays])
                    count_very_wet = 0

            nwet_array[j,i] = nwetdays
            nvery_wet_array[j,i] = nvery_wetdays

            # if we have got to the end of the year and we are in
            # the middle of the wet spell add some from the next year
            
            if count_very_wet ==360:
                nvery_wet_array[j,i]=360.
            if count_wet == 360:
                nwet_array[j,i]=360.
            if count_wet > 0 and nwetdays < 360:
                for t in range(0,179):
                    if next_year_precip[t,j,i] > 4.0:
                        count_wet = count_wet+1
                    else:
                        nwetdays = np.max([count_wet,nwetdays])
                        nwet_array[j,i] = np.min([360,nwetdays])
                        break
            
            # if we have got to the end of the year and we are in
            # the middle of the wet spell add some from the next year
            
            if count_very_wet > 0 and nvery_wetdays <360:
                for t in range(0,179):
                    if next_year_precip[t,j,i] > 8.0:
                        count_very_wet = count_very_wet+1
                    else:
                        nvery_wetdays = np.max([count_very_wet,nvery_wetdays])
                        nvery_wet_array[j,i] = np.min([360,nvery_wetdays])
                        break
    return nwet_array, nvery_wet_array

def create_cube(datareqd,cube,long_name,year):
    """
    we are going to move the data to a cube.
    but then we are going to add a new time axis for concatenation
    """
    datacube = cube.copy(data=datareqd)
    datacube.long_name = long_name
    datacube.units='days'
    datacube.coord('surface').rename('year')
    datacube = iris.util.new_axis(datacube,scalar_coord='year')
    datacube.coord('year').points = [year]

    return datacube

def calculate_extreme_spell():
    """
    this function will calculate the number of days which belong to an
    extreme spell for each gridbox in the 100 year period
    """
    startyear=1
    endyear=99

    # get mean precipitation data in mm/day 
    # (collapse the time dimension to create a cube to store data)
    this_year_cube = get_ann_precip(startyear)
    lat_lon_cube = this_year_cube.collapsed('t',iris.analysis.MEAN)

    wet_spell_allcubes = iris.cube.CubeList([])
    very_wet_spell_allcubes = iris.cube.CubeList([])
   
    for year in range(startyear,endyear):
        print(year)
        next_year_cube = get_ann_precip(year+1)
        print('1')
        (wet_spell_data,
         very_wet_spell_data)= get_wet_spell(this_year_cube.data,next_year_cube.data)
        print('2',wet_spell_data[36,36],very_wet_spell_data[36,36])
      
        wet_spell_cube = create_cube(wet_spell_data,lat_lon_cube,'wet spell duration (>4mm/day)',year)
        very_wet_spell_cube = create_cube(very_wet_spell_data,lat_lon_cube,'wet spell duration ( > 8mm/day)',year)
        
        wet_spell_allcubes.append(wet_spell_cube)
        very_wet_spell_allcubes.append(very_wet_spell_cube)
        
    wet_spell_allyears_cube = wet_spell_allcubes.concatenate_cube()
    very_wet_spell_allyears_cube = very_wet_spell_allcubes.concatenate_cube()
    difference_cube = very_wet_spell_allyears_cube - wet_spell_allyears_cube
    difference_cube.long_name = 'difference between wet spell based on 8mm/day and 4mm/day'
    outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
               EXPTNAME + '/diag_23_24/'+ EXTRA+'_very_wet_spell_duration.nc')
    iris.save([wet_spell_allyears_cube, very_wet_spell_allyears_cube,difference_cube],
               outfile, netcdf_format="NETCDF3_CLASSIC")
   
def average_and_plot_wet_spell():
    """
    will average the wet spell over all the years in the file and plot
    """
    filein = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
               EXPTNAME + '/diag_23_24/'+ EXTRA+'_wet_spell_duration.nc')
    wetcube = iris.load_cube(filein,'wet spell duration')
    very_wetcube = iris.load_cube(filein,'very_wet spell duration')

    wet_avg_cube = wetcube.collapsed('year',iris.analysis.MEAN)
    very_wet_avg_cube = very_wetcube.collapsed('year',iris.analysis.MEAN)
    
    wet_leeds = wet_avg_cube.extract(iris.Constraint(longitude=0,latitude=52.5))
    very_wet_leeds = very_wet_avg_cube.extract(iris.Constraint(longitude=0,latitude=52.5))

    print('wet',wet_leeds.data)
    print('very_wet',very_wet_leeds.data)

    

##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'
MIN_MAX = 'max' # max is warm spell duration index
                # min is cold spell duration index
print('start of program')
MONTHNAMES = {0:'ja',1:'fb',2:'mr',3:'ar',4:'my',5:'jn',6:'jl',
                  7:'ag',8:'sp',9:'ot',10:'nv',11:'dc'}
  
EXPTNAME = 'xozzm'
EXTRA='w'

# this will check if the average of the daily precipitation matches the
# monthly precipitation
#just_testing()

nextreme_spell = calculate_extreme_spell()

#average_and_plot_wet_spell()
::::::::::::::
PlioMIP_new/extremes/ETCCDI_5_growing_season_length.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will write
indices 5 (growing season length to a file.  This is:

GSL, Growing season length: Annual (1st Jan to 31st Dec in Northern Hemisphere (NH), 1st July to 30th June in Southern Hemisphere (SH)) count between first span of at least 6 days with daily mean temperature TG>5oC and first span after July 1st (Jan 1st in SH) of 6 days with TG<5oC.

Let TGij be daily mean temperature on day i in year j. Count the number of days between the first occurrence of at least 6 consecutive days with:

TGij > 5oC.

and the first occurrence after 1st July (1st Jan. in SH) of at least 6 consecutive days with:

TGij < 5oC. 
"""
import numpy as np
import iris
from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys


def get_HCM3_year_data_NH(filestart,year):
    """
    reads in the mean daily temperature for the year and puts it in 
    a single cube
    """
    meanTcubelist = iris.cube.CubeList([])
    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    for month in months:
        filename = filestart + np.str(year).zfill(2) + month + '.nc'
        # load in data
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp'))
        cube = iris.load(filename, constraints=variable_constraint)
        maxTcube = cube[0]

        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_1'))
        cube = iris.load(filename, constraints=variable_constraint)
        minTcube = cube[0]

        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_2'))
        cube = iris.load(filename, constraints=variable_constraint)
        meanTcube = cube[0]

        # check you have got maxT, meanT and minT in correct order        
        if np.max(maxTcube.data) < np.max(meanTcube.data):
            print('cubes not in right order')
            sys.exit(0)

        if np.max(meanTcube.data) < np.max(minTcube.data):
            print('cubes not in right order2')
            sys.exit(0)

        meanTcubelist.append(meanTcube-273.15)

    equalise_attributes(meanTcubelist)
    temporarycube = meanTcubelist.concatenate_cube()
    meanTyearcube = iris.util.squeeze(temporarycube)
  
    return meanTyearcube
   
def get_HCM3_year_data_SH(filestart,year):
    """
    reads in the mean daily temperature for the year and puts it in 
    a single cube
    However for the SH a year will go from July to june
    """
    meanTcubelist = iris.cube.CubeList([])
    months = ['jl','ag','sp','ot','nv','dc','ja','fb','mr','ar','my','jn',]
    for monthno, month in enumerate(months):
        if monthno <=5:  
            filename = filestart + np.str(year).zfill(2) + month + '.nc'
        else:
            filename = filestart + np.str(year+1).zfill(2) + month + '.nc'
        print(filename)
        # load in data
        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp'))
        cube = iris.load(filename, constraints=variable_constraint)
        maxTcube = cube[0]

        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_1'))
        cube = iris.load(filename, constraints=variable_constraint)
        minTcube = cube[0]

        variable_constraint = iris.Constraint(cube_func=(lambda c: c.var_name == 'temp_2'))
        cube = iris.load(filename, constraints=variable_constraint)
        meanTcube = cube[0]

        # check you have got maxT, meanT and minT in correct order        
        if np.max(maxTcube.data) < np.max(meanTcube.data):
            print('cubes not in right order')
            sys.exit(0)

        if np.max(meanTcube.data) < np.max(minTcube.data):
            print('cubes not in right order2')
            sys.exit(0)

        meanTcubelist.append(meanTcube-273.15)

    equalise_attributes(meanTcubelist)
    temporarycube = meanTcubelist.concatenate_cube()
    meanTyearcube = iris.util.squeeze(temporarycube)
  
    return meanTyearcube
   



def calc_grow_seas(meanTcube,nhshind):
    """
    note nhshind = 1.0 for nh and -1.0 for sh

    input is a cube of yearly temperature data.  We want to find the growing
    season length.  Do this as follows:

    1. find the first span of at least 6 days with temperature > 5degC
    2. find the first span after this with 6 days with temperature < 5degC
    """

    temporary_cube = meanTcube.collapsed('t',iris.analysis.SUM)
    growing_seas_arr = np.zeros(np.shape(temporary_cube.data))

    for j, lat in enumerate(meanTcube.coord('latitude').points):
        if lat * nhshind >= 0:
            for i, lon in enumerate(meanTcube.coord('longitude').points):
                timeseries = meanTcube[:, j, i].data

                growstart = -99
                growend = -99

                for day, temp in enumerate(timeseries):
                    if growstart < 0 and temp > 5.0 and day < 355 :
                       # print(day)
                        # is this start of growing seas
                        if (timeseries[day+1] > 5.0 and 
                            timeseries[day+2] > 5.0 and
                            timeseries[day+3] > 5.0 and 
                            timeseries[day+4] > 5.0 and
                            timeseries[day+5] > 5.0):
                            growstart = day
                            growend = 360
                        
                    if (growstart>=0 and growend == 360 
                        and temp < 5.0 and 180 < day < 355):
                        # is this the end of the growing season
                        # only check if end of growing season has not already
                        # been found
                     #    print('j',day, len(timeseries))
                         if (timeseries[day+1] < 5.0 and 
                            timeseries[day+2] < 5.0 and
                            timeseries[day+3] < 5.0 and 
                            timeseries[day+4] < 5.0 and
                            timeseries[day+5] < 5.0):
                            growend = day
                growing_seas_arr[j,i] = growend - growstart

    growing_seas_cube = temporary_cube.copy(data=growing_seas_arr)

    return growing_seas_cube

def get_HadCM3_diagnostics(expt, extra):
    """
    gets the diagnostics (frost days, summer days, icing days tropical nights
    from HadCM3)
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra
  
    for year in range(99, 100):
        meanTcube  = get_HCM3_year_data_NH(filestart, year)
        NH_growing_seas_len_cube = calc_grow_seas(meanTcube, 1.0)

        meanTcube  = get_HCM3_year_data_SH(filestart, year)
        SH_growing_seas_len_cube = calc_grow_seas(meanTcube, -1.0)

        growing_seas_cube = NH_growing_seas_len_cube.copy(np.maximum(NH_growing_seas_len_cube.data, SH_growing_seas_len_cube.data))

        growing_seas_cube.long_name = 'length of growing season (days)'
        growing_seas_cube.units = None
        
        outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   expt + '/' + extra + '_' + 
                   'diag5_' + np.str(year) + '.nc')
        iris.save(growing_seas_cube, outfile, netcdf_format="NETCDF3_CLASSIC")


##########################################################
def read_data(expt,extra,startyear,endyear):
    """
    reads in the data for each year, finds the sum and returns
    """
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/'

    growcubelist = iris.cube.CubeList([])
   
    for year in range(startyear,endyear):
        filename = (filestart + expt + '/diag5/' + extra + 
                    '_diag5_'+ np.str(year) + '.nc')

        growcubelist.append(iris.load_cube(filename,'length of growing season (days)'))
        
    equalise_attributes(growcubelist)
    allgrowcube = growcubelist.merge_cube()
    meangrowcube = allgrowcube.collapsed('t',iris.analysis.MEAN)
   
  
    return meangrowcube


def plot_anom(cube,ocn_mask, expt, cntl, cube_cntl, cube_expt):
    """
    this will plot the anomaly cube between the pliocene and the pi
    """

    fieldname = 'len growing seas  '
    if ocn_mask == 'y':
       maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
      
       cube.data.mask = (maskcube.data - 1.0) * (-1.0)
       cube_cntl.data.mask = (maskcube.data - 1.0) * (-1.0)
       cube_expt.data.mask = (maskcube.data - 1.0) * (-1.0)
      
    plt.subplot(2,2,1)
    qplt.contourf(cube_cntl, levels=np.arange(100,320,20), extend='both')
    plt.gca().coastlines()
    plt.title(fieldname + ':' +  TIME.get(cntl))

    plt.subplot(2,2,2)
    qplt.contourf(cube_expt, levels=np.arange(100,320,20), extend='both')
    plt.gca().coastlines()
    plt.title(fieldname + ':' +  TIME.get(expt))

    plt.subplot(2,2,3)
    cmapname = 'Reds'
    vals = np.arange(0,30,5)
    qplt.contourf(cube,cmap=cmapname, levels=vals,extend='both')
    plt.gca().coastlines()
    plt.title(fieldname + ': ' + TIME.get(expt) + '-' +TIME.get(cntl))

    plt.subplot(2,2,4)
    qplt.contourf((cube / cube_cntl) * 100.,cmap=cmapname, 
                  levels=vals,extend='both')
    plt.gca().coastlines()
    plt.title('percentage change: ' + TIME.get(expt) + '-' +TIME.get(cntl))


    plt.tight_layout()
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag5/' + 
               'grow_seas_len_' + expt + '-' + cntl)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()

       
def plot_growing_season(expt, cntl, nyears):
    """
    plots the growing season for plio and pi and difference between them
    """
    grow_expt_cube = read_data(expt,'o',0, nyears)

    grow_cntl_cube = read_data(cntl,'o',0, nyears)

    grow_anom_cube = grow_expt_cube - grow_cntl_cube
    plot_anom(grow_anom_cube,'y', expt,cntl,
              grow_cntl_cube, grow_expt_cube)
  
  
    
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'
TIME = {'tenvj' : 'mPlio', 'xozza' : 'PI', 'xozzb' : 'Plio'}


# this is for if you actually want to get the diagnostics.
# ie write them to the file /nfs/hera1/earjcti/PLIOMIP2/..ETCCDI....diags 5  
#if MODELNAME == 'HadCM3':
#    get_HadCM3_diagnostics('xozza','o')
 

# plot info about growing season.  a) growing season_pi, growing_season_plio
# and diff between them

plot_growing_season('tenvj','xozza',99)  # tenvj plio, xozza control, xozzb control
::::::::::::::
PlioMIP_new/extremes/ETCCDI_6_9.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

We are looking at ETCCDI Climate change indicies.  This program will write
indices 6-9 to a file.  These are:

6  TXx, Monthly maximum value of daily maximum temperature:

Let TXx be the daily maximum temperatures in month k, period j. The maximum daily maximum temperature each month is then:

TXxkj=max(TXxkj)

7.  TNx, Monthly maximum value of daily minimum temperature:

Let TNx be the daily minimum temperatures in month k, period j. The maximum daily minimum temperature each month is then:

TNxkj=max(TNxkj)

8.TXn, Monthly minimum value of daily maximum temperature:

Let TXn be the daily maximum temperatures in month k, period j. The minimum daily maximum temperature each month is then:

9. TXnkj=min(TXnkj)

TNn, Monthly minimum value of daily minimum temperature:

Let TNn be the daily minimum temperatures in month k, period j. The minimum daily minimum temperature each month is then:

TNnkj=min(TNnkj) 

"""
import numpy as np
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import matplotlib.pyplot as plt
import sys


def get_HCM3_year_data(filestart,year):
    """
    reads in the maximum  and minimum temperature for the year and puts it in 
    a single cube
    """
    maxmon_maxT_cubelist = iris.cube.CubeList([])
    minmon_maxT_cubelist = iris.cube.CubeList([])
    maxmon_minT_cubelist = iris.cube.CubeList([])
    minmon_minT_cubelist = iris.cube.CubeList([])

    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    for month in months:
        filename = filestart + np.str(year).zfill(2) + month + '.nc'
        # load in data
        cubes=iris.load(filename)
        temperature_cubes = iris.cube.CubeList([])
        max_temp = []
        for cube in cubes:
            if (cube.var_name == 'temp' or cube.var_name == 'temp_1' 
                or cube.var_name == 'temp_2'):
               try:
                   cube.coord('t_1').rename('t')
               except:
                   pass
               temperature_cubes.append(cube)
               max_temp.append(np.max(cube.data))
     
        maxindex = np.argmax(max_temp)
        minindex = np.argmin(max_temp)
        indexes = np.argsort(max_temp)
        minTcube = temperature_cubes[indexes[0]]
        meanTcube = temperature_cubes[indexes[1]]
        maxTcube = temperature_cubes[indexes[2]]
    
       
        # check you have got maxT, meanT and minT in correct order        
        if np.max(maxTcube.data) < np.max(meanTcube.data):
            print('cubes not in right order')
            sys.exit(0)

        if np.max(meanTcube.data) < np.max(minTcube.data):
            print('cubes not in right order2')
            sys.exit(0)

        maxmon_of_maxT_cube = maxTcube.collapsed('t', iris.analysis.MAX)
        minmon_of_maxT_cube = maxTcube.collapsed('t', iris.analysis.MIN)
        maxmon_of_minT_cube = minTcube.collapsed('t', iris.analysis.MAX)
        minmon_of_minT_cube = minTcube.collapsed('t', iris.analysis.MIN)

        maxmon_maxT_cubelist.append(iris.util.squeeze(maxmon_of_maxT_cube))
        minmon_maxT_cubelist.append(iris.util.squeeze(minmon_of_maxT_cube))
        maxmon_minT_cubelist.append(iris.util.squeeze(maxmon_of_minT_cube))
        minmon_minT_cubelist.append(iris.util.squeeze(minmon_of_minT_cube))
        
    equalise_attributes(maxmon_maxT_cubelist)
    equalise_attributes(minmon_maxT_cubelist)
    equalise_attributes(maxmon_minT_cubelist)
    equalise_attributes(minmon_minT_cubelist)

    #for cube in maxmon_maxT_cubelist:
    #    print(cube.coord('t'))
    max_maxdayTcube = maxmon_maxT_cubelist.merge_cube()
    min_maxdayTcube = minmon_maxT_cubelist.merge_cube()
    max_mindayTcube = maxmon_minT_cubelist.merge_cube()
    min_mindayTcube = minmon_minT_cubelist.merge_cube()
  
    return max_maxdayTcube, min_maxdayTcube, max_mindayTcube, min_mindayTcube 
   

def get_HadCM3_diagnostics(expt, extra):
    """
    gets the diagnostics (frost days, summer days, icing days tropical nights
    from HadCM3)
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra
  
    for year in range(0, 100):
        yearuse = np.str(year).zfill(2)
        (max_Tmax_cube, min_Tmax_cube, 
         max_Tmin_cube, min_Tmin_cube)= get_HCM3_year_data(filestart, year)

        # rename
        max_Tmax_cube.long_name = 'Monthly maximum value of daily maximum temperature'
        max_Tmin_cube.long_name = 'Monthly maximum value of daily minimum temperature'
        min_Tmax_cube.long_name = 'Monthly minimum value of daily maximum temperature'
        min_Tmin_cube.long_name = 'Monthly minimum value of daily minimum temperature'

        cubelist = [max_Tmax_cube, max_Tmin_cube,min_Tmax_cube,
                    min_Tmin_cube]

        outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   expt + '/' + extra + '_' + 
                   'diag6-9_' + np.str(year) + '.nc')
        iris.save(cubelist, outfile, netcdf_format="NETCDF3_CLASSIC")


#########################################################################
def read_data(expt,extra,startyear,endyear):
    """
    reads in the data for each year, finds the sum and returns
    """
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/'

    maxTmaxcubelist = iris.cube.CubeList([])
    maxTmincubelist = iris.cube.CubeList([])
    minTmaxcubelist = iris.cube.CubeList([])
    minTmincubelist = iris.cube.CubeList([])
 
    for year in range(startyear,endyear):
        filename = (filestart + expt + '/diag6-9/' + extra + 
                    '_diag6-9_'+ np.str(year) + '.nc')
        cube = iris.load_cube(filename,
                        'Monthly maximum value of daily maximum temperature')
        maxTmaxcubelist.append(cube[MONTH_REQ, :,:])
            
        cube = iris.load_cube(filename,
                        'Monthly maximum value of daily minimum temperature')
        maxTmincubelist.append(cube[MONTH_REQ,:,:])

        cube = iris.load_cube(filename,
                        'Monthly minimum value of daily maximum temperature')
        minTmaxcubelist.append(cube[MONTH_REQ, :, :])

        cube = iris.load_cube(filename,
                        'Monthly minimum value of daily minimum temperature')
        minTmincubelist.append(cube[MONTH_REQ, :, :])

    equalise_attributes(maxTmaxcubelist)
    all_maxTmax_cube = maxTmaxcubelist.merge_cube()
    max_maxTmax_cube = all_maxTmax_cube.collapsed('t',iris.analysis.MAX)
    mean_maxTmax_cube = all_maxTmax_cube.collapsed('t',iris.analysis.MEAN)
  
    equalise_attributes(minTmaxcubelist)
    all_minTmax_cube = minTmaxcubelist.merge_cube()
    mean_minTmax_cube = all_minTmax_cube.collapsed('t',iris.analysis.MEAN)
  
    equalise_attributes(maxTmincubelist)
    all_maxTmin_cube = maxTmincubelist.merge_cube()
    max_maxTmin_cube = all_maxTmin_cube.collapsed('t',iris.analysis.MAX)
    mean_maxTmin_cube = all_maxTmin_cube.collapsed('t',iris.analysis.MEAN)
  
    equalise_attributes(minTmincubelist)
    all_minTmin_cube = minTmincubelist.merge_cube()
    min_minTmin_cube = all_minTmin_cube.collapsed('t',iris.analysis.MIN)
    mean_minTmin_cube = all_minTmin_cube.collapsed('t',iris.analysis.MEAN)
  
  
    return (mean_maxTmax_cube, mean_minTmax_cube, mean_maxTmin_cube,
            mean_minTmin_cube, max_maxTmax_cube, min_minTmin_cube)
  
##########################################################   
def plot_extreme_extremes(meanpliocube, meanpicube,meananomcube, extrpliocube,
                          extrpicube,
                          extranomcube, plottype, ocn_mask, expt,cntl):
    """
    this will do a four panel plot
    a) pliocene_annmean extreme temperature  b) pliocene-pi anomaly of a
    c) plioceene most extreme temp in 100 yrs d) plio - pi anomaly of c
    """
    months = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']
    exptnames = {'xozzm' : 'E560', 'xozza': 'PI'}

    if ocn_mask == 'y':
       maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
      
       meanpliocube.data.mask = (maskcube.data - 1.0) * (-1.0)
       meanpicube.data.mask = (maskcube.data - 1.0) * (-1.0)
       meananomcube.data.mask = (maskcube.data - 1.0) * (-1.0)
       extrpliocube.data.mask = (maskcube.data - 1.0) * (-1.0)
       extranomcube.data.mask = (maskcube.data - 1.0) * (-1.0)
       extrpicube.data.mask = (maskcube.data - 1.0) * (-1.0)
      
    plt.subplot(2,2,1)
    if plottype == 'max':
        vals = np.arange(30,55,5)
        pivals = [40]
        picolor='white'
        cmapname = 'gist_stern_r'
    if plottype == 'min':
        vals = np.arange(-50,0,10)
        pivals = [-30]
        picolor='black'
        cmapname='gist_ncar'
    meanpliocube.convert_units('celsius')
    meanpicube.convert_units('celsius')
    meanpliocube.long_name = exptnames.get(expt) + ': Mean ' + plottype + ' of T' + plottype
   
    iplt.contourf(meanpliocube, extend='both',levels=vals,cmap=cmapname)
    iplt.contour(meanpicube, levels=pivals, linestyles='solid',
                 colors=picolor,linewidths=1)
    plt.gca().coastlines()
    print(np.str(pivals[0]))
    plt.title(exptnames.get(expt) + ' mean ' + plottype + ' ' + months[MONTH_REQ] + ' T'+
              plottype +  '(contour: pi='+ np.str(pivals[0])+'degC)',fontsize=8)

    plt.subplot(2,2,2)
    vals_a = np.arange(-12,14,2)
    meananomcube.long_name = exptnames.get(expt) + ' - ' + exptnames.get(cntl) + ': Mean ' + plottype + ' of T' + plottype  
    iplt.contourf(meananomcube, extend='both',levels=vals_a,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title(exptnames.get(expt) + '-' + exptnames.get(cntl) + 'ann mean ' + plottype + ' ' + months[MONTH_REQ] + ' T',
              fontsize=8)

    plt.subplot(2,2,3)
    extrpliocube.convert_units('celsius')
    extrpicube.convert_units('celsius')
    extrpliocube.long_name = (exptnames.get(expt) + ' ' + plottype + ' ' + months[MONTH_REQ] + 
                              ' T' + plottype + ' in ' + np.str(NYEARS) + 
                              'years')
    qplt.contourf(extrpliocube, extend='both',levels=vals,cmap=cmapname)
    iplt.contour(extrpicube, levels=pivals, linestyles='solid',
                 colors=picolor,linewidths=1)
  
    plt.gca().coastlines()
    plt.title(exptnames.get(expt) + ' ' + plottype + ' ' + months[MONTH_REQ] + ' T in '
              + np.str(NYEARS) + 'years',fontsize=8)

    plt.subplot(2,2,4)
    extranomcube.long_name = (plottype + ' ' + months[MONTH_REQ] + 
                              ' T' + plottype + ' in ' + np.str(NYEARS) + 
                              'years : '+ exptnames.get(expt) + '-' + 
                              exptnames.get(cntl))
  
    qplt.contourf(extranomcube, extend='both',levels=vals_a,cmap='RdBu_r')
  
    plt.gca().coastlines()
    plt.title(plottype + ' ' + months[MONTH_REQ] + ' T in '
              +np.str(NYEARS) + 'years: ' + exptnames.get(expt) + '-' + 
              exptnames.get(cntl) + ' anom',fontsize=8)

    
    plt.tight_layout()
   
   
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag6-9/' +
               expt + '_' + cntl + '_' + 
               plottype + 'T' + plottype + '_' + months[MONTH_REQ])
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()
    # save data to a netcdffile
   
    meanpliocube.data.mask = None
    meananomcube.data.mask = None
    extrpliocube.data.mask = None
    extranomcube.data.mask = None
  
    cubelist = [meanpliocube, meananomcube, extrpliocube, extranomcube]
    iris.save(cubelist, fileout + '.nc', netcdf_format="NETCDF3_CLASSIC")

##########################################################              
def  plot_minTmax_maxTmin(mean_minTmax_pliocube, mean_minTmax_anomcube,
                  mean_maxTmin_pliocube,mean_maxTmin_anomcube,ocn_mask,
                  expt,cntl):
    """
    this will do a four panel plot
    a) mean_minTmax_plio, b) mean_minTmax_anom
    c) mean_maxTmin_plio, d) mean_maxTmin_anom
    """
    
    months = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']
 
    mean_minTmax_pliocube.convert_units('celsius')
    mean_maxTmin_pliocube.convert_units('celsius')
    if ocn_mask == 'y':
       maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
      
       mean_minTmax_pliocube.data.mask = (maskcube.data - 1.0) * (-1.0)
       mean_minTmax_anomcube.data.mask = (maskcube.data - 1.0) * (-1.0)
       mean_maxTmin_pliocube.data.mask = (maskcube.data - 1.0) * (-1.0)
       mean_maxTmin_anomcube.data.mask = (maskcube.data - 1.0) * (-1.0)
       
    plt.subplot(2,2,1)
    vals = np.arange(-50,45,5)
    name = 'Plio minTmax:' + months[MONTH_REQ]
    mean_minTmax_pliocube.long_name = name
    qplt.contourf(mean_minTmax_pliocube, extend='both',levels=vals)
    plt.gca().coastlines()
    plt.title(name)

    plt.subplot(2,2,2)
    vals_a = np.arange(-12,14,2)
    name = 'Plio - PI: minTmax '+ months[MONTH_REQ]
    mean_minTmax_anomcube.long_name = name
    qplt.contourf(mean_minTmax_anomcube, extend='both',
                  levels=vals_a,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title(name)
    
    plt.subplot(2,2,3)
    name = 'Plio maxTmin:' + months[MONTH_REQ]
    mean_maxTmin_pliocube.long_name = name
    qplt.contourf(mean_maxTmin_pliocube, extend='both',levels=vals)
    plt.gca().coastlines()
    plt.title(name)

    plt.subplot(2,2,4)
    name = 'Plio - PI: maxTmin '+ months[MONTH_REQ]
    mean_maxTmin_anomcube.long_name = name
    qplt.contourf(mean_maxTmin_anomcube, extend='both',
                  levels=vals_a,cmap='RdBu_r')
    plt.gca().coastlines()
    plt.title(name)

    
    plt.tight_layout()
   
   
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag6-9/'+ 
               expt + '_' + cntl + '_maxTmin_minTmax' + months[MONTH_REQ])
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()
    # save data to a netcdffile
   
    mean_maxTmin_pliocube.data.mask = None
    mean_maxTmin_anomcube.data.mask = None
    mean_minTmax_pliocube.data.mask = None
    mean_minTmax_anomcube.data.mask = None
   
    cubelist = [mean_maxTmin_pliocube, mean_maxTmin_anomcube, 
                mean_minTmax_pliocube, mean_minTmax_anomcube]
    iris.save(cubelist, fileout + '.nc', netcdf_format="NETCDF3_CLASSIC")


def plot_textremes_toplevel(expt,exptextra,cntl,cntlextra):
    """
    this will plot the extreme temperatures (maximum of Tmax, minimum of Tmax,
    maximum of Tmin, minimum of Tmin) by month and show how these 
    have changed between the PI and the Pliocene
    """

    (mean_maxTmax_pliocube, mean_minTmax_pliocube, 
     mean_maxTmin_pliocube, mean_minTmin_pliocube, max_maxTmax_pliocube, 
     min_minTmin_pliocube) = read_data(expt,exptextra,0, 0+NYEARS)

    (mean_maxTmax_picube, mean_minTmax_picube, 
     mean_maxTmin_picube, mean_minTmin_picube, 
     max_maxTmax_picube, min_minTmin_picube)= read_data(cntl,cntlextra,0, 0+ NYEARS)
              

    # plot maximum temperature of the Tmax for this month
    plot_extreme_extremes(mean_maxTmax_pliocube, mean_maxTmax_picube,
                  mean_maxTmax_pliocube - mean_maxTmax_picube,
                  max_maxTmax_pliocube, max_maxTmax_picube,
                  max_maxTmax_pliocube - max_maxTmax_picube,'max','y',expt,cntl)

    # plot minimum temperature of Tmin for this month
    plot_extreme_extremes(mean_minTmin_pliocube, mean_minTmin_picube,
                  mean_minTmin_pliocube - mean_minTmin_picube,
                  min_minTmin_pliocube, min_minTmin_picube,
                  min_minTmin_pliocube - min_minTmin_picube,'min','y',expt,cntl)

    # plot minimum temperature of Tmax and maximum temperature of Tmin
    # for this month
    plot_minTmax_maxTmin(mean_minTmax_pliocube,
                  mean_minTmax_pliocube - mean_minTmax_picube,
                  mean_maxTmin_pliocube,
                  mean_maxTmin_pliocube - mean_maxTmin_picube,
                 'y',expt,cntl)

#########################################################
def plot_all_months(field, expt, cntl, ocn_mask):
    """
    this is for plotting the anomalies in the diagnostics for all months
    on one page
    ie max_Tmax_plio - max_Tmax_pi (mean value) for all months
    field : maxTmax, maxTmin, minTmax, minTmin
    """ 

    fig = plt.figure(figsize=(11.75, 13.0))
    # get all the stuff (names/ files etc) we might need
    filestart = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/' + 
                 'diag6-9/' + expt + '_' + cntl + '_')
    months = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']
    fieldname = {"maxTmax": "E560 - PI: Mean max of Tmax",
                 "minTmin": "E560 - PI: Mean min of Tmin",
                 "minTmax": "E560 - PI: minTmax ",
                 "maxTmin": "E560 - PI: maxTmin "}
    filemid = {"maxTmax": "maxTmax_", "minTmin": "minTmin_",
               "minTmax": "maxTmin_minTmax",
               "maxTmin": "maxTmin_minTmax"}
    maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
    fieldstart = fieldname.get(field)
    vals = np.arange(-12,14,2)
   
    # process and plot
    for mon in range(0,12):
        filename = filestart + filemid.get(field) + months[mon] + '.nc'
        if field == "minTmax" or field == "maxTmin":
            fielduse = fieldstart + months[mon]
        else:
            fielduse = fieldstart 
        test = iris.load(filename)
        cube = iris.load_cube(filename,fielduse)
        if ocn_mask == 'y':
           cube.data.mask = (maskcube.data - 1.0) * (-1.0)
      
        plt.subplot(4,3,mon+1)
        cs = iplt.contourf(cube, levels=vals, cmap='RdBu_r', extend='both')
        plt.gca().coastlines()
        plt.title(months[mon] + ': Plio-Pi ' + field) 
    
    # tidy up plot and add colorbar
    plt.subplots_adjust(left=0.05, bottom=0.1, right=0.95, top=0.95,
                                wspace=0.1, hspace=0.0)

    cb_ax = fig.add_axes([0.35, 0.05, 0.30, 0.02])
           
    cbar = fig.colorbar(cs, cax=cb_ax, orientation='horizontal')
    cbar.set_label('degC', fontsize=10)
         
    cbar.ax.tick_params(labelsize=10)
  

    # save to file
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag6-9/allmonths_'+  expt + '_' + cntl + '_' + field)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()
  
def extract_avg(latreq,lonreq,expt,extra):
    """
    extract average at a given location
    """
    filestart = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + expt + 
          '/diag6-9/')
    max_Tmax = []
    max_Tmin = []
    min_Tmax = []
    min_Tmin = []
    constraint = iris.Constraint(longitude=lonreq,latitude=latreq)
    for year in range(0,100):
        filename = filestart + extra + '_diag6-9_' + str(year) + '.nc'
        cubes = iris.load(filename)
        for cube in cubes:
            smallcube = cube.extract(constraint)
            if cube.long_name == 'Monthly minimum value of daily maximum temperature':
                min_Tmax.append(np.min(smallcube.data)-273.15)
            if cube.long_name == 'Monthly maximum value of daily maximum temperature':
                max_Tmax.append(np.max(smallcube.data)-273.15)
            if cube.long_name == 'Monthly minimum value of daily minimum temperature':
                min_Tmin.append(np.min(smallcube.data)-273.15)
            if cube.long_name == 'Monthly maximum value of daily minimum temperature':
                max_Tmin.append(np.max(smallcube.data)-273.15)
      
    print('mean maxTmax',np.mean(max_Tmax))
    print('mean maxTmin',np.mean(max_Tmin))
    print('mean minTmax',np.mean(min_Tmax))
    print('mean minTmin',np.mean(min_Tmin))
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

###################################################
# this is for if you actually want to get the diagnostics.
# ie write them to the file /nfs/hera1/earjcti/PLIOMIP2/..ETCCDI....diags 1-4  
#if MODELNAME == 'HadCM3':
#    get_HadCM3_diagnostics('xozzm','w')

##################################################################
###  this is for plotting the extremes on a month by month basis
#NYEARS = 100
#for MONTH_REQ in range(0,12):
#    plot_textremes_toplevel('xozzm','w','xozza','o')
 

###############################################################
### this is for plotting the anomalies in the diagnostics for all months
### on one page
### ie max_Tmax_plio - max_Tmax_pi (mean value) for all months
#field='minTmax' # values maxTmax,  maxTmin, maxTmin, minTmax
#plot_all_months(field, 'xozzb', 'xozza','y')  


############################################################
# extract average at a given location

extract_avg(52.5,0,'xozzm','w')
::::::::::::::
PlioMIP_new/extremes/month_means.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia
Used for comparing with diags 6-9
Plots the difference between the pliocene and the preindustrial mean temperature for each month
"""
import numpy as np
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import matplotlib.pyplot as plt
import sys


def get_data(expt,extra):
    """
    reads in the maximum  and minimum temperature for the year and puts it in 
    a single cube
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pd/' + expt + 'a@pd'+ extra
    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']

    allmonths_cubes = iris.cube.CubeList([])

    for i, month in enumerate(months):
        meanT_cubelist = iris.cube.CubeList([])
  
        for year in range(0,80):
            filename = filestart + np.str(year).zfill(2) + month + '.nc'
            cube=iris.load_cube(filename,'TEMPERATURE AT 1.5M')
            meanT_cubelist.append(cube)
      
        iris.util.equalise_attributes(meanT_cubelist)
        allcubes = meanT_cubelist.concatenate_cube()
        meanmoncube = allcubes.collapsed('t',iris.analysis.MEAN)
        meanmoncube.coord('ht').points=i+1
       
        allmonths_cubes.append(meanmoncube)

    return allmonths_cubes 
   



   
#########################################################
def plot_anomalies(cube, monthname, ocn_mask, plotname, plottype):
    """
    this is for plotting all the anomalies
    """ 

    cube.units = 'degC'
    maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
    vals = np.arange(-12,14,2)
    # process and plot
    if ocn_mask == 'y':
        cube.data.mask = (maskcube.data - 1.0) * (-1.0)
    cs = qplt.contourf(cube, levels=vals, cmap='RdBu_r', extend='both')
    plt.gca().coastlines()
    plt.title(plottype + ' Plio-Pi ' + monthname) 
    
   
    #cb_ax = fig.add_axes([0.35, 0.05, 0.30, 0.02])
           
    #cbar = fig.colorbar(cs, cax=cb_ax, orientation='horizontal')
    #cbar.set_label('degC', fontsize=10)
         
    #cbar.ax.tick_params(labelsize=10)
  
    # save to file
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/means/' + plotname +  monthname)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()


def plot_textremes_toplevel(expt,exptextra,cntl,cntlextra):
    """
    for each month plots the difference between the Tmean for the pliocene and the preindustrial
    """
    # mean_pliocubes is a cubelist one per month
    mean_pliocubes = get_data(expt,exptextra)
    mean_picubes = get_data(cntl,cntlextra)

    jan_anom = iris.util.squeeze(mean_pliocubes[0] - mean_picubes[0])
    feb_anom = iris.util.squeeze(mean_pliocubes[1] - mean_picubes[1])
    mar_anom = iris.util.squeeze(mean_pliocubes[2] - mean_picubes[2])
    apr_anom = iris.util.squeeze(mean_pliocubes[3] - mean_picubes[3])
    may_anom = iris.util.squeeze(mean_pliocubes[4] - mean_picubes[4])
    jun_anom = iris.util.squeeze(mean_pliocubes[5] - mean_picubes[5])
    jul_anom = iris.util.squeeze(mean_pliocubes[6] - mean_picubes[6])
    aug_anom = iris.util.squeeze(mean_pliocubes[7] - mean_picubes[7])
    sep_anom = iris.util.squeeze(mean_pliocubes[8] - mean_picubes[8])
    oct_anom = iris.util.squeeze(mean_pliocubes[9] - mean_picubes[9])
    nov_anom = iris.util.squeeze(mean_pliocubes[10] - mean_picubes[10])
    dec_anom = iris.util.squeeze(mean_pliocubes[11] - mean_picubes[11])
    plotname = 'Tanom_' + EXPTNAME +  '-' + CNTLNAME + '_' 

    # plot monthly mean temperature anomaly for this month
    plot_anomalies(jan_anom,'jan','y', plotname, 'Tmean')
    plot_anomalies(feb_anom,'feb','y', plotname, 'Tmean')
    plot_anomalies(mar_anom,'mar','y', plotname, 'Tmean')
    plot_anomalies(apr_anom,'apr','y', plotname, 'Tmean')
    plot_anomalies(may_anom,'may','y', plotname, 'Tmean')
    plot_anomalies(jun_anom,'jun','y', plotname, 'Tmean')
    plot_anomalies(jul_anom,'jul','y', plotname, 'Tmean')
    plot_anomalies(aug_anom,'aug','y', plotname, 'Tmean')
    plot_anomalies(sep_anom,'sep','y', plotname, 'Tmean')
    plot_anomalies(oct_anom,'oct','y', plotname, 'Tmean')
    plot_anomalies(nov_anom,'nov','y', plotname, 'Tmean')
    plot_anomalies(dec_anom,'dec','y', plotname, 'Tmean')

    # get max Tmax for month
    filename = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag6-9/' + 
                EXPTNAME + '_' + CNTLNAME + '_maxTmax_jan.nc')
    jancube = iris.load_cube(filename, 'Plio - PI: Mean max of Tmax')
    plotname = 'maxTmax_anom_' + EXPTNAME +  '-' + CNTLNAME + '_' 

    plot_anomalies(jancube,'jan','y', plotname, 'maxTmax')

     # get max Tmax for month
    filename = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag6-9/' + 
                EXPTNAME + '_' + CNTLNAME + '_maxTmax_jul.nc')
    jancube = iris.load_cube(filename, 'Plio - PI: Mean max of Tmax')
    plotname = 'maxTmax_anom_' + EXPTNAME +  '-' + CNTLNAME + '_' 

    plot_anomalies(jancube,'jul','y', plotname, 'maxTmax')

  
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

EXPTNAME = 'tenvj'
CNTLNAME = 'xozza'
plot_textremes_toplevel(EXPTNAME,'o',CNTLNAME,'o')
 

::::::::::::::
PlioMIP_new/extremes/plot_weather_at_location.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia


This will plot the weather at a particular location like we might see on the internet.

For each month we will get average Tmax, average Tmin and total precipitation
and plot them all

"""
import numpy as np
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys


def get_HCM3_year_data(filestart,year):
    """
    reads in the maximum  and minimum temperature for the year and puts it in 
    a single cube
    """
    maxTcubelist = iris.cube.CubeList([])
    minTcubelist = iris.cube.CubeList([])
    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    for month in months:
        filename = filestart + np.str(year).zfill(2) + month + '.nc'
        cubes=iris.load(filename)
        temperature_cubes = iris.cube.CubeList([])
        max_temp = []
        for cube in cubes:
            if (cube.var_name == 'temp' or cube.var_name == 'temp_1' 
                or cube.var_name == 'temp_2'):
               try:
                   cube.coord('t_1').rename('t')
               except:
                   pass
               cube_reg_1 = cube.extract(iris.Constraint(latitude = LAT_REQ,
                                                       longitude = LON_REQ))
               cube_reg = cube_reg_1.collapsed('t',iris.analysis.MEAN)
               temperature_cubes.append(cube_reg)
               max_temp.append(np.max(cube_reg.data))
     
        maxindex = np.argmax(max_temp)
        minindex = np.argmin(max_temp)
        indexes = np.argsort(max_temp)
        minTcube = temperature_cubes[indexes[0]]
        meanTcube = temperature_cubes[indexes[1]]
        maxTcube = temperature_cubes[indexes[2]]
    
        # check you have got maxT, meanT and minT in correct order        
        if np.max(maxTcube.data) < np.max(meanTcube.data):
            print('cubes not in right order')
            sys.exit(0)

        if np.max(meanTcube.data) < np.max(minTcube.data):
            print('cubes not in right order2')
            sys.exit(0)

        maxTcubelist.append(maxTcube-273.15)
        minTcubelist.append(minTcube-273.15)

        Tmax = []
        Tmin = []
        for cube in maxTcubelist:
            Tmax.append(cube.data[0])
        for cube in minTcubelist:
            Tmin.append(cube.data[0])
   
    return np.asarray(Tmax),np.asarray(Tmin)
   

def get_HadCM3_diagnostics(expt, extra):
    """
    gets the diagnostics (frost days, summer days, icing days tropical nights
    from HadCM3)
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra

    count=0
    summaxT = np.zeros(12)
    summinT = np.zeros(12)
    for year in range(0, 10):
        yearuse = np.str(year).zfill(2)
        maxT, minT  = get_HCM3_year_data(filestart, year)
        
        summaxT = summaxT + maxT
        summinT = summinT + minT
        count=count+1
    
    avg_maxT = summaxT / count
    avg_minT = summinT / count
    print(avg_maxT, avg_minT)
    
    plt.plot(avg_maxT)
    plt.plot(avg_minT)
    plt.show()
    sys.exit(0)

        
##########################################################
def read_data(expt,extra,startyear,endyear):
    """
    reads in the data for each year, finds the sum and returns
    """
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/'

    frostcubelist = iris.cube.CubeList([])
    summercubelist = iris.cube.CubeList([])
    icingcubelist = iris.cube.CubeList([])
    tropcubelist = iris.cube.CubeList([])
 
    for year in range(startyear,endyear):
        filename = (filestart + expt + '/diag1-4/' + extra + 
                    '_diag1-4_'+ np.str(year) + '.nc')

        frostcubelist.append(iris.load_cube(filename,'number of frost days'))
        summercubelist.append(iris.load_cube(filename,'number of summer days'))
        icingcubelist.append(iris.load_cube(filename,'number of icing days'))
        tropcubelist.append(iris.load_cube(filename,
                                           'number of tropical nights'))

    equalise_attributes(frostcubelist)
    allfrostcube = frostcubelist.merge_cube()
    meanfrostcube = allfrostcube.collapsed('t',iris.analysis.MEAN)
   
    equalise_attributes(summercubelist)
    allsummercube = summercubelist.merge_cube()
    meansummercube = allsummercube.collapsed('t',iris.analysis.MEAN)
    meansummercube.units = None
  
    equalise_attributes(icingcubelist)
    allicingcube = icingcubelist.merge_cube()
    meanicingcube = allicingcube.collapsed('t',iris.analysis.MEAN)
   
    equalise_attributes(tropcubelist)
    alltropcube = tropcubelist.merge_cube()
    meantropcube = alltropcube.collapsed('t',iris.analysis.MEAN)

    return (meanfrostcube, meansummercube, meanicingcube,meantropcube)
  
def plot_anom(cube,field,ocn_mask, expt, cntl, cube_cntl, cube_expt):
    """
    this will plot the anomaly cube between the pliocene and the pi
    """
    fieldname = {'frost' : 'number of frost days',
                 'summer': 'number of summer days',
                 'icing': 'number of icing days',
                 'tropical_nights' : 'number of tropical nights'}

    if ocn_mask == 'y':
       maskcube=iris.load_cube('/nfs/b0164/Data/LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc','LAND MASK (LOGICAL: LAND=TRUE)')
      
       cube.data.mask = (maskcube.data - 1.0) * (-1.0)
       cube_cntl.data.mask = (maskcube.data - 1.0) * (-1.0)
       cube_expt.data.mask = (maskcube.data - 1.0) * (-1.0)
      
    plt.subplot(2,2,1)
    qplt.contourf(cube_cntl, levels=np.arange(0,300,20), extend='max')
    plt.gca().coastlines()
    plt.title(fieldname.get(field) + ':' +  TIME.get(cntl))

    plt.subplot(2,2,2)
    qplt.contourf(cube_expt, levels=np.arange(0,300,20), extend='max')
    plt.gca().coastlines()
    plt.title(fieldname.get(field) + ':' +  TIME.get(expt))

    plt.subplot(2,2,3)
    if field == 'frost' or field == 'icing':
        cmapname = 'Blues_r'
        vals = np.arange(-40,5,5)
    else:
        cmapname = 'Reds'
        vals = np.arange(0,65,5)
    qplt.contourf(cube,cmap=cmapname, levels=vals,extend='both')
    plt.gca().coastlines()
    plt.title(fieldname.get(field) + ': ' + TIME.get(expt) + '-' +TIME.get(cntl))
    plt.subplot(2,2,4)
    qplt.contourf((cube / cube_cntl) * 100.,cmap=cmapname, 
                  levels=vals,extend='both')
    plt.gca().coastlines()
    plt.title('percentage change: ' + TIME.get(expt) + '-' +TIME.get(cntl))


    plt.tight_layout()
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/diag1-4/' + 
               field + '_' + expt + '-' + cntl)
    plt.savefig(fileout + '.eps')
    plt.savefig(fileout + '.png')
    plt.close()
    
    
def plot_n_extremes(expt, exptextra, cntl, cntlextra, nyears):
    """
    plots the number of days that are: 1. frost days, 2. summer days,
    3. icing days, 4. tropical nights and how it changes between plio and cntl
    """
    (frost_expt_cube, summerday_expt_cube, 
    icing_expt_cube, tropnight_expt_cube) = read_data(expt,exptextra,0, nyears)

    (frost_cntl_cube, summerday_cntl_cube, 
    icing_cntl_cube, tropnight_cntl_cube) = read_data(cntl,cntlextra,0, nyears)

    frost_anom_cube = frost_expt_cube - frost_cntl_cube
    plot_anom(frost_anom_cube,'frost','y', expt,cntl,
              frost_cntl_cube, frost_expt_cube)
  
    summer_anom_cube = summerday_expt_cube - summerday_cntl_cube
    plot_anom(summer_anom_cube,'summer','y', expt,cntl,
              summerday_cntl_cube, summerday_expt_cube)
  
    icing_anom_cube = icing_expt_cube - icing_cntl_cube
    plot_anom(icing_anom_cube,'icing','y', expt,cntl,
              icing_cntl_cube, icing_expt_cube)
  
    trop_anom_cube = tropnight_expt_cube - tropnight_cntl_cube
    plot_anom(trop_anom_cube,'tropical_nights','y', expt,cntl,
              tropnight_cntl_cube, tropnight_expt_cube)
  
    
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

TIME = {'tenvj' : 'mPlio', 'xozza' : 'PI', 'xozzb' : 'Plio','tenvs':'E560',
        'xozzm' : 'E560'}

LAT_REQ = 52.5
LON_REQ = 0.0

# this is for if you actually want to get the diagnostics.

Tmax_mon, Tmin_mon = get_HadCM3_diagnostics('xozzm','w')

# plot map of number of days that are 'extreme' according to the ETCCDI 1-4
# criteria
# plot anomaly from plio and pi

('xozzm','w','xozza','o',99)  # tenvj plio, xozza control, xozzb control
 

::::::::::::::
PlioMIP_new/extremes/pot_wet_bulb_gt35.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.02.2022 by Julia

This will count the number of days when the potential wet bulb temperature 
exceeds certain thresholds
20
25
30
35
and write to a file 
There will be one record in the file for each year
"""
import numpy as np
import iris
#from iris.experimental.equalise_cubes import equalise_attributes
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import sys


def get_HCM3_year_data(filestart,year):
    """
    reads in the wet bulb temperature for the year and puts it in 
    a single cube
    """
    wetbulb_cubelist = iris.cube.CubeList([])
    months = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    for month in months:
        filename = filestart + np.str(year).zfill(2) + month + '.nc'
        cube=iris.load_cube(filename,'WET BULB POTENTIAL TEMPERATURE')
        wetbulb_cubelist.append(cube)
        
    iris.util.equalise_attributes(wetbulb_cubelist)
    wetbulbcube = wetbulb_cubelist.concatenate_cube(wetbulb_cubelist)
   
    return wetbulbcube - 273.15

def count_wet_bulb_days(wetbulbcube, year):
    """
    input the wet bulb cube for the year
    output the number of days where the wet bulb temperature exceeds
           thresholds
    """
    wetbulbdata = wetbulbcube.data
    # find where wetbulb pot temp > vals
    wet_gt_20 = np.where(wetbulbdata > 20, 1.0, 0)
    wet_gt_25 = np.where(wetbulbdata > 25, 1.0, 0)
    wet_gt_30 = np.where(wetbulbdata > 30, 1.0, 0)
    wet_gt_35 = np.where(wetbulbdata > 35, 1.0, 0)

    # set up cubes
    wet_gt_20_cube = wetbulbcube.copy(data = wet_gt_20)
    wet_gt_25_cube = wetbulbcube.copy(data = wet_gt_25)
    wet_gt_30_cube = wetbulbcube.copy(data = wet_gt_30)
    wet_gt_35_cube = wetbulbcube.copy(data = wet_gt_35)

    # count how many days
    
    ndays_wet_gt_20_cube = wet_gt_20_cube.collapsed('t',iris.analysis.SUM)
    ndays_wet_gt_25_cube = wet_gt_25_cube.collapsed('t',iris.analysis.SUM)
    ndays_wet_gt_30_cube = wet_gt_30_cube.collapsed('t',iris.analysis.SUM)
    ndays_wet_gt_35_cube = wet_gt_35_cube.collapsed('t',iris.analysis.SUM)

    # set up for concatenation by year
    ndays_wet_gt_20_cube.coord('p').rename('year')
    ndays_wet_gt_25_cube.coord('p').rename('year')
    ndays_wet_gt_30_cube.coord('p').rename('year')
    ndays_wet_gt_35_cube.coord('p').rename('year')

    ndays_wet_gt_20_cube.coord('year').points = [year]
    ndays_wet_gt_25_cube.coord('year').points = [year]
    ndays_wet_gt_30_cube.coord('year').points = [year]
    ndays_wet_gt_35_cube.coord('year').points = [year]

    ndays_wet_gt_20_cube.remove_coord('t')
    ndays_wet_gt_25_cube.remove_coord('t')
    ndays_wet_gt_30_cube.remove_coord('t')
    ndays_wet_gt_35_cube.remove_coord('t')
    
    return (ndays_wet_gt_20_cube, ndays_wet_gt_25_cube, ndays_wet_gt_30_cube,
            ndays_wet_gt_35_cube)

def get_HadCM3_diagnostics(expt, extra):
    """
    gets the diagnostics (frost days, summer days, icing days tropical nights
    from HadCM3)
    """
    filestart = '/nfs/hera1/earjcti/um/' + expt + '/pb/' + expt + 'a@pb' + extra
    
    allcubes_wbgt20 = iris.cube.CubeList([])
    allcubes_wbgt25 = iris.cube.CubeList([])
    allcubes_wbgt30 = iris.cube.CubeList([])
    allcubes_wbgt35 = iris.cube.CubeList([])

    for year in range(1, 79):
        print(year)
        yearuse = np.str(year).zfill(2)
        wetbulbcube  = get_HCM3_year_data(filestart, year)

      
        (wet_gt_20_cube, 
         wet_gt_25_cube, 
         wet_gt_30_cube, 
         wet_gt_35_cube) = count_wet_bulb_days(wetbulbcube, year)
        

        allcubes_wbgt20.append(wet_gt_20_cube)
        allcubes_wbgt25.append(wet_gt_25_cube)
        allcubes_wbgt30.append(wet_gt_30_cube)
        allcubes_wbgt35.append(wet_gt_35_cube)
    
    iris.util.equalise_attributes(allcubes_wbgt20)
    print(allcubes_wbgt20[0],allcubes_wbgt20[1])
    wbgt20_cube = allcubes_wbgt20.concatenate_cube()
    wbgt20_cube.long_name='ndays with wetbulb pottemp gt 20degC'
    
    wbgt25_cube = allcubes_wbgt25.concatenate_cube()
    wbgt25_cube.long_name='ndays with wetbulb pottemp gt 25degC'

    wbgt30_cube = allcubes_wbgt30.concatenate_cube()
    wbgt30_cube.long_name='ndays with wetbulb pottemp gt 30degC'
   
    wbgt35_cube = allcubes_wbgt35.concatenate_cube()
    wbgt35_cube.long_name='ndays with wetbulb pottemp gt 35degC'
   
    
    cubelist = [wbgt20_cube, wbgt25_cube, wbgt30_cube, wbgt35_cube]

    outfile = ('/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/ETCCDI/' + 
                   expt + '/wetbulb_thresholds_' + extra + '.nc')
    iris.save(cubelist, outfile, netcdf_format="NETCDF3_CLASSIC")

  
    
##########################################################
# main program
MODELNAME = 'HadCM3'  # 'CESM2', 'IPSLCM6A', 'COSMOS', 'EC-Earth3.3', 
                      # 'CESM1.2', 'IPSLCM5A', 'MIROC4m', 'IPSLCM5A2',
                      # 'HadCM3', 'GISS2.1G', 'CCSM4',  'CCSM4-Utr', 
                      # 'CCSM4-UoT','NorESM-L', 'MRI2.3', 'NorESM1-F'

TIME = {'tenvj' : 'mPlio', 'xozza' : 'PI', 'xozzb' : 'Plio','tenvs':'E560',
        'xozzm' : 'E560'}
# this is for if you actually want to get the diagnostics.
# ie write them to the file /nfs/hera1/earjcti/PLIOMIP2/..ETCCDI....diags 1-4  
get_HadCM3_diagnostics('xozzb','p')


::::::::::::::
PlioMIP_new/extremes/standard_dev_ratio.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

As a first test for looking at the climate extremes we will plot 
 standard deviation EOI400
 =========================
 standard deviation E280

firstly for temperature. 

If the ratio is approximately 1.0 then the standard deviation is the same.  If it is >1 then there is more interannual variability in the Pliocene (which could be worrying).  If it is less than 1 there is less interannual variability in the Pliocene.


"""

import os
import sys
import numpy as np
#import matplotlib as mp
import matplotlib.pyplot as plt
#from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#import netCDF4
#from netCDF4 import Dataset, MFDataset
import iris
import iris.quickplot as qplt
import iris.analysis.cartography
import iris.coord_categorisation

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def getmodelfield(modelname, period):
    """
    get the mean values from the model data
    inputs: modelname (ie HadCM3)
            period (likely EOI400 or E280)
    returns:  a cube contatining the mean data from the model

    """

    modfile = (FILESTART + 'regridded100/' + modelname + '/' +
               period + '.' + FIELDNAME + '.sd_month.nc')

    tempcube = iris.load(modfile)
    cube2 = tempcube[0]
    cube2.units = UNITS
    print(cube2)
    print(cube2.coord('time'))
    
    cube = cube2[MONTH_REQ, :, :]
   
    #this will make all the dimensions of all the cubes match.


    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube


class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, timeperiod, anom_cubes):
        self.nmodels = len(MODELNAMES)
        self.filestart = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/'+ 
                          'extremes/sd_ratio/' + timeperiod + '_' + 
                          FIELDNAME + '_' + MONTHNAMES.get(MONTH_REQ))
        self.timeperiod = timeperiod
        self.anom_cubes = anom_cubes

        if timeperiod == 'Ratio':
            self.valmin = 0.5
            self.valmax = 1.6
            self.diff = 0.1
            self.colormap = 'RdBu_r'
            self.cbarlabel = 'ratio'
        else:            
            self.valmin = 0.0
            self.valmax = 2.0
            self.diff = 0.2
            self.colormap = 'RdBu_r'
            self.cbarlabel = 'sdev'


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        fig = plt.figure(figsize=(5.5, 4.25))
        for i in range(0, self.nmodels):

            cubedata = self.anom_cubes[i].data
            latitudes = self.anom_cubes[i].coord('latitude').points
            lon = self.anom_cubes[i].coord('longitude').points
            datatoplot, longitudes = (shiftgrid(180., cubedata,
                                                lon, start=False))
            #if (np.mod(i, 8) + 1) == 1:
            #    title_ = (MODELNAMES[i] + ':' +
            #              self.timeperiod + ' (model - MMM)')
            #else:
            #    title_ = (MODELNAMES[i])

            title_ = (MODELNAMES[i])
            self.plotmap(i, title_,
                         datatoplot, longitudes, latitudes, fig)


        return

    def plotmap(self, i, titlename, datatoplot, longitudes, latitudes, fig):
        """
        will plot the data in a map format

        """

        xplot = 4
        yplot = 4


        plotpos = np.mod(i, xplot * yplot) + 1
        plt.subplot(xplot, yplot, plotpos)
        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=-90.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)

        V = np.arange(self.valmin, self.valmax, self.diff)
        cs = map.contourf(x, y, datatoplot, V, cmap=self.colormap,
                          extend='both')
        plt.title(titlename,fontsize=8)


        if plotpos == (xplot * yplot) or (i + 1) == self.nmodels:
             # Shrink current axis by 20% and put a legend to the right
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.82, top=0.9,
                                wspace=0.1, hspace=0.0)

            cb_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])
           
            cbar = fig.colorbar(cs, cax=cb_ax, orientation='vertical')
            #cbar = plt.colorbar(fig, orientation='horizontal')
            #fig.colorbar(fix, ax=axs[:, col], shrink=0.6)
            cbar.ax.tick_params(labelsize=8)
            print('plotted colorbar')
            #plt.show()
            #plt.tight_layout()
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.eps')
            plt.savefig(fileout, bbox_inches='tight')

            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.pdf')

            plt.savefig(fileout, bbox_inches='tight')
            plt.close()

def plot_mmm(mean_ratio, mean_pi, mean_plio, ocn_mask):
    """
    plots the multimodel mean we will mask out the ocean if necessary
    """
    land_ocn = {'y' : '_land', 'n': '_globe'}

    if ocn_mask == 'y':
       tempcube=iris.load_cube('/nfs/hera1/earjcti/regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
       cubegrid=iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
       exptlsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
      
       tempcube=iris.load_cube(FILESTART+'regridded/PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
       cntllsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())

       mean_ratio.data.mask = (exptlsmcube.data - 1.0) * (-1.0)
       mean_pi.data.mask = (cntllsmcube.data - 1.0) * (-1.0)
       mean_plio.data.mask = (exptlsmcube.data - 1.0) * (-1.0)
    

    qplt.contourf(mean_ratio, levels=np.arange(0.5, 1.6, 0.1), 
                  cmap='RdBu_r',extend='both')
    plt.title(MONTHNAMES.get(MONTH_REQ) + 'Ratio:  Plio /  PI st_dev : ' + FIELDNAME)
    plt.gca().coastlines()
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/sd_ratio/ratio_MMM_' + FIELDNAME + land_ocn.get(ocn_mask) + '_' + MONTHNAMES.get(MONTH_REQ) + '.eps', bbox_inches='tight')
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/sd_ratio/ratio_MMM_' + FIELDNAME + land_ocn.get(ocn_mask) + '_' + MONTHNAMES.get(MONTH_REQ) + '.png', bbox_inches='tight')
    plt.close()

  
    qplt.contourf(mean_pi, levels=np.arange(0.0, 2.1, 0.1), 
                  cmap='RdBu_r',extend='max')
    plt.title(MONTHNAMES.get(MONTH_REQ) + 'PI st_dev : ' + FIELDNAME)
    plt.gca().coastlines()
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/sd_ratio/pi_sdev_MMM_' + FIELDNAME + land_ocn.get(ocn_mask) + '_' + MONTHNAMES.get(MONTH_REQ) + '.eps', bbox_inches='tight')
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/sd_ratio/pi_sdev_MMM_' + FIELDNAME + land_ocn.get(ocn_mask) + '_' + MONTHNAMES.get(MONTH_REQ) + '.png', bbox_inches='tight')
    plt.close()

    qplt.contourf(mean_plio, levels=np.arange(0.0, 2.1, 0.1), 
                  cmap='RdBu_r',extend='max')
    plt.title(MONTHNAMES.get(MONTH_REQ) + 'Plio st_dev : ' + FIELDNAME)
    plt.gca().coastlines()
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/sd_ratio/plio_sdev_MMM_' + FIELDNAME + land_ocn.get(ocn_mask) + '_' + MONTHNAMES.get(MONTH_REQ) + '.eps', bbox_inches='tight')
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/extremes/sd_ratio/plio_sdev_MMM_' + FIELDNAME + land_ocn.get(ocn_mask) + '_' + MONTHNAMES.get(MONTH_REQ) + '.png', bbox_inches='tight')
    plt.close()

  
   
     

def get_mean(pi_sdev_cubes, plio_sdev_cubes):
    """
    this will get the mean standard deviation from all the models and
    find the ratio between them.
    Note to find the mean standard deviation we want to add up all the variances    and then squareroot
    """
    count=0
    for i, cube in enumerate(pi_sdev_cubes):
        if i == 0:
            pi_var_cube = cube * cube
            count=count+1
        else:
            print(pi_var_cube)
            print(cube)
            pi_var_cube = pi_var_cube + (cube * cube)
            count=count+1
    pi_var_cube = pi_var_cube / count
    mean_pi_sd = pi_var_cube.copy(data=np.sqrt(pi_var_cube.data))
   

    count=0
    for i, cube in enumerate(plio_sdev_cubes):
        if i == 0:
            plio_var_cube = cube * cube
            count=count+1
        else:
            plio_var_cube = plio_var_cube + (cube * cube)
            count=count+1
    plio_var_cube = plio_var_cube / count
    mean_plio_sd = plio_var_cube.copy(data=np.sqrt(plio_var_cube.data))
   
  
    return mean_plio_sd / mean_pi_sd, mean_pi_sd, mean_plio_sd 
##########################################################
# main program
# set up variable information
FIELDNAME = 'NearSurfaceTemperature'
UNITS = 'Celsius'
#FIELDNAME = 'SST'
#UNITS = 'Celsius'
#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'
#FIELDNAME = 'SST'
LINUX_WIN = 'l'

MONTHNAMES = {0:'Jan',1:'Feb',2:'Mar',3:'Apr',4:'May',5:'Jun',6:'Jul',7:'Aug',8:'Sep',9:'Oct',10:'Nov',11:'Dec',}
MONTH_REQ = 6

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
             ]

#MODELNAMES = ['NorESM-L']



# set up cubelists to store data
mpwp_sdev_cubes = iris.cube.CubeList([])
pi_sdev_cubes = iris.cube.CubeList([])
ratio_sdev_cubes = iris.cube.CubeList([])

#################################################
# get standard deviation data

for model, modelname in enumerate(MODELNAMES):
    model_plio_cube = getmodelfield(modelname, 'EOI400')
    model_pi_cube = getmodelfield(modelname, 'E280')
  
    if modelname == 'EC-Earth3.1' and FIELDNAME == 'SST':
       model_pi_cube.coord('latitude').bounds = None
       model_pi_cube.coord('longitude').bounds = None

    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_sdev_cubes.append(model_plio_cube)
    pi_sdev_cubes.append(model_pi_cube)
    ratio_sdev_cubes.append(model_plio_cube / model_pi_cube)

##################################################
# plot the cubes for the model anomalies relative to the mean

#obj = Plotalldata('mPWP', mpwp_sdev_cubes)
#obj.plotdata()

#obj = Plotalldata('PI', pi_sdev_cubes)
#obj.plotdata()

#obj = Plotalldata('ratio', ratio_sdev_cubes)
#obj.plotdata()


################################################################
# get mean ratios 
# note that the mean standard deviation is the square root of the sum
# of the variances

mean_ratio, mean_pi_sd, mean_plio_sd = get_mean(pi_sdev_cubes, mpwp_sdev_cubes)
plot_mmm(mean_ratio, mean_pi_sd, mean_plio_sd,'y')
plot_mmm(mean_ratio, mean_pi_sd, mean_plio_sd,'n')


#
::::::::::::::
PlioMIP_new/ITCZ/ITCZ_diagnostics_regional.py
::::::::::::::
#!/usr/bin/env python3
"""
#NAME
#    ITCZ diagnostics
#PURPOSESS
#    This program will find the ITCZ in a given region based on the
#    Stanfield et al 2015 definition.  It will find the centerline width
#    and intensity and plot these by season for the Pliocene and the PI.
#
# search for 'main program' to find end of functions
# Julia August 2018
#
# Notes This program is like ITCZ_diagnostics_regional.  However it allows
# the ITCZ to be discontinuous.
"""

import os
os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
#os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap
import iris
import iris.plot as iplt
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
import numpy as np
#import matplotlib as mp
import matplotlib.pyplot as plt
import sys


def plotmean_newaxis(cube, modelno_):
    """
    add a new axis to cube for help with concatenation
    """
    tempcube = iris.util.new_axis(cube)
    tempcube.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                standard_name='model_level_number', 
                                                long_name='model',
                                                var_name='model',
                                                units=None,
                                                bounds=None,
                                                coord_system=None,
                                                circular=False), 0)
    return tempcube

def recentre(cube, upper_bound_index, lower_bound_index,
             most_intense_precip_index):
    """
    recentre so that all the indexes go from -180 to 180
    """

    lons = cube.coord('longitude').points
    lons_recentre = np.where(lons < 180., lons, lons -360.)
    indexes = lons_recentre.argsort() # of the recentered array

    lats = cube.coord('latitude').points

    lat_upper = np.full((NMONTHS, len(lons)), np.nan)
    lat_lower = np.full((NMONTHS, len(lons)), np.nan)
    lat_max = np.full((NMONTHS, len(lons)), np.nan)

    for month in range(len(MONTHNAMES)):
        for i in range(len(lons)):
            if not upper_bound_index.mask[month, i]:
                lat_upper[month, i] = lats[upper_bound_index[month, i] - 1]
            if not lower_bound_index.mask[month, i]:
                lat_lower[month, i] = lats[lower_bound_index[month, i]]
            if not most_intense_precip_index.mask[month, i]:
                lat_max[month, i] = lats[most_intense_precip_index[month, i]]

        lat_upper[month, :] = lat_upper[month, indexes]
        lat_lower[month, :] = lat_lower[month, indexes]
        lat_max[month, :] = lat_max[month, indexes]

    return(lons_recentre[indexes], lat_upper, lat_lower, lat_max)


class GetITCZ:
    """
    the class object is the region name
    this class will deal with finding the itcz in this region (N/S bounds
    and maximum intensity)
    """
    def __init__(self, region):

        # l = land, b=both if not set then it is o=ocean
        landoceanind = {"IndianOceanLand" : "b",
                        "Globe" : "b",
                        "Africa" : "l",
                        "CentralAmerica" : "l",
                        "EastAtlantic" : "b",
                        "Indian" : "l", "EastAsia" : "l", "Indonesia" : "b",
                        "IndonesiaLand" : "l", "Australia" : "l",
                        "SouthAmerica" : "l", "WesternPacific" : "b",
                        "EasternPacific" : "b", "EastPacificExt" : "b"
                        }
        latmin = {"Globe" : -30., "Africa" : -30., "CentralAmerica" : -25.,
                  "EastAtlantic" : -25., "Indian" : 10.0,
                  "EastAsia" : 0.0, "Indonesia" : -12.0,
                  "IndonesiaLand" : -10.0, "Australia" : -30.0,
                  "SouthAmerica" : -20.0, "WesternPacific" : -30.0,
                  "EasternPacific" : -30.0, "EastPacificExt" : -10.0}

        latmax = {"IndianOcean" : 30, "IndianOceanLand" : 30,
                  "Globe" : 45.0, "Africa" : 35.0,
                  "CentralAmerica" : 25.0, "EastAtlantic" : 25.0,
                  "Indian" : 30.0, "EastAsia" : 50.0, "Indonesia" : 10.0,
                  "IndonesiaLand" : 0.0, "Australia" : -12.0,
                  "SouthAmerica" : 15.0, "WesternPacific" :30.0,
                  "EasternPacific" : 30.0, "EastPacificExt" : 30.0}

        lonmin = {"AtlanticOcean" : -60.0, "IndianOcean" : 30,
                  "IndianOceanLand" : 60.0, "Globe" : 0, "Africa" : -20.0,
                  "CentralAmerica" : -120.0, "EastAtlantic" : -120.0,
                  "Indian" : 70.0, "EastAsia" : 60.0, "Indonesia" : 110.0,
                  "IndonesiaLand" : 130.0, "Australia" : 110,
                  "SouthAmerica" : -80.0, "WesternPacific" : 150.0,
                  "EasternPacific" : -120.0, "EastPacificExt" : -120.0}

        lonmax = {"AtlanticOcean" : 20.0, "IndianOcean" : 120,
                  "IndianOceanLand" : 90.0, "Globe" : 360.0, "Africa" : 45.0,
                  "CentralAmerica" : -80.0, "EastAtlantic" : -80.0,
                  "Indian" : 110.0, "EastAsia" : 150.0, "Indonesia" : 150,
                  "IndonesiaLand" : 150.0, "Australia" : 160.0,
                  "SouthAmerica" : -30.0, "WesternPacific" : 200.0,
                  "EasternPacific" : -90.0, "EastPacificExt" : -60.0}

        reg_threshold = {"IndianOceanLand" : 4.0, "Globe" : 4.0,
                         "Africa" : 2.0, "CentralAmerica" : 0.0,
                         "EastAtlantic" : 4.0, "Indian" : 0.0,
                         "EastAsia" : 2.0, "Indonesia" : 0.0,
                         "IndonesiaLand" : 0.0, "Australia" : 1.0,
                         "SouthAmerica" : 4.0, "WesternPacific" : 4.0,
                         "EasternPacific" : 4.0, "EastPacificExt" : 4.0}

        self.regionname = region
        self.land_ocean_ind = landoceanind.get(region, 'o')
        self.latmin = latmin.get(region, -20.0)
        self.latmax = latmax.get(region, 20.0)
        self.latmax = self.latmax + 0.5 # correct grid
        self.latmin = self.latmin - 0.5
        self.lonmin = lonmin.get(region)
        self.lonmax = lonmax.get(region)
        self.threshold = THRESHOLD
        self.threshold[:] = reg_threshold.get(region, THRESHOLD)


    def get_region_AOI(self):
        """

        Returns
        -------
        The area of interest incase the calling program needs it

        """

        lons = np.arange(self.lonmin, self.lonmax, 1.0)
        lons = np.where(lons < 180., lons, lons-360.)
        return(np.min(lons), np.max(lons), self.latmin, self.latmax)


    def read_cube(self, period, model, modelno):
        """
        reads the cube and extracts the required region and equalises all the
        attributes
        """

        if period == 'E280':
            cubelsmo = iris.load_cube(E280LSM)
        if period == 'Eoi400':
            cubelsmo = iris.load_cube(EOI400LSM)

        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        cube_lsm = cubelsmo.regrid(cubegrid, iris.analysis.Linear())

        # get precipitation data in mm/day

        filename = (FILESTART + 'regridded/' + model + '/' + period  +
                    '.TotalPrecipitation.mean_month.nc')
        cube_precip = iris.load_cube(filename)


        # mask out land or ocean as appropriate.
        if np.array_equal(cube_lsm.coord('longitude').points,
                          cube_precip.coord('longitude').points):
            if self.land_ocean_ind == 'l':
                cube_precip.data = cube_precip.data * cube_lsm.data
            if self.land_ocean_ind == 'o':
                cube_precip.data = cube_precip.data * np.abs(cube_lsm.data - 1.0)
        else:
            print('error lon/lat of land sea mask dont match')
            print('lsm long', cube_lsm.coord('longitude'))
            print('precip long', cube_precip.coord('longitude'))
            sys.exit()


        # decompose grid to that defined by the region

        sample_points = [('longitude',
                          np.arange(self.lonmin, self.lonmax + 1.0, 1.0)),
                         ('latitude',
                          np.arange(self.latmin, self.latmax + 1.0, 1.0))]

        cube_precip.coord('latitude').var_name = 'latitude'
        cube_precip.coord('longitude').var_name = 'longitude'
        cube_precip.coord('latitude').long_name = None
        cube_precip.coord('longitude').long_name = None

        cube_area = cube_precip.interpolate(sample_points, iris.analysis.Linear())
        cube_area.var_name = "precip"
        cube_area.standard_name = None
        cube_area.long_name = "precip"
          # remove auxillary coordinate year and time
        print(model, period, cube_area.aux_coords)
        for coord in cube_area.aux_coords:
            if coord.standard_name == None:
                name = coord.var_name
            else:
                name = coord.standard_name
            cube_area.remove_coord(name)
        # remove attributes from month
        cube_area.coord('month').attributes = None

        for coord in cube_area.coords():
            coord.points = coord.points.astype('float32')
        cube_area.data = cube_area.data.astype('float32')

        print(cube_area.coord('month').attributes)
        # add scalar coordinate for merge
        new_cube = plotmean_newaxis(cube_area, modelno)
        new_cube.cell_methods = None
        
        return new_cube


    def find_ITCZ(self, period, model, modelno):
        """
        #  to find the ITCZ we find precipitation in the given region.
        #  we then find the longest continuous stretch of precipitation above a certain
        #  threshold for each longitude.
        """

        print('in find_ITCZ')


        cube_AOI = self.read_cube(period, model, modelno)

        #


        # for each longitude find the longest continuous band of precipitation
        # above the threshold

        (upper_bound_index,
         lower_bound_index,
         most_intense_precip_index) = self.get_indexes(cube_AOI)



        # plot precipitation map for each month all on one page with bands shown
        # return some of the things we are plotting

        (lons_recentre,
         lat_upper,
         lat_lower,
         lat_max) = recentre(cube_AOI, upper_bound_index,
                             lower_bound_index,
                             most_intense_precip_index)



        return (cube_AOI, lons_recentre, lat_upper, lat_lower,
                lat_max)


    def get_indexes(self, cube):
        """
        the main part of the class which actually gets the indexes
        """

        def correct_undefined():
            """
            # where ITCZ is undefined set to NAN
            # if lower and upper are same set to undefined
            """
            nlats = len(lats)
            for j, upper_int in enumerate(upper_bound_index):
                for i, upper in enumerate(upper_int):
  #                  if upper == lower_bound_index[j,i] and upper == 0.0:
                    if upper == lower_bound_index[j, i]:
                        upper_bound_index[j, i] = np.ma.masked
                        lower_bound_index[j, i] = np.ma.masked
                        most_intense_precip_index[j, i] = np.ma.masked
                    if upper == 0 or lower_bound_index[j, i] == 0:
                        upper_bound_index[j, i] = np.ma.masked
                        lower_bound_index[j, i] = np.ma.masked
                        most_intense_precip_index[j, i] = np.ma.masked
                    if upper >= nlats-1 or lower_bound_index[j, i] >= nlats-1:
                        upper_bound_index[j, i] = np.ma.masked
                        lower_bound_index[j, i] = np.ma.masked
                        most_intense_precip_index[j, i] = np.ma.masked

            return upper_bound_index, lower_bound_index, most_intense_precip_index

        def correct_discontinuity(precip_AOI, bound_index, ns_ind):
            """
            # ns ind = 1.0 for upper bound, -1.0 for lower bound
            #  correct a very small (=< 4 gridboxes of longitude) discontinuity
            # a large discontinuity is > 6 degrees
            """

            for index, bound in np.ndenumerate(bound_index):
                mon = index[0]
                i = index[1]
                if bound_index.mask[mon, i]:
                    break

                bound_prev = bound_index[mon, i - 1]
                bound_diff = np.abs(bound - bound_prev)

                if bound_diff > 6 and bound_prev != 0:


                    # large difference will it return to previous position within
                    # n degrees (min diff = minumum difference over next 10 longitudes)
                    n = 11
                    min_diff = np.min(np.abs(
                            bound_index[mon, i + 1: i  + n]
                            - bound_prev))

                    if min_diff < bound_diff / 1.3:

                        # initially set to same as previous
                        bound_index[index] = bound_index[mon, i - 1]
                        # if new upper bound index has precipitation
                        # greater than threshold see if you can expand
                        # northwards if upper bound, southwards if lower bound
                        j1 = bound_index[index]
                        if precip_AOI[mon, bound_index[index], i] >= self.threshold[mon]:

                            j2 = len(AOI_lat)
                            if ns_ind < 0.0: j2 = 0
                            for j in range(j1, j2, ns_ind):
                                if precip_AOI[mon, j, i] >= self.threshold[mon]:
                                    bound_index[mon, i] = j # move northwards/southwards
                                else:
                                    break # no longer try and move northwards


                        # if new upper bound index has precipitation that is less
                        # then the threshold then you will have to move it
                        # southwards for the upper bound or northwards for the lower bound
                        else:
                            j2 = len(AOI_lat)
                            if ns_ind > 0: j2 = 0
                            for j in range(j1, j2, -1 * ns_ind):
                                if precip_AOI[mon, j, i] >= self.threshold[mon]:
                                    bound_index[mon, i] = j # move southwards/northwards
                                    break


            return bound_index



        def get_most_intense(precip_AOI):
            """
            gets the index of the most intense precipitation between the
            lower and upper bounds
            """
            most_intense_precip_index = np.ma.zeros(np.shape(lower_bound_index), dtype=int)
            for index, lower in np.ndenumerate(lower_bound_index):

                if (np.isfinite(lower) and np.isfinite(upper_bound_index[index])
                    and lower < upper_bound_index[index]):

                    max_val = 0.
                    for j in range(lower, upper_bound_index[index] + 1):
                        if precip_AOI[index[0], j, index[1]] > max_val:
                            max_val = precip_AOI[index[0], j, index[1]]
                            most_intense_precip_index[index] = j
                else:
                    most_intense_precip_index[index] = np.ma.masked


            return most_intense_precip_index



        print('in get_indexes')

        lons = cube.coord('longitude').points
        nlons = len(lons)
        lats = cube.coord('latitude').points
        nlats = len(lats)
        months = cube.coord('month').points
        nmonths = len(months)

        max_count_lons = np.zeros((nmonths, nlons), dtype=int)
        upper_bound_index = np.ma.zeros((nmonths, nlons), dtype=int)
        lower_bound_index = np.ma.zeros((nmonths, nlons), dtype=int)
        most_intense_precip_index = np.ma.zeros((nmonths, nlons), dtype=int)



        for mon in range(0, nmonths):
            smallcube = iris.util.squeeze(cube)
            AOI_precip = smallcube.data[mon, :, :]
            AOI_lat = cube.coord('latitude').points

            for i, lon in enumerate(lons):
                count_lons = 0
                jmax = len(lats) - 1
                jmin = 0
                # check it is not picking up mid latitude storm tracks in pacific
                # or atlantic by reducing range to -20 20 ignore for south america
                if ((lon > 180. or lon < 0) and (self.regionname != 'SouthAmerica')):
                    jmax = (np.abs(lats - 20.)).argmin()
                    jmin = (np.abs(lats + 20.)).argmin()
                # restrict to 20S in summer
                if 3 <= mon <= 8:
                    jmin = (np.abs(lats + 20.)).argmin()

                for j in range(jmax-1, jmin, -1):
                    # set up j-1 and j+1 for checking values either side
                    jmin1 = j - 1
                    jpl1 = j + 1
                    if jmin1 < 0:
                        jmin1 = 0
                    if jpl1 > nlats-1:
                        jpl1 = nlats-1
                    # check precipitation greater than threshold but include if there
                    # is only a single latitude blip

                    if (AOI_precip[j, i] >= self.threshold[mon] or
                            (AOI_precip[jmin1, i] > self.threshold[mon] and
                             AOI_precip[jpl1, i] > self.threshold[mon])):
                        count_lons = count_lons+1

                    # if the number of longitude is greater than the previous
                    # maximum then this is the new ITCZ.  However we are
                    # constraining the
                    # lower bound latitude to be less than 20N
                        if (count_lons > max_count_lons[mon, i]
                                and lats[j] < 20.):
                            max_count_lons[mon, i] = count_lons
                            lower_bound_index[mon, i] = j

                    else:
                        count_lons = 0

                #############################################
                # set up upper bound and get masimum intensity index
                upper_bound_index[mon, i] = lower_bound_index[mon, i] + max_count_lons[mon, i]


             ###########################################
            # CORRECTIONS TO THE ALGORITHM
            # make sure picks up upper bound in west pacific
            # don't use this as it is too complicated and doesn't make enough difference

            #(lower_bound_new,
            # upper_bound_new) = correct_west_pacific(lower_bound_index[mon, :],
            #                                         upper_bound_index[mon, :],
            #                                         mon)
            #lower_bound_index[mon, :] = lower_bound_new
            #upper_bound_index[mon, :] = upper_bound_new



        (upper_bound_index,
         lower_bound_index,
         most_intense_precip_index) = correct_undefined()

        upper_bound_index = correct_discontinuity(smallcube.data,
                                                  upper_bound_index, 1) # for upper bound

        lower_bound_index = correct_discontinuity(smallcube.data,
                                                  lower_bound_index, -1) # for upper bound

        most_intense_precip_index = get_most_intense(smallcube.data)


        #print('j1',upper_bound_index[1],lower_bound_index[1])
        return [upper_bound_index, lower_bound_index,
                most_intense_precip_index]


def plot_map(cubelist, lons, upper_lat, lower_lat,
             max_lat, period):
    """
    For each month plots the ICZ for each model.  Showing
    tropical precipitation and the upper and lower bounds

    """

    print('in plot map')
    fig = plt.figure(figsize=[12.0, 8.0])

    for month, monthname in enumerate(MONTHNAMES):
        fig = plt.figure(figsize=[12.0, 8.0])
        # plot all models for each month
        for i, modelname in enumerate(MODELNAMES):
            cube = cubelist[i]

            plt.subplot(4, 4, i + 1)
            cubeplot = iris.util.squeeze(cube)
            cubeplot = cubeplot[month, :, :]

            V = np.arange(0, 15, 1)
            cs = iplt.contourf(cubeplot, V, cmap='Blues', extend='max')
            plt.gca().coastlines()

            plt.plot(lons, upper_lat[i][month, :], color='red', linewidth=1)
            plt.plot(lons, lower_lat[i][month, :], color='red', linewidth=1)
            plt.plot(lons, max_lat[i][month, :], color='white', linewidth=1)
            plt.title(modelname)


        # sort out all subplots and print out to a file
        plt.subplots_adjust(left=0.1, bottom=0.2, right=0.9, top=0.9, wspace=0.1, hspace=0.0)
        cb_ax = fig.add_axes([0.1, 0.1, 0.8, 0.05])
        cbarname = period + ' ' + monthname + ' precip (mm/day)'
        cbar = fig.colorbar(cs, cax=cb_ax, orientation="horizontal")
        cbar.set_label(cbarname, fontsize=15)

        fileoutstart = (FILESTART + 'ITCZ/' + REGION + '/' + period + '_'
                        + np.str(month) + monthname + '_precipitation.')

        if LINUX_WIN == 'w':
            plt.savefig(fileoutstart + 'png')
        else:
            plt.savefig(fileoutstart + 'pdf')

        plt.close()


    return

def plot_map_mmm(cubelist, lons, period):
    """
    For each month plots the multimodel mean ITCZ.  Showing
    tropical precipitation and the upper and lower bounds

    """

    print('in plot map')
    fig = plt.figure(figsize=[12.0, 8.0])

    # get average
    equalise_attributes(cubelist)
    cube = cubelist.concatenate_cube()

    cube_avg = cube.collapsed('model_level_number', iris.analysis.MEAN)

    # get indexfrom MMM
    Regioninfo = GetITCZ(REGION)

    (upper_bound_index,
     lower_bound_index,
     max_index) = Regioninfo.get_indexes(cube_avg)

    (lons_recentre, lat_upper,
     lat_lower, lat_max) = recentre(cube_avg, upper_bound_index,
                                    lower_bound_index, max_index)

    # plot
    fig = plt.figure(figsize=[12.0, 8.0])
    for month, monthname in enumerate(MONTHNAMES):

        plt.subplot(3, 4, month + 1)
        cubeplot = cube_avg[month, :, :]
        V = np.arange(0, 15, 1)
        cs = iplt.contourf(cubeplot, V, cmap='Blues', extend='max')
        plt.gca().coastlines()

        plt.plot(lons, lat_upper[month, :], color='red', linewidth=1)
        plt.plot(lons, lat_lower[month, :], color='red', linewidth=1)
        plt.plot(lons, lat_max[month, :], color='white', linewidth=1)
        plt.title(monthname)



    # sort out all subplots and print out to a file
    plt.subplots_adjust(left=0.1, bottom=0.2, right=0.9, top=0.9, wspace=0.1, hspace=0.0)
    cb_ax = fig.add_axes([0.1, 0.1, 0.8, 0.05])
    cbarname = period + 'MMM precip (mm/day)'
    cbar = fig.colorbar(cs, cax=cb_ax, orientation="horizontal")
    cbar.set_label(cbarname, fontsize=15)

    fileoutstart = (FILESTART + 'ITCZ/' + REGION + '/MMM_' + period + '_'
                        + '_precipitation.')

    if LINUX_WIN == 'w':
        plt.savefig(fileoutstart + 'png')
    else:
        plt.savefig(fileoutstart + 'pdf')

    plt.close()


    # write means out so we can reaccess them
    write_info((FILESTART + 'ITCZ/' + REGION + '/data_MMM_'
                + period + '.txt'),
               lons, lat_upper, lat_lower, lat_max)

    fileout = (FILESTART + 'ITCZ/' + REGION + '_multimodelmean.nc')
    iris.save(cubeplot,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)



    return




def plot_lats(lons, lat_pi, lat_plio, title, lonmin, lonmax, latmin, latmax,
              filename):
    """
    will do a latitude vs longitude line on a map for both the pliocene and
    the preindustrial over 12 months
    """

    print(lons)
    print(lons[0])
    print(np.shape(lons))

    for monno, monname in enumerate(MONTHNAMES):
        fig = plt.figure(figsize=[12.0, 8.0])
        print(filename, monno)
        for i, model in enumerate(MODELNAMES):
            plt.subplot(4, 4, i+1)
            map = Basemap(llcrnrlon=lonmin, urcrnrlon=lonmax, llcrnrlat=latmin,
                          urcrnrlat=latmax, projection='cyl', resolution='c')
            map.drawcoastlines()
            plt.plot(lons[i], lat_pi[i][monno, :], label='pi')
            plt.plot(lons[i], lat_plio[i][monno, :], label='plio')
            plt.title(model)

        plt.legend()
        plt.subplots_adjust(left=0.1, bottom=0.2, right=0.9, top=0.9, wspace=0.1, hspace=0.0)
        fig.text(0.1, 0.9, title, fontsize=20)

        fileout = (FILESTART + '/ITCZ/' + REGION + '/' + filename +
                   np.str(monno) + monname)
        if LINUX_WIN == 'w':
            plt.savefig(fileout + '.png')
        else:
            plt.savefig(fileout + '.pdf')

        plt.close()

def write_info(filename, lons, upper, lower, maximum):
    """
    # write information to a file

    """
    f1 = open(filename, 'w+')
    f1.write('longitude, upper bound latitude, lower bound latitude, ' +
                 'max_precip latitude \n')

    for mon, month in enumerate(MONTHNAMES):
        f1.write('Data for ' + month +  '\n')
        for i, lon in enumerate(lons):
            f1.write(np.str(np.around(lon, 2)) + ',' +
                     np.str(np.around(upper[mon, i], 2)) + ',' +
                     np.str(np.around(lower[mon, i], 2)) + ',' +
                     np.str(np.around(maximum[mon, i], 2)) + '\n')
    f1.close()


def main():
    print('in main')


    cubelist_AOI_pi = iris.cube.CubeList([])
    lons_pi_allmods = []
    upper_bound_lat_pi_allmods = []
    lower_bound_lat_pi_allmods = []
    most_intense_lat_pi_allmods = []

    cubelist_AOI_plio = iris.cube.CubeList([])
    lons_plio_allmods = []
    upper_bound_lat_plio_allmods = []
    lower_bound_lat_plio_allmods = []
    most_intense_lat_plio_allmods = []

    Regioninfo = GetITCZ(REGION)
    lonmin, lonmax, latmin, latmax = Regioninfo.get_region_AOI()

    for modno, modname in enumerate(MODELNAMES):

        (cube_AOI_pi, lons_pi, upper_bound_lat_pi, lower_bound_lat_pi,
         most_intense_precip_lat_pi) = Regioninfo.find_ITCZ('E280',
                                                            modname, modno)

        cubelist_AOI_pi.append(cube_AOI_pi)
        lons_pi_allmods.append(lons_pi)
        upper_bound_lat_pi_allmods.append(upper_bound_lat_pi)
        lower_bound_lat_pi_allmods.append(lower_bound_lat_pi)
        most_intense_lat_pi_allmods.append(most_intense_precip_lat_pi)

        (cube_AOI_plio, lons_plio, upper_bound_lat_plio, lower_bound_lat_plio,
         most_intense_precip_lat_plio) = Regioninfo.find_ITCZ('Eoi400',
                                                              modname, modno)

        cubelist_AOI_plio.append(cube_AOI_plio)
        lons_plio_allmods.append(lons_pi)
        upper_bound_lat_plio_allmods.append(upper_bound_lat_plio)
        lower_bound_lat_plio_allmods.append(lower_bound_lat_plio)
        most_intense_lat_plio_allmods.append(most_intense_precip_lat_plio)


        write_info(FILESTART + 'ITCZ/' + REGION + '/data_' + modname + '_E280.txt',
                   lons_pi, upper_bound_lat_pi, lower_bound_lat_pi,
                   most_intense_precip_lat_pi)

        write_info(FILESTART + 'ITCZ/' + REGION + '/data_' + modname + '_Eoi400.txt',
                   lons_plio, upper_bound_lat_plio, lower_bound_lat_plio,
                   most_intense_precip_lat_plio)


    # plot a map with the itcz on for each month and the multimodel mean

    plot_map_mmm(cubelist_AOI_plio, lons_pi, 'Pliocene')

    plot_map_mmm(cubelist_AOI_pi, lons_pi, 'Preindustrial')

    plot_map(cubelist_AOI_pi, lons_pi, upper_bound_lat_pi_allmods,
             lower_bound_lat_pi_allmods,
             most_intense_lat_pi_allmods,
             'Preindustrial')

    plot_map(cubelist_AOI_plio, lons_plio, upper_bound_lat_plio_allmods,
             lower_bound_lat_plio_allmods,
             most_intense_lat_plio_allmods,
             'Pliocene')




    # plot the maximum intensity latitude for each longitude for pliocene
    # and preindustrial for each month

    plot_lats(lons_pi_allmods, most_intense_lat_pi_allmods,
              most_intense_lat_plio_allmods,
              'Latitude of maximum precipitation',
              lonmin, lonmax, latmin, latmax,
              'Lat_max_precip_')

    # plot upper and lower bounds
    plot_lats(lons_pi_allmods, upper_bound_lat_pi_allmods,
              upper_bound_lat_plio_allmods,
              'Northern boundary of ITCZ', lonmin, lonmax, latmin, latmax,
              'Northern_bound_ITCZ_')

    plot_lats(lons_pi_allmods, lower_bound_lat_pi_allmods,
              lower_bound_lat_plio_allmods,
              'Sourthern boundary of ITCZ', lonmin, lonmax, latmin, latmax,
              'Southern_bound_ITCZ_')











########     END OF MAIN ###########

MONTHNAMES = ['January', 'February', 'March', 'April', 'May', 'June',
              'July', 'August', 'September', 'October', 'November', 'December']
#MONTHNAMES = ['ja','fb','mr','ar','my','jn','jl']
NMONTHS = len(MONTHNAMES)
LINUX_WIN = 'w'
REGION = 'Globe'

THRESHOLD = np.zeros(12, dtype=float)
THRESHOLD[0:4] = 4.
THRESHOLD[4:12] = 6.

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS',
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4',
              'CCSM4-Utr', 'CCSM4-UoT',
              'NorESM-L', 'MRI2.3', 'NorESM1-F']
#MODELNAMES = ['HadCM3', 'CESM2']

if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'

EOI400LSM = FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
E280LSM = FILESTART+'regridded/PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'




main()
::::::::::::::
PlioMIP_new/ITCZ/ITCZ_southern_branch_diagnostics_old.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    ITCZ diagnostics
#PURPOSESS
#    This program is based on ITCZ_diagnostics.py (described below)
#    However it will try and find the southern branch so instead of using 
#    2S-21N we will use 30S-2N
#    have also changed threshold to 6 for all months
#
#    ORIGINAL description
#    This program will find the ITCZ in the North Pacific using the 
#    Stanfield et al 2015 definition.  It will find the centerline width 
#    and intensity and plot these by season for the Pliocene and the PI.
#
# search for 'main program' to find end of functions
# Julia May 2018



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
from mpl_toolkits.basemap import Basemap, shiftgrid, maskoceans


#functions are:
#  def plotdata
#  def annmean
#  def seasmean

# functions start here
def plotdata(plotdata,fileno,lon,lat,titlename,minval,maxval,valinc,V,uselog,cbarname,land_ocn_ind):
    lons, lats = np.meshgrid(lon,lat)
    if fileno !=99:        plt.subplot(2,2,fileno+1)


    if land_ocn_ind == 'l':
        plotnew=maskoceans(lons,lats,plotdata)
        plotdata=plotnew
        if cbarname=='mm/day':
            minval=minval/2.
            maxval=maxval/2.
            valinc=valinc/2.

   # this is good for a tropical region
   # map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')
   # this is good for the globe
    map=Basemap(llcrnrlon=170.0,urcrnrlon=260.0,llcrnrlat=-30.0,urcrnrlat=5.0,projection='cyl',resolution='c')
    x, y = map(lons, lats)
    map.drawcoastlines()

    plotdata2=plotdata
    #plotdata=maskoceans(x,y,plotdata)
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-32,vmax=32),cmap='RdBu',extend='both')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='both')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu',extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                if uselog =='ra':
                    cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r',extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")
                else:
                    print(np.shape(plotdata))
                    cs = map.contourf(x,y,plotdata,V,extend='both')
                    cbar = plt.colorbar(cs,orientation="horizontal")


    if fileno != 99:
        plt.title(titlename)
        cbar.set_label(cbarname,labelpad=-40)
    else:
        cbar.set_label(cbarname,labelpad=-70,size=20)
        cbar.ax.tick_params(labelsize=20)
        plt.title(titlename,loc='left',fontsize=20)
   

    plotdata=plotdata2

    if land_ocn_ind == 'l':
        map.drawmapboundary(fill_color='white')
    else:
        map.drawmapboundary

#end def plotdata

def find_ITCZ(exptname,extra,monthname,threshold):
#  to find the ITCZ we find precipitation in a box  2N-21S 180W-110 W.
#  we then find the longest continuous stretch of precipitation above a certain
#  threshold for each longitude.  The monthly thresholds are 4mm/day from 
#  January to April and 6mm/day from May-December

   # read in data from multiple files and calculate average precipitation
   # in mm/day

    print('/nfs/hera1/earjcti/um/HadGEM_data/'+exptname+'/precip_data/'+exptname+'a@pd'+extra+'[5-9]?'+monthname+'_precip.nc')

    f=MFDataset('/nfs/hera1/earjcti/um/HadGEM_data/'+exptname+'/precip_data/'+exptname+'a@pd'+extra+'[5-9]?'+monthname+'_precip.nc')
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]
    aprecip=f.variables['precip_1'][:]
    aprecip=np.squeeze(aprecip)
    ntimes,ny,nx=np.shape(aprecip)
    print(ntimes,ny,nx)
    f.close()

    avg_precip=np.mean(aprecip,axis=0)
    avg_precip=avg_precip * 60. * 60. * 24. 

    # decompose grid to 180W-110W (180E-250E) 2S-21N

    nlon=0
    nlat=0
    lat_first=0
    lon_first=0

    for i in range (0,len(lon)):
        if 180 <= lon[i] <= 250:
            nlon=nlon+1
            if lon_first==0:
                lon_first=i


    for j in range (0,len(lat)):
        if -30.0 <= lat[j] <= 2:
            nlat=nlat+1
            if lat_first==0:
                lat_first=j


    print(lat_first,lon_first)
    AOI_precip=np.zeros((nlat,nlon))
    AOI_lon=np.zeros(nlon)
    AOI_lat=np.zeros(nlat)


    for i in range(0,len(lon)):
        if 180<= lon[i] < 250:
          AOI_lon[i-lon_first]=lon[i]
          for j in range(0,len(lat)):
              if -30.0 <= lat[j] <= 2:
                  if i==lon_first:
                      AOI_lat[j-lat_first]=lat[j]
                  AOI_precip[j-lat_first,i-lon_first]=avg_precip[j,i]

    

    

    # for each longitude find the longest continuous band of precipitation 
    # above the threshold


    max_count_lons=np.zeros(len(AOI_lon),dtype=int)
    upper_bound_index=np.zeros(len(AOI_lon),dtype=int)
    lower_bound_index=np.zeros(len(AOI_lon),dtype=int)
    most_intense_precip_index=np.zeros(len(AOI_lon),dtype=int)

    upper_bound_index[:]=np.nan
    lower_bound_index[:]=np.nan
    most_intense_precip_index[:]=np.nan


    for i in range(0,len(AOI_lon)):
        count_lons=0
        max_val=0
        for j in range(len(AOI_lat)-1,0,-1):
            if AOI_precip[j,i] >= threshold: # check greater then threshold
                count_lons=count_lons+1
                if count_lons > max_count_lons[i]: # set up maximum
                    max_count_lons[i]=count_lons
                    lower_bound_index[i]=j
                    if AOI_precip[j,i] > max_val: # find most intense precip
                                                  #in north itcz
                        max_val=AOI_precip[j,i]
                        most_intense_precip_index[i]=j
         
            else:
                count_lons=0
          
        if max_count_lons[i]==0:
            lower_bound_index[i]=-9999
            upper_bound_index[i]=-9999
            most_intense_precip_index[i]=-9999
        else:
            upper_bound_index[i]=lower_bound_index[i]+max_count_lons[i]-1
        #print(AOI_lon[i],i,upper_bound_index[i],lower_bound_index[i],max_count_lons[i])
        #print(AOI_lat[upper_bound_index[i]],AOI_lat[lower_bound_index[i]])
        #for j in range(0,len(AOI_lat)):
        #          print(AOI_lat[j],AOI_precip[j,i])
        #sys.exit()

    #area of interest check 

    titlename=exptname+' '+monthname
    plotdata(AOI_precip,99,AOI_lon,AOI_lat,titlename,0.0,10.0,0.5,0,'n','mm/day','b')


    # overplot location of itcz but must reduce arrays to account for
    # missing data
    nredu=sum(float(num) >=0 for num in lower_bound_index)
    print('nredu',nredu)
    AOI_lon_redu=np.zeros(nredu,dtype=int)
    upper_bound_index_redu=np.zeros(nredu,dtype=int)
    lower_bound_index_redu=np.zeros(nredu,dtype=int)
    most_intense_precip_index_redu=np.zeros(nredu,dtype=int)
    
    count=0
    for i in range(0,len(AOI_lon)):
        if upper_bound_index[i]>=0:
            AOI_lon_redu[count]=AOI_lon[i]
            upper_bound_index_redu[count]=upper_bound_index[i]
            lower_bound_index_redu[count]=lower_bound_index[i]
            most_intense_precip_index_redu[count]=most_intense_precip_index[i]
            count=count+1
    
    print('upper',upper_bound_index_redu)
    print('lower',lower_bound_index_redu)
    print('upper full',upper_bound_index)
    print('lower full',lower_bound_index)
    plt.plot(AOI_lon_redu,AOI_lat[upper_bound_index_redu],color='orange',linewidth=3)
    plt.plot(AOI_lon_redu,AOI_lat[lower_bound_index_redu],color='red',linewidth=3)
    plt.plot(AOI_lon_redu,AOI_lat[most_intense_precip_index_redu],color='white',linewidth=3)

    # overplot lower and upper bounds
    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/ITCZ/map_south_'+exptname+'_'+monthname+'.eps'
    plt.savefig(fileout)
    plt.close()


    lon_res=AOI_lon[1]-AOI_lon[0]
    lat_res=AOI_lat[1]-AOI_lat[0]
   
    max_AOI_precip=np.zeros(len(AOI_lon))
    mean_AOI_precip=np.zeros(len(AOI_lon)) # in mm/day (kg/m2/day)
    total_AOI_precip=np.zeros(len(AOI_lon)) # in kg/day (so we need to multiply by area of gridbox)
    for i in range(0,len(AOI_lon)):
        if most_intense_precip_index[i]>=0:
            max_AOI_precip[i]=AOI_precip[most_intense_precip_index[i],i]
            weightamt=0.
            for j in range(lower_bound_index[i],upper_bound_index[i]+1):
                mean_AOI_precip[i]=(mean_AOI_precip[i]+
                      AOI_precip[j,i]*np.cos(np.radians(AOI_lat[j])))
                weightamt=weightamt + np.cos(np.radians(AOI_lat[j]))
                total_AOI_precip[i]=(total_AOI_precip[i]+
                  AOI_precip[j,i]*111000.*lon_res*np.cos(np.radians(AOI_lat[j]))
                              *111000.*lat_res)
            
            mean_AOI_precip[i]=mean_AOI_precip[i] / weightamt

   
    retdata=[AOI_lon,max_AOI_precip,mean_AOI_precip,total_AOI_precip]
    return retdata


#end def find_ITCZ


     


#end def seasmean

################################
# main program

monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']


threshold=np.zeros(12,dtype=float)
threshold[0]=4.
threshold[1]=4.
threshold[2]=4.
threshold[3]=4.
threshold[4]=6.
threshold[5]=6.
threshold[6]=6.
threshold[7]=6.
threshold[8]=6.
threshold[9]=6.
threshold[10]=6.
threshold[11]=6.
threshold[:]=6  # have changed threshold to 6

# get data for all months
for i in range(0,len(monthnames)):
 
    ITCZ_data=find_ITCZ('xkvje','n',monthnames[i],threshold[i])
    if i == 0:
        AOI_lon=ITCZ_data[0]
        # amount of rainfall arrays
        max_intensity_amt_pi=np.zeros((12,len(AOI_lon)))
        max_intensity_amt_plio=np.zeros((12,len(AOI_lon)))
        mean_itcz_precip_amt_pi=np.zeros((12,len(AOI_lon)))
        mean_itcz_precip_amt_plio=np.zeros((12,len(AOI_lon)))
        total_itcz_precip_amt_pi=np.zeros((12,len(AOI_lon)))
        total_itcz_precip_amt_plio=np.zeros((12,len(AOI_lon)))
    max_intensity_amt_pi[i,:]=ITCZ_data[1]
    mean_itcz_precip_amt_pi[i,:]=ITCZ_data[2]
    total_itcz_precip_amt_pi[i,:]=ITCZ_data[3]

   
    ITCZ_data=find_ITCZ('xkvjg','n',monthnames[i],threshold[i])
    max_intensity_amt_plio[i,:]=ITCZ_data[1]
    mean_itcz_precip_amt_plio[i,:]=ITCZ_data[2]
    total_itcz_precip_amt_plio[i,:]=ITCZ_data[3]



# plot some averages over area by month

mean_loc_max_intensity_plio=np.zeros(len(monthnames))
mean_loc_max_intensity_pi=np.zeros(len(monthnames))
mean_loc_upper_bound_plio=np.zeros(len(monthnames))
mean_loc_upper_bound_pi=np.zeros(len(monthnames))
mean_loc_lower_bound_plio=np.zeros(len(monthnames))
mean_loc_lower_bound_pi=np.zeros(len(monthnames))
mean_amt_max_intensity_plio=np.zeros(len(monthnames))
mean_amt_max_intensity_pi=np.zeros(len(monthnames))
mean_amt_itcz_plio=np.zeros(len(monthnames))
mean_amt_itcz_pi=np.zeros(len(monthnames))
total_amt_itcz_plio=np.zeros(len(monthnames))
total_amt_itcz_pi=np.zeros(len(monthnames))
for mon in range(0,len(monthnames)):
    mean_loc_max_intensity_plio[mon]=np.mean(max_intensity_lat_plio[mon,:])
    mean_loc_max_intensity_pi[mon]=np.mean(max_intensity_lat_pi[mon,:])
    mean_loc_upper_bound_plio[mon]=np.mean(upper_bound_plio[mon,:])
    mean_loc_upper_bound_pi[mon]=np.mean(upper_bound_pi[mon,:])
    mean_loc_lower_bound_plio[mon]=np.mean(lower_bound_plio[mon,:])
    mean_loc_lower_bound_pi[mon]=np.mean(lower_bound_pi[mon,:])
    mean_amt_max_intensity_plio[mon]=np.mean(max_intensity_amt_plio[mon,:])
    mean_amt_max_intensity_pi[mon]=np.mean(max_intensity_amt_pi[mon,:])
    mean_amt_itcz_plio[mon]=np.mean(mean_itcz_precip_amt_plio[mon,:])
    mean_amt_itcz_pi[mon]=np.mean(mean_itcz_precip_amt_pi[mon,:])
    total_amt_itcz_plio[mon]=np.sum(total_itcz_precip_amt_plio[mon,:])
    total_amt_itcz_pi[mon]=np.sum(total_itcz_precip_amt_pi[mon,:])

plt.subplot(2,2,1)  
plt.plot(mean_loc_max_intensity_plio,label='plio')
plt.plot(mean_loc_max_intensity_pi,label='pi')
plt.title('lat maximum ITCZ intensity')
plt.xlabel('month number')
plt.ylabel('latitude')
plt.legend()


plt.subplot(2,2,2)  
plt.plot(mean_loc_upper_bound_plio,label='plio')
plt.plot(mean_loc_upper_bound_pi,label='pi')
plt.title('north lat of ITCZ')
plt.xlabel('month number')
plt.ylabel('latitude')
plt.legend()


plt.subplot(2,2,4)  
plt.plot(mean_loc_lower_bound_plio,label='plio')
plt.plot(mean_loc_lower_bound_pi,label='pi')
plt.title('south lat of ITCZ')
plt.xlabel('month number')
plt.ylabel('latitude')
plt.legend()

plt.subplot(2,2,3)  
plt.plot(mean_loc_upper_bound_plio-mean_loc_lower_bound_plio,label='plio')
plt.plot(mean_loc_upper_bound_pi-mean_loc_lower_bound_pi,label='pi')
plt.title('width of ITCZ')
plt.xlabel('month number')
plt.ylabel('degrees')
plt.legend()

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/ITCZ/avg_loc_by_month_south.eps'
plt.savefig(fileout)
plt.close()


# we will now plot intensity by month.
# 1. average amount of rain at the location of maximum intensity.

plt.subplot(2,2,1)  
plt.plot(mean_amt_max_intensity_plio,label='plio')
plt.plot(mean_amt_max_intensity_pi,label='pi')
plt.title('rain amt at max ITCZ intensity')
plt.xlabel('month number')
plt.ylabel('mm/day')
plt.legend()

# 2. mean amount of rain in mm/day

plt.subplot(2,2,2)  
plt.plot(mean_amt_itcz_plio,label='plio')
plt.plot(mean_amt_itcz_pi,label='pi')
plt.title('mean itcz precip')
plt.xlabel('month number')
plt.ylabel('mm/day')
plt.legend()

# 3. total amount of rain in kg

plt.subplot(2,2,3)  
plt.plot(total_amt_itcz_plio/1.0E12,label='plio')
plt.plot(total_amt_itcz_pi/1.0E12,label='pi')
plt.title('total itcz precip')
plt.xlabel('month number')
plt.ylabel('kg (x 10^12)')
plt.legend()

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/ITCZ/rain_amt_by_month_south.eps'
plt.savefig(fileout)
plt.close()


sys.exit()
####

::::::::::::::
PlioMIP_new/ITCZ/plot_tropical_precipitation.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Sat Jun  6 08:43:12 2020

@author: julia
"""

#import sys
import iris
import iris.plot as iplt
import matplotlib.pyplot as plt
import numpy as np


def get_filenames(model):
    """
    gets the preindustrial and the pliocene filename for the given model
    """
    file_pi = (FILESTART + 'regridded/' + model +
               '/E280.TotalPrecipitation.mean_month.nc')
    file_plio = (FILESTART + 'regridded/' + model +
                 '/EOI400.TotalPrecipitation.mean_month.nc')

    return file_pi, file_plio

def get_zm_cube(file_pi, file_plio):
    """
    gets the zonal mean of the cube that is contained in the file
    """

    cube_pi = iris.load_cube(file_pi)
    cube_plio = iris.load_cube(file_plio)

    cube_zm_pi = cube_pi.collapsed(['longitude'], iris.analysis.MEAN)
    cube_zm_plio = cube_plio.collapsed(['longitude'], iris.analysis.MEAN)

    # remove auxillary coordinate year and time
    cube_pi.remove_coord('year')
    cube_plio.remove_coord('year')
    cube_pi.remove_coord('time')
    cube_plio.remove_coord('time')

    # remove attributes from month coordinate
    cube_pi.coord('month').attributes = None
    cube_plio.coord('month').attributes = None

    return cube_zm_pi, cube_zm_plio, cube_pi, cube_plio

def plot_cubes(cubelist_zm_pi, cubelist_zm_plio):
    """
    plots the preindustrial and pliocene zonal mean precipitation that
    is in the cubes
    """

    for mon, monthname in enumerate(MONTHNAMES):
        fig = plt.figure(figsize=[12.0, 8.0])
        for i, cube_pi in enumerate(cubelist_zm_pi):
            axis = plt.subplot(4, 4, i+1)
            cube_plio = cubelist_zm_plio[i]
            axis.plot(cube_pi[mon, :].data, cube_pi.coord('latitude').points,
                      color='blue')
            axis.plot(cube_plio[mon, :].data, cube_pi.coord('latitude').points,
                      color='orange')
            plt.text(5, 20, MODELNAMES[i], fontsize=10)
            plt.ylim(-30, 30)
            plt.xlim(0, 10)
            plt.axhline(y=0, linestyle='dashed', color='black')
            if i < 12: # only show xaxmis on bottom plot
                axis.axes.xaxis.set_visible(False)
            if i % 4 != 0: # only show yaxis on left plot
                axis.axes.yaxis.set_visible(False)
        fig.text(0, 0.95,
                 'Tropical precipitation (plio-orange, pi-blue) for:'
                 + monthname,
                 fontsize=25)
        fig.text(0.5, 0.05, 'mm/day', fontsize=20)
        fig.text(0.05, 0.5, 'latitude', fontsize=20, rotation=90)


        fileout = (FILESTART + 'ITCZ/Tropical_precip/absolute_values_'
                   +str(mon) + monthname + '.pdf')
        plt.savefig(fileout)
        fileout = (FILESTART + 'ITCZ/Tropical_precip/absolute_values_'
                   + str(mon) + monthname + '.png')
        plt.savefig(fileout)
        plt.close()


def plot_anomaly_cubes(cubelist_zm_pi, cubelist_zm_plio):
    """
    plots the preindustrial and pliocene zonal mean precipitation that
    is in the cubes
    """

    for mon, monthname in enumerate(MONTHNAMES):
        fig = plt.figure(figsize=[12.0, 8.0])
        for i, cube_pi in enumerate(cubelist_zm_pi):
            axis = plt.subplot(4, 4, i+1)
            cube_plio = cubelist_zm_plio[i]
            anomaly_data = cube_plio[mon, :].data - cube_pi[mon, :].data
            axis.plot(anomaly_data, cube_pi.coord('latitude').points)
            plt.text(0.5, 20, MODELNAMES[i], fontsize=10)
            plt.ylim(-30, 30)
            plt.xlim(-2, 2)
            plt.axhline(y=0, linestyle='dashed', color='black')
            plt.axvline(x=0, linestyle='dashed', color='black')
            if i < 12: # only show xaxmis on bottom plot
                axis.axes.xaxis.set_visible(False)
            if i % 4 != 0: # only show yaxis on left plot
                axis.axes.yaxis.set_visible(False)
        fig.text(0, 0.95, 'Tropical precipitation anomaly (plio - pi) for '  + monthname,
                 fontsize=25)
        fig.text(0.5, 0.05, 'mm/day', fontsize=20)
        fig.text(0.05, 0.5, 'latitude', fontsize=20, rotation=90)


        fileout = (FILESTART + 'ITCZ/Tropical_precip/anomalies_'
                   +str(mon) + monthname + '.pdf')
        plt.savefig(fileout)
        fileout = (FILESTART + 'ITCZ/Tropical_precip/anomalies_'
                   + str(mon) + monthname + '.png')
        plt.savefig(fileout)
        plt.close()


def plot_map_cubes(cubelist_pi, cubelist_plio):
    """
    plots the preindustrial and pliocene mean precipitation that
    is in the cubes for the globe
    """

    for mon, monthname in enumerate(MONTHNAMES):
        fig = plt.figure(figsize=[12.0, 6.0])
        for i, cube_pi in enumerate(cubelist_pi):
            plt.subplot(4, 4, i+1)
            cube_plio = cubelist_plio[i]
            anomaly_cube = cube_plio - cube_pi
            trop_anom_month = anomaly_cube.extract(iris.Constraint(month=mon+1,
                                                                   latitude = lambda cell: -30 < cell < 30))
            V = np.arange(-5, 6, 1)
            cs = iplt.contourf(trop_anom_month, levels=V, cmap='RdBu', extend='both')
            plt.gca().coastlines()
            plt.title(MODELNAMES[i], fontsize=10)

        # add colorbar and full plot title
        plt.subplots_adjust(left=0.0, bottom=0.2, right=0.9, top=0.9, wspace=0.1, hspace=0.0)
        cb_ax = fig.add_axes([0.10, 0.15, 0.7, 0.05])
        cbar = fig.colorbar(cs, cax=cb_ax, orientation='horizontal')
        cbar.set_label('mm/day', fontsize=15)
        fig.text(0, 0.95, 'Tropical precipitation anomaly (plio - pi) for '  + monthname,
                 fontsize=25)



        fileout = (FILESTART + 'ITCZ/Tropical_precip/global_anomalies_'
                   +str(mon) + monthname + '.pdf')
        plt.savefig(fileout)
        fileout = (FILESTART + 'ITCZ/Tropical_precip/global_anomalies_'
                   + str(mon) + monthname + '.png')
        plt.savefig(fileout)
        plt.close()




def main():
    """
    Will plot the tropical precipitation for the pliocene and the preindustrial
    from each of the models
    """
    cubelist_zm_e280 = iris.cube.CubeList([])
    cubelist_zm_eoi400 = iris.cube.CubeList([])
    cubelist_e280 = iris.cube.CubeList([])
    cubelist_eoi400 = iris.cube.CubeList([])
    for modname in MODELNAMES:
        filein_e280, filein_eoi400 = get_filenames(modname)
        (cube_zm_e280, cube_zm_eoi400,
         cube_e280, cube_eoi400) = get_zm_cube(filein_e280, filein_eoi400)
        cubelist_zm_e280.append(cube_zm_e280)
        cubelist_zm_eoi400.append(cube_zm_eoi400)
        cubelist_e280.append(cube_e280)
        cubelist_eoi400.append(cube_eoi400)

    #plot_cubes(cubelist_zm_e280, cubelist_zm_eoi400)
    #plot_anomaly_cubes(cubelist_zm_e280, cubelist_zm_eoi400)
    plot_map_cubes(cubelist_e280, cubelist_eoi400)




LINUX_WIN = 'w'
MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS',
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4',
              'CCSM4-Utr', 'CCSM4-UoT',
              'NorESM-L', 'MRI2.3', 'NorESM1-F'
             ]
#MODELNAMES = ['CESM2']
MONTHNAMES = ['January', 'February', 'March', 'April', 'May', 'June',
              'July', 'August', 'September', 'October', 'November', 'December']
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'

main()
::::::::::::::
PlioMIP_new/large_scale_features/Arctic_amp_plots.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
#Created on 05/03/2020


#@author: earjcti

This program will plot the polar amplification diagrams
requested by IPCC in particular

Arctic amplification: I suggest reporting the
average mean annual land + sea temperature for six,
30 deg latitude bands,
 or at least the temperatures for 60-90N vs 0-90N.

So we will do:

For land/sea/total the temperature anomaly for the 6 bands
"""
import sys
import warnings
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import re

warnings.filterwarnings("ignore")

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match.
    """

    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube

def get_lsm(filein):
    """
    returns a  numpy array of land points and one of sea points
    """
    tempcube = iris.load_cube(filein)
    cubegrid = iris.load_cube('one_lev_one_deg.nc')
    lsmcube = tempcube.regrid(cubegrid, iris.analysis.Linear())
    landpoints = lsmcube.data
    seapoints = (lsmcube.data - 1.0) * (-1.0)

    return landpoints, seapoints

def get_mean_data(model, expt):
    """
    gets the cube of mean data for a single model

    Parameters
    ----------
    model : the name of the model we are interested in
    expt : whether it is the experiment or the control

    Returns
    -------
    a cube with the mean data from this file
    grid_areas = the size of the grid for averaging
    """

    filename = (FILESTART + 'regridded/' + model
                + '/' + expt + '.' + FIELD + '.allmean.nc')

    cube = iris.load_cube(filename)
    cube2 = resort_coords(cube)

    cube2.coord('latitude').guess_bounds()
    cube2.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube2)

    return cube2, grid_areas


def get_region(band_upper, band_lower, cube, mask, grid_areas):
    """
    Gets the average temeprature over land within the
    bounded range

    Parameters
    ----------
    band_upper : scalar value denoting the upper latitude of
                 the band
    band_lower : scalar value denoting the lower latitude of
                 the band
    cube : the cube containing average temperatures that
           we want to get the land temperature over
    mask : numpy array containing the mask we want.  Could be a land mask
           a sea mask or ones everywhere (ie all points)
    grid_areas : the size of each gridpoint for weighting

    Returns
    -------
    bound_land_avg : scalar containing the average land
                    temperature over the bounded region

    """

    grid_areas_mask = grid_areas * mask
    grid_areas_band = np.zeros(grid_areas.shape)
    lats = cube.coord('latitude').points

    for j, lat in enumerate(lats):
        if band_lower <= lat <= band_upper:
            grid_areas_band[j, :] = grid_areas_mask[j, :]

    bound_mask_avg = cube.collapsed(['longitude', 'latitude'],
                                    iris.analysis.MEAN,
                                    weights=grid_areas_band)

    return bound_mask_avg.data

def get_pliomip1_data():
    """
    Gets the pliomip1 data for each of the bands

    Returns
    -------
    pliomip1 data

    """
    if LINUX_WIN == 'l':
        PLIOMIP1_FILE = (FILESTART + '/PLIOMIP/means_for_' + FIELD + '.txt')
    else:
        PLIOMIP1_FILE = FILESTART + 'PLIOMIP1/means_for_' + FIELD + '.txt'
    f1 = open(PLIOMIP1_FILE)
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    # find line index which has the title 'modelname, latband mean '
    string = 'modelname, latband mean'
    for i, line in enumerate(lines):
        if line[0:23] == string:
            index = i
        
    # get bands by splitting the line
    bands_line = lines[index + 1]
    bands_str_array = bands_line[10:]
    print(bands_str_array)
    res = bands_str_array.split("], [")
    res = [x.strip('[') for x in res]
    res = [x.strip(']') for x in res]
    nbands = len(res)
    bands_array = np.zeros((nbands, 2))
    for i, x in enumerate(res):
        x1, x2 = x.split(',')
        bands_array[i, 0] = x1
        bands_array[i, 1] = x2
     
    # get the data from the next lines find the mean, min and max for each band
    # for the anomaly only
    minval = np.zeros(nbands)
    minval[:] = 100.
    maxval = np.zeros(nbands)
    meanval = np.zeros(nbands)
    
    for i in range(index + 2, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break      
        modname, eoi400, e280, anom = line.split(',')

        eoi400_val = np.array(eoi400.strip('[]').split(), dtype=float)
        e280_val = np.array(e280.strip('[]').split(), dtype=float)
        
        anom_val = np.array(anom.strip('[]').split(), dtype=float)
        
        for i, anom in enumerate(anom_val):
            if anom < minval[i]:
                minval[i] = anom
            if anom > maxval[i]:
                maxval[i] = anom
        if modname == 'MEAN':
            meanval = anom_val
        
    return meanval, minval, maxval
  
   
   
   
def plot_temp_by_lat(anomaly, uppervals, lowervals, plottype, fileoutstart,
                     mean_p1, min_p1, max_p1):
    """
    plot the temperature anomaly vs the region on one plot

    Parameters
    ----------
    anomaly : temperature anomaly to plot np.shape= nmodels, nbounds
    uppervals : the upper limit of the boundary range (np.shape = nbounds)
    lowervals : the lower limit of the boundary range (np.shape = nbounds)
    plottype : 'Land' 'Ocean' or '' if empty it is all surface types

    Returns
    -------
    None.

    """

    titlename = 'MPWP - PI ' + plottype + ' SAT anomaly'
    latns = {-90.0 : '90S', -60.0: '60S', -30.0 : '30S', 0.0 : '0N',
             30.0 : '30N', 60.0 : '60N', 90.0 : '90N'}


    labels = []
    for i, upval in enumerate(uppervals):
        labels.append(latns.get(upval) + '-' + latns.get(lowervals[i]))

    ax = plt.subplot(1, 1, 1)
    for i, model in enumerate(MODELNAMES):
        if i < len(MODELNAMES) / 2.0:
            ax.plot(anomaly[i, :], labels, label=model)
        else:
            ax.plot(anomaly[i, :], labels, label=model, linestyle='dashed')


    ax.plot(np.mean(anomaly, axis=0), labels, color='black',
            linestyle='dashed',
            linewidth=2, label='avg')
    plt.title(titlename)
    plt.xlabel(UNITS)

    if PLIOMIP1 == 'y' and plottype == '':
        ax.plot(mean_p1, labels, label = 'PlioMIP1', color='black',
                linestyle = 'dotted', linewidth=2)
        ax.fill_betweenx(labels, min_p1, max_p1, alpha=0.2, 
                        color="grey")
       
    FILETXT = (FILESTART + '/regridded/alldata/data_for_supp_fig2_' 
               + plottype + '.txt')

    txtout = open(FILETXT, "w+")
    
    writedata = 'modelname'
    for j in range(0, len(labels)):
        writedata = writedata + ',' + labels[j]
    writedata = writedata + '\n'
    txtout.write(writedata)

    for i, mod in enumerate(MODELNAMES):
        writedata = mod 
        for j in range(0, len(labels)):
            writedata = writedata + ',' + (np.str(np.around(anomaly[i,j],2)))
        writedata = writedata + '\n'
        txtout.write(writedata)
    txtout.close()
   

    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    fileout = (fileoutstart + '/polar_amplification'  + plottype + '_anomaly.eps')
    plt.savefig(fileout)
    fileout = (fileoutstart + '/polar_amplification'  + plottype + '_anomaly.pdf')
    plt.savefig(fileout)
    plt.show()
    plt.close()


#####################################
def main():
    """
    Tha main control of the program to plot the
    polar amplification by temperature by latitude band

    """

    if LINUX_WIN == 'w':
        fileoutstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD + '\\')
        exptlsm = (FILESTART + 'regridded/PlioMIP2_Boundary_conds' +
                   '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'regridded/PlioMIP2_Boundary_conds' +
                   '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
    else:
        fileoutstart = '/nfs/hera1/earjcti/regridded/allplots/' + FIELD + '/'
        dataoutstart = '/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm = (FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh' +
                   '/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'PlioMIP2_Boundary_conds/Modern_std/' +
                   'Modern_std/Modern_std_LSM_v1.0.nc')


    ########################################################
    # setup: get the lsm for the land sea contrast plot

    exptland, exptsea = get_lsm(exptlsm)
    cntlland, cntlsea = get_lsm(cntllsm)
    maskall = np.ones(np.shape(exptland))


    #########################################################
    # need to get data from annual mean plot

    bandmax = [-60., -30., 0., 30., 60., 90.]
    bandmin = [-90., -60., -30., 0., 30., 60.]


    land_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))

    land_expt = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_expt = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_expt = np.zeros((len(MODELNAMES), len(bandmax)))

    land_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    
    land_expt_avg = np.zeros((len(MODELNAMES)))
    sea_expt_avg = np.zeros((len(MODELNAMES)))
    expt_avg = np.zeros((len(MODELNAMES)))

    land_cntl_avg = np.zeros((len(MODELNAMES)))
    sea_cntl_avg = np.zeros((len(MODELNAMES)))
    cntl_avg = np.zeros((len(MODELNAMES)))

    

    for modelno, modeluse in enumerate(MODELNAMES):

        # get mean data
        (exptcube, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME)
        (cntlcube, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME)

        # get data within bounds
        for i, band_upper in enumerate(bandmax):
            band_lower = bandmin[i]
            land_expt[modelno, i] = get_region(band_upper, band_lower,
                                               exptcube, exptland,
                                               grid_areas_expt)

            land_cntl[modelno, i] = get_region(band_upper, band_lower,
                                               cntlcube, cntlland,
                                               grid_areas_cntl)

            sea_expt[modelno, i] = get_region(band_upper, band_lower,
                                              exptcube, exptsea,
                                              grid_areas_expt)

            sea_cntl[modelno, i] = get_region(band_upper, band_lower,
                                              cntlcube, cntlsea,
                                              grid_areas_cntl)

            bands_expt[modelno, i] = get_region(band_upper, band_lower,
                                                exptcube, maskall,
                                                grid_areas_expt)

            bands_cntl[modelno, i] = get_region(band_upper, band_lower,
                                                cntlcube, maskall,
                                                grid_areas_cntl)
            
        #get average data for calculating polar amplification
        land_expt_avg[modelno] = get_region(90.0, -90.0, exptcube, exptland,
                                            grid_areas_expt)
        land_cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, cntlland,
                                            grid_areas_cntl)
        sea_expt_avg[modelno] = get_region(90.0, -90.0, exptcube, exptsea,
                                            grid_areas_expt)
        sea_cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, cntlsea,
                                            grid_areas_cntl)
        expt_avg[modelno] = get_region(90.0, -90.0, exptcube, maskall,
                                            grid_areas_expt)
        cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, maskall,
                                            grid_areas_cntl)

    land_anomaly = land_expt - land_cntl
    sea_anomaly = sea_expt - sea_cntl
    bands_anomaly = bands_expt - bands_cntl
    
    ##############################################################
    #  get pliomip1 data if required.  Note we will only get annual mean
    
    if PLIOMIP1 == 'y':
        pliomip1_mean, pliomip1_min, pliomip1_max = get_pliomip1_data()
    else:
        pliomip1_mean = 0
        pliomip1_min = 0
        pliomip1_max = 0

    #print polar amplification
    print('SH polar amplification')
    print('model, land amplification, sea amp, all amp')
    #for i, model in enumerate(MODELNAMES):
    #    print('bands',bandmax[0],bandmin[0])
    #    print(model, land_anomaly[i, 0] / (land_expt_avg[i] - land_cntl_avg[i]), 
    #          sea_anomaly[i, 0] / (sea_expt_avg[i] - sea_cntl_avg[i]),
    #          bands_anomaly[i,0] / (expt_avg[i] - cntl_avg[i]))
    all_sh_land_amp = [x1 / x2 for (x1, x2) in zip(land_anomaly[:,0], 
                                                   land_expt_avg - land_cntl_avg)]
    all_sh_sea_amp = [x1 / x2 for (x1, x2) in zip(sea_anomaly[:,0], 
                                                  sea_expt_avg - sea_cntl_avg)]
    all_sh_amp = [x1 / x2 for (x1, x2) in zip(bands_anomaly[:,0], 
                                              expt_avg - cntl_avg)]
    print('mean', 
          np.mean(land_anomaly[:, 0]) / np.mean(land_expt_avg[:] - land_cntl_avg[:]), 
          np.mean(sea_anomaly[:, 0]) / np.mean(sea_expt_avg[:] - sea_cntl_avg[:]),
          np.mean(bands_anomaly[:,0] / np.mean(expt_avg[:] - cntl_avg[:])))
    print('median', np.median(all_sh_land_amp), np.median(all_sh_sea_amp), 
          np.median(all_sh_amp))
    
    print('checking land',  np.mean(land_anomaly[:, 0]), np.mean(land_expt[:,0]),
          np.mean(land_cntl[:,0]))
          #,np.mean(land_expt_avg[:])-
          #np.mean(land_cntl_avg[:]))
    print('checking sea',  np.mean(sea_anomaly[:, 0]), np.mean(sea_expt[:,0]),
          np.mean(sea_cntl[:,0]))#,np.mean(sea_expt_avg[:])-
        #  np.mean(sea_cntl_avg[:]))
    print('checking avg',  np.mean(bands_anomaly[:, 0]),np.mean(bands_expt[:,0]),
          np.mean(bands_cntl[:,0]))#,np.mean(expt_avg[:])-
        #  np.mean(cntl_avg[:]))
      
    print(' ')    
    print('NH polar amplification')
    all_nh_land_amp = [x1 / x2 for (x1, x2) in zip(land_anomaly[:,5], 
                                                   land_expt_avg - land_cntl_avg)]
    all_nh_sea_amp = [x1 / x2 for (x1, x2) in zip(sea_anomaly[:,5], 
                                                  sea_expt_avg - sea_cntl_avg)]
    all_nh_amp = [x1 / x2 for (x1, x2) in zip(bands_anomaly[:,5], 
                                              expt_avg - cntl_avg)]
    print('model, land amp, sea amp, all amp')
    #for i, model in enumerate(MODELNAMES):
    #    print('bands',bandmax[5],bandmin[5])
    #    print(model, land_anomaly[i, 5] / (land_expt_avg[i] - land_cntl_avg[i]), 
    #          sea_anomaly[i, 5] / (sea_expt_avg[i] - sea_cntl_avg[i]),
    #          bands_anomaly[i,5] / (expt_avg[i] - cntl_avg[i]))
    print('mean', 
          np.mean(land_anomaly[:, 5]) / np.mean(land_expt_avg[:] - land_cntl_avg[:]), 
          np.mean(sea_anomaly[:, 5]) / np.mean(sea_expt_avg[:] - sea_cntl_avg[:]),
          np.mean(bands_anomaly[:,5] / np.mean(expt_avg[:] - cntl_avg[:])))
    print('median', np.median(all_nh_land_amp), np.median(all_nh_sea_amp), 
          np.median(all_nh_amp))

    # plot everything on one plot.

    plot_temp_by_lat(land_anomaly, bandmax, bandmin, 'Land', 
                     fileoutstart, 0, 0, 0)
    plot_temp_by_lat(sea_anomaly, bandmax, bandmin, 'Ocean', 
                     fileoutstart, 0, 0, 0)
    plot_temp_by_lat(bands_anomaly, bandmax, bandmin, '', 
                     fileoutstart, pliomip1_mean, pliomip1_min, pliomip1_max)



##########################################################
# DEFINITIONS

LINUX_WIN = 'l'
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'


MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]

PLIOMIP1 = 'y' # overplot PlioMIP1 models.

#MODELNAMES=['HadCM3','NorESM-L']
FIELD = 'NearSurfaceTemperature'
UNITS = 'degC'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
EXPTNAME = 'EOI400'
CNTLNAME = 'E280'

if FIELD == 'NearSurfaceTemperature':
    FILEOUT = FILESTART + 'regridded/alldata/data_for_arctic_amplification.txt'


main()
::::::::::::::
PlioMIP_new/large_scale_features/average_HadISST_NOAA.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Jul 25 16:18:28 2019

@author: earjcti
This program will calculate a 30 year (1870-1900) average
of HadISST or HadSST4 data and write it to a file.
This can then be compared with modelled SST data

"""

import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import sys


def get_var():
    """
    gets variable names depending on whether we are using
    HadISST or HadSST4.0

    returns, FILENAME, OUTSTART, FILECUBE
    """

    if DATASET == 'HadISST':
        filename = '/nfs/hera1/earjcti/PLIOMIP2/HadISST/HadISST_sst.nc'
        varname = 'sea_surface_temperature'
        fieldname = 'SST'

    if DATASET == 'HadSST4':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/HadSST4/' +
                    'HadSST.4.0.0.0_median.nc')
        varname = 'Sea water temperature anomaly at a depth of 20cm'
        fieldname = 'SST'


    if DATASET == 'NOAAERSST5':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/NOAAERSST5/' +
                    'sst.mnmean.nc')
        varname = 'Monthly Means of Sea Surface Temperature'
        fieldname = 'SST'


    if DATASET == 'CRUTEMP':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/CRU_DATA/' +
                    'cru_ts4.04.1901.2019.tmp.dat.nc')
        varname = 'near-surface temperature'
        fieldname = 'NearSurfaceTemperature'


    if DATASET == 'CRUPRECIP':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/CRU_DATA/' +
                    'cru_ts4.04.1901.2019.pre.dat.nc')
        varname = 'precipitation'
        fieldname = 'TotalPrecipitation'


    outstart = ('/nfs/hera1/earjcti/regridded/'
                + DATASET + '/E280.' + fieldname + '.')
    filecube = iris.load_cube(filename, varname)


    return [filename, outstart, filecube]

def extract_nyrs(cube, startyear, endyear):
    """
    Extract data between startyear and endyear
    from a cube of monthly data

    Parameters:
    cube (iris cube): The cube containing a number of years
    startyear,endyear  (integer): years we wish to extract

    Returns:
    newcube (iris cube): The cube containing the subset of data
    """
    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        iris.coord_categorisation.add_year(t_slice, 'time', name='year')
        year = t_slice.coord('year').points

        if startyear <= year <= endyear:
            print('processing', year)
            t_slice.remove_coord('year')
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            # we have missing data at lsm and also some points are -1000.
            # change points that are -1000. to missing data
            newdata = np.ma.masked_where(t_slice2.data < -900., t_slice2.data)
            t_slice2.data = newdata
            cubelist.append(t_slice2)

    newcube = cubelist.concatenate_cube()

    return newcube


def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]

def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
        or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):
            mon_slice.coord('latitude').guess_bounds()
            mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):

        allmeancube.coord('latitude').guess_bounds()
        allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):

        yearmeancube.coord('latitude').guess_bounds()
        yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]


def textout(meanmon, stdevmon):
    """
    write out all the means to a text file
    this includes global means monthly means and latitudinal means
    """
    textfile = OUTSTART + 'data.txt'
    file1 = open(textfile, "w")
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2)) + ',' +
                np.str(np.round(stdevann, 3)) + '\n')

    # write monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    for i in range(0, 12):
        file1.write(np.str(i+1) + ','+np.str(np.round(meanmon[i], 2)) + ','
                    + np.str(np.round(stdevmon[i], 3))+'\n')

    # write latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')

    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i]) +
                    ',' + np.str(np.round(meanlat[i], 2)) +
                    ',' + np.str(np.round(stdevlat[i], 3)) + '\n')

    file1.close()

def plot_to_check(cube):
    """
    A test program to check that we have averaged properly.
    """


    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube=subcube_mean_mon.copy(data=)  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas = iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],
                             iris.analysis.MEAN, weights=grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt / 12)
    print(nyears)

    for i in range(0, nyears):
        tstart = i * 12
        tend = (i+1) * 12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color='r')

    # global mean from average

    grid_areas = iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean = mean_mon_data.collapsed(['latitude', 'longitude'],
                                            iris.analysis.MEAN, weights=grid_areas)

    plt.plot(temporal_mean.data, color='b', label='avg')
    plt.title('globavg HadISST')
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0] >= 32. >= bounds[i, 1] or bounds[i, 0] <= 32. < bounds[i, 1]):
            index = i

    subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color='r')

    #mean at 30N
    slice_30N = mean_mon_data.extract(iris.Constraint(latitude=32))
    mean_30N = slice_30N.collapsed(['longitude'], iris.analysis.MEAN)


    plt.plot(mean_30N.data, color='b', label='avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()

def writeout():
    """
    write the monthly mean and global mean cubes out to a netcdf file
    """

    outfile = OUTSTART + 'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)



##########################################################
# MAIN PROGRAM

"""
This program will average over the first 30 years of the HadISST
dataset and write results out to a file
"""

DATASET = 'CRUPRECIP' # HadISST HadSST4 NOAAERSST5 CRUTEMP CRUPRECIP

# read in HadISST data
FILENAME, OUTSTART, FILECUBE = get_var()

# extract the years from filecube likely 30 years
#cube30 = extract_nyrs(FILECUBE, 1870, 1899) # where available
cube30 = extract_nyrs(FILECUBE, 1901, 1930)

# create averages
 # add year and month time axis
iris.coord_categorisation.add_month_number(cube30, 'time', name='month')
iris.coord_categorisation.add_year(cube30, 'time', name='year')
mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(cube30)

# create info about each month and get average monthly data
# stdevmon is a numpy array of standard deviation, monthly_mean is a numpy array
# of means
monthly_mean = mon_avg(mean_mon_data)
monthly_standard_deviation = get_monthly_sd(cube30)


# get global and latitinal means for writing to text file
# plot for

meanann, stdevann, meanlat, stdevlat = area_means(mean_data, mean_year_data)

# write means to a text file
textout(monthly_mean, monthly_standard_deviation)

# write iris cubes out to a file
writeout()

# plot to check we have averaged properly
plot_to_check(cube30)
::::::::::::::
PlioMIP_new/large_scale_features/calculate_land_sea_contrast_HadGEM3.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import netCDF4
import sys
#import os


###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """
    f = Dataset(LSM, "r")
    lsm_data = f.variables['lsm'][:]
    lats = f.variables['latitude'][:] 
    lons = f.variables['longitude'][:]
    f.close()
  
    lsm_data = np.squeeze(lsm_data)
    land_mask = lsm_data
    sea_mask = (lsm_data -1.0) * 1.0

    return land_mask, sea_mask, lats, lons

    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """

    f = Dataset(FILENAME_PLIO, "r")
    plio_data_all = f.variables[FIELDNAME][:]
    lats = f.variables['latitude'][:] 
    lons = f.variables['longitude'][:]
    f.close()

    plio_data = (np.mean(plio_data_all,axis=0)-273.15)

    f = Dataset(FILENAME_PI, "r")
    pi_data_all = f.variables[FIELDNAME][:]
    lats2 = f.variables['latitude'][:] 
    lons2 = f.variables['longitude'][:]
    f.close()

    pi_data = (np.mean(pi_data_all,axis=0)-273.15)


    for i, lat in enumerate(lats):
        if lat != lats2[i]:
            print('lats dont match', lat, i, lats2[i])
            sys.exit(0)

    for i, lon in enumerate(lons):
        if lon != lons2[i]:
           print('lons dont match', lon, i, lons2[i])
           sys.exit(0)

  
    return plio_data, pi_data, lats, lons


def get_global_avg(land_mask, sea_mask, dataarr, lats, lons):
    """
    gets global average temperature, and also global avg for
    the land and the ocean
    """
  
    grid_areas_lat = np.zeros(len(lats))
    for j, lat in enumerate(lats):
        grid_areas_lat[j] = np.cos(2. * np.pi * lat / 360.)

    print(np.shape(land_mask))
    global_mean = 0.
    global_mean_weights = 0.
    global_mean_land = 0.
    global_mean_land_weights = 0.
    global_mean_sea = 0.
    global_mean_sea_weights = 0.

    for j in range(0, len(lats)):
        for i in range(0, len(lons)):
            global_mean = global_mean + (dataarr[j, i] * grid_areas_lat[j])
            global_mean_weights = global_mean_weights + grid_areas_lat[j]
            if land_mask[j, i] == 1.0:
                global_mean_land = (global_mean_land + 
                                   (dataarr[j, i] * grid_areas_lat[j]))
                global_mean_land_weights = (global_mean_land_weights + 
                                            grid_areas_lat[j])
            else:
                global_mean_sea = (global_mean_sea + 
                                   (dataarr[j, i] * grid_areas_lat[j]))
                global_mean_sea_weights = (global_mean_sea_weights + 
                                            grid_areas_lat[j])

    global_mean = global_mean / global_mean_weights
    global_mean_land = global_mean_land / global_mean_land_weights
    global_mean_sea = global_mean_sea / global_mean_sea_weights

   
    return global_mean, global_mean_land, global_mean_sea, grid_areas_lat


def get_regional_landsea(rmax, rmin, land_mask, dataarr, lats, lons,
                         grid_areas):

    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    grid_areas_use = grid_areas * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j] = 0.0
    #grid_areas_land = grid_areas_use * land_cube.data
    #grid_areas_sea = grid_areas_use * sea_cube.data
    
    reg_mean = 0.
    reg_mean_weights = 0.
    reg_mean_land = 0.
    reg_mean_land_weights = 0.
    reg_mean_sea = 0.
    reg_mean_sea_weights = 0.

    for j in range(0, len(lats)):
        if grid_areas_use[j] != 0.0:
            for i in range(0, len(lons)):
                reg_mean = reg_mean + (dataarr[j, i] * grid_areas_use[j])
                reg_mean_weights = reg_mean_weights + grid_areas_use[j]
                if land_mask[j, i] == 1.0:
                    reg_mean_land = (reg_mean_land + 
                                     (dataarr[j, i] * grid_areas_use[j]))
                    reg_mean_land_weights = (reg_mean_land_weights + 
                                             grid_areas_use[j])
                else:
                    reg_mean_sea = (reg_mean_sea + 
                                    (dataarr[j, i] * grid_areas_use[j]))
                    reg_mean_sea_weights = (reg_mean_sea_weights + 
                                            grid_areas_use[j])
    print(rmax, rmin)
    print(rmax, rmin, reg_mean / reg_mean_weights)
    print(reg_mean, reg_mean_land, reg_mean_sea)
    print(reg_mean_weights, reg_mean_land_weights, reg_mean_sea_weights)
  
    reg_mean = reg_mean / reg_mean_weights
    reg_mean_land = reg_mean_land / reg_mean_land_weights
    reg_mean_sea = reg_mean_sea / reg_mean_sea_weights

    return [reg_mean, reg_mean_land, reg_mean_sea]
           
                                                       
def write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin):
    """
    write the information to a pandas dataframe
    """
    print(avg_T_anom, MODELNAME)

    data = [['Global', avg_T_anom], ['Global (over land)', avg_T_landanom],
            ['Global (over sea)', avg_T_seaanom]]

    for i, rmax in enumerate(regionmax):
        if rmax > 0:
            latrange = np.str(np.around(rmax)) + 'N'
        else:
            latrange = np.str(np.around(np.abs(rmax))) + 'S'
        if regionmin[i] > 0.:
            latrange = latrange + np.str(np.around(regionmin[i])) + 'N'
        else:
            latrange = latrange + np.str(np.around(np.abs(regionmin[i]))) + 'S'
        
        data.append(['glob_' + latrange, land_sea_anom[0,i]])
        data.append(['land_' + latrange, land_sea_anom[1,i]])
        data.append(['sea_' + latrange, land_sea_anom[2,i]])
        

    df = pd.DataFrame(data, columns = ['Simulated temperature', MODELNAME])
 

    # save dataframe as a excel file
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/IPCC/' + MODELNAME + 
          'mPWP_CMIP6_land_sea.csv')
    #df.to_excel(filename)
    df.to_csv(filename)


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM)

    # get land and sea mask
    land_mask, sea_mask, lsm_lats, lsm_lons = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    plio_data, pi_data, lats, lons = get_nsat_data()

    # check grid
    for i, lat in enumerate(lsm_lats):
        if lat != lats[i]:
           print('lsm lat doesnt match', i, lat, lats[i])
           sys.exit(0)
    for i, lon in enumerate(lsm_lons):
        if lon !=lons[i]:
           print('lsm lon doesnt match', i, lon, lons[i])
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, avg_land_T_plio, 
     avg_sea_T_plio, grid_areas) = get_global_avg(land_mask, sea_mask, 
                                                 plio_data, lats, lons)
                                    
    (avg_T_pi, avg_land_T_pi,
     avg_sea_T_pi, grid_areas) = get_global_avg(land_mask, sea_mask,
                                                pi_data, lats, lons)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask, pi_data,
                                                        lats, lons, 
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask, plio_data,
                                                        lats, lons,
                                                        grid_areas)

       
    
 # write to a spreadsheet
    land_sea_anom = land_sea_region_plio - land_sea_region_pi

    write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin)
    


    

   


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "HadGEM3"
FIELDNAMEIN = ['tas']


START = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
FILENAME_PI = START + 'climatologies/E280/atmos/clims_hadgem3_pi_airtemp_final.nc'
FILENAME_PLIO = START + 'climatologies/Eoi400/atmos/clims_hadgem3_pliocene_airtemp_final.nc'
FIELDNAME = 'temp'
LSM = START + 'hadgem3.mask.nc'
FIELDLSM = 'land_binary_mask'#

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

get_land_sea_contrast()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/calculate_land_sea_contrast.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import netCDF4
import sys
#import os


###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d

    ############################################
    if MODELNAME == 'IPSLCM5A' or MODELNAME == 'IPSLCM5A2':
        plio_lsm_cube = get_ipsl_lsm(LSM_PLIO, FIELDLSM)
        pi_lsm_cube = get_ipsl_lsm(LSM_PI, FIELDLSM)
    elif MODELNAME == 'HadGEM3':
        test = iris.fileformats.netcdf.load_cubes(LSM_PLIO, callback=None)
        print(test)
        for data in test:
            print(data)
        sys.exit(0)
        f = netCDF4.Dataset(LSM_PLIO, "r")
        print(f)
        sys.exit(0)
    else:
        plio_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PI, FIELDLSM))
        pi_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PLIO, FIELDLSM))
   
    plio_lsm_cube2 = change_to_2d(plio_lsm_cube)
    pi_lsm_cube2 = change_to_2d(pi_lsm_cube)
   
    plio_lsm_data = plio_lsm_cube2.data
    pi_lsm_data = pi_lsm_cube2.data

    if MODELNAME == 'IPSLCM6A':
        plio_lsm_data = plio_lsm_data / 100.0
        pi_lsm_data = pi_lsm_data / 100.0

    if MODELNAME == 'EC-Earth3.3':
        plio_lsm_data = np.where(plio_lsm_data > 0.5, 1.0, 0.0)
        pi_lsm_data = np.where(pi_lsm_data > 0.5, 1.0, 0.0)
  
  
    land_mask = np.zeros(np.shape(plio_lsm_data))
    sea_mask = np.zeros(np.shape(plio_lsm_data))

    for ix, plio_mask in np.ndenumerate(plio_lsm_data):
        if plio_mask == 1.0 and pi_lsm_data[ix] == 1.0:
            land_mask[ix] = 1.0
        #if pi_lsm_data[ix] > 0:
        #    land_mask[ix] = 1.0
        if plio_mask == 0.0 and pi_lsm_data[ix] == 0.0:
            sea_mask[ix] = 1.0

    land_cube = plio_lsm_cube2.copy(data=land_mask)
    land_cube.var_name = 'land_mask'
    land_cube.long_name = 'land_mask'
    print('getting sea mask')
    sea_cube = pi_lsm_cube2.copy(data=sea_mask)
    sea_cube.var_name = 'sea_mask'
    sea_cube.long_name = 'sea_mask'
    print('got sea mask')

    return land_cube, sea_cube

def get_hadcm3_data(filestart):
    """
    gets the nsat data from HadCM3 and MRI
    called by get_nsat_data
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if MODELNAME  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filestart + yearuse + '.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if MODELNAME  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if MODELNAME  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')
  
    if MODELNAME  == 'HadCM3':
        cube_temp.coord('ht').rename('surface')

    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))

    return cube

def get_ipslcm6a_data(file):
    """
    for ipslcm6a we have 200 years in the file.  but we only need 100 years
    """
    cube = iris.load_cube(file, FIELDNAME)
    # reduce number of years
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube100yr = cubelist.concatenate_cube()
  
    return cube100yr

def get_ipsl5_data(filename, exptname):
    """
    gets nsat data from ipsl
    there is a bit of an error in the file calendar so we will
    """
# copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        elif ncattr !='_FillValue':
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = 'Temperature 2m'

        cube = iris.load_cube('temporary.nc', fieldreq)

        cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm5a2_data(filename):
    """
    gets the data for ipslcm5a2
    and removes all auxillary coordinates
    """
    cubelist = iris.cubeList
    cube = iris.load_cube(filename, FIELDNAME)
    for coord in cube.aux_coords:
        coord.rename('toremove')
        cube.remove_coord('toremove')
    return cube

def get_giss_data(filenames):
    """
    gets giss data: this is in two files
    """ 
    allcubes = iris.cube.CubeList([])
    for file in filenames:
        cubetemp = iris.load_cube(file, FIELDNAME)
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
  
    return(cube)
    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """
        

    if MODELNAME == 'HadCM3' or MODELNAME == 'MRI-CGCM2.3':
        cube_plio = get_hadcm3_data(FILENAME_PLIO)
        cube_pi = get_hadcm3_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM6A':
        cube_plio = get_ipslcm6a_data(FILENAME_PLIO)
        cube_pi = get_ipslcm6a_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM5A':
        cube_pi = iris.load(FILENAME_PI)[0]
        cube_plio = get_ipsl5_data(FILENAME_PLIO,'Eoi400')
    elif MODELNAME == 'IPSLCM5A2':
        cube_plio = get_ipslcm5a2_data(FILENAME_PLIO)
        cube_pi = get_ipslcm5a2_data(FILENAME_PI)
    elif MODELNAME == 'GISS2.1G':
        cube_plio = get_giss_data(FILENAME_PLIO)
        cube_pi = get_giss_data(FILENAME_PI)
    else:
        cube_plio = iris.load_cube(FILENAME_PLIO, FIELDNAME)
        cube_pi = iris.load_cube(FILENAME_PI, FIELDNAME)
    
   
    cube_plio_avg = cube_plio.collapsed('time', iris.analysis.MEAN)
    cube_pi_avg = cube_pi.collapsed('time', iris.analysis.MEAN)
  
    return cube_plio_avg, cube_pi_avg


def get_global_avg(land_cube, sea_cube, cube_nsat):
    """
    get's global average temperature, and also global avg for
    the land and the ocean
    """


    if cube_nsat.coord('latitude').has_bounds():
        cube_nsat.coord('latitude').bounds
    else:
        cube_nsat.coord('latitude').guess_bounds()

    if cube_nsat.coord('longitude').has_bounds():
        cube_nsat.coord('longitude').bounds
    else:
        cube_nsat.coord('longitude').guess_bounds()

  
    grid_areas = iris.analysis.cartography.area_weights(cube_nsat)
    grid_areas_land = grid_areas * land_cube.data
    grid_areas_sea = grid_areas * sea_cube.data

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)

    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15
   
    return avg_temp_data, avg_temp_land, avg_temp_sea, grid_areas


def get_regional_landsea(rmax, rmin, land_cube, sea_cube, cube_nsat,
                         grid_areas_region):
    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    lats = cube_nsat.coord('latitude').points
    grid_areas_use = grid_areas_region * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j, :] = 0.0
    grid_areas_land = grid_areas_use * land_cube.data
    grid_areas_sea = grid_areas_use * sea_cube.data
    

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_use)
   
    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)
   
    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15

    return [avg_temp_data, avg_temp_land, avg_temp_sea]
           
                                                       
def write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin):
    """
    write the information to a pandas dataframe
    """
    print(avg_T_anom, MODELNAME)

    data = [['Global', avg_T_anom], ['Global (over land)', avg_T_landanom],
            ['Global (over sea)', avg_T_seaanom]]

    for i, rmax in enumerate(regionmax):
        if rmax > 0:
            latrange = np.str(np.around(rmax)) + 'N'
        else:
            latrange = np.str(np.around(np.abs(rmax))) + 'S'
        if regionmin[i] > 0.:
            latrange = latrange + np.str(np.around(regionmin[i])) + 'N'
        else:
            latrange = latrange + np.str(np.around(np.abs(regionmin[i]))) + 'S'
        
        data.append(['glob_' + latrange, land_sea_anom[0,i]])
        data.append(['land_' + latrange, land_sea_anom[1,i]])
        data.append(['sea_' + latrange, land_sea_anom[2,i]])
        

    df = pd.DataFrame(data, columns = ['Simulated temperature', MODELNAME])
 

    # save dataframe as a excel file
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/IPCC/' + MODELNAME + 
          'mPWP_CMIP6_land_sea.csv')
    #df.to_excel(filename)
    df.to_csv(filename)


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM_PLIO)

    # get land and sea mask
    land_mask_cube, sea_mask_cube = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    cube_nsat_plio, cube_nsat_pi = get_nsat_data()
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, 
     avg_land_T_plio, 
     avg_sea_T_plio,
     gridareas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                      cube_nsat_plio)

 

    (avg_T_pi, 
     avg_land_T_pi,
     avg_sea_T_pi,
     grid_areas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                     cube_nsat_pi)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))

    #plt.subplot(2,1,1)
    #V = [0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2]
    #qplt.contourf(cube_nsat_plio - cube_nsat_pi, levels=V)
    #plt.subplot(2,1,2)
    #qplt.contourf((sea_mask_cube + land_mask_cube),  levels=V)
    #plt.show()
    #sys.exit(0)
   


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_pi,
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_plio,
                                                        grid_areas)

       
    
 # write to a spreadsheet
    land_sea_anom = land_sea_region_plio - land_sea_region_pi

    write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin)
    


    

   

#############################################################################
def getnames():

# this program will get the names of the files and the field for each
# of the model

  
    # get names for each model

    if MODELNAME == 'CESM2':
        file_e280 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                         'f09_g17.CMIP6-piControl.' + 
                         '001.cam.h0.TREFHT.110001-120012.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                           'f09_g17.PMIP4-midPliocene-eoi400.' + 
                           '001.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                      'f09_g17.PMIP4-midPliocene-eoi400.001.' + 
                      'cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'COSMOS':
        file_e280 = (FILESTART + 'AWI/COSMOS/E280/E280.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        file_eoi400 = (FILESTART + 'AWI/COSMOS/Eoi400/Eoi400.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        fielduse =  "2m temperature"
        lsm_e280 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                    "E280_et_al/E280.slf.atm.nc")
        lsm_eoi400 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                      "Eoi400_et_al/Eoi400.slf.atm.nc")
        fieldlsm = "SLF"

    if MODELNAME == 'EC-Earth3.3':
        file_e280 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_surface.nc'
        file_eoi400 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_surface.nc'
        fielduse = 'Air temperature at 2m'
        lsm_e280 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc'
        lsm_eoi400 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc'
        fieldlsm = 'Land/sea mask'

    if MODELNAME == 'CESM1.2':
        file_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0701.0800.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if MODELNAME   ==  'MIROC4m':
        file_e280 = (FILESTART + 'MIROC4m/tas/MIROC4m_E280_Amon_tas.nc')
        file_eoi400 = (FILESTART + 'MIROC4m/tas/MIROC4m_Eoi400_Amon_tas.nc')
        fielduse = "tas"
        lsm_e280 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc')
        lsm_eoi400 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc')
        fieldlsm = "sftlf"

    if MODELNAME  == 'HadCM3':
        file_e280 = (FILESTART+'LEEDS/HadCM3/e280/NearSurfaceTemperature/' + 
                     'e280.NearSurfaceTemperature.')
        file_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/NearSurfaceTemperature/' + 
                     'eoi400.NearSurfaceTemperature.')
        fielduse = "TEMPERATURE AT 1.5M"
        lsm_e280 = (FILESTART+'LEEDS/HadCM3/e280/qrparm.mask.nc')
        lsm_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc')
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if MODELNAME == 'CCSM4':
        file_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0081.0180.nc')
        file_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.TREFHT.1001.1100.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'CCSM4_Utr':
        file_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                     'tas_Amon_CESM1.0.5_E280_r1i1p1f1_gn_275001-285012.nc')  
        file_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                       'tas_Amon_CESM1.0.5_Eoi400_r1i1p1f1_gn_190001-200012.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                    'land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC' + 
                    '_control_r1i1p1f1_gn.nc')
        lsm_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                      'land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_' + 
                      'f19g16_NESSC_control_r1i1p1f1_gn.nc')
        fieldlsm = 'LANDMASK[D=1]'
  
    if MODELNAME == 'CCSM4_UoT':
        start = FILESTART + 'UofT/UofT-CCSM4/'
        file_e280 = (start + '/E280/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_piControl_r1i1p1f1_gn_150101-160012.nc')  
        file_eoi400 = (start + '/Eoi400/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_midPliocene-eoi400_r1i1p1f1_gn_' + 
                       '160101-170012.nc') 
        fielduse = 'air_temperature'
        lsm_e280 = start + 'for_julia/E_mask.nc'
        lsm_eoi400 = start + 'for_julia/Eoi_mask.nc'
        fieldlsm = 'gridbox land fraction'
      
    if MODELNAME == 'NorESM-L':
       file_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_TREFHT.nc')
       file_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_TREFHT.nc')
       fielduse = 'Reference height temperature'
       lsm_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc')
       lsm_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc')
       fieldlsm = 'Fraction of sfc area covered by land'


    if MODELNAME  == 'MRI-CGCM2.3':
        file_e280 = (FILESTART + 'MRI-CGCM2.3/tas/e280.tas.')
        file_eoi400 = (FILESTART + 'MRI-CGCM2.3/tas/eoi400.tas.')
        fielduse = 'near surface air temperature [degC]'
        lsm_e280 = (FILESTART + 'MRI-CGCM2.3/sftlf.nc')
        lsm_eoi400 = lsm_e280
        fieldlsm = 'landsea mask [0 - 1]'


    if MODELNAME  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        mid = 'e280/tas_Amon_GISS-E2-1-G_piControl_r1i1p1f1_gn_'
        file_e280 = ([start + mid + '490101-495012.nc', 
                      start + mid + '495101-500012.nc'])
        mid = 'eoi400/tas_Amon_GISS-E2-1-G_midPliocene-eoi400_r1i1p1f1_gn_'
        file_eoi400 = ([start + mid + '305101-310012.nc',
                        start + mid + '310101-315012.nc'])
        fielduse = 'air_temperature'
        lsm_e280 = start + 'e280/NASA-GISS_PIctrl_all_fland.nc'
        lsm_eoi400 = start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc'
        fieldlsm = 'fland'

    if MODELNAME == 'NorESM1-F':
        file_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_TREFHT.nc'
        file_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_TREFHT.nc'
        lsm_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc'
        lsm_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc'
        fielduse = 'Reference height temperature'
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if MODELNAME == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        file_e280 = start + 'tas_Amon_IPSL-CM6A-LR_piControl_r1i1p1f1_gr_285001-304912.nc'
        file_eoi400 = (start + 'tas_Amon_IPSL-CM6A-LR_midPliocene-eoi400_' + 
                       'r1i1p1f1_gr_185001-204912.nc')
        lsm_e280 = start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc'
        lsm_eoi400 = start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc'
        fielduse = 'air_temperature'
        fieldlsm = 'land_area_fraction'

    if MODELNAME == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A/PI.NearSurfaceTemp_tas_3600_3699_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A/Eoi400.NearSurfaceTemp_tas_3581_3680_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Tas'
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if MODELNAME == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A2/PI.NearSurfaceTemp_tas_6110_6209_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A2/Eoi400.NearSurfaceTemp_tas_3381_3480_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Temperature 2m'
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if MODELNAME == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        file_e280 = start + 'climatologies/E280/clims_hadgem3_pi_airtemp_final.nc'
        file_eoi400 = start + 'climatologies/Eoi400/clims_hadgem3_pliocene_airtemp_final.nc'
        fielduse = 'temp'
        lsm_e280 = start + 'hadgem3.mask.nc'
        lsm_eoi400 = lsm_e280
        fieldlsm = 'land_binary_mask'
            
            
      
    retdata = [fielduse, file_e280, file_eoi400,
               fieldlsm, lsm_e280, lsm_eoi400]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "HadGEM3" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                  
FIELDNAMEIN = ['tas']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


# call program to get model dependent names
# fielduse,  and  filename
retdata = getnames()

FIELDNAME = retdata[0]
FILENAME_PI = retdata[1]
FILENAME_PLIO = retdata[2]
FIELDLSM = retdata[3]
LSM_PI = retdata[4]
LSM_PLIO = retdata[5]

print('fieldname is',FIELDNAME)

get_land_sea_contrast()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/CCSM4_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
"""
This program will average all of the CCSM4 models.
We will average the data file (text files) and also the mean average temperature file
"""

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys


#########################################################################
# stuff for doing text file is here  
def get_text_data(filename):
    """
    gets the text data for each filename
    returns globalmean, monthly mean, latitudinal mean
    """
    f=open(filename,"r")
    f1=f.readlines()
    f2 = [x.replace('\n', '') for x in f1]
            
    # get the means according to their position in the file
    all_mean_sd=f2[2]
    all_mon_mean_sd=f2[5:5+12]
    all_lat_mean_sd=f2[20:20+180]
           
    # extract global mean
    meanglob,sd=all_mean_sd.split(',')
    
   
    monmeans = np.zeros(12)
    latmeans = np.zeros(180)
    lats = np.zeros(180)
    # extract monthly means
    for x in all_mon_mean_sd:
        mon,mean,sd=x.split(',')
        monmeans[int(mon)-1]=float(mean)
            
            
    # extract latitude means
    for x in all_lat_mean_sd:
        lat,mean,sd=x.split(',')
        latss=int(float(lat)+89.5) # convert latitude to a subscript
               
        if mean != ' --' and mean != '--':
            latmeans[latss]=float(mean) # stores latitudinal means
        else:
            latmeans[latss]=np.nan
            
        lats[latss]=lat # stores latitudes
      
   
    return meanglob, monmeans, latmeans, lats
   
def writeout_textdata(mean_global, mean_mon, mean_lat, lats, expt):
    """
    writeout the text data to a file
    replace stdev with -999
    """
    
    textout = FILESTART + OUTMODEL + '/' + expt + '.' + FIELDNAME + '.data.txt'
    file1 =  open(textout, "w")

    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
   
    # write out global temperautre
    file1.write(np.str(np.round(mean_global, 2))+', -99.99\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(mean_mon[i], 2))+', -99.99\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(mean_lat)):
        file1.write(np.str(lats[i])+', '+np.str(np.round(mean_lat[i], 2))+', -99.99\n')

    file1.close()
    

def main_avg_text(expt):
    """
    loop over all the models and extract mean, monthlymeans, latitudinal means
    average all the means
    write out to a file in the format of the input.  (Put standard deviation to -999.999)
    """
    
    for i, model in enumerate(MODELNAMES):
        filename = FILESTART + model + '/' + expt + '.' + FIELDNAME + '.data.txt'
        
        globmean, monmeans, latmeans, lats = get_text_data(filename)
        
        if i == 0:
            allmeans = np.zeros(NMODELS)
            allmonmeans = np.zeros((NMODELS, len(monmeans)))
            alllatmeans = np.zeros((NMODELS, len(latmeans)))
        
        allmeans[i] = globmean
        allmonmeans[i, :] = monmeans
        alllatmeans[i, :] = latmeans
        
    mean_global = np.mean(allmeans)
    mean_mon = np.mean(allmonmeans, axis=0)
    mean_lat = np.mean(alllatmeans, axis=0)
    
    writeout_textdata(mean_global, mean_mon, mean_lat, lats, expt)
 
###############################################
## stuff for doing netcdf file is here
def main_avg_netcdf(expt):
    """
    loop over all the models and extract the global average netcdf file
    average all the means
    write out to a file in the format of the input.  
    """ 
    all_cubes=iris.cube.CubeList([])     
    for i, model in enumerate(MODELNAMES):
        print(i)
        filename = FILESTART + model + '/' + expt + '.' + FIELDNAME + '.allmean.nc'
        cube = iris.load_cube(filename)
        modelcube = resort_coords(cube, i)
        modelcube.data=modelcube.data.astype('float32') 
        all_cubes.append(modelcube)
    
    
    iris.experimental.equalise_cubes.equalise_attributes(all_cubes)
  
    cat_cubes = all_cubes.concatenate_cube()
    meancube = cat_cubes.collapsed(['model_level_number'], iris.analysis.MEAN)  
    
    fileout = FILESTART + OUTMODEL + '/' + expt + '.' + FIELDNAME + '.allmean.nc'
    iris.save(meancube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)
  
    
def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
       
       
     
    newcube=iris.util.new_axis(cube)
    newcube.add_dim_coord(iris.coords.DimCoord(levelno, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
   
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    newcube.rename('tas')
    
        
    return newcube      
    
    
def main():
    """ 
    main program
    1. average the text files for each of the models and writeout
    2. average the netcdf files from each of the models and writeout
    """
    
    for i, expt in enumerate(EXPTNAMES):
        avgtext = main_avg_text(expt)
        avgnetcdf = main_avg_netcdf(expt)
    
    
   

##########################################################
# fixed constants
        
LINUX_WIN='w'
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'#
else:
    FILESTART = ' '

MODELNAMES=['CCSM4-1deg', 'CCSM4-2deg','CCSM4-UoT']
NMODELS = len(MODELNAMES)
OUTMODEL = 'CCSM4-avg'

FIELDNAME='NearSurfaceTemperature'
EXPTNAMES=['EOI400','E280']
#EXPTNAMES=['EOI400']


main()::::::::::::::
PlioMIP_new/large_scale_features/climate_sensitivity_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
import sys



#####################################
def  climate_sensitivity_analysis(modelnames,fieldname,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    else:
        filestart='/nfs/hera1/earjcti/regridded/'
     
        
    # set up a dictionary for the climate sensitivity
   
    # from my stuff
    #clim_sens ={'NorESM-L': 3.1,
    #             'NorESM1-F':2.29,
    #             'IPSLCM6A': 4.8,
    #             'IPSLCM5A':3.4,
    #             'HadCM3': 3.7,
    #             'MIROC4m':3.9,
    #             'COSMOS':4.1,
    #             'UofT':3.8,
    #             'EC-Earth3.1':3.2,
    #             'MRI-CGCM2.3':2.8,
    #             'CESM1.0.5': 3.1,
    #             'GISS': 3.31
    #             }
        
    # from Alan's table provided by authors
    clim_sens ={'NorESM-L': 3.1,
                 'NorESM1-F':2.3,
                 'IPSLCM6A': 4.8,
                 'IPSLCM5A2':3.6,
                 'IPSLCM5A':4.1,
                 'HadCM3': 3.5,
                 'MIROC4m':3.9,
                 'COSMOS':4.7,
                 'CCSM4-UoT':3.2,
                 'EC-Earth3.1':3.2,
                 'MRI2.3':2.8, # from my investigation
                 'CCSM4-2deg': 3.2,
                 'GISS2.1G': 3.3,
                 'CESM2': 5.3,
                 'CESM1.2' :4.1,
                 'CCSM4-1deg' :3.2
                 }
        
     
    # first get the data.  We need climate sensitivity, 
    # global temperature anomaly, latitude temperature anomaly
    # and gridbox by gridbox temperature anomaly
    climdiff=np.zeros(len(modelnames))
    climdiffmon=np.zeros((len(modelnames),12))
    climdifflat=np.zeros((len(modelnames),180))
    sensitivity_array=np.zeros(len(modelnames))
    alllats=np.arange(0,180,1)-89.5
    
    for mod in range(0,len(modelnames)):
        sensitivity_array[mod]=clim_sens.get(modelnames[mod])
        
        # get data from experiment file
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.data.txt'
        file1= open(fileexpt,"r")
        lines=list(file1)
       
        
        meanexpt,sdexpt=lines[2].split(",")
        monmeanexpt=np.zeros(12)
        latmeanexpt=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeanexpt[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeanexpt[index]=np.float(mean)
       
        
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.data.txt'
        file2= open(filecntl,"r")
        lines=list(file2)
        meancntl,sdexpt=lines[2].split(",")
        monmeancntl=np.zeros(12)
        latmeancntl=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeancntl[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeancntl[index]=np.float(mean)
        
        climdiff[mod]=np.float(meanexpt)-np.float(meancntl)
        
        climdiffmon[mod,:]=monmeanexpt-monmeancntl
        climdifflat[mod,:]=latmeanexpt-latmeancntl
        #print(modelnames[mod],climdifflat[mod])
    
     
    ########################################################
    # plot the climate sensitivity vs the global mean
    fig = plt.figure(figsize=(7.0, 4.0))
    plt.plot(climdiff,sensitivity_array,'x')
    plt.xlabel('Plio_Core - PI_CTL SAT anomaly',fontsize=15)
    plt.ylabel('ECS', fontsize=15)
    plt.xlim(np.floor(np.min(climdiff)),np.ceil(np.max(climdiff)))
    
    # do a linear regression
    print(modelnames)
    print(climdiff)
    print(sensitivity_array)
    print(np.mean(climdiff)*1.94, np.mean(sensitivity_array))
    sys.exit(0)
   
    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(climdiff, sensitivity_array)
    xarray=np.arange(0,5,1)
    yarray=intercept+(slope*xarray)
    plt.plot(xarray,yarray)
    plt.tick_params(axis='x',  labelsize=15)
    plt.tick_params(axis='y',  labelsize=15)
    #plt.title("R-squared: %2f, p-value: %2f" %'{:06.2f}'.format(r_value**2,), np.round(p_value, 2)), fontsize=15)
    plt.title("R-squared: " + np.str(np.around((r_value**2.), 2)) 
            + ",  p-value: " + np.str(np.around(p_value, 2)) , fontsize=15) 
    

    if redu == '':
        figtext = 'a)'
    else:
        figtext = 'a)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
  
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom' + 
             redu + '.eps')
    
    plt.tight_layout()
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    plt.close()

    rsq_std=r_value**2.
    
    ########################################################
    # plot the correlation between climate sensitivity vs the monthly mean
    rvals=np.zeros(12)
    pvals=np.zeros(12)
    for mon in range(0,12):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdiffmon[:,mon], sensitivity_array))
        rvals[mon]=r_value**2.
        pvals[mon]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    

    color = 'tab:red'
    ax1.set_xlabel('month', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=15)
    ax1.plot(np.arange(1,13,1), rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=15)
    ax1.tick_params(axis='x',  labelsize=15)
    #ax1.plot([0,13],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axi
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=15)  # we already handled the x-label with ax1
    ax2.plot(np.arange(1,13,1), pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=15)
    plt.title('ECS vs Plio_Core - PI_CTL by month', fontsize=15)
    
    
    if redu == '':
        figtext = 'c)'
    else:
        figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
   
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom' + 
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    plt.close()
    
    
     ########################################################
    # plot the correlation between climate sensitivity vs the latitudinal mean
    rvals=np.zeros(180)
    pvals=np.zeros(180)
    for lat in range(0,180):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdifflat[:,lat], sensitivity_array))
        rvals[lat]=r_value**2.
        pvals[lat]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))

    color = 'tab:red'
    ax1.set_xlabel('latitude', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=15)
    ax1.plot(alllats, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=15)
    ax1.tick_params(axis='x', labelsize=15)
    #ax1.plot([-90,90],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])
    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=15)  # we already handled the x-label with ax1
    ax2.plot(alllats, pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=15)
    ax2.set_ylim(0,0.1)
    plt.title('ECS vs Plio_Core - PI_CTL by latitude', fontsize=15)
    
    if redu == '':
        figtext = 'e)'
    else:
        figtext = 'c)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom' +
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    
    
    # now get the global data and do a correlation
    for mod in range(0,len(modelnames)):
        # get average anomaly
        
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.allmean.nc'
        exptcube=iris.load_cube(fileexpt)
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.allmean.nc'
        cntlcube=iris.load_cube(filecntl)
        
        ny,nx=np.shape(exptcube.data)
        if mod==0:
            anommap=np.zeros((len(modelnames),ny,nx))
           
       
        anommap[mod,:,:]=exptcube.data-cntlcube.data
    
        
    
    rsqmap=np.zeros((ny,nx))
    pvalmap = np.zeros((ny, nx))
    slopemap = np.zeros((ny,nx))
    interceptmap = np.zeros((ny,nx))
    for j in range(0,ny):
         for i in range(0,nx):
             slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(anommap[:,j,i], sensitivity_array))
             rsqmap[j,i]=r_value**2.
             pvalmap[j,i] = p_value
             slopemap[j,i] = slope
             interceptmap[j,i] = intercept
           
             
    rsqmapcube=exptcube.copy(data=rsqmap) 
    rsqmapcube.units=None
    
    slopecube=exptcube.copy(data=slopemap) 
    slopecube.units=None
    
    interceptcube=exptcube.copy(data=interceptmap) 
    interceptcube.units=None
    
    temparr = np.where(pvalmap < 0.05, 1, 0) 
    significance_cube = rsqmapcube.copy(data=temparr)
    
    
    plt.subplot(1,1,1)
    V=np.arange(0.0,1,0.05)
    qplt.contourf(rsqmapcube,V,cmap='YlGnBu')
    iplt.contourf(significance_cube, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(significance_cube, 1, hatches=[None, '\\\''], colors='none')
    #titlename=modeluse+' '+exptname+': '+field
    #bar=plt.colorbar(cs,orientation="horizontal")
    plt.title('correlation between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    
    #plt.title(titlename,fontsize=8)
    plt.gca().coastlines()
 
    if redu == '':
        figtext = 'g)'
    else:
        figtext = 'd)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe'+
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe' + 
             redu + '.pdf')
    plt.savefig(fileout)  
    plt.close()    
    
        
    # print out the slope and the intercept at each gridpoint
    plt.subplot(1,1,1)
    qplt.contourf(slopecube)
    plt.title('slope between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_slope_'+
                redu + '.eps')  
 
    plt.close()
    
    plt.subplot(1,1,1)
    qplt.contourf(interceptcube)
    plt.title('intercept between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_intercept_'+
                redu + '.eps')  
 
    plt.close()
    
    outfile = (filestart + 'allplots/' + 
              fieldname + '/climate_sensitivity_relationships'+
                redu + '.txt')
    
    txtfile1 = open(outfile,"w+") 
    txtfile1.write("longitude latitude rsq pvalue intercept slope \n")
    lons = interceptcube.coord('longitude').points
    lats = interceptcube.coord('latitude').points
    
    for j in range(0,ny):
         for i in range(0,nx):
             writestring = (np.str(np.around(lons[i],2)) + ',' + 
                            np.str(np.around(lats[j],2)) + ',' + 
                            np.str(np.around(rsqmap[j, i],2)) + ',' +
                            np.str(np.around(pvalmap[j, i],2)) + ',' +
                            np.str(np.around(interceptmap[j, i],2)) + ',' +
                            np.str(np.around(slopemap[j,i],2)) + '\n')
             txtfile1.write(writestring)
    txtfile1.close

##########################################################
# main program
        
filename=' '
linux_win='l'
#modelnames=['MIROC4m','COSMOS']   # MIROC4m  COSMOS UofT EC-Earth3.1

#modelnames=['CCSM4-2deg','COSMOS', 'CCSM4-1deg',
#            'EC-Earth3.1', 'CESM1.2',
#            'GISS2.1G','HadCM3',
#            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
#            'MIROC4m','MRI2.3',
#            'NorESM-L','NorESM1-F',
#            'CCSM4-UoT'
#            ]
#redu = ''

modelnames=['COSMOS', 'CESM1.2', 'CCSM4-1deg',
            'GISS2.1G','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI2.3',
            'NorESM-L','NorESM1-F',
            'CCSM4-UoT'
            ]
redu = '_redu'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
#fieldnames = ['SST']
units=['degC']
exptname='EOI400'
cntlname='E280'

for field in range(0,len(fieldnames)):
    climate_sensitivity_analysis(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)::::::::::::::
PlioMIP_new/large_scale_features/climate_sensitivity.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

#os.environ["PROJ_LIB"] = r'C:\Users\julia\Miniconda2\pkgs\proj4-5.2.0-hc56fc5f_1003\Library\share'
#from mpl_toolkits.basemap import Basemap, shiftgrid
import sys



#####################################
def  climate_sensitivity_analysis(modelnames,fieldname,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
        datatext = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\data_for_7a-b.txt'
        netcdfout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\data_for_7c.nc'
    else:
        filestart='/nfs/hera1/earjcti/regridded/'
        datatext = '/nfs/hera1/earjcti/regridded/alldata/data_for_7a-b.txt'
        netcdfout = '/nfs/hera1/earjcti/regridded/alldata/data_for_7c.nc'
     
        
    # set up a dictionary for the climate sensitivity
   
    # from my stuff
    #clim_sens ={'NorESM-L': 3.1,
    #             'NorESM1-F':2.29,
    #             'IPSLCM6A': 4.8,
    #             'IPSLCM5A':3.4,
    #             'HadCM3': 3.7,
    #             'MIROC4m':3.9,
    #             'COSMOS':4.1,
    #             'UofT':3.8,
    #             'EC-Earth3.1':3.2,
    #             'MRI-CGCM2.3':2.8,
    #             'CESM1.0.5': 3.1,
    #             'GISS': 3.31
    #             }
        
    # from Alan's table provided by authors
    clim_sens ={'NorESM-L': 3.1,
                 'NorESM1-F':2.3,
                 'IPSLCM6A': 4.8,
                 'IPSLCM5A2':3.6,
                 'IPSLCM5A':4.1,
                 'HadCM3': 3.5,
                 'MIROC4m':3.9,
                 'COSMOS':4.7,
                 'CCSM4-UoT':3.2,
                 'EC-Earth3.1':3.2,
                 'EC-Earth3.3':4.3,
                 'MRI2.3':2.8, # from my investigation
                 'CCSM4-Utr': 3.2,
                 'GISS2.1G': 3.3,
                 'CESM2': 5.3,
                 'CESM1.2' :4.1,
                 'CCSM4' :3.2,
                 'CCSM4-avg' : 3.2
                 }
        
     
    # first get the data.  We need climate sensitivity, 
    # global temperature anomaly, latitude temperature anomaly
    # and gridbox by gridbox temperature anomaly
    climdiff=np.zeros(len(modelnames))
    climdiffmon=np.zeros((len(modelnames),12))
    climdifflat=np.zeros((len(modelnames),180))
    sensitivity_array=np.zeros(len(modelnames))
    alllats=np.arange(0,180,1)-89.5
    
    for mod in range(0,len(modelnames)):
        sensitivity_array[mod]=clim_sens.get(modelnames[mod])
        
        # get data from experiment file
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.data.txt'
        file1= open(fileexpt,"r")
        lines=list(file1)
       
        
        meanexpt,sdexpt=lines[2].split(",")
        monmeanexpt=np.zeros(12)
        latmeanexpt=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeanexpt[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeanexpt[index]=np.float(mean)
       
        
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.data.txt'
        file2= open(filecntl,"r")
        lines=list(file2)
        meancntl,sdexpt=lines[2].split(",")
        monmeancntl=np.zeros(12)
        latmeancntl=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeancntl[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeancntl[index]=np.float(mean)
        
        climdiff[mod]=np.float(meanexpt)-np.float(meancntl)
        
        climdiffmon[mod,:]=monmeanexpt-monmeancntl
        climdifflat[mod,:]=latmeanexpt-latmeancntl
        #print(modelnames[mod],climdifflat[mod])
    
     
    ########################################################
    # plot the climate sensitivity vs the global mean
   
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    ax1.plot(climdiff,sensitivity_array,'x')
    ax1.set_xlabel('Plio_Core - PI_CTL SAT anomaly',fontsize=15)
    ax1.set_ylabel('ECS', fontsize=15)
    ax1.set_xlim(np.floor(np.min(climdiff)),np.ceil(np.max(climdiff)))
    
    # do a linear regression
    print(modelnames)
    print(climdiff)
    print(sensitivity_array)
   
    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(climdiff, sensitivity_array)
    xarray=np.arange(0,10,1)
    yarray=intercept+(slope*xarray)
    ax1.plot(xarray,yarray)
    ax1.tick_params(axis='x',  labelsize=15)
    ax1.tick_params(axis='y',  labelsize=15)
    #plt.title("R-squared: " + np.str(np.around((r_value**2.), 2)) 
    #        + ",  p-value: " + np.str(np.around(p_value, 2)) , fontsize=15) 
    
    print('julia',slope, intercept, r_value, p_value)
   # sys.exit(0)
    figtext = 'a)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])
  
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom.png')
    
    #plt.tight_layout()
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom.pdf')
    plt.savefig(fileout)
    plt.close()

    rsq_std=r_value**2.
    
    f1 = openx=open(datatext,'w')
    f1.write('Data for Figure 7a\n')
    f1.write('model name, Plio_core - PI_Cntl, ECS\n')
    for i in range(0,len(modelnames)):
        f1.write(modelnames[i] + ',' + np.str(np.round(climdiff[i],2)) + ',' + 
                 np.str(np.round(sensitivity_array[i],2)) + '\n')
   

    
    ########################################################
    # plot the correlation between climate sensitivity vs the monthly mean
    rvals=np.zeros(12)
    pvals=np.zeros(12)
    for mon in range(0,12):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdiffmon[:,mon], sensitivity_array))
        rvals[mon]=r_value**2.
        pvals[mon]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

    color = 'tab:red'
    #ax1.set_xlabel('month', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=12)
    ax1.plot(labels, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=12)
    ax1.tick_params(axis='x',  labelsize=12)
    #ax1.plot([0,13],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axi
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=12)  # we already handled the x-label with ax1
    ax2.plot(labels, pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)
    #plt.title('ECS vs Plio_Core - PI_CTL by month', fontsize=15)
    
    
    figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
   
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom.pdf')
    plt.savefig(fileout)
    plt.close()
    
    
    
     ########################################################
    # plot the correlation between climate sensitivity vs the latitudinal mean
    rvals=np.zeros(180)
    pvals=np.zeros(180)
    for lat in range(0,180):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdifflat[:,lat], sensitivity_array))
        rvals[lat]=r_value**2.
        pvals[lat]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))

    color = 'tab:red'
    ax1.set_xlabel('latitude', fontsize=12)
    ax1.set_ylabel('Rsq', color=color, fontsize=12)
    ax1.plot(alllats, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=12)
    ax1.tick_params(axis='x', labelsize=12)
    #ax1.plot([-90,90],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])
    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=12)  # we already handled the x-label with ax1
    ax2.plot(alllats, pvals, color=color)
    #for i, lat in enumerate(alllats):
    #    print(lat, pvals[i], rvals[i])
   
    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)
    #ax2.set_ylim(0,0.1)
    #plt.title('ECS vs Plio_Core - PI_CTL by latitude', fontsize=15)
    
    figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom.pdf')
    plt.savefig(fileout)
    
    f1.write('\n')
    f1.write('Data for Figure 7b\n')
    f1.write('latitude, Pvalue, Rsqvalue\n')
    for i in range(0,len(alllats)):
        f1.write(np.str(np.round(alllats[i],1)) + ',' + np.str(np.round(pvals[i],3)) + ',' + 
                 np.str(np.round(rvals[i],2)) + '\n')
    f1.close()
    
    #############################################################################
    # now get the global data and do a correlation
    
    cubelist = iris.cube.CubeList([])
    for mod in range(0,len(modelnames)):
        # get average anomaly
        
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.allmean.nc'
        exptcube=iris.load_cube(fileexpt)
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.allmean.nc'
        cntlcube=iris.load_cube(filecntl)
        
        ny,nx=np.shape(exptcube.data)
        if mod==0:
            anommap=np.zeros((len(modelnames),ny,nx))
           
       
        anommap[mod,:,:]=exptcube.data-cntlcube.data
    
        
    
    rsqmap=np.zeros((ny,nx))
    pvalmap = np.zeros((ny, nx))
    slopemap = np.zeros((ny,nx))
    interceptmap = np.zeros((ny,nx))
    for j in range(0,ny):
         for i in range(0,nx):
             slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(anommap[:,j,i], sensitivity_array))
             rsqmap[j,i]=r_value**2.
             pvalmap[j,i] = p_value
             slopemap[j,i] = slope
             interceptmap[j,i] = intercept
           
             
    rsqmapcube=exptcube.copy(data=rsqmap) 
    rsqmapcube.units=None
    rsqmapcube.long_name = 'Rsq'
    rsqmapcube.standard_name = None
    rsqmapcube.var_name = 'Rsq'

    
    slopecube=exptcube.copy(data=slopemap) 
    slopecube.units=None
    
    interceptcube=exptcube.copy(data=interceptmap) 
    interceptcube.units=None
    
    temparr = np.where(pvalmap < 0.05, 1, 0) 
    significance_cube = rsqmapcube.copy(data=temparr)
    pval_cube = rsqmapcube.copy(data=pvalmap)
    pval_cube.units = None
    pval_cube.long_name = 'pvalue'
    pval_cube.standard_name = None
    pval_cube.var_name = 'pvalue'
    
    cubelist.append(rsqmapcube)
    cubelist.append(pval_cube)
    print(cubelist)
    iris.save(cubelist, netcdfout, netcdf_format='NETCDF3_CLASSIC')

    
    # plot the map with Rsq and the significance
    
    fig = plt.subplots(figsize=(7.0, 5.0))
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(0.0,1,0.05)
    
    qplt.contourf(rsqmapcube,V,cmap='YlGnBu')
    iplt.contourf(significance_cube, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(significance_cube, 1, hatches=[None, '\\\''], colors='none')
    #titlename=modeluse+' '+exptname+': '+field
    #bar=plt.colorbar(cs,orientation="horizontal")
    #plt.title('correlation between climate sensitivity and \n mPWP-PI anomaly '
    #          'at this point')
    plt.title('')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    
    #plt.title(titlename,fontsize=8)
    plt.gca().coastlines()
 
    figtext = 'c)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe.pdf')
    plt.savefig(fileout)  
    plt.close()    
    
        
    # print out the slope and the intercept at each gridpoint
    plt.subplot(1,1,1)
    qplt.contourf(slopecube)
    plt.title('slope between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_slope.eps')  
 
    plt.close()
    
    plt.subplot(1,1,1)
    
    qplt.contourf(interceptcube)
    plt.title('intercept between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_intercept.eps')  
 
    plt.close()
    
    outfile = (filestart + 'allplots/' + 
              fieldname + '/climate_sensitivity_relationships.txt')
    
    txtfile1 = open(outfile,"w+") 
    txtfile1.write("longitude latitude rsq pvalue intercept slope \n")
    lons = interceptcube.coord('longitude').points
    lats = interceptcube.coord('latitude').points
    
    for j in range(0,ny):
         for i in range(0,nx):
             writestring = (np.str(np.around(lons[i],2)) + ',' + 
                            np.str(np.around(lats[j],2)) + ',' + 
                            np.str(np.around(rsqmap[j, i],2)) + ',' +
                            np.str(np.around(pvalmap[j, i],2)) + ',' +
                            np.str(np.around(interceptmap[j, i],2)) + ',' +
                            np.str(np.around(slopemap[j,i],2)) + '\n')
             txtfile1.write(writestring)
    txtfile1.close

##########################################################
# main program
        
filename=' '
linux_win='l'
#modelnames=['MIROC4m','COSMOS']   # MIROC4m  COSMOS UofT EC-Earth3.1

modelnames=['CCSM4-Utr','COSMOS', 'CCSM4',
            'EC-Earth3.3', 'CESM1.2','CESM2',
            'GISS2.1G','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI2.3',
            'NorESM-L','NorESM1-F',
            'CCSM4-UoT'
            ]

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
#fieldnames = ['SST']
units=['degC']
exptname='EOI400'
cntlname='E280'

for field in range(0,len(fieldnames)):
    climate_sensitivity_analysis(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/create_grid.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 14 15:53:19 2019

#@author: earjcti

# create a blank netcdf file on the correct grid for putting the pliomip data on


import netCDF4
import numpy as np
from netCDF4 import Dataset

dataset=Dataset('one_lev_one_deg.nc', 'w',format='NETCDF3_CLASSIC') 
#create dimensions
level = dataset.createDimension('level', 1) 
latitude = dataset.createDimension('latitude', 180)
longitude = dataset.createDimension('longitude', 360) 
time = dataset.createDimension('time', None)

# create variables
times = dataset.createVariable('time', np.float64, ('time',)) 
levels = dataset.createVariable('level', np.int32, ('level',)) 
latitudes = dataset.createVariable('latitude', np.float32,('latitude',))
longitudes = dataset.createVariable('longitude', np.float32,('longitude',)) 
# Create the actual 4-d variable
dummy = dataset.createVariable('dummy', np.float32, ('time','level','latitude','longitude')) 

# Variable Attributes  
latitudes.units = 'degree_north'  
longitudes.units = 'degree_east'  
levels.units = 'Surface' 
dummy.units = 'None' 
times.units = 'hours since 0001-01-01 00:00:00'  
times.calendar = 'gregorian' 

# add variables
lats = np.linspace(-89.5,89.5,num=180) 
print(lats)
lons = np.arange(0,360,1.0)
print(len(lats)) 
print(len(lons))
latitudes[:] = lats  
longitudes[:] = lons 
dummy[:,:,:,:]=0.0
levels[0]=0

dataset.close()
::::::::::::::
PlioMIP_new/large_scale_features/diagnostics_to_one_file.py
::::::::::::::
#!/usr/bin/env python3
# created 08/12/2022 by Julia
#
# This program will put all the regridded data for pliomip2 in one file 
# so they are easier for sharing.

import iris
from iris.cube import CubeList
import numpy as np

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F','HadGEM3'
            ]

FIELD = 'SST'

EXPTNAME = 'E280'

FILEINSTART = '/nfs/hera1/earjcti/regridded/'

cubelist = CubeList([])
for model in MODELNAMES:
    filename = FILEINSTART + model + '/' + EXPTNAME + '.SST.allmean.nc'
    cube = iris.load_cube(filename)
    print(cube)
    for coord in cube.coords():
        coord.bounds = None
    cube.long_name = model + '_' + FIELD
    cube.data = np.where(cube.data.mask == True, -99999., cube.data)
    cubelist.append(cube)
    


fileout = FILEINSTART + FIELD + '_' + EXPTNAME + '_allmodels.nc'
iris.save(cubelist,fileout,fill_value = -99999.)
::::::::::::::
PlioMIP_new/large_scale_features/dmc_by_latitude.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
This program will do a DMC plot.  But it will be latitude vs deltSST

"""
import pandas as pd
import matplotlib as mp
import matplotlib.pyplot as plt
import numpy as np
import iris
import sys

def is_number(s):
    try:
        float(s)
        return True
    except ValueError:
        return False

def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints
    
def plot_data(lats, Tanom, stdev):
    """
    plots the data and the errorbars
    """
    
    stdevplot = np.zeros(len(stdev))
    for i in range(0, len(stdev)):
       numeric = is_number(stdev[i])
       if numeric:
            stdevplot[i] = stdev[i]
    
    print(stdevplot)

    plt.errorbar(lats, Tanom, yerr=stdevplot, fmt='o')
    

def get_multimodel_mean(fieldname):
    """
    gets the multimodel mean and calculates a zonal average
    """
    mmm_cube = iris.load_cube(MULTIMODELMEAN, fieldname)
    zm_cube = mmm_cube.collapsed(['longitude'], iris.analysis.MEAN)
    zm_cube_max = mmm_cube.collapsed(['longitude'], iris.analysis.MAX)
    zm_cube_min = mmm_cube.collapsed(['longitude'], iris.analysis.MIN)
    
    return zm_cube, zm_cube_max, zm_cube_min
    
def plot_zm(cubemean, cubemax, cubemin, max_cubemax, min_cubemin):
    """
    latitudinal plot + zonal range of multimodel mean
    cubemean, cubemax, cubemin are the mean and the range from the multimodel mean
    max_cubemax is the maximum longitude from the model with the maximum difference
    min_cubemin is the minimum longitude from the model with the minimum difference
    """
    
    lats = cubemean.coord('latitude').points
    datamean = cubemean.data
    datamax = cubemax.data
    datamin = cubemin.data
    
    fig, ax = plt.subplots() 
    ax.plot(lats, datamean)
    ax.fill_between(lats, min_cubemin.data, max_cubemax.data, alpha=0.4)
    ax.fill_between(lats, datamin, datamax, alpha=0.4)
    ax.set_ylim(-5.0,20.0)
   
    ax.set_xlabel('latitude')
    ax.set_ylabel('SST anomaly')
    


def main():
    """
    1. get the data
    2. get multimodel mean SST 
    3. plot the data on the same file as the MMM
    """

    lats, lons, data_tanom, data_stdev, Npoints = get_data()
    #
    zonal_mean_cube, zonal_max_cube, zonal_min_cube = get_multimodel_mean('SSTmean_anomaly')
    max_zonal_mean_cube, max_zonal_max_cube, max_zonal_min_cube = get_multimodel_mean('SSTmax_anomaly')
    min_zonal_mean_cube, min_zonal_max_cube, min_zonal_min_cube = get_multimodel_mean('SSTmin_anomaly')
   
   
    plot_zm(zonal_mean_cube, zonal_max_cube, 
            zonal_min_cube, max_zonal_max_cube, min_zonal_min_cube)
    plot_data(lats, data_tanom, data_stdev)
    
    
    plt.savefig(OUTNAME + '.eps')
    plt.savefig(OUTNAME + '.pdf')
    plt.close()

DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
MULTIMODELMEAN = '/nfs/hera1/earjcti/regridded/SST_multimodelmean.nc'
OUTNAME = '/nfs/hera1/earjcti/regridded/allplots/SST/dmc_by_latitude'

main()
::::::::::::::
PlioMIP_new/large_scale_features/DMC_for_IPCC.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on August 2020


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
#import matplotlib as mp
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    import matplotlib as mpl
    import numpy as np
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=60, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

  
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    model_anom_cube, lsmplio_cube = get_model_data()

    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata() 
        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])

    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'H' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/DMC_for_IPCC_with_land.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    #plt.show()
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         land_lats, land_lons, land_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/DMC_for_IPCC_with_land_split.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 2021
# note this differs from DMC_for_IPCC_land in that it will produce 
# seperate plots for the land and the ocean
# it will also produce a plot where the ocean mmm and data disagree.


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons


def outside_x(x, lats, lons_shift, data_tanom, model_anom_cube):
    """
    reduces data points to those that are more than x degs away from the model
    """

    cubelats = model_anom_cube.coord('latitude').points
    cubelons = model_anom_cube.coord('longitude').points

    new_lats = []
    new_lons = []
    new_data = []

    for i, lat in enumerate(lats):
        lon = lons_shift[i]
      # find nearest latitude and lontiude to the value
        latix = np.abs(cubelats-lat).argmin()
        lonix = np.abs(cubelons-lon).argmin()

        model_slice  =  model_anom_cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
           
        modelanom = model_slice.data

        if np.abs(data_tanom[i] - modelanom) > x:
            new_lats.append(lat)
            new_lons.append(lon)
            new_data.append(data_tanom[i])
        
    return new_lats, new_lons, new_data


def plot(model_cube, mask_cube, lats, lons, data, fileout):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data 
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


  
    #plt.show()
    plt.savefig(fileout + '.png')
    plt.savefig(fileout + '.eps')
    plt.close()


def plot_2figs(model_cube, mask_cube, land_lats, land_lons, 
         land_data, ocean_lats, ocean_lons, ocean_data):
    """
    prepares a 2 part figure with land and ocean 
    """

    fig = plt.figure(figsize=[8.0,8.0])


    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    # OCEAN DATA
    # model
    ax = fig.add_subplot(2,1,1, projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('a) Ocean DMC')
    

    # overplot data   
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    plt.scatter(ocean_lons, ocean_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())
    plt.scatter(ocean_lons, ocean_lats, c=ocean_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
    print('length',np.shape(ocean_lons))
    sys.exit(0)


    # LAND DATA
    # model
    ax = fig.add_subplot(2,1,2, projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('b) Land DMC')
    

    # overplot data 
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())
    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

    # colorbar
  #  fig.tight_layout()
    fig.subplots_adjust(bottom=0.15)
    cbar_ax = fig.add_axes([0.15, 0.10, 0.75, 0.03])
    cbar = fig.colorbar(cs, cax=cbar_ax,orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C',fontsize=15)
    cbar.ax.tick_params(labelsize=10)

    plt.savefig('/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_ocn.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_ocn.eps')
    plt.close()

    
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        print('n mgca',len(lats))
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        print('n uk37',len(lats_UK37))
       
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)

    # put land and ocean on same figure
    plot_2figs(model_anom_cube, lsmplio_cube, land_lats, land_lons, 
         land_tanom, lats, lons_shift, data_tanom)
    
    # put land and ocean on seperate figures
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land'
    plot(model_anom_cube, lsmplio_cube, land_lats, land_lons, 
         land_tanom,fileout)
    
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/ocean'
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         fileout)

    # reduce points to those not within x deg of data
    x=2
    (llat_new, llon_new, ldata_new) = outside_x(x, land_lats, land_lons, 
                                               land_tanom, model_anom_cube)
    (olat_new, olon_new, odata_new) = outside_x(x, lats, lons_shift, 
                                               data_tanom, model_anom_cube)

    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_unmatched'
    plot(model_anom_cube, lsmplio_cube, llat_new, llon_new, 
         ldata_new,fileout)
    
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/ocean_unmatched'
    plot(model_anom_cube, lsmplio_cube, olat_new, olon_new, odata_new,
         fileout)

   
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/DMC_for_paper_with_land_alt.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            print(dfs.iloc[rl,0])
  
            if (dfs.iloc[rl,0] == 'Lake Baikal'):
                temps.append(temp - 5.8)
                print('LB found')
            elif (dfs.iloc[rl,0] == 'Lost Chicken Mine'):
                temps.append(temp - 4.0)
                print('LCM found')
            elif (dfs.iloc[rl,0] == 'James Bay Lowland'):
                temps.append(temp - 4.0)
                print('JBL found')
            else:
                temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    plt.savefig('/nfs/hera1/earjcti/regridded/alternative_IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/alternative_IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         land_lats, land_lons, land_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/emergent_constraints_old.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu)
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    print('mean',np.mean(new_clim_sens_redu))
    print('stdev',np.std(new_clim_sens_redu))
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
    

   
    return lats, lons, sstanom_min, sstanom_max


def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    mod_lat, mod_lon, mod_minsst, mod_maxsst =  readmodel()

                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
  
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
PlioMIP_new/large_scale_features/emergent_constraints.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu,
                                                 proxy_sst_anom,
                                                 proxylat, proxylon)  
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    allsstanom = np.zeros((37, 17)) # the 17th is the MMM
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    line = lines[0]
    titles = line.split(',')
    modnames = titles[3:20]
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
      allsstanom[i-1, :] = np.asarray(vals[3:20], dtype = float)
    

   
    return lats, lons, sstanom_min, sstanom_max, allsstanom, modnames

def print_rmse(mod_allsst, proxy_allsst, modlats, modlons, 
               proxylats, proxylons, modnames):
    """
    just prints out the rmse for all the models

    """
    print('shape proxy', np.shape(proxy_allsst))
    print('shape data', np.shape(mod_allsst))
    npoints, nmods = np.shape(mod_allsst)
    
    f1 = open('ind_model_dmc.txt','w+')
    f1.write('model        rmse      bias   within 2deg/ 1deg/ 0.5deg \n')
    for i in range(0, nmods):
        sumsq = 0.0
        avger = 0.0
        count = 0.0
        within_1deg = 0
        within_2deg = 0
        within_05deg = 0
        for j in range(0, npoints):
            sumsq = sumsq + ((proxy_allsst[j] - mod_allsst[j, i])**2.0)
            avger = avger + (proxy_allsst[j] - mod_allsst[j, i])
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 1.0):
                within_1deg = within_1deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 2.0):
                within_2deg = within_2deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 0.5):
                within_05deg = within_05deg + 1
            count = count + 1.0
            
        rmse = np.sqrt(sumsq / count)
        avger = avger / count
      
        f1.write(modnames[i].ljust(12) + ',' +  np.str(np.around(rmse,2))
                 + ',   ' +  np.str(np.around(avger,2)) + ',   ' 
                 + np.str(within_2deg) +  ',' +  np.str(within_1deg)
                 +  ',' +  np.str(within_05deg) + '\n')
        print(modnames[i].ljust(12),',',np.around(rmse,2),
                 ',   ',np.around(avger,2),',   ',within_2deg,  
               '   ',within_1deg,',   ',within_05deg)
        
    f1.close()
   

def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens, 
               proxysst_full, proxy_fulllat, proxy_fulllon):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    (mod_lat, mod_lon, mod_minsst, mod_maxsst, 
    mod_allsst, modnames) =  readmodel()
    print_rmse(mod_allsst, proxysst_full, mod_lat, mod_lon, proxy_fulllat, 
               proxy_fulllon, modnames)
                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
PlioMIP_new/large_scale_features/emergent_contstraints_old2.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu,
                                                 proxy_sst_anom,
                                                 proxylat, proxylon)  
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    allsstanom = np.zeros((37, 17)) # the 17th is the MMM
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    line = lines[0]
    titles = line.split(',')
    modnames = titles[3:20]
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
      allsstanom[i-1, :] = np.asarray(vals[3:20], dtype = float)
    

   
    return lats, lons, sstanom_min, sstanom_max, allsstanom, modnames

def print_rmse(mod_allsst, proxy_allsst, modlats, modlons, 
               proxylats, proxylons, modnames):
    """
    just prints out the rmse for all the models

    """
    print('shape proxy', np.shape(proxy_allsst))
    print('shape data', np.shape(mod_allsst))
    npoints, nmods = np.shape(mod_allsst)
    
    f1 = open('ind_model_dmc.txt','w+')
    f1.write('model        rmse      bias   within 2deg/ 1deg/ 0.5deg \n')
    for i in range(0, nmods):
        sumsq = 0.0
        avger = 0.0
        count = 0.0
        within_1deg = 0
        within_2deg = 0
        within_05deg = 0
        for j in range(0, npoints):
            sumsq = sumsq + ((proxy_allsst[j] - mod_allsst[j, i])**2.0)
            avger = avger + (proxy_allsst[j] - mod_allsst[j, i])
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 1.0):
                within_1deg = within_1deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 2.0):
                within_2deg = within_2deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 0.5):
                within_05deg = within_05deg + 1
            count = count + 1.0
            
        rmse = np.sqrt(sumsq / count)
        avger = avger / count
      
        f1.write(modnames[i].ljust(12) + ',' +  np.str(np.around(rmse,2))
                 + ',   ' +  np.str(np.around(avger,2)) + ',   ' 
                 + np.str(within_2deg) +  ',' +  np.str(within_1deg)
                 +  ',' +  np.str(within_05deg) + '\n')
        print(modnames[i].ljust(12),',',np.around(rmse,2),
                 ',   ',np.around(avger,2),',   ',within_2deg,  
               '   ',within_1deg,',   ',within_05deg)
        
    f1.close()
   

def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens, 
               proxysst_full, proxy_fulllat, proxy_fulllon):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    (mod_lat, mod_lon, mod_minsst, mod_maxsst, 
    mod_allsst, modnames) =  readmodel()
    print_rmse(mod_allsst, proxysst_full, mod_lat, mod_lon, proxy_fulllat, 
               proxy_fulllon, modnames)
                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
PlioMIP_new/large_scale_features/extract_data_locations_monthly.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_02102019.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.outend = 'modeloutput_pliovar'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'modeloutput_CSCD_localities'
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'test_localities'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []

        with open(self.filenamein) as csvfile:
            print(self.filenamein)
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:#
                print(row)
                print(' ')
                print(' ')
                if count != 0: # not titleline
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period
        self.nmonths = 12

        if test == 'y':
            self.modelnames = ['NorESM-L']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]
        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros((len(self.lonlist), self.nmonths))
        model_data_near = np.zeros((len(self.lonlist), self.nmonths)) # values near the point
        near_distance = np.zeros((len(self.lonlist), self.nmonths)) # how far away we have to look to get data
        ngbox_avg = np.zeros((len(self.lonlist), self.nmonths)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            data_slice_months = data_slice.data
            for j in range(0,12):
                if -100. < data_slice_months[j] < 100.: 
                    model_data[i, j] = data_slice_months[j]
                    model_data_near[i, j] = model_data[i, j]
                else:
                    model_data[i, j] = float('NaN')
                    model_data_near[i, j] = float('NaN')

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            # check at month zero as we assume that lsm is same for all months
            while np.isnan(model_data_near[i, 0]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i, :] = neardata

            near_distance[i, :] = count_near_gb # how far away are we looking for data
            ngbox_avg[i, :] = ngboxes


        returndata = [model_data, model_data_near, near_distance, ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)

        sitenear = np.zeros((nmodels, npoints, self.nmonths)) # data near point
        sitevals = np.zeros((nmodels, npoints, self.nmonths)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints, self.nmonths)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints, self.nmonths)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.' + fieldnames + '.mean_month.nc')

            # get model points and how far away they are from data
            (sitevals[model, :, :], 
             sitenear[model, :, :], 
             sitenear_dist[model, :, :], 
             sitenear_ngbox_avg[model, :,: ]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'lat')
    sheet.write(0,1,'lon')
    for model in range(0,nmodels):
        sheet.write(0,2+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,latlist[i],style)
        sheet.write(i+1,1,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,2+model,datawrite[model,i],style)

    sheet.write(0,2+nmodels,'MMM')
    sheet.write(0,3+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,2+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,3+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

   
    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,4+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,4+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, monthno):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    print('writing out monthno',monthno, np.shape(eoi400))
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400[:, :, monthno])
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280[:, :, monthno])
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels, :, monthno]-e280[0:nmodels, :, monthno])

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near[:, :, monthno])
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near[:, :, monthno])
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels, :, monthno]-e280_near[0:nmodels, :, monthno])

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance[:, :, monthno])
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance[:, :, monthno])

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg[:, :, monthno])
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg[:, :, monthno])


    monthname = {0:'jan', 1:'feb', 2:'mar',3:'apr', 4:'may', 
                 5:'jun',6:'jul', 7:'aug', 8:'sep',9:'oct', 
                 10:'nov', 11:'dec' }

    fileoutwrite = fileout + monthname.get(monthno) + '.xls'

    # remove output file if it exists
    exists  =  os.path.isfile(fileoutwrite)
    if exists:
        os.remove(fileoutwrite)
    wb.save(fileoutwrite)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Erin' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
modelstart,outputfile,longitudes,latitudes,longitudes_alt = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points




##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
nmonths = len(eoi400[0, 0, :])

#######################################
# write data out to a workbook

for mon in range(0, 12):
    write_to_book(outputfile,longitudes,latitudes,longitudes_alt, mon)

###################################
# plot model points
for model in range(0,1):
        #plot points from january
    plotpoints(longitudes_alt,latitudes,eoi400[model,:,0])
          #plot points from july
    plotpoints(longitudes_alt,latitudes,eoi400[model,:,6])

::::::::::::::
PlioMIP_new/large_scale_features/extract_data_locations_pliomip1.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations 
#   from Aislings PlioMIP1 data
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_alan.csv'
            self.latcolumn = 2
            self.loncolumn = 3
            self.outend = 'PlioMIP1output_pliovar.xls'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'PlioMIP1output_CSCD_localities.xls'

        if self.linuxwin == 'l':
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []

        with open(self.filenamein) as csvfile:
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:
                if count != 0: # not titleline
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.fileout,lonlist, latlist, lonlist_alt
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, field, latlist, lonlist, lonlist_alt, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.lonlist_alt=lonlist_alt
        self.period = period

        if test == 'y':
            self.modelnames = ['COSMOS']
        else:
            self.modelnames = ['COSMOS', 'GISS', 'HAD', 
                               'IPSL', 'MIROC', 'MRI',
                               'NOR'
                               ]

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, allcube_,fieldreq_):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        
        ncubes = len(allcube_)
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]

      
        print(cube,'found')
        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist_alt[i])).argmin()
            
            print(self.lonlist_alt[i],self.latlist[i],latix,lonix)

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            model_data[i] = data_slice.data
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = np.nan # if no data near set to nan

        return data_near,count_finite

    def get_fieldreq(self,model_):
        
        PeriodE280Use={
                       "COSMOS" : "Ctrl",
                       "GISS" : "Ctrl",
                       "HAD" : "ctrl",
                       "IPSL" : "ctrl",
                       "MIROC" : "ctrl",
                       "MRI" : "ctrl",
                       "NOR" : "ctrl"
                       }
        
        PeriodEoi400Use={
                         "COSMOS" : "Plio",
                         "GISS" : "Plio",
                         "HAD" : "plio",
                         "IPSL" : "plio",
                         "MIROC" : "plio",
                         "MRI" : "plio",
                         "NOR" : "plio"
                         }
        
        fieldname={
                   "COSMOS" : "SST",
                   "GISS" : "SST",
                   "HAD" : "sst",
                   "IPSL" : "sst",
                   "MIROC" : "sst",
                   "MRI" : "sst",
                   "NOR" : "sst"
                   }
         
        if self.period == 'E280':
            self.fieldreq = (model_ + '_' + 
                        PeriodE280Use.get(model_)+
                        '_' + fieldname.get(model_))
        if self.period == 'EOI400':
            self.fieldreq = (model_ + '_' + 
                        PeriodEoi400Use.get(model_)+
                        '_' + fieldname.get(model_)) 
            
            

    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
          
            filename = ('/nfs/hera1/earjcti/PLIOMIP/PlioMIP1_regridded.nc')
            self.get_fieldreq(self.modelnames[model])
            print('fieldreq is',self.fieldreq)
            
            allcube = iris.load(filename)
            
            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(allcube,self.fieldreq)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'lat')
    sheet.write(0,1,'lon')
    for model in range(0,nmodels):
        sheet.write(0,2+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,latlist[i],style)
        sheet.write(i+1,1,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,2+model,datawrite[model,i],style)

    # add multimodel mean and multimodel standard deviation
    sheet.write(0,2+nmodels,'MMM')
    sheet.write(0,3+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,2+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,3+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,4+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,4+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels])

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near)
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near)
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels]-e280_near[0:nmodels])

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance)
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance)

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg)
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Harry' # could have Harry or Erin
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
outputfile,longitudes,latitudes,longitudes_alt = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points


######################################
# setup a map and plot the points

#if linuxwin = ='l':
#    plotpoints(lonlist_alt,latlist,np.zeros(count))


##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,fieldnames,
                       latitudes,longitudes,longitudes_alt,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,fieldnames,
                       latitudes,longitudes,longitudes_alt,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt)

###################################
# plot model points
for model in range(0,len(modelnames_eoi400)):
    #plotpoints(lonlist,latlist,eoi400[model,:]-e280[model,:])

    plotpoints(longitudes_alt,latitudes,eoi400[model,:])

::::::::::::::
PlioMIP_new/large_scale_features/extract_data_locations.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
import csv
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_02102019.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.sitecolumn = 1
            self.outend = 'modeloutput_pliovar.xls'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'modeloutput_CSCD_localities.xls'
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'test_localities.xls'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend

 
    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []
        sitelist = []

        with open(self.filenamein) as csvfile:
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:
                if count != 0: # not titleline
                    sitelist.append(row[self.sitecolumn])
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt, sitelist
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period

        if test == 'y':
            self.modelnames = ['NorESM1-F', 'HadCM3']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3','HadCM3_new',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]

        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            print(self.latlist[i], self.lonlist[i])
            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            
            data_slice_data = data_slice.data
            if -100. < data_slice_data < 100.: 
                model_data[i] = data_slice_data
            else:
                model_data[i] = float('NaN')
        
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.SST.allmean.nc')

            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite, sitename):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'site')
    sheet.write(0,1,'lat')
    sheet.write(0,2,'lon')
    for model in range(0,nmodels):
        sheet.write(0,3+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,sitename[i],style)
        sheet.write(i+1,1,latlist[i],style)
        sheet.write(i+1,2,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,3+model,datawrite[model,i],style)

    sheet.write(0,3+nmodels,'MMM')
    sheet.write(0,4+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,3+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,4+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,5+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,5+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, sitename):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400, sitename)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280, sitename)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels], sitename)

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near, sitename)
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near, sitename)
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels]-e280_near[0:nmodels], sitename)

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance, sitename)
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance, sitename)

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg, sitename)
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg, sitename)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Harry' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
(modelstart, outputfile, longitudes, 
 latitudes, longitudes_alt, sitenames) = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points


######################################
# setup a map and plot the points

#if linuxwin = ='l':
#    plotpoints(lonlist_alt,latlist,np.zeros(count))


##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt, sitenames)

###################################
# plot model points
#for model in range(0,len(modelnames_eoi400)):
    #plotpoints(lonlist,latlist,eoi400[model,:]-e280[model,:])

#    plotpoints(longitudes_alt,latitudes,eoi400[model,:])

::::::::::::::
PlioMIP_new/large_scale_features/extract_HadCM3_MOC.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 18 14:13:50 2019

#@author: earjcti
#
# This program will extract the fields needed by Zhongshi Zhang for the
# PlioMIP MOC paper.  It will extract the monthly averages from
# steve hunters processed data 
#
#
#

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.plot as iplt
import sys

#####################################
def extract_fields(filename,fieldnames,fileoutstart,filefieldnames):

    # load required cubes
    cubes=iris.load_cubes(filename,fieldnames)
    
    for i in range(0,len(fieldnames)):
        fileout=fileoutstart+filefieldnames[i]+'.nc'
        if filefieldnames[i]=='SSS':
            subcube=cubes[i]
            cube_extract= subcube.extract(iris.Constraint(z2=5))
            iris.save(cube_extract,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
        else:
            iris.save(cubes[i],fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)


def avg_MOC(fileMOC,fileout,fieldname):
    cubes=iris.load(fileMOC,fieldname)
  
    print(np.shape(cubes[0]))
    print(cubes)
    print(cubes[0].data)
    avgcube=cubes[0].data
    for i in range(1,len(cubes)):
        subcube=cubes[i]
        subcube_1d=subcube[0,:,50]
        iplt.plot(subcube_1d,color='r')
        avgcube=avgcube+cubes[i].data
        cubedata=cubes[i].data
       
    avgcube=avgcube/len(cubes)
    # put average cube in subcube area
    subcube.data=avgcube
    subcube_1d=subcube[0,:,50]
    iplt.plot(subcube_1d,color='blue')
    iris.save(subcube,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
    
    

    
   
   

##########################################################
# main program

#####################################################
# this is extracting fields from a mean file

fieldnames=['OCN TOP-LEVEL TEMPERATURE K',
            'SALINITY (OCEAN) (PSU)',
            'AICE : ICE CONCENTRATION',
            'SALINITY (OCEAN) (PSU)',]
filefieldnames=['SST','SSS','iceconc','salinity']
#filename='C:\\Users\\julia\\OneDrive\\DATA\\HadCM3_DATA\\Eoi400_2400-2499_Monthly.nc'
#fileoutstart='C:\\Users\\julia\\OneDrive\\DATA\\HadCM3_DATA\\EOI400_2400_2499_Monthly_'

filename='/nfs/a103/palaeo_share/PlioMIP2/processed/Eoi400_2400-2499_Monthly.nc'
fileoutstart='/nfs/hera1/earjcti/um/tenvj/pk2/EOI400_2400_2499_Monthly_'
extract_fields(filename,fieldnames,fileoutstart,filefieldnames)

print('here')
filename='/nfs/a103/palaeo_share/PlioMIP2/processed/Preind_E280_2900-2999_Monthly.nc'
fileoutstart='/nfs/hera1/earjcti/um/tenvo/pk2/E280_2900_2999_Monthly_'
extract_fields(filename,fieldnames,fileoutstart,filefieldnames)


####################################################
# this is averaging MOC from the MOC scripts
#Basin=['Atlantic','Indian','Pacific','Global']

#for i in range(0,len(Basin)):
#    fieldname='Meridional Overturning Stream Function ('+Basin[i]+')'

#    fileMOC='/nfs/hera1/earjcti/um/tenvo/pk2/tenvoo@pgt*c1.nc'
#    fileout='/nfs/hera1/earjcti/um/tenvo/pk2/E280_avg_'+Basin[i]+'_MOC.nc'
#    avg_MOC(fileMOC,fileout,fieldname)

#    fileMOC='/nfs/hera1/earjcti/um/tenvj/pk2/tenvjo@pgo*c1.nc'
#    fileout='/nfs/hera1/earjcti/um/tenvj/pk2/EOI400_avg_'+Basin[i]+'_MOC.nc'
#    avg_MOC(fileMOC,fileout,fieldname)

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/extract_HadCM3_PlioMIP.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 18 14:13:50 2019

#@author: earjcti1
#
# This program will extract the fields needed for the PlioMIP2 database
#  It will extract the monthly averages from
# steve hunters processed data 
#
#
#

import os
import numpy as np
import scipy as sp
#import cf
import iris
from iris.cube import CubeList
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import sys
import warnings

warnings.filterwarnings("ignore")

def simplify_cube(cube):
    """
    gets cube and makes sure dimensions are longitude, latitude surface and
    t. 
    """    
    for coord in cube.coords():
        if coord.var_name == 'level275':
            coord.var_name = 'surface'
    
    cube.coord('surface').points = 0.0
    cube.coord('surface').units = 'm'
    cube.coord('surface').attributes = None
    
    cube.data = np.where(cube.data > 1.0E10, 0., cube.data)
    return cube

def get_evap(filename_):
    """
    will add up all the fluxes that make evaporation and returns
    total evaporation within a cube
    The fluxes are:
      evaporation from canopy
      evaporation from sea
      transpiration (this is exactly the same as evaporation from soil
                     I have checked some examples)
      sublim from surface
    """

    varnames_sec = ["EVAPORATION FROM SEA (GBM)   KG/M2/S",
                "TRANSPIRATION RATE           KG/M2/S"]
    
    varnames_ts = ["EVAP FROM CANOPY - AMOUNT   KG/M2/TS",
                "SUBLIM. FROM SURFACE (GBM)  KG/M2/TS"]
    
    
    for i, var in enumerate(varnames_sec):
        cube = iris.load_cube(filename_,var)
        cube = simplify_cube(cube)
        if i == 0:
            cubetot = cube
        else:
            cubetot = cubetot + cube
        
        
    for i, var in enumerate(varnames_ts):
        cube = iris.load_cube(filename_,var)
        cube = simplify_cube(cube)
        cube.data = cube.data / (30. * 60.)
        cube.units = 'kg m-2 s-1'
        cubetot = cubetot + cube
     
    return cubetot

#####################################
def extract_fields(filestart,expt,filetype,extra,startyear,endyear,timeperiod,
                   fileoutstart,varnamein,varnameout):

    # load required cubes
    #cubes=iris.load(filename)
    #print(cubes)
    #sys.exit(0)
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    
    
    # loop over years
    
   
    for year in range(startyear,endyear):
        allcubes=CubeList([])
        stringyear=np.str(year).zfill(2)
        for mon in range(0,len(monthnames)):
            print(mon)
            if format == '#':
                filename=(filestart + expt + filetype + 
                      extra + stringyear + monthnames[mon] + '+.nc')
            else:
                filename=(filestart + expt + filetype + 
                      extra + stringyear + monthnames[mon] + '.nc')
         
            if varnamein == 'TOTAL EVAPORATION':
                cube = get_evap(filename)
            else:
                print(filename,varnamein)
                cube=iris.load_cube(filename,varnamein)
                cube.coord('t').points=mon+1
            allcubes.append(cube)
            if mon==11:
                print(allcubes)
            
        #make sure the metadata on all cubes are the same
        iris.util.equalise_attributes(allcubes)
        catcube=allcubes.concatenate()[0]
        
        # if precipitation convert to mm/day
        if varnameout=='TotalPrecipitation':
            catcube.data=catcube.data * 60.*60.*24.
            catcube.long_name='TOTAL PRECIPITATION RATE    MM/DAY'
            catcube.units='mm/day'
       
        if varnameout=='evap':
            catcube.data=catcube.data * 60.*60.*24.
            catcube.long_name='TOTAL EVAPORATION    MM/DAY'
            catcube.units='mm/day'
      
        if varnameout=='so':
            catcube.data=(catcube.data * 1000.) + 35.
            catcube.long_name='SALMINTY (OCEAN)     (PSU)'
            catcube.units='ppt'
      
            
        # if temperature convert to Celcius
        print(varnameout)
        if varnameout=='SurfaceTemperature':
            print('convert to celcius')
            catcube.convert_units('celsius')
       
    
        stringyear=np.str(year).zfill(3)
        print(stringyear)
        fileout=fileoutstart+varnameout+'/'+timeperiod+'.'+varnameout+'.'+stringyear+'.nc'        
        iris.save(catcube,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
     
    
   

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning
            
fileextra = {
		"SURFACE TEMPERATURE AFTER TIMESTEP" : "a#pd",
        "TOTAL PRECIPITATION RATE     KG/M2/S" : "a#pd",
        "TEMPERATURE AT 1.5M": "a@pd",
        "OCN TOP-LEVEL TEMPERATURE          K" : "o@pf",
        "AICE : ICE CONCENTRATION" : "o@pf", 
                "TOTAL CLOUD AMOUNT - RANDOM OVERLAP" : "a#pd",
       
		"U COMPNT OF WIND ON PRESSURE LEVELS" : "a@pc",
		"V COMPNT OF WIND ON PRESSURE LEVELS" : "a@pc", 
        	"NET DOWN SURFACE SW FLUX: SW TS ONLY" : "a@pd",
		"NET DOWN SURFACE LW RAD FLUX" : "a@pd",
        	"SURFACE & B.LAYER HEAT FLUXES   W/M2" : "a@pd",
		"SURFACE LATENT HEAT FLUX        W/M2" : "a@pd",
        "OMEGA ON PRESSURE LEVELS" : "a@pc",
		"SPECIF HUM;P LEVS;U GRID.  USE MACRO" : "a@pc",
		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS" : "a@pc",
		"TEMPERATURE ON PRESSURE LEVELS" : "a@pc",
		"SURFACE LATENT HEAT FLUX        W/M2" : "a@pd",
        "DOWNWARD LW RAD FLUX: SURFACE" : "a@pd",
        "TOTAL DOWNWARD SURFACE SW FLUX" : "a@pd",
        "INCOMING SW RAD FLUX (TOA): ALL TSS" : "a@pd",
        "OUTGOING SW RAD FLUX (TOA)" : "a@pd",
        "OUTGOING LW RAD FLUX (TOA)" : "a@pd",
        "SURFACE & B.LAYER HEAT FLUXES   W/M2" : "a@pd",
		"PRESSURE AT MEAN SEA LEVEL" : "a@pd",
		"PSTAR AFTER TIMESTEP" : "a@pd",
		"TOTAL EVAPORATION" : "a@pd",
		"X-COMP OF SURF & BL WIND STRESS N/M2" : "a@pd",
                "Y-COMP OF SURF & BL WIND STRESS N/M2" : "a@pd",
		"TOTAL OCEAN U-VELOCITY      CM S**-1" : "o@pf",
		"TOTAL OCEAN V-VELOCITY      CM S**-1" : "o@pf",
		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S" : "o@pf",
                "POTENTIAL TEMPERATURE (OCEAN)  DEG.C" : "o@pf",
                "SALINITY (OCEAN)       (PSU-35)/1000" : "o@pf"
	}

shortname = {
		"SURFACE TEMPERATURE AFTER TIMESTEP" : "SurfaceTemperature",
        "TOTAL PRECIPITATION RATE     KG/M2/S" : "TotalPrecipitation",
        "TEMPERATURE AT 1.5M": "NearSurfaceTemperature",
        "TOTAL CLOUD AMOUNT - RANDOM OVERLAP" : "totcloud",
        "OCN TOP-LEVEL TEMPERATURE          K" : "SST",
        "AICE : ICE CONCENTRATION" : "SeaIceConc", 
		"U COMPNT OF WIND ON PRESSURE LEVELS" : "ua",
		"V COMPNT OF WIND ON PRESSURE LEVELS" : "va",
        	"NET DOWN SURFACE SW FLUX: SW TS ONLY" : "fsns",
		"NET DOWN SURFACE LW RAD FLUX" : "flns",
        "OMEGA ON PRESSURE LEVELS" : "wap",
		"SPECIF HUM;P LEVS;U GRID.  USE MACRO" : "spechumid",
		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS" : "zg",
		"TEMPERATURE ON PRESSURE LEVELS" : "ta",
		"SURFACE LATENT HEAT FLUX        W/M2" : "hfls",
        "DOWNWARD LW RAD FLUX: SURFACE" : "rlds",
        "TOTAL DOWNWARD SURFACE SW FLUX" : "rsds",
        "INCOMING SW RAD FLUX (TOA): ALL TSS" : "rsdt",
        "OUTGOING SW RAD FLUX (TOA)" : "rsut",
        "OUTGOING LW RAD FLUX (TOA)" : "rlut",
        "SURFACE & B.LAYER HEAT FLUXES   W/M2" : "surfheatflux",
        "CLEAR-SKY (II) UPWARD LW FLUX (TOA)" : "rlutcs",
                "CLEAR-SKY (II) UPWARD SW FLUX (TOA)" : "rsutcs",
		"PRESSURE AT MEAN SEA LEVEL" : "MSLP",
		"PSTAR AFTER TIMESTEP" : "ps",
		"TOTAL EVAPORATION" : "evap",
		"X-COMP OF SURF & BL WIND STRESS N/M2" : "tauu",
                "Y-COMP OF SURF & BL WIND STRESS N/M2" : "tauv",
		"TOTAL OCEAN U-VELOCITY      CM S**-1" : "uo",
		"TOTAL OCEAN V-VELOCITY      CM S**-1" : "vo",
		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S" : "wo",
                "POTENTIAL TEMPERATURE (OCEAN)  DEG.C" : "thetao",
                "SALINITY (OCEAN)       (PSU-35)/1000" : "so"
	}


exptname = {
        "e280" : "tenvo",
        "e400" : "tenvq",
        "e560":"tenvs",
        "eoi400" : "tenvj",
        "eoi350" : "tenvk",
        "eoi450" : "tenvl",
        "eoi280" : "tenvm"
      
}

extraname = {
        "e280" : "t",
        "e400" : "t",
        "eoi400" : "o",
        "eoi350" : "o",
        "eoi450" : "o",
        "eoi280" : "o",
        "e560" : "v",
        "xozza" : "p",
        "xozzb" : "o",
        "xozzc" : "o",
        "xozzd" : "o",
        "xozze" : "o",
        "xozzf" : "o",
        "xpkma" : "00000",
        "xpkmb" : "00000",
        "xpkmc" : "00000"}

#fieldname=["Y-COMP OF SURF & BL WIND STRESS N/M2","X-COMP OF SURF & BL WIND STRESS N/M2" 
#           ]
fieldname = ["SURFACE TEMPERATURE AFTER TIMESTEP",
#         "TOTAL PRECIPITATION RATE     KG/M2/S",
#        "TEMPERATURE AT 1.5M",
#         "OCN TOP-LEVEL TEMPERATURE          K",
        # "AICE : ICE CONCENTRATION", 
#          "TOTAL CLOUD AMOUNT - RANDOM OVERLAP",
       
#		"U COMPNT OF WIND ON PRESSURE LEVELS",
#		"V COMPNT OF WIND ON PRESSURE LEVELS",
#        	"NET DOWN SURFACE SW FLUX: SW TS ONLY",
#		"NET DOWN SURFACE LW RAD FLUX",
#        	"SURFACE & B.LAYER HEAT FLUXES   W/M2",
#		"SURFACE LATENT HEAT FLUX        W/M2",
#        "OMEGA ON PRESSURE LEVELS",
#		"SPECIF HUM;P LEVS;U GRID.  USE MACRO",
#		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS",
#		"TEMPERATURE ON PRESSURE LEVELS",
	#	"SURFACE LATENT HEAT FLUX        W/M2",
#        "DOWNWARD LW RAD FLUX: SURFACE",
#        "TOTAL DOWNWARD SURFACE SW FLUX",
#        "INCOMING SW RAD FLUX (TOA): ALL TSS",
#        "OUTGOING SW RAD FLUX (TOA)",
#        "OUTGOING LW RAD FLUX (TOA)",
#         "CLEAR-SKY (II) UPWARD SW FLUX (TOA)",
#         "CLEAR-SKY (II) UPWARD LW FLUX (TOA)",
#        "SURFACE & B.LAYER HEAT FLUXES   W/M2",
#		"PRESSURE AT MEAN SEA LEVEL",
#		"PSTAR AFTER TIMESTEP",
#		"TOTAL EVAPORATION",
#		"X-COMP OF SURF & BL WIND STRESS N/M2",
 #               "Y-COMP OF SURF & BL WIND STRESS N/M2",
#		"TOTAL OCEAN U-VELOCITY      CM S**-1",
#		"TOTAL OCEAN V-VELOCITY      CM S**-1",
#		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S"
#           "POTENTIAL TEMPERATURE (OCEAN)  DEG.C",
#             "SALINITY (OCEAN)       (PSU-35)/1000" 
	]
	       
#fieldname=["V COMPNT OF WIND ON PRESSURE LEVELS"]

linux_win='l'
startyear=0
endyear=100
timeperiod='xozzf'
expt=exptname.get(timeperiod,timeperiod)
extra=extraname.get(timeperiod)
format = '@'  # is the filename xxxxx#pd or xxxxx@pd

if linux_win=='w':
    filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\HadCM3\\'+exptname.get(timeperiod)+'/'
    fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\HadCM3_UPLOAD\\'+timeperiod+'/'
else:
    filestart='/nfs/hera1/earjcti/um/'+expt+'/pf/'
    fileoutstart='/nfs/hera1/earjcti/um/'+expt + '/'
    #fileoutstart='/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/'+timeperiod+'/'

for i in range(0,len(fieldname)):
    varnamein=fieldname[i]
    varnameout=shortname.get(varnamein)
    filetype=fileextra.get(varnamein,'a'+ format+ 'pd')

    extract_fields(filestart,expt,filetype,extra,startyear,endyear,timeperiod,fileoutstart,varnamein,varnameout)

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/extract_ipcc_data.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with all the data that we had in the supplementary and
have asked for it extracted as spreadsheet.  This program will do this. 

This is superceeded by calculate_land_sea_contrast.py

@author: julia
"""


import pandas as pd
import numpy as np
import sys

def get_global_temperature_anomaly():
    """
    
    extract model name and global_tanom from data_for_fig1a
    
    """
    
    data = [['CESM2',19.31,0.21,14.14,0.2],
            ['IPSLCM6A',16.02,0.23,12.54,0.25],
            ['COSMOS',16.83,0.48,13.5,0.46],
            ['EC-Earth3.3',18.17,0.15,13.33,0.19],
            ['CESM1.2',17.33,0.17,13.3,0.21],
            ['IPSLCM5A',14.38,0.22,12.09,0.27],
            ['MIROC4m',15.91,0.18,12.77,0.22],
            ['IPSLCM5A2',15.33,0.27,13.16,0.34],
            ['HadCM3',16.95,0.2,14.06,0.21],
            ['GISS2.1G',15.91,0.31,13.78,0.27],
            ['CCSM4',15.98,0.15,13.37,0.19],
            ['CCSM4-Utr',18.52,0.13,13.78,0.27],
            ['CCSM4-UoT',16.8,0.13,13.01,0.21],
            ['NorESM-L',14.57,0.13,12.48,0.14],
            ['MRI2.3',15.12,0.26,12.67,0.2],
            ['NorESM1-F',16.22,0.15,14.49,0.14]]
    
    modelnames = ['Simulated Temperature Changes', 'Fraction of Global Area']
    global_tanom = ['GLOBAL', 1.0]
    
    for row in data:
        modelnames.append(row[0])
        plio_temp = row[1]
        pi_temp = row[3]
        temp_anom = plio_temp - pi_temp
        global_tanom.append(temp_anom)
        
    
    return [modelnames, global_tanom]

def get_global_land_sea():
    """
    
    extract model name and global_tanom from data_for_fig3b
    
    """
    
    data = [['CESM2',6.58,4.69,4.23,3.52],
            ['IPSLCM6A',4.61,3.14,2.72,2.21],
            ['COSMOS',4.97,2.74,3.33,2.35],
            ['EC-Earth3.3',6.64,4.21,3.9,2.96],
            ['CESM1.2',5.09,3.72,2.89,2.46],
            ['IPSLCM5A',3.37,1.96,2.17,1.62],
            ['MIROC4m',4.63,2.6,2.95,2.14],
            ['IPSLCM5A2',3.15,1.87,1.99,1.51],
            ['HadCM3',4.47,2.34,2.97,1.69],
            ['GISS2.1G',2.57,2.08,1.46,1.09],
            ['CCSM4',3.51,2.36,1.64,1.42],
            ['CCSM4-Utr',5.6,4.5,2.9,2.67],
            ['CCSM4-UoT',4.77,3.51,2.32,2.13],
            ['NorESM-L',2.64,2.0,0.95,1.08],
            ['MRI2.3',3.71,2.04,2.04,1.42],
            ['NorESM1-F',2.52,1.52,1.13,1.07]]
    
    modelnames = ['Simulated Temperature Changes', 'Fraction of Global Area']
    global_landanom = ['GLOBAL (over land)', 1.0]
    global_seaanom = ['GLOBAL (over ocean)', 1.0]
    
    for row in data:
        modelnames.append(row[0])
        global_landanom.append(row[1])
        global_seaanom.append(row[2])
        
        
    land_dict = {} # set up dictionaries for new dataframe row
    sea_dict = {}
    for i, model in enumerate(modelnames):
        land_dict[model] = global_landanom[i]
        sea_dict[model] = global_seaanom[i]
    
        
    return [land_dict, sea_dict]


def get_latitude_bands_global(glob_l_s_ind):
    """
    data from data-for_supp_2
    """
    
    if glob_l_s_ind == 'g':
        data = [['CESM2',10.78,4.57,3.74,3.8,5.62,10.52],
                ['IPSLCM6A',8.68,2.87,2.14,2.5,3.79,7.73],
                ['COSMOS',6.87,2.01,2.45,2.76,3.88,7.26],
                ['EC-Earth3.3',8.45,3.37,3.07,3.67,6.64,11.36],
                ['CESM1.2',9.32,3.67,2.58,2.68,4.19,9.76],
                ['IPSLCM5A',4.11,1.29,1.5,2.03,3.04,5.18],
                ['MIROC4m',6.41,2.05,2.3,2.48,3.59,7.07],
                ['IPSLCM5A2',4.34,1.41,1.49,1.79,2.59,4.89],
                ['HadCM3',6.55,1.85,1.83,2.36,3.92,5.22],
                ['GISS2.1G',7.32,2.38,1.26,1.2,1.79,3.93],
                ['CCSM4',6.28,2.57,1.46,1.56,2.85,6.6],
                ['CCSM4-Utr',12.89,4.56,2.82,2.71,5.25,10.45],
                ['CCSM4-UoT',8.97,3.6,2.06,2.31,4.2,9.96],
                ['NorESM-L',7.59,2.26,1.11,1.06,1.66,4.82],
                ['MRI2.3',5.39,1.05,1.47,1.84,3.17,7.37],
                ['NorESM1-F',3.18,1.58,1.12,1.1,1.87,5.02]]
        titleend = ''
        frac = [0.067, 0.183, 0.25, 0.25, 0.183, 0.067]
        
    if glob_l_s_ind == 'l':
        data = [['CESM2',6.7,4.61,4.4,4.47,6.1,10.17],
                ['IPSLCM6A',4.55,2.61,2.6,3.01,4.03,7.34],
                ['COSMOS',5.82,2.11,3.9,3.31,3.92,7.32],
                ['EC-Earth3.3',5.24,3.92,4.13,4.68,7.14,10.7],
                ['CESM1.2',4.77,3.24,3.03,3.16,4.39,8.95],
                ['IPSLCM5A',1.63,1.61,1.98,2.5,2.97,4.97],
                ['MIROC4m',4.1,2.37,3.43,3.02,3.95,7.18],
                ['IPSLCM5A2',1.51,1.74,1.95,2.2,2.55,4.72],
                ['HadCM3',3.43,2.51,3.25,3.07,4.4,5.22],
                ['GISS2.1G',4.19,2.19,1.64,1.6,1.43,3.17],
                ['CCSM4',2.69,2.04,1.71,1.96,2.94,6.36],
                ['CCSM4-Utr',7.68,3.72,3.2,2.86,5.3,9.61],
                ['CCSM4-UoT',3.48,2.74,2.3,2.95,4.59,9.02],
                ['NorESM-L',5.31,1.71,1.14,0.94,1.71,4.67],
                ['MRI2.3',2.94,1.92,2.21,2.23,2.82,6.63],
                ['NorESM1-F',0.36,1.56,1.31,1.2,1.83,5.03]]
        titleend = ' (over land)'
        frac = [0.118, 0.312, 0.247, 0.196, 0.037, 0.09]
    
    if glob_l_s_ind == 's':
        data = [['CESM2',9.23,4.55,3.53,3.53,5.35,11.17],
                ['IPSLCM6A',6.91,2.87,2.02,2.33,3.87,8.69],
                ['COSMOS',3.85,1.98,2.01,2.55,4.04,7.36],
                ['EC-Earth3.3',6.53,3.31,2.77,3.27,6.41,12.45],
                ['CESM1.2',7.89,3.69,2.46,2.51,4.27,11.27],
                ['IPSLCM5A',1.78,1.25,1.36,1.86,3.37,5.09],
                ['MIROC4m',3.92,2.01,1.96,2.27,3.44,6.97],
                ['IPSLCM5A2',2.24,1.36,1.35,1.64,2.89,4.82],
                ['HadCM3',4.45,1.79,1.4,2.07,3.63,5.36],
                ['GISS2.1G',5.1,2.37,1.15,1.04,2.41,4.98],
                ['CCSM4',5.03,2.59,1.41,1.43,3.03,6.86],
                ['CCSM4-Utr',12.35,4.6,2.72,2.68,5.44,11.26],
                ['CCSM4-UoT',8.51,3.64,2.02,2.09,4.08,11.16],
                ['NorESM-L',5.25,2.28,1.11,1.15,1.91,4.96],
                ['MRI2.3',2.77,0.97,1.26,1.69,3.73,7.45],
                ['NorESM1-F',1.31,1.57,1.08,1.08,2.18,5.19]]
        titleend = ' (over sea)'
        frac = [0.045, 0.129, 0.251, 0.273, 0.244, 0.057]
    
    modelnames = ['Simulated Temperature Changes','Fraction of Global Area']
    l60S_90S = ['90-60oS ' + titleend, frac[5]]
    l30S_60S = ['60-30oS ' + titleend, frac[4]]
    l0_30S = ['30oS-0 ' + titleend, frac[3]]
    l30N_0N = ['0-30oN ' + titleend, frac[2]]
    l60N_30N = ['30-60oN ' + titleend, frac[1]]
    l90N_60N = ['60-90oN' + titleend, frac[0]]
    
    for row in data:
        modelnames.append(row[0])
        l60S_90S.append(row[1])
        l30S_60S.append(row[2])
        l0_30S.append(row[3])
        l30N_0N.append(row[4])
        l60N_30N.append(row[5])
        l90N_60N.append(row[6])
    
    dict_60S90S = {} # set up dictionaries for new dataframe row
    dict_30S60S = {}
    dict_0S30S = {}
    dict_30N0 = {}
    dict_60N30N = {}
    dict_90N60N = {}
    
    for i, model in enumerate(modelnames):
        print(l60S_90S[i], model, i)
        dict_60S90S[model] = l60S_90S[i]
        dict_30S60S[model] = l30S_60S[i]
        dict_0S30S[model] = l0_30S[i]
        dict_30N0[model] = l30N_0N[i]
        dict_60N30N[model] = l60N_30N[i]
        dict_90N60N[model] = l90N_60N[i]
    
        
    return [dict_60S90S, dict_30S60S, dict_0S30S, dict_30N0,
            dict_60N30N, dict_90N60N]

    
################################
modelnames, global_tanom = get_global_temperature_anomaly()
land_dfrow, sea_dfrow =  get_global_land_sea()
[glob_60S90S, glob_30S60S, glob_0S30S, glob_30N0,
            glob_60N30N, glob_90N60N] = get_latitude_bands_global('g')
[land_60S90S, land_30S60S, land_0S30S, land_30N0,
            land_60N30N, land_90N60N] = get_latitude_bands_global('l')
[sea_60S90S, sea_30S60S, sea_0S30S, sea_30N0,
            sea_60N30N, sea_90N60N] = get_latitude_bands_global('s')

#print(len(modelnames))
#print(len(global_tanom))
#print(modelnames)
#print(global_tanom)
print(land_dfrow)

# create the dataframe
df = pd.DataFrame([global_tanom], columns = modelnames, dtype=float)
df = df.append([land_dfrow], ignore_index=True)
df = df.append(sea_dfrow, ignore_index=True)
df = df.append(glob_90N60N,ignore_index= True)
df = df.append(land_90N60N,ignore_index= True)
df = df.append(sea_90N60N,ignore_index= True)
df = df.append(glob_60N30N, ignore_index= True)
df = df.append(land_60N30N, ignore_index= True)
df = df.append(sea_60N30N, ignore_index= True)
df = df.append(glob_30N0,ignore_index= True)
df = df.append(land_30N0,ignore_index= True)
df = df.append(sea_30N0,ignore_index= True)
df = df.append(glob_0S30S, ignore_index= True)
df = df.append(land_0S30S, ignore_index= True)
df = df.append(sea_0S30S, ignore_index= True)
df = df.append(glob_30S60S, ignore_index= True)
df = df.append(land_30S60S, ignore_index= True)
df = df.append(sea_30S60S, ignore_index= True)
df = df.append(glob_60S90S, ignore_index= True)
df = df.append(land_60S90S, ignore_index= True)
df = df.append(sea_60S90S, ignore_index= True)



# save dataframe as a csv file
df.to_csv('C:/Users/julia/OneDrive/WORK/MY_PAPERS/PlioMIP2/IPCC_box/' + 
          'mPWP_CMIP6_land_sea_by_latitude_jct.csv')
    
    


::::::::::::::
PlioMIP_new/large_scale_features/get_means_from_pliomip1_and_regrid.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Jul  5 15:11:26 2019
@author: earjcti

 This program will get the means from the PlioMIP1 models that
 can be added to the pliomip2 figures

#
 """

#import os
import warnings
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#import matplotlib as mpl
#import matplotlib.cm as cm
#from matplotlib.colors import Normalize
import numpy as np
import iris
from iris.cube import CubeList
#import xlwt
#from xlwt import Workbook

warnings.filterwarnings("ignore")


###############################################################################

class Getmodeldata:
    """
    get all of the data from the model with the required averaing
    ie e280 or eoi400
    """
    def __init__(self, field, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day",
            "NearSurfaceTemperature" : "degC"
                        }


        self.fieldname = field
        self.period = period
        if period == 'E280':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Ctrl_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Ctrl_landmasks_p3grid.nc'
        if period == 'EOI400':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Plio_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Plio_landmasks_p3grid.nc'


        self.units = fieldunits.get(field)



    def get_fieldreq(self, model_):
        """
        All fields are in a single file.  This will get the fieldname
        required via a number of dictionaries
        """


        PeriodE280Use = {
            "CCSM" : "ctrl", "COSMOS" : "Ctrl", "GISS" : "Ctrl",
            "HAD" : "ctrl", "IPSL" : "ctrl", "MIROC" : "ctrl",
            "MRI" : "ctrl", "NOR" : "ctrl"
                        }

        PeriodEoi400Use = {
            "CCSM" : "plio", "COSMOS" : "plio", "GISS" : "Plio",
            "HAD" : "plio", "IPSL" : "plio", "MIROC" : "plio",
            "MRI" : "plio", "NOR" : "plio"
                          }

        fieldname_sst = {
            "CCSM" : "sst", "COSMOS" : "SST", "GISS" : "SST",
            "HAD" : "sst", "IPSL" : "sst", "MIROC" : "sst",
            "MRI" : "sst", "NOR" : "sst"
                        }

        fieldname_sat = {
            "CCSM" : "sat", "COSMOS" : "sat", "GISS" : "SAT",
            "HAD" : "SAT", "IPSL" : "sat", "MIROC" : "sat",
            "MRI" : "sat", "NOR" : "sat"
                        }


        if FIELDNAME == 'NearSurfaceTemperature':
            fielduse = fieldname_sat.get(model_)
        if FIELDNAME == 'TotalPrecipitation':
            fielduse = 'precip'
        if FIELDNAME == 'SST':
            fielduse = fieldname_sst.get(model_)

        if self.period == 'E280':
            fieldreq = (model_ + '_' +
                             PeriodE280Use.get(model_) + '_' + fielduse)
        if self.period == 'EOI400':
            fieldreq = (model_ + '_' +
                             PeriodEoi400Use.get(model_) + '_' + fielduse)


        return fieldreq

    def extract_cube(self, allcube_, fieldreq_):
        """
        will extract the cube from the list of cubes
        this is needed because the cube comes from varname not long name
        """
        ncubes = len(allcube_)

        # look for the cube
        cube_found = 'n'
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]
                cube_found = 'y'

        # if cube not found then change the capitalisation of the first
        # letter of the time period (ie plio ==> Plio,  Plio ==> plio)
        if cube_found == 'n':
            newstring = ' '
            if fieldreq_.find("Plio") > 0:
                newstring = fieldreq_.replace("Plio", "plio")
            if fieldreq_.find("plio") > 0:
                newstring = fieldreq_.replace("plio", "Plio")
            if fieldreq_.find("ctrl") > 0:
                newstring = fieldreq_.replace("ctrl", "Ctrl")
            if fieldreq_.find("Ctrl") > 0:
                newstring = fieldreq_.replace("Ctrl", "ctrl")
            for i in range(0, ncubes):
                if allcube_[i].var_name == newstring:
                    cube = allcube_[i]
                    cube_found = 'y'

        cube = iris.util.squeeze(cube)

        return cube

    def get_lsm(self, modname, datacube):
        """
        gets the lsm from the lsm file
        """

        alllsm = iris.load(self.lsm_file)
        ncubes = len(alllsm)
        fieldreq = modname + '_landmask'
        cube_found = 'n'

        time = {'E280': 'Ctrl',
                'EOI400' : 'Plio'}
        for i in range(0, ncubes):
            if alllsm[i].var_name == fieldreq:
                cube = alllsm[i]
                cube = iris.util.squeeze(cube)
                cube_found = 'y'

        # if cube not found then add the time period before the landseamask
        if cube_found == 'n':
            newstring = fieldreq.replace("landmask",
                                         time.get(self.period) + "_landmask")
            for i in range(0, ncubes):
                if alllsm[i].var_name == newstring:
                    cube = alllsm[i]
                    cube_found = 'y'

        if cube_found == 'n': #cube still not found then error
            print('cannot find lsm ' + fieldreq + ' or ' + newstring)
            sys.exit(0)

        # check coords of lsm cube are the same as the
        # coordinates of the data cube
        if np.array_equal(datacube.coord('longitude').points,
                          cube.coord('longitude').points):
            pass

        else:
            print('longitudes dont match')
            sys.exit(0)

        if np.array_equal(datacube.coord('latitude').points,
                          cube.coord('latitude').points):
            pass
        else:
            print('latitudes dont match')
            sys.exit(0)


        return np.squeeze(cube.data)

    def get_globalmean(self, cube):
        """
        calculates the area weighted global mean from the cube given
        """
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)

        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas)
        if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 270.:
            meancube.data = meancube.data - 273.15

        return meancube.data, grid_areas

    def get_latmean(self, cube, grid_areas, nbands):
        """
        calculates the area weighted mean in latitude bounds
        from the cube given
        """

        model_latbands = np.zeros((nbands))
        for boundno, thisbound in enumerate(LATBANDS):
            grid_areas_thisbound = np.zeros((np.shape(grid_areas)))
            for j, lat in enumerate(cube.coord('latitude').points):
                if thisbound[0] <= lat < thisbound[1]:
                    if cube.ndim == 3:
                        grid_areas_thisbound[:, j, :] = grid_areas[:, j, :]
                    if cube.ndim == 4:
                        grid_areas_thisbound[:, :, j, :] = grid_areas[:, :, j, :]
                    if cube.ndim == 2:
                        grid_areas_thisbound[j, :] = grid_areas[j, :]

            meancube = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_thisbound)

            if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 250.:
                meancube.data = meancube.data - 273.15

            model_latbands[boundno] = meancube.data


        return model_latbands

    def get_land_sea_means(self, cube, grid_areas, modname):
        """
        extract the land and the sea anomaly
        """

        # get lsm
        lsm = self.get_lsm(modname, cube)


        # get grid_areas (_20 means from 20N-20S)
        grid_areas_land = grid_areas * lsm
        grid_areas_sea = grid_areas - grid_areas_land
        grid_areas_land_20 = np.zeros(np.shape(grid_areas))
        grid_areas_sea_20 = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if -20 <= lat <= 20:
                grid_areas_land_20[j, :] = grid_areas_land[j, :]
                grid_areas_sea_20[j, :] = grid_areas_sea[j, :]

        mean_land = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_land)
        mean_sea = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_sea)
        mean_land_20 = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_land_20)
        mean_sea_20 = cube.collapsed(['longitude', 'latitude'],
                                     iris.analysis.MEAN,
                                     weights=grid_areas_sea_20)

        if FIELDNAME == 'NearSurfaceTemperature' and mean_land.data > 250.:
            mean_land.data = mean_land.data - 273.15
            mean_sea.data = mean_sea.data - 273.15
            mean_land_20.data = mean_land_20.data - 273.15
            mean_sea_20.data = mean_sea_20.data - 273.15

        return (mean_land.data, mean_sea.data, mean_land_20.data,
                mean_sea_20.data)

    def get_highlatmeans(self, cube, grid_areas, latval):
        """
        gets the means polewards of latval(north) and polewards of latval(S)
        """
        grid_areas_NH = np.zeros(np.shape(grid_areas))
        grid_areas_SH = np.zeros(np.shape(grid_areas))
        for j, lat in enumerate(cube.coord('latitude').points):
            if lat >= latval:
               grid_areas_NH[j, :] = grid_areas[j, :]
            if lat <= (-1.0) * latval:
                grid_areas_SH[j, :] = grid_areas[j, :]
                
        mean_NH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_NH)
        mean_SH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_SH)

        return (mean_NH.data, mean_SH.data)

    def extract_means(self):
        """
        the top level function for this class.  This will return all the
        means to the main program
        """
        # arrays to store data
        meanvals = np.zeros(len(MODELNAMES))
        meanvals_land = np.zeros(len(MODELNAMES))
        meanvals_sea = np.zeros(len(MODELNAMES))
        meanvals_land_20 = np.zeros(len(MODELNAMES))
        meanvals_sea_20 = np.zeros(len(MODELNAMES))
        meanvals_NH45 = np.zeros(len(MODELNAMES))
        meanvals_SH45 = np.zeros(len(MODELNAMES))
        meanvals_NH60 = np.zeros(len(MODELNAMES))
        meanvals_SH60 = np.zeros(len(MODELNAMES))

        nbands, nlims = np.shape(LATBANDS)
        latmeans = np.zeros((len(MODELNAMES), nbands))

        filename = (FILESTART + 'PlioMIP1_regridded.nc')
        allcubes = iris.load(filename)
        regridded_cubes = CubeList([])

        for i, model in enumerate(MODELNAMES):


            fieldreq = self.get_fieldreq(model)

            # extract the cube we want
            cube = self.extract_cube(allcubes, fieldreq)
            cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
            rg_cube = cube.regrid(cubegrid, iris.analysis.Nearest())
            if FIELDNAME == 'SST':
                newdata = np.where(rg_cube.data < 1E20, rg_cube.data, -999.999)
                newdata2 = np.where(newdata > -500, newdata, -999.999)
                regridded_cube= rg_cube.copy(data=newdata2)
                regridded_cubes.append(regridded_cube)
            else:
                regridded_cubes.append(rg_cube)
            
            meanvals[i], grid_areas = self.get_globalmean(cube)
            latmeans[i, :] = self.get_latmean(cube, grid_areas, nbands)

            (meanvals_land[i],
             meanvals_sea[i],
             meanvals_land_20[i],
             meanvals_sea_20[i]) = self.get_land_sea_means(cube, grid_areas, model)
            
            (meanvals_NH45[i], 
             meanvals_SH45[i]) = self.get_highlatmeans(cube,grid_areas, 45.0)
            
            (meanvals_NH60[i], 
             meanvals_SH60[i]) = self.get_highlatmeans(cube,grid_areas, 60.0)


        return (meanvals, latmeans, meanvals_land,
                meanvals_sea, meanvals_land_20, meanvals_sea_20,
                meanvals_NH45, meanvals_SH45,
                meanvals_NH60, meanvals_SH60, regridded_cubes)




# end of class Getmodeldata
##################################################################################
def get_nh_seascyc():
    """
    get the NH seasonal cycle from each model
    """


    shortfield = {"NearSurfaceTemperature" : "SAT",
                  "TotalPrecipitation" : "precip"}
    longfield_sat = {"CCSM" : "Reference height temperature",
                     "COSMOS" : "2m temperature",
                     "IPSL" : "t2m",
                     "MIROC": "tas",
                     "MRI" : "near surface air temperature [degC]"
                     }

    longfield_precip = {
        "CCSM" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "COSMOS" : "total precipitation",
        "IPSL" : "IPSL_precip",
        "MIROC": "MIROC_precip",
        "MRI" : "total precipitation [mm/day]"
                       }
    nh_means = np.zeros((len(MODELNAMES), 12))

    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + '/' + FIELDNAME + '/' +
                    model + '_Exp2_anom_' +
                    shortfield.get(FIELDNAME) + '_p3grid.nc')

        if FIELDNAME == "NearSurfaceTemperature":
            fieldreq = longfield_sat.get(model, "air_temperature")
        if FIELDNAME == "TotalPrecipitation":
            fieldreq = longfield_precip.get(model, "precipitation_flux")

        cube = iris.load_cube(filename, fieldreq)
        cube = iris.util.squeeze(cube)

        # get grid areas for seasonal average
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)
        grid_areas_nh = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if lat > 0:
                grid_areas_nh[:, j, :] = grid_areas[:, j, :]

        # get seasonal average
        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_nh)

        nh_means[i, :] = meancube.data
        plt.plot(meancube.data, label=model)
    plt.plot(np.mean(nh_means, axis=0), label='mean')
    plt.legend()
    #plt.show()

    return nh_means


def write_global_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write global means from each model to a text file

    """

    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)

    filetext.write("modelname, global mean EOI400," +
                   "global mean E280, anomaly \n")

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(np.mean(modelmean_eoi400), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_e280), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_anomaly), 3)) + '\n')



def write_lat_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write the latitudinal mean from each model to a textfile
    """


    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)
    mean_eoi400 = np.mean(modelmean_eoi400, axis=0)
    mean_e280 = np.mean(modelmean_e280, axis=0)
    mean_anomaly = mean_eoi400 - mean_e280

    filetext.write("modelname, latband mean EOI400," +
                   "latband mean E280, latband anomaly \n")
    filetext.write("bands are " + np.str(LATBANDS) + '\n')

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(mean_eoi400, 3)) + ',' +
                   np.str(np.around(mean_e280, 3)) +',' +
                   np.str(np.around(mean_anomaly, 3)) + '\n')

def write_nh_seascycle(filetext, seasanom):
    """
    write the NH seasonal anomaly from each model to a textfile
    """


    filetext.write("modelname, [jan feb mar apr may jun jul aug sep oct nov dec] \n")
    mean_anomaly = np.mean(seasanom, axis=0)

    for i, model in enumerate(MODELNAMES_FULL):
        anom = np.str(np.around(seasanom[i], 3))

        filetext.write((model + ',' + anom + '\n'))

    filetext.write('MEAN,' +  np.str(np.around(mean_anomaly, 3)) + '\n')

def write_landsea_means(filetext, landmean_eoi400_allmodels,
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, region):
    """
    write the land sea averages to a text file
    """
    filetext.write("modelname" + region + "[mean_ocean_eoi400, meanocean_e280, meanocean_anom, " +
                   "mean_land_eoi400, mean_land_e280, mean_land_anom ] \n")

    eoi400_sea_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels), 3))
    e280_sea_mean = np.str(np.around(np.mean(seamean_e280_allmodels), 3))
    eoi400_land_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels), 3))
    e280_land_mean = np.str(np.around(np.mean(landmean_e280_allmodels), 3))
    sea_anom_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels)
                                     - np.mean(seamean_e280_allmodels), 3))
    land_anom_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels)
                                      - np.mean(landmean_e280_allmodels), 3))

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400_sea = np.str(np.around(seamean_eoi400_allmodels[i], 3))
        e280_sea = np.str(np.around(seamean_e280_allmodels[i], 3))
        eoi400_land = np.str(np.around(landmean_eoi400_allmodels[i], 3))
        e280_land = np.str(np.around(landmean_e280_allmodels[i], 3))
        sea_anom = np.str(np.around((seamean_eoi400_allmodels[i]
                                     - seamean_e280_allmodels[i]), 3))
        land_anom = np.str(np.around((landmean_eoi400_allmodels[i]
                                      - landmean_e280_allmodels[i]), 3))


        filetext.write(model + ',' + eoi400_sea + ',' + e280_sea + ',' + sea_anom +
                       ',' + eoi400_land + ',' + e280_land + ',' + land_anom + '\n')

    filetext.write('MEAN,' + eoi400_sea_mean + ',' + e280_sea_mean + ',' + sea_anom_mean +
                   ',' + eoi400_land_mean + ',' + e280_land_mean + ',' + land_anom_mean + '\n')

def write_hemisphere_anom(filetext, NH_anomaly45,SH_anomaly45,
                          NH_anomaly60, SH_anomaly60):
    """
    writes the anomalies polewards of 45N and 45S and 60S and 60N
    """
    filetext.write("modelname, 45N-90N_anom, 45S-90S_anom, 60N-90N_anom, " +
                   "60S-90S_anom \n")
    for i, model in enumerate(MODELNAMES_FULL):

        filetext.write(model + ',' + np.str(np.around(NH_anomaly45[i], 3)) + ',' +
                       np.str(np.around(SH_anomaly45[i],3)) + ',' +
                       np.str(np.around(NH_anomaly60[i],3)) + ',' +
                       np.str(np.around(SH_anomaly60[i],3)) + '\n')
    filetext.write('MEAN,' + np.str(np.around(np.mean(NH_anomaly45), 3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly45),3)) + ',' +
                   np.str(np.around(np.mean(NH_anomaly60),3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly60),3)) +  '\n')


def write_regridded(regriddedcubes,exptname):
    """
    write the data on a 1x1 grid to a file
    """
    fileoutstart = FILESTART + FIELDNAME + '_regridded/'
    for i, model in enumerate(MODELNAMES_FULL):
        filename = (fileoutstart + FIELDNAME + '_' + model + '_' + 
                    exptname + '.nc')
        iris.save(regriddedcubes[i], filename, netcdf_format="NETCDF3_CLASSIC",
                  fill_value = -999.999)

    

    
  
def main():

    # get data means and latitudinal anomalies
    modeldata = Getmodeldata(FIELDNAME, 'EOI400')
    (globalmean_eoi400_allmodels,
     latmean_eoi400_allmodels,
     landmean_eoi400_allmodels,
     seamean_eoi400_allmodels,
     landmean20_eoi400_allmodels,
     seamean20_eoi400_allmodels,
     NH45_eoi400_anomaly, SH45_eoi400_anomaly,
     NH60_eoi400_anomaly, SH60_eoi400_anomaly,
     regridded_eoi400_cubes) = modeldata.extract_means()

    modeldata = Getmodeldata(FIELDNAME, 'E280')
    (globalmean_e280_allmodels,
     latmean_e280_allmodels,
     landmean_e280_allmodels,
     seamean_e280_allmodels,
     landmean20_e280_allmodels,
     seamean20_e280_allmodels,
     NH45_e280_anomaly, SH45_e280_anomaly,
     NH60_e280_anomaly, SH60_e280_anomaly,
     regridded_e280_cubes) = modeldata.extract_means()


    # write regridded data
    write_regridded(regridded_eoi400_cubes,'Expt2')
    write_regridded(regridded_e280_cubes,'Cntl')
    sys.exit(0)


    # get seasonal cycle
    NH_seas_anom = get_nh_seascyc()


    # write to text

    filetext = open((FILESTART + '/means_for_' + FIELDNAME + '.txt'), "w+")
    write_global_means(filetext, globalmean_eoi400_allmodels,
                       globalmean_e280_allmodels)
    write_lat_means(filetext, latmean_eoi400_allmodels,
                    latmean_e280_allmodels)
    write_nh_seascycle(filetext, NH_seas_anom)
    write_landsea_means(filetext, landmean_eoi400_allmodels, # global land sea
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, "global")
    write_landsea_means(filetext, landmean20_eoi400_allmodels, # global land sea
                        seamean20_eoi400_allmodels, landmean20_e280_allmodels,
                        seamean20_e280_allmodels, "20N-20S")
    write_hemisphere_anom(filetext, NH45_eoi400_anomaly - NH45_e280_anomaly,
                          SH45_eoi400_anomaly - SH45_e280_anomaly,
                          NH60_eoi400_anomaly - NH60_e280_anomaly,
                          SH60_eoi400_anomaly - SH60_e280_anomaly)

    filetext.close()

   
####################################
# definitions

LINUX_WIN = 'l'
#FIELDNAME = 'TotalPrecipitation'
#FIELDNAME = 'NearSurfaceTemperature'
FIELDNAME = 'SST'
if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/PLIOMIP_old/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\PLIOMIP1\\'

MODELNAMES = ['CCSM', 'COSMOS', 'GISS', 'HAD', 'IPSL',
              'MIROC', 'MRI', 'NOR']

CONVERT_MODELS = {'CCSM' : 'CCSM4',
                  'GISS' : 'GISS-E2-R',
                  'HAD'  : 'HadCM3',
                  'IPSL' : 'IPSLCM5A',
                  'MIROC': 'MIROC4m',
                  'MRI' : 'MRI2.3',
                  'NOR' : 'NORESM-L'}

MODELNAMES_FULL = []
for MOD in MODELNAMES:
    MODELNAMES_FULL.append(CONVERT_MODELS.get(MOD, MOD))

LATBANDS = [[-90., -60.], [-60., -30.], [-30., 0.], [0., 30],
            [30., 60.], [60., 90.]]


main()

::::::::::::::
PlioMIP_new/large_scale_features/get_means_from_pliomip1.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Jul  5 15:11:26 2019
@author: earjcti

 This program will get the means from the PlioMIP1 models that
 can be added to the pliomip2 figures

#
 """

#import os
import warnings
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#import matplotlib as mpl
#import matplotlib.cm as cm
#from matplotlib.colors import Normalize
import numpy as np
import iris
#import xlwt
#from xlwt import Workbook

warnings.filterwarnings("ignore")


###############################################################################

class Getmodeldata:
    """
    get all of the data from the model with the required averaing
    ie e280 or eoi400
    """
    def __init__(self, field, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day",
            "NearSurfaceTemperature" : "degC"
                        }


        self.fieldname = field
        self.period = period
        if period == 'E280':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Ctrl_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Ctrl_landmasks_p3grid.nc'
        if period == 'EOI400':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Plio_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Plio_landmasks_p3grid.nc'


        self.units = fieldunits.get(field)



    def get_fieldreq(self, model_):
        """
        All fields are in a single file.  This will get the fieldname
        required via a number of dictionaries
        """


        PeriodE280Use = {
            "CCSM" : "ctrl", "COSMOS" : "Ctrl", "GISS" : "Ctrl",
            "HAD" : "ctrl", "IPSL" : "ctrl", "MIROC" : "ctrl",
            "MRI" : "ctrl", "NOR" : "ctrl"
                        }

        PeriodEoi400Use = {
            "CCSM" : "plio", "COSMOS" : "plio", "GISS" : "Plio",
            "HAD" : "plio", "IPSL" : "plio", "MIROC" : "plio",
            "MRI" : "plio", "NOR" : "plio"
                          }

        fieldname_sst = {
            "CCSM" : "sst", "COSMOS" : "SST", "GISS" : "SST",
            "HAD" : "sst", "IPSL" : "sst", "MIROC" : "sst",
            "MRI" : "sst", "NOR" : "sst"
                        }

        fieldname_sat = {
            "CCSM" : "sat", "COSMOS" : "sat", "GISS" : "SAT",
            "HAD" : "SAT", "IPSL" : "sat", "MIROC" : "sat",
            "MRI" : "sat", "NOR" : "sat"
                        }


        if FIELDNAME == 'NearSurfaceTemperature':
            fielduse = fieldname_sat.get(model_)
        if FIELDNAME == 'TotalPrecipitation':
            fielduse = 'precip'

        if self.period == 'E280':
            fieldreq = (model_ + '_' +
                             PeriodE280Use.get(model_) + '_' + fielduse)
        if self.period == 'EOI400':
            fieldreq = (model_ + '_' +
                             PeriodEoi400Use.get(model_) + '_' + fielduse)


        return fieldreq

    def extract_cube(self, allcube_, fieldreq_):
        """
        will extract the cube from the list of cubes
        this is needed because the cube comes from varname not long name
        """
        ncubes = len(allcube_)

        # look for the cube
        cube_found = 'n'
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]
                cube_found = 'y'

        # if cube not found then change the capitalisation of the first
        # letter of the time period (ie plio ==> Plio,  Plio ==> plio)
        if cube_found == 'n':
            newstring = ' '
            if fieldreq_.find("Plio") > 0:
                newstring = fieldreq_.replace("Plio", "plio")
            if fieldreq_.find("plio") > 0:
                newstring = fieldreq_.replace("plio", "Plio")
            if fieldreq_.find("ctrl") > 0:
                newstring = fieldreq_.replace("ctrl", "Ctrl")
            if fieldreq_.find("Ctrl") > 0:
                newstring = fieldreq_.replace("Ctrl", "ctrl")
            for i in range(0, ncubes):
                if allcube_[i].var_name == newstring:
                    cube = allcube_[i]
                    cube_found = 'y'

        cube = iris.util.squeeze(cube)

        return cube

    def get_lsm(self, modname, datacube):
        """
        gets the lsm from the lsm file
        """

        alllsm = iris.load(self.lsm_file)
        ncubes = len(alllsm)
        fieldreq = modname + '_landmask'
        cube_found = 'n'

        time = {'E280': 'Ctrl',
                'EOI400' : 'Plio'}
        for i in range(0, ncubes):
            if alllsm[i].var_name == fieldreq:
                cube = alllsm[i]
                cube = iris.util.squeeze(cube)
                cube_found = 'y'

        # if cube not found then add the time period before the landseamask
        if cube_found == 'n':
            newstring = fieldreq.replace("landmask",
                                         time.get(self.period) + "_landmask")
            for i in range(0, ncubes):
                if alllsm[i].var_name == newstring:
                    cube = alllsm[i]
                    cube_found = 'y'

        if cube_found == 'n': #cube still not found then error
            print('cannot find lsm ' + fieldreq + ' or ' + newstring)
            sys.exit(0)

        # check coords of lsm cube are the same as the
        # coordinates of the data cube
        if np.array_equal(datacube.coord('longitude').points,
                          cube.coord('longitude').points):
            pass

        else:
            print('longitudes dont match')
            sys.exit(0)

        if np.array_equal(datacube.coord('latitude').points,
                          cube.coord('latitude').points):
            pass
        else:
            print('latitudes dont match')
            sys.exit(0)


        return np.squeeze(cube.data)

    def get_globalmean(self, cube):
        """
        calculates the area weighted global mean from the cube given
        """
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)

        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas)
        if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 270.:
            meancube.data = meancube.data - 273.15

        return meancube.data, grid_areas

    def get_latmean(self, cube, grid_areas, nbands):
        """
        calculates the area weighted mean in latitude bounds
        from the cube given
        """

        model_latbands = np.zeros((nbands))
        for boundno, thisbound in enumerate(LATBANDS):
            grid_areas_thisbound = np.zeros((np.shape(grid_areas)))
            for j, lat in enumerate(cube.coord('latitude').points):
                if thisbound[0] <= lat < thisbound[1]:
                    if cube.ndim == 3:
                        grid_areas_thisbound[:, j, :] = grid_areas[:, j, :]
                    if cube.ndim == 4:
                        grid_areas_thisbound[:, :, j, :] = grid_areas[:, :, j, :]
                    if cube.ndim == 2:
                        grid_areas_thisbound[j, :] = grid_areas[j, :]

            meancube = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_thisbound)

            if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 250.:
                meancube.data = meancube.data - 273.15

            model_latbands[boundno] = meancube.data


        return model_latbands

    def get_land_sea_means(self, cube, grid_areas, modname):
        """
        extract the land and the sea anomaly
        """

        # get lsm
        lsm = self.get_lsm(modname, cube)


        # get grid_areas (_20 means from 20N-20S)
        grid_areas_land = grid_areas * lsm
        grid_areas_sea = grid_areas - grid_areas_land
        grid_areas_land_20 = np.zeros(np.shape(grid_areas))
        grid_areas_sea_20 = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if -20 <= lat <= 20:
                grid_areas_land_20[j, :] = grid_areas_land[j, :]
                grid_areas_sea_20[j, :] = grid_areas_sea[j, :]

        mean_land = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_land)
        mean_sea = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_sea)
        mean_land_20 = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_land_20)
        mean_sea_20 = cube.collapsed(['longitude', 'latitude'],
                                     iris.analysis.MEAN,
                                     weights=grid_areas_sea_20)

        if FIELDNAME == 'NearSurfaceTemperature' and mean_land.data > 250.:
            mean_land.data = mean_land.data - 273.15
            mean_sea.data = mean_sea.data - 273.15
            mean_land_20.data = mean_land_20.data - 273.15
            mean_sea_20.data = mean_sea_20.data - 273.15

        return (mean_land.data, mean_sea.data, mean_land_20.data,
                mean_sea_20.data)

    def get_highlatmeans(self, cube, grid_areas, latval):
        """
        gets the means polewards of latval(north) and polewards of latval(S)
        """
        grid_areas_NH = np.zeros(np.shape(grid_areas))
        grid_areas_SH = np.zeros(np.shape(grid_areas))
        for j, lat in enumerate(cube.coord('latitude').points):
            if lat >= latval:
               grid_areas_NH[j, :] = grid_areas[j, :]
            if lat <= (-1.0) * latval:
                grid_areas_SH[j, :] = grid_areas[j, :]
                
        mean_NH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_NH)
        mean_SH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_SH)

        return (mean_NH.data, mean_SH.data)

    def extract_means(self):
        """
        the top level function for this class.  This will return all the
        means to the main program
        """
        # arrays to store data
        meanvals = np.zeros(len(MODELNAMES))
        meanvals_land = np.zeros(len(MODELNAMES))
        meanvals_sea = np.zeros(len(MODELNAMES))
        meanvals_land_20 = np.zeros(len(MODELNAMES))
        meanvals_sea_20 = np.zeros(len(MODELNAMES))
        meanvals_NH45 = np.zeros(len(MODELNAMES))
        meanvals_SH45 = np.zeros(len(MODELNAMES))
        meanvals_NH60 = np.zeros(len(MODELNAMES))
        meanvals_SH60 = np.zeros(len(MODELNAMES))

        nbands, nlims = np.shape(LATBANDS)
        latmeans = np.zeros((len(MODELNAMES), nbands))

        filename = (FILESTART + 'PlioMIP1_regridded.nc')
        allcubes = iris.load(filename)

        for i, model in enumerate(MODELNAMES):


            fieldreq = self.get_fieldreq(model)

            # extract the cube we want
            cube = self.extract_cube(allcubes, fieldreq)
            meanvals[i], grid_areas = self.get_globalmean(cube)
            latmeans[i, :] = self.get_latmean(cube, grid_areas, nbands)

            (meanvals_land[i],
             meanvals_sea[i],
             meanvals_land_20[i],
             meanvals_sea_20[i]) = self.get_land_sea_means(cube, grid_areas, model)
            
            (meanvals_NH45[i], 
             meanvals_SH45[i]) = self.get_highlatmeans(cube,grid_areas, 45.0)
            
            (meanvals_NH60[i], 
             meanvals_SH60[i]) = self.get_highlatmeans(cube,grid_areas, 60.0)


        return (meanvals, latmeans, meanvals_land,
                meanvals_sea, meanvals_land_20, meanvals_sea_20,
                meanvals_NH45, meanvals_SH45,
                meanvals_NH60, meanvals_SH60)




# end of class Getmodeldata
##################################################################################
def get_nh_seascyc():
    """
    get the NH seasonal cycle from each model
    """


    shortfield = {"NearSurfaceTemperature" : "SAT",
                  "TotalPrecipitation" : "precip"}
    longfield_sat = {"CCSM" : "Reference height temperature",
                     "COSMOS" : "2m temperature",
                     "IPSL" : "t2m",
                     "MIROC": "tas",
                     "MRI" : "near surface air temperature [degC]"
                     }

    longfield_precip = {
        "CCSM" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "COSMOS" : "total precipitation",
        "IPSL" : "IPSL_precip",
        "MIROC": "MIROC_precip",
        "MRI" : "total precipitation [mm/day]"
                       }
    nh_means = np.zeros((len(MODELNAMES), 12))

    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + '/' + FIELDNAME + '/' +
                    model + '_Exp2_anom_' +
                    shortfield.get(FIELDNAME) + '_p3grid.nc')

        if FIELDNAME == "NearSurfaceTemperature":
            fieldreq = longfield_sat.get(model, "air_temperature")
        if FIELDNAME == "TotalPrecipitation":
            fieldreq = longfield_precip.get(model, "precipitation_flux")

        cube = iris.load_cube(filename, fieldreq)
        cube = iris.util.squeeze(cube)

        # get grid areas for seasonal average
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)
        grid_areas_nh = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if lat > 0:
                grid_areas_nh[:, j, :] = grid_areas[:, j, :]

        # get seasonal average
        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_nh)

        nh_means[i, :] = meancube.data
        plt.plot(meancube.data, label=model)
    plt.plot(np.mean(nh_means, axis=0), label='mean')
    plt.legend()
    #plt.show()

    return nh_means


def write_global_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write global means from each model to a text file

    """

    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)

    filetext.write("modelname, global mean EOI400," +
                   "global mean E280, anomaly \n")

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(np.mean(modelmean_eoi400), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_e280), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_anomaly), 3)) + '\n')



def write_lat_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write the latitudinal mean from each model to a textfile
    """


    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)
    mean_eoi400 = np.mean(modelmean_eoi400, axis=0)
    mean_e280 = np.mean(modelmean_e280, axis=0)
    mean_anomaly = mean_eoi400 - mean_e280

    filetext.write("modelname, latband mean EOI400," +
                   "latband mean E280, latband anomaly \n")
    filetext.write("bands are " + np.str(LATBANDS) + '\n')

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(mean_eoi400, 3)) + ',' +
                   np.str(np.around(mean_e280, 3)) +',' +
                   np.str(np.around(mean_anomaly, 3)) + '\n')

def write_nh_seascycle(filetext, seasanom):
    """
    write the NH seasonal anomaly from each model to a textfile
    """


    filetext.write("modelname, [jan feb mar apr may jun jul aug sep oct nov dec] \n")
    mean_anomaly = np.mean(seasanom, axis=0)

    for i, model in enumerate(MODELNAMES_FULL):
        anom = np.str(np.around(seasanom[i], 3))

        filetext.write((model + ',' + anom + '\n'))

    filetext.write('MEAN,' +  np.str(np.around(mean_anomaly, 3)) + '\n')

def write_landsea_means(filetext, landmean_eoi400_allmodels,
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, region):
    """
    write the land sea averages to a text file
    """
    filetext.write("modelname" + region + "[mean_ocean_eoi400, meanocean_e280, meanocean_anom, " +
                   "mean_land_eoi400, mean_land_e280, mean_land_anom ] \n")

    eoi400_sea_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels), 3))
    e280_sea_mean = np.str(np.around(np.mean(seamean_e280_allmodels), 3))
    eoi400_land_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels), 3))
    e280_land_mean = np.str(np.around(np.mean(landmean_e280_allmodels), 3))
    sea_anom_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels)
                                     - np.mean(seamean_e280_allmodels), 3))
    land_anom_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels)
                                      - np.mean(landmean_e280_allmodels), 3))

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400_sea = np.str(np.around(seamean_eoi400_allmodels[i], 3))
        e280_sea = np.str(np.around(seamean_e280_allmodels[i], 3))
        eoi400_land = np.str(np.around(landmean_eoi400_allmodels[i], 3))
        e280_land = np.str(np.around(landmean_e280_allmodels[i], 3))
        sea_anom = np.str(np.around((seamean_eoi400_allmodels[i]
                                     - seamean_e280_allmodels[i]), 3))
        land_anom = np.str(np.around((landmean_eoi400_allmodels[i]
                                      - landmean_e280_allmodels[i]), 3))


        filetext.write(model + ',' + eoi400_sea + ',' + e280_sea + ',' + sea_anom +
                       ',' + eoi400_land + ',' + e280_land + ',' + land_anom + '\n')

    filetext.write('MEAN,' + eoi400_sea_mean + ',' + e280_sea_mean + ',' + sea_anom_mean +
                   ',' + eoi400_land_mean + ',' + e280_land_mean + ',' + land_anom_mean + '\n')

def write_hemisphere_anom(filetext, NH_anomaly45,SH_anomaly45,
                          NH_anomaly60, SH_anomaly60):
    """
    writes the anomalies polewards of 45N and 45S and 60S and 60N
    """
    filetext.write("modelname, 45N-90N_anom, 45S-90S_anom, 60N-90N_anom, " +
                   "60S-90S_anom \n")
    for i, model in enumerate(MODELNAMES_FULL):

        filetext.write(model + ',' + np.str(np.around(NH_anomaly45[i], 3)) + ',' +
                       np.str(np.around(SH_anomaly45[i],3)) + ',' +
                       np.str(np.around(NH_anomaly60[i],3)) + ',' +
                       np.str(np.around(SH_anomaly60[i],3)) + '\n')
    filetext.write('MEAN,' + np.str(np.around(np.mean(NH_anomaly45), 3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly45),3)) + ',' +
                   np.str(np.around(np.mean(NH_anomaly60),3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly60),3)) +  '\n')

def main():

    # get data means and latitudinal anomalies
    modeldata = Getmodeldata(FIELDNAME, 'EOI400')
    (globalmean_eoi400_allmodels,
     latmean_eoi400_allmodels,
     landmean_eoi400_allmodels,
     seamean_eoi400_allmodels,
     landmean20_eoi400_allmodels,
     seamean20_eoi400_allmodels,
     NH45_eoi400_anomaly, SH45_eoi400_anomaly,
     NH60_eoi400_anomaly, SH60_eoi400_anomaly) = modeldata.extract_means()

    modeldata = Getmodeldata(FIELDNAME, 'E280')
    (globalmean_e280_allmodels,
     latmean_e280_allmodels,
     landmean_e280_allmodels,
     seamean_e280_allmodels,
     landmean20_e280_allmodels,
     seamean20_e280_allmodels,
     NH45_e280_anomaly, SH45_e280_anomaly,
     NH60_e280_anomaly, SH60_e280_anomaly) = modeldata.extract_means()

    # get seasonal cycle
    NH_seas_anom = get_nh_seascyc()


    # write to text

    filetext = open((FILESTART + '/means_for_' + FIELDNAME + '.txt'), "w+")
    write_global_means(filetext, globalmean_eoi400_allmodels,
                       globalmean_e280_allmodels)
    write_lat_means(filetext, latmean_eoi400_allmodels,
                    latmean_e280_allmodels)
    write_nh_seascycle(filetext, NH_seas_anom)
    write_landsea_means(filetext, landmean_eoi400_allmodels, # global land sea
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, "global")
    write_landsea_means(filetext, landmean20_eoi400_allmodels, # global land sea
                        seamean20_eoi400_allmodels, landmean20_e280_allmodels,
                        seamean20_e280_allmodels, "20N-20S")
    write_hemisphere_anom(filetext, NH45_eoi400_anomaly - NH45_e280_anomaly,
                          SH45_eoi400_anomaly - SH45_e280_anomaly,
                          NH60_eoi400_anomaly - NH60_e280_anomaly,
                          SH60_eoi400_anomaly - SH60_e280_anomaly)

    filetext.close()


####################################
# definitions

LINUX_WIN = 'l'
#FIELDNAME = 'TotalPrecipitation'
FIELDNAME = 'NearSurfaceTemperature'
if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/PLIOMIP_old/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\PLIOMIP1\\'

MODELNAMES = ['CCSM', 'COSMOS', 'GISS', 'HAD', 'IPSL',
              'MIROC', 'MRI', 'NOR']

CONVERT_MODELS = {'CCSM' : 'CCSM4',
                  'GISS' : 'GISS-E2-R',
                  'HAD'  : 'HadCM3',
                  'IPSL' : 'IPSLCM5A',
                  'MIROC': 'MIROC4m',
                  'MRI' : 'MRI2.3',
                  'NOR' : 'NORESM-L'}

MODELNAMES_FULL = []
for MOD in MODELNAMES:
    MODELNAMES_FULL.append(CONVERT_MODELS.get(MOD, MOD))

LATBANDS = [[-90., -60.], [-60., -30.], [-30., 0.], [0., 30],
            [30., 60.], [60., 90.]]


main()

::::::::::::::
PlioMIP_new/large_scale_features/globalmean_timeseries.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 21 2020

#
# This program will produce a regridded 1X1degree timeseries of a given field.  
# We will remove the annual cycle in order to look for interannual variability 
# etc.  


import numpy as np
from netCDF4 import Dataset
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
from scipy import stats
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname, lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse == "sst":
        if exptname == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'

        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data = np.ma.array(cube.data, mask=mymask)
    return cube

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u = unit.Unit('days since 0800-01-01 00:00:00',
                      calendar=unit.CALENDAR_360_DAY)
        if model == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model == 'HadCM3' and fielduse == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model == 'HadCM3' and fielduse == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube = cube_temp.extract(iris.Constraint(surface=0.))

    return cube

def get_noresm_ocn(exptnamein, fieldnamein):
    """
    get noresm ocean things.  

    """
    
    print('need to do this')
    print('it is in monthly data and on a tripolar grid')
    sys.exit(0)

    return cube

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src, Dataset("temporary.nc", "w", format='NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name, src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name, dimension in src.dimensions.items():

            if name != 'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name, (len(dimension)))

        # copy all file data
        for name, variable in src.variables.items():
            print('name is', name, variable)
            print('datatype',variable.datatype)
            print('dimensions',variable.dimensions)
            if name not in ('time_counter_bnds', 'time_centered'):
                x = dst.createVariable(name, variable.datatype,
                                       variable.dimensions)
                if name == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:] = ((src.variables[name][:] / (60.*60.*24))
                                              -(src.variables[name][0] / (60.*60.*24)) + 15.)
                else:
                    dst.variables[name][:] = src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    #print(ncattr, attribute, exptname)
                    if ncattr == 'calendar' and exptname == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr == 'units' and name == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            if ncattr != "_FillValue":
                                dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein in ('ts', 'tas'):
            cube.convert_units('Celsius')

        if exptname == 'Eoi400':
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_360_DAY)
        else:
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return cube

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return cube

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return cube

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return cube


def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """

    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]

    # put units as celcius if required
    if fielduse == 'tas':
        cube.units = 'Celsius'

    cube2 = iris.util.new_axis(cube, 'time')

    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """

    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

    cube.add_dim_coord(iris.coords.DimCoord(points,
                                            standard_name='time', long_name='time',
                                            var_name='time',
                                            units=u,
                                            bounds=None,
                                            coord_system=None, circular=False), 0)

    if fieldnamein == 'pr':
        cube.data = cube.data * 60. *60. *24. *1000.
        cube.name = 'Total precipitation'
        cube.long_name = 'Total precipitation'
        cube.units = 'mm/day'


    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]

    return singlecube

def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """

    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
        cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
        cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
        if fieldnameout == 'TotalPrecipitation':
            cube = cube1 + cube2
        if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
            cube1.convert_units('Celsius')
            cubedata = np.where((cube2.data > 0.01),
                                -1.8, cube1.data)
            cube = cube1.copy(data=cubedata)
    else:
        cube = get_cesm12_singlecube(filename, fielduse)

    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b.e21.B1850.f09_g17.' +
                           'PMIP4-midPliocene-eoi400.001.'+
                           'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' +
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280':
            #if modelname == 'CESM2':
            #    filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
           #                'b.e21.B1850.f09_g17.' +
           #                'CMIP6-piControl.001.cam.h0.'+
           #                'LANDFRAC.1300.1399.nc')
           # else:
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                       'b.e12.B1850.f09_g16.preind.' +
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data = np.ma.array(cube.data, mask=mymask)


    return cube



######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube

    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years


    """

    if modelname in ('CCSM4', 'CESM1.2', 'CESM2','NorESM1-F','NorESM-L'):
        print('CCSM',cube.coord('month').points)
        months = cube.coord('month').points
        months = months -1
        for i, month in enumerate(months):
            if month == 0: months[i] = 12
        print(months)
        cube.coord('month').points = months
 
   


    return cube

######################################################
def cube_avg(cube):
    """
    Extract global annual averaged data from an array

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanyear (numpy array): the global mean of the field

    """

    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    
    meanyearcube.coord('latitude').guess_bounds()
    meanyearcube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(meanyearcube)
    
    yearglobavg_cube = (meanyearcube.collapsed(['longitude', 'latitude'],
                                       iris.analysis.MEAN, weights = grid_areas))
    global_avg = yearglobavg_cube.data
  

    return yearglobavg_cube.data




##############################################
def get_timeseries_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

    # outfile
    if linux_win == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/' + modelname +
                    '/timeseries/' + exptnameout + '.' + fieldnameout + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
                    + modelname + '\\timeseries\\' + exptnameout 
                    + '.' + fieldnameout + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


    #####################################
    # get all data in a single cube
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3'): # all fields in one file
        cube = get_ecearth_cube(exptnamein, lsmstart)
    elif modelname in ('HadCM3', 'MRI2.3'):
        cube = get_hadcm3_cube(modelname)
    elif modelname in ('IPSLCM5A', 'IPSLCM5A2') and fieldnamein != 'tos':
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif modelname in ('NorESM1-F', 'NorESM-L') and fieldnamein == 'tos':
        cube = get_noresm_ocn(exptnamein, fieldnamein)
    elif modelname == 'IPSLCM6A':
        cube = get_ipslcm6()
    elif modelname in ('MIROC4m', 'tos'):
        cube = get_miroc_tos()
    elif modelname == 'GISS2.1G':
        cube = get_giss()
    elif modelname == 'CCSM4-Utr':
        cube = get_ccsm4_2deg()
    elif modelname in ('CESM1.2', 'CCSM4', 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube = get_ccsm4_uot(fieldnamein)
    else:
        cube = iris.load_cube(filename)

    

    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid

    if ((modelname == 'CCSM4-UoT')
            or (modelname == 'IPSLCM5A' and fieldnamein == 'tos')
            or (modelname == 'IPSLCM5A2' and fieldnamein == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A',
                     'IPSLCM5A2', 'IPSLCM6A', 'CCSM4-Utr', 'GISS2.1G'):
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if modelname in ('NorESM1-F', 'NorESM-L', 'CESM1.2', 'CESM2', 'CCSM4'):

       # if precipitation is in m/s convert to mm/day
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. * 60. * 24. * 1000.
            cube.data = cube.data * 60. * 60. * 24. * 1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if modelname in ('CCSM4-UoT', 'NorESM1-F', 'NorESM-L', 'IPSLCM6A',
                     'EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A', 'IPSLCM5A2',
                     'HadCM3', 'GISS2.1G'):
         # convert to celcius
        if fieldnamein in ('tas', 'tos'):
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname in ('COSMOS', 'MIROC4m', 'IPSLCM6A', 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate


    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube, 'time', name='month')
    iris.coord_categorisation.add_year(regridded_cube, 'time', name='year')


     # correct the start month if required
    regridded_cube = correct_start_month(regridded_cube)
    
    # calculate averages
    mean_year_array = cube_avg(regridded_cube)
    
    return mean_year_array




#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                     }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }

    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }

    NorESM_FIELDS = {"pr" : "PRECT",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "sst"
                    }

    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                      }

    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA = {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                    "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }

    CESM2_EXTRA = {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                   "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                   }

    CCSM4_EXTRA = {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                   "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                   }

    ECearth_EXPT = {"Eoi400": "mPlio",
                    "E280":"PI"
                   }

    CESM12_EXPT = {"Eoi400": "PlioMIP2",
                   "E280":"PI"
                   }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
                     "E280":"PI"
                    }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }

    CESM2_TIME = {"E280" : ".110001-120012",
                  "Eoi400" : ".1101.1200"
                 }

    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
                     "E280":"3600_3699"
                    }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
                       "E280":"6110_6209",
                      }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
                     "E280":"piControl_r1i1p1f1_gr_285001-304912",
                     }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                         "E280":"piControl_r1i1p1f1_gn_285001-304912",
                        }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
                  "E280":"piControl_r1i1p1f1_gn_490101-495012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
                 }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
                       "E280":"piControl_r1i1p1f1_gr1_150101-160012",
                       "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                      }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
                  "E280":"piControl_r1i1p1f1_gn_495101-500012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                  }
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"}
    cosmos_version = {"tas": "",
                      "pr": "",
                      "tos":"_remapbil"}

    # get names for each model
    if modelname == 'MIROC4m':
        filename = filestart + modelname + '/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename + fielduse + '/MIROC4m_'+exptnamein
                    + '_' + atm_ocn_ind.get(fieldnamein) + '_' + fielduse + '.nc')
    if modelname == 'COSMOS':
        if linux_win == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename + exptnamein + '.' + fielduse +
                    '_CMIP6_name_' + fieldnamein +
                    '_2650-2749_monthly_mean_time_series' +
                    cosmos_version.get(fieldnamein) + '.nc')
    if modelname == 'CCSM4-UoT':
        if linux_win == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' +
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart + 'UofT-CCSM4\\' + exptnamein + '\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)

        filename = (filename + fielduse +
                    '_' + atm_ocn_ind.get(fieldnamein) +
                    '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart + 'LEEDS/HadCM3/' + exptuse + '/' + fielduse + '/'
                    + exptuse + '.' + fielduse + '.')
    if modelname == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                    +exptuse+'.'+fielduse+'.')
    if modelname == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname
                    + '_'
                    + ECearth_EXPT.get(exptnamein)
                    + fileend)
    if modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse 
                        + '_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse + '_monthly_TS.nc')

    if modelname == 'NorESM1-F' or modelname == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' +fielduse + '/'
                    + modelname + '_' + exptnamein + '.' + fielduse + '.')
        else:
            filename = (filestart + modelname + '/' + modelname + '_' +
                    exptnamein + '_' + fielduse + '.nc')
    if modelname == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' + fielduse +
                        '_Omon_IPSL-CM6A-LR_' + IPSLCM6A_TIME_ALT.get(exptnamein) 
                        + '_rectilinear.nc')
        else:
            filename = (filestart + modelname + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) + '_IPSL-CM6A-LR_' 
                        + IPSLCM6A_TIME.get(exptnamein) + '.nc')
    if modelname == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart + modelname + '/' + exptuse + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME1.get(exptnamein)
                        + '.nc')
        filename.append(filestart + modelname + '/' + exptuse + '/' 
                        + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME2.get(exptnamein) + '.nc')

    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein

    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                      CESM2_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECL' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']

        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'TS' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'ICEFRAC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']

        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' +
                      CCSM4_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)


    retdata = [fielduse, filename]
    return retdata

def checkdrift(plio_ts, pi_ts):
    """
    this program needs a pliocene timeseries and a preindustrial timeseries
    it will calculate the linear regression and use the slope to find
    the expected drift over 100 years (in the format ??degC / century)
    
    it will do this for the pliocene the preindustrial and the anomaly
    """
    
    nyears = len(plio_ts)
    allyears = np.arange(0.0, nyears, 1.0)
    nyears_pi = len(pi_ts)
    allyears_pi = np.arange(0.0, nyears_pi, 1.0)
    nyears_min = np.min([nyears, nyears_pi])
    allyears_min = np.arange(0.0, nyears_min, 1.0)
     
    print(nyears, nyears_pi, nyears_min)
  
    print(modelname)
    print('=======')
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears_min, plio_ts[0:nyears_min])
    print('pliocene drift = ' + np.str(np.around((slope * 100.),2)) 
          + 'dec C / centuary')
    
    
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears_min, pi_ts[0:nyears_min])
    print('pi drift = ' + np.str(np.around((slope * 100.), 2))
          + 'dec C / centuary')
    
    anomaly = plio_ts[0:nyears_min] - pi_ts[0:nyears_min]
    allyears = np.arange(0.0, nyears_min, 1.0)
    
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears, anomaly)
    print('anomaly drift = ' +  np.str(np.around((slope * 100.),2) )
              + 'dec C / centuary')


##########################################################
# main program

filename =  ' '
linux_win =  'l'
modelname = "CESM2" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"
            }

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"
            }

fieldname = {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']

fieldnamein = 'tas'
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


if (modelname in ('IPSLCM5A', 'IPSLCM5A2', 'CCSM4-Utr') 
    and fieldnamein == 'tos'):
        filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
if modelname in ('IPSLCM6A', 'GISS2.1G'):
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/'

# call program to get model dependent names
# fielduse,  and  filename and process for eoi400
fielduse, filename = getnames(modelname, filestart, fieldnamein, 'Eoi400')
fieldnameout = fieldname.get(fieldnamein)
exptnameout = exptname.get('Eoi400')
plio_timeseries = get_timeseries_data(fieldnamein, 'Eoi400')

# call program to get model dependent names
# fielduse,  and  filename and process for e280
fielduse, filename = getnames(modelname, filestart, fieldnamein, 'E280')
fieldnameout = fieldname.get(fieldnamein)
exptnameout = exptname.get('E280')
pi_timeseries = get_timeseries_data(fieldnamein, 'E280')

checkdrift(plio_timeseries, pi_timeseries)




::::::::::::::
PlioMIP_new/large_scale_features/means_all_models_pre_hadgem.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import iris.util
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

import sys


def plotmean_newaxis(cube, modelno_):
     tempcube=iris.util.new_axis(cube)
     tempcube.add_dim_coord(iris.coords.DimCoord(modelno_, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
     return tempcube
 

def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
         
    newcube = plotmean_newaxis(cube, [levelno])
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    
        
    return newcube
###########################################
def get_NH_mean(modelname, expt, field):
     """
     gets the mean of the NH for model - modelname
                             and expt - experiment name (ie PI)
     returns a numpy array of length 12 with the average for each month
     """
     # read data into iris cube
     filename = (FILESTART + modelname + 
                  '/' + expt + '.' + field + 
                  '.mean_month.nc')
     
     if (modelname == 'GISS2.1G' or modelname == 'IPSLCM6A'
         or (modelname == 'IPSLCM5A' and field == 'SST')
         or (modelname == 'IPSLCM5A2' and field == 'SST')
         or (modelname == 'NorESM-L' and field == 'SST')
         or (modelname == 'NorESM1-F' and field == 'SST')):
          cubes = iris.load(filename)
          cube = cubes[0]
     else:
          cube = iris.load_cube(filename, field)
        
     # get weights and average over NH
     cube.coord('latitude').guess_bounds()
     cube.coord('longitude').guess_bounds()
     grid_areas = iris.analysis.cartography.area_weights(cube)
     grid_areas_nh = np.zeros(grid_areas.shape)
     for j, lat in enumerate(cube.coord('latitude').points):
          if lat > 0:
               grid_areas_nh[:, j, :] = grid_areas[:, j, :]

     cube_nh = (cube.collapsed(['longitude', 'latitude'],
                iris.analysis.MEAN, weights = grid_areas_nh))
     
     return cube_nh.data
 
def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART[:-10] + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    
    # means are at the start of the file
    allanoms_list = []
    for i in range(1, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break    # we have now got all the means
        modname, eoi400, e280, anom = line.split(',')
        if modname == 'MEAN':
            meananom=anom
        else:
            allanoms_list.append(anom)
            
    # find line which contains 'jan feb mar ' which is the start of the seasonal cycle
    string = 'jan feb mar'
    min_seas_cyc = np.zeros(12) + 1000.
    max_seas_cyc = np.zeros(12) - 1000.
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        modname, anomstr = line.split(',')
        anom_arr = np.array(anomstr.strip('[]').split(), dtype=float)
        if modname == 'MEAN':
            mean_seas_cyc = anom_arr
        else:
            for j in range(0,12):
                min_seas_cyc[j] = np.min([anom_arr[j], min_seas_cyc[j]]) 
                max_seas_cyc[j] = np.max([anom_arr[j], max_seas_cyc[j]]) 
    

    allanoms = np.asarray(allanoms_list, dtype=float)
    
        
    return allanoms, min_seas_cyc, max_seas_cyc, mean_seas_cyc
  
   

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units,individual_plot):
   
    
    
    names = {"EOI400" : "Plio_Core",
             "E280" : "PI_Ctrl"
            }
    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }
    
    # set up lists to store all values for both experiment and control    
    model_global_mean=[]
    model_global_sd=[]
    
    # store months in a numpy array (model, experiment,month)
    monmeans=np.zeros((len(modelnames),2,12))   
    monsd=np.zeros((len(modelnames),2,12))
    
    # store latitudes in a numpy array (model, experiment,latitude)
    latmeans=np.zeros((len(modelnames),2,180))   
    latsd=np.zeros((len(modelnames),2,180))
    lats=np.zeros(180)
     
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        filenames=[]
        filenames.append(FILESTART+modeluse+'/'+exptname+'.'+field+'.data.txt')
        filenames.append(FILESTART+modeluse+'/'+cntlname+'.'+field+'.data.txt')
       
        # set up temporary lists to store data from each experiment
        means=[]
        sds=[]
        nmon=12
       
        for fileno in range(0,len(filenames)):
            
            f=open(filenames[fileno],"r")
            f1=f.readlines()
            f2 = [x.replace('\n', '') for x in f1]
            
            # get the means according to their position in the file
            all_mean_sd=f2[2]
            all_mon_mean_sd=f2[5:5+12]
            all_lat_mean_sd=f2[20:20+180]
           
            # extract global mean
            mean,sd=all_mean_sd.split(',')
            means.append(mean)
            sds.append(sd)
            
            # extract monthly means 
            for x in all_mon_mean_sd:
                mon,mean,sd=x.split(',')
                monmeans[modelno,fileno,int(mon)-1]=float(mean)
                monsd[modelno,fileno,int(mon)-1]=float(sd)
            
            
            # extract latitude means
            for x in all_lat_mean_sd:
                lat,mean,sd=x.split(',')
                latss=int(float(lat)+89.5) # convert latitude to a subscript
               
                if mean != ' --' and mean != '--':
                    latmeans[modelno,fileno,latss]=float(mean) # stores latitudinal means
                else:
                    latmeans[modelno,fileno,latss]=np.nan
                if sd != ' --' and sd != '--' :
                    latsd[modelno,fileno,latss]=float(sd)
                else:
                    latsd[modelno,fileno,latss]=np.nan
                lats[latss]=lat # stores latitudes
            
            
        model_global_mean.append(means)
        model_global_sd.append(sds)
           
    ############################################################
    # get the monthly means for the NH
    # 
    # store months in a numpy array (model, experiment,month)
    # ss 1 is experiment ss 2 is control

    monmeans_NH = np.zeros((len(modelnames),2,12))   
    monsd = np.zeros((len(modelnames),2,12))

    for i, model in enumerate(modelnames):
         expt_mon_mean = get_NH_mean(model, exptname, field)
         monmeans_NH[i, 0, :] = expt_mon_mean

         cntl_mon_mean = get_NH_mean(model, cntlname, field)
         monmeans_NH[i, 1, :] = cntl_mon_mean

    
    #===============================================================
    # if pliomip1 is set get pliomip1 data
    if PLIOMIP1 == 'y':
        (mean_pliomip1, min_seas_pliomip1,
         max_seas_pliomip1, mean_seas_pliomip1) = get_pliomip1_data(field)
    
    #############################################################
    # plot the global mean and error bars from each model.
    
   
    expt_global_mean=[float(item[0]) for item in model_global_mean]
    expt_global_2sigma=([float(item[0])*2.0 for item in model_global_sd])

    cntl_global_mean=[float(item[1]) for item in model_global_mean]
    cntl_global_2sigma=([float(item[1])*2.0 for item in model_global_sd])
  
    
   
   
    #fig,ax=plt.subplots(2,1,1)
    ax=plt.subplot(2,1,1)
   
    ax.errorbar(modelnames,expt_global_mean,
                yerr=expt_global_2sigma,fmt='x',label=names.get(exptname))
    ax.errorbar(modelnames,cntl_global_mean,
                yerr=cntl_global_2sigma,fmt='x',label=names.get(cntlname))
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.15*box.height), box.width * 0.8, box.height])
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    titlename='Global mean ' + namefield.get(field)
    plt.title(titlename)
    ax.tick_params(axis='x',labelbottom='False')
    plt.ylabel(units)
    
    
    
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)],'x')
    print('means',[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    anomalies = [x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)]
    print('multimodelmean = ',
          np.mean(expt_global_mean) - np.mean(cntl_global_mean), np.mean(anomalies))
    print('multimodelmedian = ', np.median(anomalies))
    print('percentiles 10/50/90',np.percentile(anomalies, 10),
          np.percentile(anomalies,50), np.percentile(anomalies,90))
    sys.exit(0)
    
    sorted_anomalies = np.sort(anomalies)
    print(sorted_anomalies)
    #print('means % change',[((x1 - x2) *100 / x2) for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    #print('multimodelmean = ',(np.mean(expt_global_mean) - np.mean(cntl_global_mean)) *100. / np.mean(cntl_global_mean))
     # if pliomip1 is set overplot pliomip1 means as grey horizontal bars
    if (PLIOMIP1 == 'y' and field !='SST'):
        for mean_mod in mean_pliomip1:
            ax.axhline(y=mean_mod, color='grey', alpha=0.4)
    # Shrink axis as appropriate
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, box.height*0.8])
    plt.title('Global mean ' + namefield.get(field) + ' anomaly')
    plt.ylabel(units)
    plt.xticks(rotation='90', fontsize=8)
    #plt.xticks(x, labels, rotation='vertical')
   

    fileout=FILESTART+'allplots/'+field+'/global_means.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/global_means.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out data
    
    txtfile1 = open(FILEOUT,"w+")
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_5a \n")
        
    txtfile1.write('modelname, Pliocore_global_mean, pliocore_global_mean_2sigma, ' + 
                   'picntl_global_mean, picntl_global_mean_2sigma \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + np.str(np.around(expt_global_mean[i],2)) + 
                       ',' + np.str(np.around(expt_global_2sigma[i],2)) + 
                       ',' + np.str(np.around(cntl_global_mean[i],2)) + 
                       ',' + np.str(np.around(cntl_global_2sigma[i],2)) + '\n'))
        
    txtfile1.write('\n')
    
      
    
    ################################################################
    #  plot the NH seasonal cycle from each model
   

    plt.subplot(2,1,1)
    for i in range(0,len(monmeans_NH[:,0,0])):
        # plot experiment data
       
        plt.plot(monmeans_NH[i,0,:],color='r')
        # plot control data
        plt.plot(monmeans_NH[i,1,:],color='b')
        plt.title('NH annual cycle of '+field)
        plt.ylabel(units)
        
    plt.subplot(2,1,2)
    plt.plot(np.mean(monmeans_NH[:,0,:],axis=0),label=exptname,color='r')
    plt.plot(np.mean(monmeans_NH[:,1,:],axis=0),label=cntlname,color='b')
    plt.ylabel(units)
    plt.xlabel('month')    
    plt.legend()
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.pdf'
    plt.savefig(fileout)
    plt.close
    
    
    ax=plt.subplot(1,1,1)
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    for i in range(0,len(monmeans_NH[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
           ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i])
        else:
            ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i],
                    linestyle='dashed')
        
        
    ax.plot(labels,np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0),
        color='black',linestyle='dashed',linewidth=2,label='avg')
    
    print('monthmeans',np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0))
   
    # plot pliomip1 data if appropriate
    if PLIOMIP1 == 'y':
        ax.plot(labels, mean_seas_pliomip1, color='black', linestyle='dotted',
                linewidth=2, label='PlioMIP1')
        ax.fill_between(labels, min_seas_pliomip1, max_seas_pliomip1, alpha=0.2, 
                        color="grey")
        
    
    plt.title(exptname+'-'+cntlname+': NH '+ field + ' anomaly')
    plt.ylabel(units)
    #plt.xlabel('month') 
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' NH anomaly')
    
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_3a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_6a \n")
       
        
    txtfile1.write('modelname, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + 
                        np.str(np.around((monmeans_NH[i,0,0] - monmeans_NH[i,1,0]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,1] - monmeans_NH[i,1,1]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,2] - monmeans_NH[i,1,2]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,3] - monmeans_NH[i,1,3]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,4] - monmeans_NH[i,1,4]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,5] - monmeans_NH[i,1,5]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,6] - monmeans_NH[i,1,6]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,7] - monmeans_NH[i,1,7]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,8] - monmeans_NH[i,1,8]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,9] - monmeans_NH[i,1,9]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,10] - monmeans_NH[i,1,10]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,11] - monmeans_NH[i,1,11]),2)) + '\n'))
        
    txtfile1.write('\n')
    txtfile1.close

    ###################################################################
    # plot the latitudinal range from each model
    
    # won't print this out as it doesn't look very useful.
    # absolute value of temperature by latitude
    #plt.subplot(2,1,1)
    #for i in range(0,len(latmeans[:,0,0])):
    #    # plot experiment data
    #   
    #    plt.plot(latmeans[i,0,:],lats,color='r')
    #    # plot control data
    #    plt.plot(latmeans[i,1,:],lats,color='b')
    #    plt.title('latitudinal average of '+field)
    #    plt.xlabel(units)
    #    
    #plt.subplot(2,1,2)
    #plt.plot(np.mean(latmeans[:,0,:],axis=0),lats,label=exptname,color='r')
    #plt.plot(np.mean(latmeans[:,1,:],axis=0),lats,label=cntlname,color='b')
    #plt.xlabel(units)
    #plt.ylabel('latitude')    
    #plt.legend()
    
    
    ax=plt.subplot(1,1,1)
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    for i in range(0,len(latmeans[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i])
        else:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i],
                    linestyle='dashed')
    ax.plot(np.mean(latmeans[:,0,:],axis=0)-np.mean(latmeans[:,1,:],axis=0),
             lats,label='avg',color='black',linestyle='dashed',linewidth=2)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' anomaly')
    plt.xlabel(units)
    plt.ylabel('latitude') 
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
  
     # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1c \n")
    elif field == 'SST':
        txtfile1.write("zonal mean data for SST \n")
   
   
        for i, model in enumerate(modelnames):
            txtfile1.write(model + '\n')
            txtfile1.write('latitude, Tanom \n')
            for j, lat in enumerate(lats):
                txtfile1.write(np.str(np.around(lat,2)) + ',' + 
                               np.str(np.around(latmeans[i,0,j] - latmeans[i,1,j],2)) + '\n')
        
        txtfile1.write('\n')
        
    txtfile1.close
   
    #########################################################
    # plot a picture of the monthly anomaly from each model
    
    
    if field=='SurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='NearSurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='SST':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -5.
            datamax = 32.
            dataincr = 2.
    if field=='TotalPrecipitation':
            valmin=-1.4
            valmax=1.6
            incr=0.2
            cmapname='RdBu'
            
            datamin = 0.
            datamax = 5.
            dataincr = 0.1
    Vanom=np.arange(valmin,valmax,incr)
    Vdata= np.arange(datamin, datamax, dataincr)
    
   
    
    #########################################################
    # plot a picture of the change from each model and the 
    # multimodelmean
    
    
    anom_cubes=iris.cube.CubeList([])
    expt_cubes=iris.cube.CubeList([])
    cntl_cubes=iris.cube.CubeList([])
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        
        exptcube=iris.load_cube(exptfile)
        cntlcube=iris.load_cube(cntlfile)
       
       
        if modeluse == 'EC-Earth3.1' and field == 'SST':
           cntlcube.coord('latitude').bounds = None
           cntlcube.coord('longitude').bounds = None
           
        if modeluse == 'CCSM4-UoT' and field == 'TotalPrecipitation':
           cntlcube.coord('latitude').var_name = 'latitude'
           cntlcube.coord('longitude').var_name = 'longitude'
           exptcube.coord('latitude').var_name = 'latitude'
           exptcube.coord('longitude').var_name = 'longitude'
           cntlcube.coord('latitude').long_name = None
           cntlcube.coord('longitude').long_name = None
           exptcube.coord('latitude').long_name = None
           exptcube.coord('longitude').long_name = None
           cntlcube.coord('latitude').points = cntlcube.coord('latitude').points.astype('float32')
           cntlcube.coord('longitude').points = cntlcube.coord('longitude').points.astype('float32')
           exptcube.coord('latitude').points =  exptcube.coord('latitude').points .astype('float32')
           exptcube.coord('longitude').points = exptcube.coord('longitude').points.astype('float32')
        
        diffcube=exptcube-cntlcube
        
        # check float 32 for concatenation
        diffcube.data=diffcube.data.astype('float32') 
        exptcube.data=exptcube.data.astype('float32') 
        cntlcube.data=cntlcube.data.astype('float32') 
        if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                diffcube.units='Celsius'
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                diffcube.convert_units('Celsius')
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')
        
        # remove scalar coordinates so that we can concatenate
        # also add a new axis with the model number
        
        newcube = resort_coords(diffcube,modelno)
        newdata = newcube.data
        anom_cubes.append(newcube)    
    
        newcube = resort_coords(exptcube,modelno)
        newcube.rename(field)
        print(modeluse)
        print('MINIMUM',np.min(newcube.data))
        expt_cubes.append(newcube)
        
        newcube = resort_coords(cntlcube,modelno)
        newcube.rename(field)
        cntl_cubes.append(newcube)          
         
        
        # plot individual values if required
        if individual_plot=='y':
            plt.subplot(1,2,1)
            cs=iplt.contourf(exptcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+exptname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
        
            plt.subplot(1,2,2)
            cs=iplt.contourf(cntlcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
       
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.pdf'
            plt.savefig(fileout)
            plt.close()
        
            # plot a picture of the anomaly from each of the models
            plt.subplot(1,1,1)
            
            
            cs=iplt.contourf(exptcube-cntlcube,Vanom,extend='both',cmap=cmapname)
            titlename=modeluse+' '+exptname+'-'+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.pdf'
            plt.savefig(fileout)
            plt.close()
        
   
       
    #############################    
    # get the multi-modelmean and standard deviation
    
   
    iris.experimental.equalise_cubes.equalise_attributes(expt_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_cubes)
    allpliocube = expt_cubes.concatenate_cube()
   
    meanplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanplio.rename(field + 'mean_mPWP')
    maxplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxplio.rename(field + 'max_mPWP')
    minplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MIN)
    minplio.rename(field + 'min_mPWP')
    stdplio = allpliocube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdplio.rename(field + 'std_mPWP')
    
    allpicube = cntl_cubes.concatenate_cube()
    meanpi = allpicube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpi.rename(field + 'mean_pi')
    maxpi = allpicube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxpi.rename(field + 'max_pi')
    minpi = allpicube.collapsed(['model_level_number'], iris.analysis.MIN)
    minpi.rename(field + 'min_pi')
    stdpi = allpicube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdpi.rename(field + 'std_pi')
    
    allmeancube=anom_cubes.concatenate_cube() # all model mean anomalies
    meancube=allmeancube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meancube.rename(field + 'mean_anomaly')
    maxcube=allmeancube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxcube.rename(field + 'max_anomaly')
    mincube=allmeancube.collapsed(['model_level_number'], iris.analysis.MIN)
    mincube.rename(field + 'min_anomaly')
    stdcube=allmeancube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdcube.rename(field + 'anomaly_multimodel_stddev')
    
    ###############################################
    # write out the mean and standard deviation to a netcdf file
    
    cubelist = iris.cube.CubeList([meanplio, stdplio, maxplio, minplio,
                                   meanpi, stdpi, maxpi, minpi,
                                   meancube, stdcube, maxcube, mincube])
    fileout = (FILESTART + field + '_multimodelmean.nc')
    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    
    
    ###########################
    # plot the mean value and standard deviation
    
   
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(valmin,valmax,incr)
    mycmap = plt.cm.get_cmap(cmapname,len(V+2))
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    if linux_win == 'l':
        exptlsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        exptlsm = FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.figtext(0.02, 0.97,'b)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(namefield.get(field) +' anomaly: multimodel mean')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.pdf')
    plt.savefig(fileout)
    plt.close()
    
    if field=='NearSurfaceTemperature':
        V=np.arange(0,5.0,incr)
        textout = 'd)'
    if field=='TotalPrecipitation':
        textout = 'c)'
        V=np.arange(0,1.3, 0.1)
    ax = plt.axes(projection = ccrs.PlateCarree())
    qplt.contourf(stdcube, V,extend='both',cmap='plasma')
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.title(namefield.get(field) +' anomaly:Standard Deviation')
    plt.figtext(0.02, 0.97,textout,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.pdf')
    plt.savefig(fileout)
    plt.close()
     

    cubelist = iris.cube.CubeList([meancube, stdcube])
    iris.save(cubelist,FILEOUTNC,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
        
        
    ##########################################################
    # plot the monthly anomaly from each model
    if linux_win=='l' and individual_plot=='y':
        for modelno in range(0,len(modelnames)):
            modeluse=modelnames[modelno]
            exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.mean_month.nc'
            cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.mean_month.nc'
            exptcube=iris.load_cube(exptfile)
            cntlcube=iris.load_cube(cntlfile)
     
            for mon in range(0,12):
                anom=exptcube.data[mon,:,:]-cntlcube.data[mon,:,:]
                lat=exptcube.coord('latitude').points
                lon=exptcube.coord('longitude').points
                lons,lats=np.meshgrid(lon,lat)
                map=Basemap(llcrnrlon=0.0,urcrnrlon=360.0,
                         llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',
                         resolution='c')
                x, y = map(lons, lats)
                map.drawcoastlines()
                V=np.arange(valmin,valmax,incr)
                cs = map.contourf(x,y,anom,V,cmap=cmapname,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
                plt.title(modeluse+':'+'month is '+str(mon+1))
                cbar.set_label(units)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.eps')
                plt.savefig(fileout)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.pdf')
                plt.savefig(fileout)
                plt.close()
          
        
  


##########################################################
# main program
        
filename=' '
linux_win='l'

modelnames=['CESM2', 'HadGEM3','IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]

#modelnames = ['CCSM4-Utr']
            
PLIOMIP1 = 'n'
#modelnames=['COSMOS',
#            'CCSM4-UoT']   

fieldnames=['TotalPrecipitation']
units=['mm/day']
#fieldnames=['NearSurfaceTemperature']
#units=['degC']
#fieldnames=['SST']
#units=['degC']
exptname='EOI400'
cntlname='E280'
individual_plot='n' # do you want to plot the anomalies for all of the individual models

if linux_win=='w':
   FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded/'
else:
   FILESTART='/nfs/hera1/earjcti/regridded/'

FILEOUT = FILESTART + 'dummy.txt'
FILEOUTNC = FILESTART + 'dummy.nc'

for field in range(0,len(fieldnames)):

    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'alldata/data_for_5a_6a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_5b_5c.nc'
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'alldata/data_for_1a_1c_3a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_1b_1d.nc'
    if fieldnames[field] == 'SST':
        FILEOUT = FILESTART + 'alldata/zonal_mean_SST_data.txt'
  
   
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field],individual_plot)

#sys.exit(0)
\
::::::::::::::
PlioMIP_new/large_scale_features/means_all_models.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import iris.util
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

import sys


def plotmean_newaxis(cube, modelno_):
     tempcube=iris.util.new_axis(cube)
     tempcube.add_dim_coord(iris.coords.DimCoord(modelno_, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
     return tempcube
 

def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
         
    newcube = plotmean_newaxis(cube, [levelno])
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    
        
    return newcube
###########################################
def get_NH_mean(modelname, expt, field):
     """
     gets the mean of the NH for model - modelname
                             and expt - experiment name (ie PI)
     returns a numpy array of length 12 with the average for each month
     """
     # read data into iris cube
     filename = (FILESTART + modelname + 
                  '/' + expt + '.' + field + 
                  '.mean_month.nc')
     
     if (modelname == 'GISS2.1G' or modelname == 'IPSLCM6A'
         or (modelname == 'IPSLCM5A' and field == 'SST')
         or (modelname == 'IPSLCM5A2' and field == 'SST')
         or (modelname == 'NorESM-L' and field == 'SST')
         or (modelname == 'NorESM1-F' and field == 'SST')):
          cubes = iris.load(filename)
          cube = cubes[0]
     else:
          print(filename,field)
          cube = iris.load_cube(filename, field)
        
     # get weights and average over NH
     cube.coord('latitude').guess_bounds()
     cube.coord('longitude').guess_bounds()
     grid_areas = iris.analysis.cartography.area_weights(cube)
     grid_areas_nh = np.zeros(grid_areas.shape)
     for j, lat in enumerate(cube.coord('latitude').points):
          if lat > 0:
               grid_areas_nh[:, j, :] = grid_areas[:, j, :]

     cube_nh = (cube.collapsed(['longitude', 'latitude'],
                iris.analysis.MEAN, weights = grid_areas_nh))
     
     return cube_nh.data
 
def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART[:-10] + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    print('julia',PLIOMIP1_FILE)
    sys.exit(0)
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    
    # means are at the start of the file
    allanoms_list = []
    for i in range(1, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break    # we have now got all the means
        modname, eoi400, e280, anom = line.split(',')
        if modname == 'MEAN':
            meananom=anom
        else:
            allanoms_list.append(anom)
            
    # find line which contains 'jan feb mar ' which is the start of the seasonal cycle
    string = 'jan feb mar'
    min_seas_cyc = np.zeros(12) + 1000.
    max_seas_cyc = np.zeros(12) - 1000.
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        modname, anomstr = line.split(',')
        anom_arr = np.array(anomstr.strip('[]').split(), dtype=float)
        if modname == 'MEAN':
            mean_seas_cyc = anom_arr
        else:
            for j in range(0,12):
                min_seas_cyc[j] = np.min([anom_arr[j], min_seas_cyc[j]]) 
                max_seas_cyc[j] = np.max([anom_arr[j], max_seas_cyc[j]]) 
    

    allanoms = np.asarray(allanoms_list, dtype=float)
    
        
    return allanoms, min_seas_cyc, max_seas_cyc, mean_seas_cyc
  
   

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units,individual_plot):
   
    
    
    names = {"EOI400" : "Plio_Core",
             "E280" : "PI_Ctrl"
            }
    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }
    
    # set up lists to store all values for both experiment and control    
    model_global_mean=[]
    model_global_sd=[]
    
    # store months in a numpy array (model, experiment,month)
    monmeans=np.zeros((len(modelnames),2,12))   
    monsd=np.zeros((len(modelnames),2,12))
    
    # store latitudes in a numpy array (model, experiment,latitude)
    latmeans=np.zeros((len(modelnames),2,180))   
    latsd=np.zeros((len(modelnames),2,180))
    lats=np.zeros(180)
     
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        print(modeluse)
        filenames=[]
        filenames.append(FILESTART+modeluse+'/'+exptname+'.'+field+'.data.txt')
        filenames.append(FILESTART+modeluse+'/'+cntlname+'.'+field+'.data.txt')
       
        # set up temporary lists to store data from each experiment
        means=[]
        sds=[]
        nmon=12
       
        for fileno in range(0,len(filenames)):
            
            f=open(filenames[fileno],"r")
            f1=f.readlines()
            f2 = [x.replace('\n', '') for x in f1]
            
            # get the means according to their position in the file
            all_mean_sd=f2[2]
            all_mon_mean_sd=f2[5:5+12]
            all_lat_mean_sd=f2[20:20+180]
           
            # extract global mean
            mean,sd=all_mean_sd.split(',')
            means.append(mean)
            sds.append(sd)
            
            # extract monthly means 
            for x in all_mon_mean_sd:
                mon,mean,sd=x.split(',')
                monmeans[modelno,fileno,int(mon)-1]=float(mean)
                monsd[modelno,fileno,int(mon)-1]=float(sd)
            
            
            # extract latitude means
            for x in all_lat_mean_sd:
                lat,mean,sd=x.split(',')
                latss=int(float(lat)+89.5) # convert latitude to a subscript
               
                if mean != ' --' and mean != '--':
                    latmeans[modelno,fileno,latss]=float(mean) # stores latitudinal means
                else:
                    latmeans[modelno,fileno,latss]=np.nan
                if sd != ' --' and sd != '--' :
                    latsd[modelno,fileno,latss]=float(sd)
                else:
                    latsd[modelno,fileno,latss]=np.nan
                lats[latss]=lat # stores latitudes
            
            
        model_global_mean.append(means)
        model_global_sd.append(sds)
           
    ############################################################
    # get the monthly means for the NH
    # 
    # store months in a numpy array (model, experiment,month)
    # ss 1 is experiment ss 2 is control

    monmeans_NH = np.zeros((len(modelnames),2,12))   
    monsd = np.zeros((len(modelnames),2,12))

    for i, model in enumerate(modelnames):
         expt_mon_mean = get_NH_mean(model, exptname, field)
         monmeans_NH[i, 0, :] = expt_mon_mean

         cntl_mon_mean = get_NH_mean(model, cntlname, field)
         monmeans_NH[i, 1, :] = cntl_mon_mean

    
    #===============================================================
    # if pliomip1 is set get pliomip1 data
    if PLIOMIP1 == 'y':
        (mean_pliomip1, min_seas_pliomip1,
         max_seas_pliomip1, mean_seas_pliomip1) = get_pliomip1_data(field)
    
    #############################################################
    # plot the global mean and error bars from each model.
    
   
    expt_global_mean=[float(item[0]) for item in model_global_mean]
    expt_global_2sigma=([float(item[0])*2.0 for item in model_global_sd])

    cntl_global_mean=[float(item[1]) for item in model_global_mean]
    cntl_global_2sigma=([float(item[1])*2.0 for item in model_global_sd])
  
    
   
   
    #fig,ax=plt.subplots(2,1,1)
    ax=plt.subplot(2,1,1)
   
    ax.errorbar(modelnames,expt_global_mean,
                yerr=expt_global_2sigma,fmt='x',label=names.get(exptname))
    ax.errorbar(modelnames,cntl_global_mean,
                yerr=cntl_global_2sigma,fmt='x',label=names.get(cntlname))
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.15*box.height), box.width * 0.8, box.height])
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    titlename='Global mean ' + namefield.get(field)
    plt.title(titlename)
    ax.tick_params(axis='x',labelbottom='False')
    plt.ylabel(units)
    
    
    
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)],'x')
    print('means',[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    anomalies = [x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)]
    print('multimodelmean = ',
          np.mean(expt_global_mean) - np.mean(cntl_global_mean), np.mean(anomalies))
    print('multimodelmedian = ', np.median(anomalies))
    print('percentiles 10/50/90',np.percentile(anomalies, 10),
          np.percentile(anomalies,50), np.percentile(anomalies,90))
    print('percentiles 5/95/90',np.percentile(anomalies, 5),
          np.percentile(anomalies,95))
      
    
    sorted_anomalies = np.sort(anomalies)
    print(sorted_anomalies)
    print('means % change',[((x1 - x2) *100 / x2) for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    print('multimodelmean = ',(np.mean(expt_global_mean) - np.mean(cntl_global_mean)) *100. / np.mean(cntl_global_mean))
     # if pliomip1 is set overplot pliomip1 means as grey horizontal bars
    if (PLIOMIP1 == 'y' and field !='SST'):
        for mean_mod in mean_pliomip1:
            ax.axhline(y=mean_mod, color='grey', alpha=0.4)
    # Shrink axis as appropriate
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, box.height*0.8])
    plt.title('Global mean ' + namefield.get(field) + ' anomaly')
    plt.ylabel(units)
    plt.xticks(rotation='90', fontsize=8)
    #plt.xticks(x, labels, rotation='vertical')
   

    fileout=FILESTART+'allplots/'+field+'/global_means.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/global_means.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out data
    
    txtfile1 = open(FILEOUT,"w+")
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_5a \n")
        
    txtfile1.write('modelname, Pliocore_global_mean, pliocore_global_mean_2sigma, ' + 
                   'picntl_global_mean, picntl_global_mean_2sigma \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + np.str(np.around(expt_global_mean[i],2)) + 
                       ',' + np.str(np.around(expt_global_2sigma[i],2)) + 
                       ',' + np.str(np.around(cntl_global_mean[i],2)) + 
                       ',' + np.str(np.around(cntl_global_2sigma[i],2)) + '\n'))
        
    txtfile1.write('\n')
    
      
    
    ################################################################
    #  plot the NH seasonal cycle from each model
   

    plt.subplot(2,1,1)
    for i in range(0,len(monmeans_NH[:,0,0])):
        # plot experiment data
       
        plt.plot(monmeans_NH[i,0,:],color='r')
        # plot control data
        plt.plot(monmeans_NH[i,1,:],color='b')
        plt.title('NH annual cycle of '+field)
        plt.ylabel(units)
        
    plt.subplot(2,1,2)
    plt.plot(np.mean(monmeans_NH[:,0,:],axis=0),label=exptname,color='r')
    plt.plot(np.mean(monmeans_NH[:,1,:],axis=0),label=cntlname,color='b')
    plt.ylabel(units)
    plt.xlabel('month')    
    plt.legend()
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.pdf'
    plt.savefig(fileout)
    plt.close
    
    
    ax=plt.subplot(1,1,1)
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    for i in range(0,len(monmeans_NH[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
           ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i])
        else:
            ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i],
                    linestyle='dashed')
        
        
    ax.plot(labels,np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0),
        color='black',linestyle='dashed',linewidth=2,label='avg')
    
    print('monthmeans',np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0))
   
    # plot pliomip1 data if appropriate
    if PLIOMIP1 == 'y':
        ax.plot(labels, mean_seas_pliomip1, color='black', linestyle='dotted',
                linewidth=2, label='PlioMIP1')
        ax.fill_between(labels, min_seas_pliomip1, max_seas_pliomip1, alpha=0.2, 
                        color="grey")
        
    
    plt.title(exptname+'-'+cntlname+': NH '+ field + ' anomaly')
    plt.ylabel(units)
    #plt.xlabel('month') 
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' NH anomaly')
    
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_3a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_6a \n")
       
        
    txtfile1.write('modelname, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + 
                        np.str(np.around((monmeans_NH[i,0,0] - monmeans_NH[i,1,0]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,1] - monmeans_NH[i,1,1]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,2] - monmeans_NH[i,1,2]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,3] - monmeans_NH[i,1,3]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,4] - monmeans_NH[i,1,4]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,5] - monmeans_NH[i,1,5]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,6] - monmeans_NH[i,1,6]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,7] - monmeans_NH[i,1,7]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,8] - monmeans_NH[i,1,8]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,9] - monmeans_NH[i,1,9]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,10] - monmeans_NH[i,1,10]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,11] - monmeans_NH[i,1,11]),2)) + '\n'))
        
    txtfile1.write('\n')
    txtfile1.close

    ###################################################################
    # plot the latitudinal range from each model
    
    # won't print this out as it doesn't look very useful.
    # absolute value of temperature by latitude
    #plt.subplot(2,1,1)
    #for i in range(0,len(latmeans[:,0,0])):
    #    # plot experiment data
    #   
    #    plt.plot(latmeans[i,0,:],lats,color='r')
    #    # plot control data
    #    plt.plot(latmeans[i,1,:],lats,color='b')
    #    plt.title('latitudinal average of '+field)
    #    plt.xlabel(units)
    #    
    #plt.subplot(2,1,2)
    #plt.plot(np.mean(latmeans[:,0,:],axis=0),lats,label=exptname,color='r')
    #plt.plot(np.mean(latmeans[:,1,:],axis=0),lats,label=cntlname,color='b')
    #plt.xlabel(units)
    #plt.ylabel('latitude')    
    #plt.legend()
    
    
    ax=plt.subplot(1,1,1)
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    for i in range(0,len(latmeans[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i])
        else:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i],
                    linestyle='dashed')
    ax.plot(np.mean(latmeans[:,0,:],axis=0)-np.mean(latmeans[:,1,:],axis=0),
             lats,label='avg',color='black',linestyle='dashed',linewidth=2)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' anomaly')
    plt.xlabel(units)
    plt.ylabel('latitude') 
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
  
     # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1c \n")
    elif field == 'SST':
        txtfile1.write("zonal mean data for SST \n")
   
   
        for i, model in enumerate(modelnames):
            txtfile1.write(model + '\n')
            txtfile1.write('latitude, Tanom \n')
            for j, lat in enumerate(lats):
                txtfile1.write(np.str(np.around(lat,2)) + ',' + 
                               np.str(np.around(latmeans[i,0,j] - latmeans[i,1,j],2)) + '\n')
        
        txtfile1.write('\n')
        
    txtfile1.close
   
    #########################################################
    # plot a picture of the monthly anomaly from each model
    
    
    if field=='SurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='NearSurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='SST':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -5.
            datamax = 32.
            dataincr = 2.
    if field=='TotalPrecipitation':
            valmin=-1.4
            valmax=1.6
            incr=0.2
            cmapname='RdBu'
            
            datamin = 0.
            datamax = 5.
            dataincr = 0.1
    Vanom=np.arange(valmin,valmax,incr)
    Vdata= np.arange(datamin, datamax, dataincr)
    
   
    
    #########################################################
    # plot a picture of the change from each model and the 
    # multimodelmean
    
    
    anom_cubes=iris.cube.CubeList([])
    expt_cubes=iris.cube.CubeList([])
    cntl_cubes=iris.cube.CubeList([])
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        
        exptcube=iris.load_cube(exptfile)
        cntlcube=iris.load_cube(cntlfile)
       
       
        if modeluse == 'EC-Earth3.1' and field == 'SST':
           cntlcube.coord('latitude').bounds = None
           cntlcube.coord('longitude').bounds = None
           
        if modeluse == 'CCSM4-UoT' and field == 'TotalPrecipitation':
           cntlcube.coord('latitude').var_name = 'latitude'
           cntlcube.coord('longitude').var_name = 'longitude'
           exptcube.coord('latitude').var_name = 'latitude'
           exptcube.coord('longitude').var_name = 'longitude'
           cntlcube.coord('latitude').long_name = None
           cntlcube.coord('longitude').long_name = None
           exptcube.coord('latitude').long_name = None
           exptcube.coord('longitude').long_name = None
           cntlcube.coord('latitude').points = cntlcube.coord('latitude').points.astype('float32')
           cntlcube.coord('longitude').points = cntlcube.coord('longitude').points.astype('float32')
           exptcube.coord('latitude').points =  exptcube.coord('latitude').points .astype('float32')
           exptcube.coord('longitude').points = exptcube.coord('longitude').points.astype('float32')
        
        diffcube=exptcube-cntlcube
        
        # check float 32 for concatenation
        diffcube.data=diffcube.data.astype('float32') 
        exptcube.data=exptcube.data.astype('float32') 
        cntlcube.data=cntlcube.data.astype('float32') 
        if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'HadGEM3')
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                diffcube.units='Celsius'
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                diffcube.convert_units('Celsius')
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')
        
        # remove scalar coordinates so that we can concatenate
        # also add a new axis with the model number
        
        newcube = resort_coords(diffcube,modelno)
        newdata = newcube.data
        anom_cubes.append(newcube)    
    
        newcube = resort_coords(exptcube,modelno)
        newcube.rename(field)
        print(modeluse)
        print('MINIMUM',np.min(newcube.data))
        expt_cubes.append(newcube)
        
        newcube = resort_coords(cntlcube,modelno)
        newcube.rename(field)
        cntl_cubes.append(newcube)          
         
        
        # plot individual values if required
        if individual_plot=='y':
            plt.subplot(1,2,1)
            cs=iplt.contourf(exptcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+exptname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
        
            plt.subplot(1,2,2)
            cs=iplt.contourf(cntlcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
       
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.pdf'
            plt.savefig(fileout)
            plt.close()
        
            # plot a picture of the anomaly from each of the models
            plt.subplot(1,1,1)
            
            
            cs=iplt.contourf(exptcube-cntlcube,Vanom,extend='both',cmap=cmapname)
            titlename=modeluse+' '+exptname+'-'+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.pdf'
            plt.savefig(fileout)
            plt.close()
        
   
       
    #############################    
    # get the multi-modelmean and standard deviation
    
   
    iris.experimental.equalise_cubes.equalise_attributes(expt_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_cubes)
    allpliocube = expt_cubes.concatenate_cube()
   
    meanplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanplio.rename(field + 'mean_mPWP')
    maxplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxplio.rename(field + 'max_mPWP')
    minplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MIN)
    minplio.rename(field + 'min_mPWP')
    stdplio = allpliocube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdplio.rename(field + 'std_mPWP')
    
    allpicube = cntl_cubes.concatenate_cube()
    meanpi = allpicube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpi.rename(field + 'mean_pi')
    maxpi = allpicube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxpi.rename(field + 'max_pi')
    minpi = allpicube.collapsed(['model_level_number'], iris.analysis.MIN)
    minpi.rename(field + 'min_pi')
    stdpi = allpicube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdpi.rename(field + 'std_pi')
    
    allmeancube=anom_cubes.concatenate_cube() # all model mean anomalies
    meancube=allmeancube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meancube.rename(field + 'mean_anomaly')
    maxcube=allmeancube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxcube.rename(field + 'max_anomaly')
    mincube=allmeancube.collapsed(['model_level_number'], iris.analysis.MIN)
    mincube.rename(field + 'min_anomaly')
    stdcube=allmeancube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdcube.rename(field + 'anomaly_multimodel_stddev')
    
    ###############################################
    # write out the mean and standard deviation to a netcdf file
    
    cubelist = iris.cube.CubeList([meanplio, stdplio, maxplio, minplio,
                                   meanpi, stdpi, maxpi, minpi,
                                   meancube, stdcube, maxcube, mincube])
    fileout = (FILESTART + field + '_multimodelmean.nc')
    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    
    
    ###########################
    # plot the mean value and standard deviation
    
   
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(valmin,valmax,incr)
    mycmap = plt.cm.get_cmap(cmapname,len(V+2))
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    if linux_win == 'l':
        exptlsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        exptlsm = FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.figtext(0.02, 0.97,'b)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(namefield.get(field) +' anomaly: multimodel mean')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.pdf')
    plt.savefig(fileout)
    plt.close()
    
    if field=='NearSurfaceTemperature':
        V=np.arange(0,5.0,incr)
        textout = 'd)'
    if field=='TotalPrecipitation':
        textout = 'c)'
        V=np.arange(0,1.3, 0.1)
    ax = plt.axes(projection = ccrs.PlateCarree())
    qplt.contourf(stdcube, V,extend='both',cmap='plasma')
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.title(namefield.get(field) +' anomaly:Standard Deviation')
    #plt.figtext(0.02, 0.97,textout,
    # horizontalalignment='left',
    # verticalalignment='top',
    # fontsize=20)
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.pdf')
    plt.savefig(fileout)
    plt.close()
     

    cubelist = iris.cube.CubeList([meancube, stdcube])
    iris.save(cubelist,FILEOUTNC,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
        
        
    ##########################################################
    # plot the monthly anomaly from each model
   
    expt_month_cubes=iris.cube.CubeList([])
    cntl_month_cubes=iris.cube.CubeList([])

    for modelno in range(0,len(modelnames)):
         modeluse=modelnames[modelno]
         exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.mean_month.nc'
         cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.mean_month.nc'

         cube=iris.load_cube(exptfile)
         cube.data=cube.data.astype('float32') 
         exptcube = resort_coords(cube,modelno)
         exptcube.rename(field)
        
         cube=iris.load_cube(cntlfile)
         cube.data=cube.data.astype('float32') 
         cntlcube = resort_coords(cube,modelno)
         cntlcube.rename(field)

         if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'HadGEM3')
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')

         expt_month_cubes.append(exptcube)
         cntl_month_cubes.append(cntlcube)

         if linux_win=='l' and individual_plot=='y':
   
            for mon in range(0,12):
                anom=exptcube.data[mon,:,:]-cntlcube.data[mon,:,:]
                lat=exptcube.coord('latitude').points
                lon=exptcube.coord('longitude').points
                lons,lats=np.meshgrid(lon,lat)
                map=Basemap(llcrnrlon=0.0,urcrnrlon=360.0,
                         llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',
                         resolution='c')
                x, y = map(lons, lats)
                map.drawcoastlines()
                V=np.arange(valmin,valmax,incr)
                cs = map.contourf(x,y,anom,V,cmap=cmapname,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
                plt.title(modeluse+':'+'month is '+str(mon+1))
                cbar.set_label(units)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.eps')
                plt.savefig(fileout)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.pdf')
                plt.savefig(fileout)
                plt.close()
        
    iris.experimental.equalise_cubes.equalise_attributes(expt_month_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_month_cubes)
     
    allpimonthcubes = cntl_month_cubes.concatenate_cube()
    meanpimonth = allpimonthcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpimonth.rename(field + 'mean_pi')
    meanpimonth.add_dim_coord(iris.coords.DimCoord(np.arange(1,13),
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                bounds = None,
                coord_system = None,  circular = False), 0)
   
  
    allpliomonthcubes = expt_month_cubes.concatenate_cube()
    meanpliomonth = allpliomonthcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpliomonth.rename(field + 'mean_plio')
    meanpliomonth.add_dim_coord(iris.coords.DimCoord(np.arange(1,13),
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                bounds = None,
                coord_system = None,  circular = False), 0)


    meananommonth = meanpliomonth - meanpimonth
    meananommonth.rename(field + 'plio - pi')
   
    cubelist = iris.cube.CubeList([meanpimonth, meanpliomonth, meananommonth])
    fileout = (FILESTART + field + '_multimodelmean_month.nc')

    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
  
    print('end of program')


##########################################################
# main program
        
filename=' '
linux_win='l'

modelnames=['CESM2', 'HadGEM3','IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]

#modelnames = ['CCSM4-Utr']
            
PLIOMIP1 = 'y'
#modelnames=['COSMOS',
#            'CCSM4-UoT']   

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
units=['degC']
#fieldnames=['SST']
#units=['degC']
exptname='EOI400'
cntlname='E280'
individual_plot='n' # do you want to plot the anomalies for all of the individual models

if linux_win=='w':
   FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded/'
else:
   FILESTART='/nfs/hera1/earjcti/regridded/'

FILEOUT = FILESTART + 'dummy.txt'
FILEOUTNC = FILESTART + 'dummy.nc'

for field in range(0,len(fieldnames)):

    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'alldata/data_for_5a_6a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_5b_5c.nc'
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'alldata/data_for_1a_1c_3a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_1b_1d.nc'
    if fieldnames[field] == 'SST':
        FILEOUT = FILESTART + 'alldata/zonal_mean_SST_data.txt'
  
   
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field],individual_plot)

#sys.exit(0)
\
::::::::::::::
PlioMIP_new/large_scale_features/model_rankings.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on October 2019


#@aauthor: earjcti
"""
This program will plot model diagnostics against model information
the information it will use is
1. year of first use
2. IPCC report where it was first used
3. resolution
"""


import numpy as np
import scipy as sp
import iris
import matplotlib.pyplot as plt



#####################################
def plot_regress(field1, field2, cluster, xtitle, ytitle, fileout, xmin, xmax):
    """
    plots the regression of the field vs the ranking
    """
    # plot the climate sensitivity vs the global mean

   
    ax = plt.subplot(111)
    for i, fielduse in enumerate(field2):
        model = MODELNAMES[i]
        print('j1',i,'j2', field1[i], 'j3',fielduse,'j4', MODELNAMES[i])
        if i % 4 == 0: # i divides 4 with no remainder
            ax.scatter(field1[i], fielduse, label = model) 
        elif i % 4 == 1 :
            ax.scatter(field1[i], fielduse, label = model, marker='^') 
        elif i % 4 == 2 :
            ax.scatter(field1[i], fielduse, label = model, marker='<') 
        else:
            ax.scatter(field1[i], fielduse, label = model, marker='v') 
   
    plt.xlabel(xtitle)
    plt.ylabel(ytitle)
    #plt.legend()

    plt.xlim(xmin, xmax)


    slope, intercept, r_value, p_value, std_err = (
        sp.stats.linregress(field1, field2))

    xarray = np.arange(xmin, xmax, 1)
    yarray = intercept+(slope*xarray)
    #ax.plot(xarray,yarray)
    rsq_val_small = np.str(np.around(r_value **2, 2))
    p_value_small = np.str(np.around(p_value, 2))

    if FIELDNAME == 'NearSurfaceTemperature':
       if xtitle == 'year':
           subplotno = 'a'
       else:
           subplotno = 'a'
    if FIELDNAME == 'TotalPrecipitation':
       if xtitle == 'year':
           subplotno = 'b'
       else:
           subplotno = 'b'
    plt.title((subplotno + ") R-squared: "  + rsq_val_small + 
              "  p-value: " + p_value_small))
              

    box = ax.get_position()
    ax.set_position([box.x0, box.y0,
                     box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))

    #plt.show()
    plt.savefig(fileout+'.eps')
    plt.savefig(fileout+'.pdf')
    plt.close()


def get_model_years():
    """
    get the year when the model was first used from a dictionary
    """

    year = {'NorESM-L': 2011,
            'NorESM1-F':2017,
            'IPSLCM6A': 2018,
            'IPSLCM5A2':2017,
            'IPSLCM5A':2010,
            'HadCM3': 1997,
            'MIROC4m':2004,
            'COSMOS':2009,
            'CCSM4-UoT':2011,
            'EC-Earth3.3':2019,
            'MRI2.3': 2006, # from my investigation
            'CCSM4-Utr': 2011,
            'GISS2.1G': 2019,
            'CESM1.2' :2013,
            'CESM2': 2019,
            'CCSM4' :2011
            }

    allyears = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        allyears[i] = year.get(model)

    return allyears

def get_resolution():
    """
    get the resolution of the model
    """

    xres_a = {'NorESM-L': 3.75,
              'NorESM1-F':2.5,
              'IPSLCM6A': 2.5,
              'IPSLCM5A2':3.75,
              'IPSLCM5A':3.75,
              'HadCM3': 3.75,
              'MIROC4m':2.8,
              'COSMOS':3.75,
              'CCSM4-UoT':1.25,
              'EC-Earth3.3':1.125,
              'MRI-CGCM2.3':2.8, # from my investigation
              'CESM4-Utr': 2.5,
              'CESM2' : 1.25,
              'GISS': 2.5,
              'CESM1.2' :1.25,
              'CCSM4' :1.25
              }

    yres_a = {'NorESM-L': 3.75,
              'NorESM1-F':1.9,
              'IPSLCM6A': 1.26,
              'IPSLCM5A2':1.9,
              'IPSLCM5A':1.9,
              'HadCM3': 2.5,
              'MIROC4m':2.8,
              'COSMOS':3.75,
              'CCSM4-UoT':0.9,
              'EC-Earth3.3':1.125,
              'MRI-CGCM2.3':2.8, # from my investigation
              'CESM-Utr': 1.9,
              'CESM2' : 0.95,
              'GISS': 2.0,
              'CESM1.2' :0.9,
              'CCSM4' :0.9
              }

    horiz_boxes = {'NorESM-L': 4608,
                   'NorESM1-F':13824,
                   'IPSLCM6A': 20592,
                   'IPSLCM5A2':9216,
                   'IPSLCM5A':9216,
                   'HadCM3': 7008,
                   'MIROC4m':8192,
                   'COSMOS':4608,
                   'CCSM4-UoT':55296,
                   'EC-Earth3.3':51200,
                   'MRI2.3':8192, # from my investigation
                   'CCSM4-Utr': 13824,
                   'GISS2.1G': 12960,
                   'CESM1.2' :55296,
                   'CESM2' : 55296,
                   'CCSM4' :55296
                   }

    horiz_gb_atm = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        horiz_gb_atm[i] = horiz_boxes.get(model)

    return horiz_gb_atm

def get_cluster():
    """
    this will put models of the same family into a cluster
    """

    year ={'NorESM-L': 1,
           'NorESM1-F': 1,
           'IPSLCM6A': 2,
           'IPSLCM5A2':2,
           'IPSLCM5A':2,
           'HadCM3': 0,
           'MIROC4m': 0,
           'COSMOS': 0,
           'CCSM4-UoT':3,
           'EC-Earth3.3': 0,
           'MRI2.3': 0, # from my investigation
           'CCSM4-Utr': 3,
           'GISS2.1G': 0,
           'CESM1.2' : 3,
           'CESM2' : 3, 
           'CCSM4' : 3
           }

    allyears = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        allyears[i] = year.get(model)

    return allyears

def get_model_info():
    """
    gets information about the model
    returns modelyears
            cluster this will group models of the same family
    """
    years = get_model_years()
    resolution = get_resolution()
    cluster = get_cluster()

    return [years, resolution, cluster]

def read_means_file(filename):
    """
    read all the data from the file containing the means
    returns the mean
    """

    file1 = open(filename, "r")
    lines = list(file1)

    meanval, sdval = lines[2].split(",")

    meanint = np.float(meanval)

    return meanint

def get_anomaly():
    """
    gets the anomaly in the field for each of the models
    """

    climdiff = np.zeros(len(MODELNAMES))

    for i, model in enumerate(MODELNAMES):

        print(FIELDNAME)
        print(model)
        meanexpt = read_means_file(FILESTART + model + '/EOI400.' +
                                   FIELDNAME + '.data.txt')
        meancntl = read_means_file(FILESTART + model + '/E280.' +
                                   FIELDNAME + '.data.txt')

        climdiff[i] = meanexpt - meancntl


    return climdiff



def main():
    """
    1. get model information.  For example the year of the model
    2. second the anomaly in the field
    3. plot
    """
    modelyears, model_res, modelcluster = get_model_info()

    fieldanom = get_anomaly()

    # regress anomaly against model year
    plot_regress(modelyears, fieldanom, modelcluster, 'year',
                 FIELDNAME + ' anom',
                 FILESTART + '/allplots/' + FIELDNAME +
                 '/' + FIELDNAME + '_anom_vs_modelyear' + SUBSCRIPT,
                 1990., 2020.)

    # regress anomaly against model resolution
    plot_regress(model_res, fieldanom, modelcluster, 'horizontal gridboxes',
                 FIELDNAME + ' anom',
                 FILESTART + '/allplots/' + FIELDNAME +
                 '/' + FIELDNAME + '_anom_vs_nbox_atm' + SUBSCRIPT,
                 5000., 60000.)



##########################################################
# main program

LINUX_WIN = 'l'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/regridded/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]
SUBSCRIPT = ''

#MODELNAMES = ['CCSM4-UoT',
#              'CCSM4',
#              'CESM1.2',
#              'IPSLCM6A',
#              'IPSLCM5A2',
#              'IPSLCM5A',
#              'NorESM-L',
#              'NorESM1-F',
#              'COSMOS',
#              'GISS',
#              'HadCM3',
#              'MIROC4m',
#              'MRI-CGCM2.3'
#            ]
#SUBSCRIPT = '_redu'

FIELDNAME = 'NearSurfaceTemperature'
#FIELDNAME = 'TotalPrecipitation'


main()
::::::::::::::
PlioMIP_new/large_scale_features/noregrid_ocn_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019

#based on regrid_ocn_50yr_avg - but will calculate the means without regridding

#
#

import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    print('julia',fielduse)
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    print(cube.data)
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm_400(fieldname):
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename, fieldname)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    print(allcube)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def getmeans_data(fieldnamein, exptnamein):
    """
    gets the means from  the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A_origgrid'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        print('before',filename)
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F' and exptnamein == 'E400'):
        cube100 = get_noresm_400(fielduse)
    else:
        cube100 = iris.load_cube(filename)

   
     

    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = cube.coord('time').points
        newpoints = origpoints/24.
        cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A' or modelname == 'HadGEM3'
             or modelname  == 'IPSLCM5A2'
             or modelname  == 'IPSLCM6A_origgrid'
             or modelname == 'CCSM4-Utr' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            cube.data = cube.data * 60. *60. *24.
            cube.name = 'Total precipitation'
            cube.long_name = 'Total precipitation'
            cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4'):
        print('cube.units',cube.units)
        print('j1',cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            cube.data = cube.data * 60. *60. *24. *1000.
            print('j2',cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',cube.data[:,0])
            cube.name = 'Total precipitation'
            cube.long_name = 'Total precipitation'
            cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A_origgrid' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A_origgrid' or 
        modelname  == 'EC-Earth3.1'):
          cube.coord('time').units = refdate


       
    print(cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(cube,  'time',  name = 'year')
    
    # correct the start month if required
    cube=correct_start_month(cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  cube.extract(iris.Constraint(month = 1))
    feb_slice  =  cube.extract(iris.Constraint(month = 2))
    mar_slice  =  cube.extract(iris.Constraint(month = 3))
    apr_slice  =  cube.extract(iris.Constraint(month = 4))
    may_slice  =  cube.extract(iris.Constraint(month = 5))
    jun_slice  =  cube.extract(iris.Constraint(month = 6))
    jul_slice  =  cube.extract(iris.Constraint(month = 7))
    aug_slice  =  cube.extract(iris.Constraint(month = 8))
    sep_slice  =  cube.extract(iris.Constraint(month = 9))
    oct_slice  =  cube.extract(iris.Constraint(month = 10))
    nov_slice  =  cube.extract(iris.Constraint(month = 11))
    dec_slice  =  cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the new dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                    "E400": "b.e21.B1850.f09_g17.CMIP6-piControl.400.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : ".110001-120012",
                  "E400" : ".0801.0900",
                  "Eoi400" : ".110001.120012"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        if linux_win  == 'l':
#            filename = filestart + 'UofT/'
#            filename = (filename + 'UofT-CCSM4/for_julia/' + 
#                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
            filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        
        
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
        if exptnamein == 'E400':
            filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_TREFHT_PRECT_month.nc')
            fielduse = NorESM_FIELDS.get(fieldnamein + 'E400')
            if fieldnamein == 'tos':
                filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_SST_month.nc')
          
            

    if modelname  == 'IPSLCM6A_origgrid':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+'IPSLCM6A/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(exptnamein, filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+'IPSLCM6A/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            if exptnamein == 'Eoi400' or exptnamein == 'E400':
                filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename = [filename1, filename2]
                fielduse = ['PRECC', 'PRECL']
            if exptnamein == 'E280':
                filename = (filestart + 'NCAR/b.e21.B1850.f09_g17.' + 
                            'CMIP6-piControl.001.cam.h0.PRECT.110001-120012.nc')
                fielduse = 'PRECT'
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            print(exptnamein)
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
        if fieldnamein =='totcloud':
            filestart='/nfs/hera1/earjcti/PLIOMIP2/CESM2/clt_Amon_CESM2_'
            fielduse = 'clt'
            if exptnamein == 'Eoi400':
                filename = (filestart + 'midPliocene-eoi400_r1i1p1f1_'+
                            'gn_015101-020012.nc')
            if exptnamein == 'E280':
                filename = (filestart +'piControl_r1i1p1f1_gn_090001-099912.nc')
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "IPSLCM6A_origgrid" 

                   # note that if you change the names of any of the models
                   # listed below make sure you carry it through to the program

                   # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A_origgrid GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

fieldnamein = ['pr']
#fieldnamein = ['tos'] # ocean tempeature or sst
exptnamein = ['E280']

#fieldnamein = ['tos']
#exptnamein = ['Eoi400', 'E280','E560']
#exptnamein = ['E400']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A_origgrid' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
        print(fielduse,filename)
       

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
#        sys.exit(0)
        getmeans_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/PLIOVAR_gradients.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the SST meridional gradient temperature change
vs global SSTtemperature change for each of the PlioMIP models

changed on October 17th to add the data to the figure

"""

import sys
import iris
import iris.quickplot as qplt
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


class GetProxy:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = FILESTARTP + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = FILESTARTP +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = FILESTARTP + 'pliovar_metadata_global_02102019.csv'
        self.pifile = FILESTARTP + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        # put the temperature anomalies into latitude bounds
        self.boundmin = -90. + (np.arange(0, 12) * 15.)
        self.boundmax = -75. + (np.arange(0, 12) * 15)
       
        boundtemp, nval = self.put_data_to_bounds(self.sitetemp.values - self.temppi)
        boundtemp_bs, nval_bs = self.put_data_to_bounds(self.sitetemp_bs.values - self.temppi)
       

        
        return boundtemp, boundtemp_bs, self.boundmin, self.boundmax, nval, nval_bs
    
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
    
    
    def put_data_to_bounds(self, tanom):
       """
       we now have the longitude, latitude and temperature of each datapoint
       we now put them into 15deg latitude bounded regions (defined by self.bounds)
       and find the average temperature in each region
       also return the number of points in each region
       """  
      
       boundtemp = np.zeros(12)
       count_boundtemp = np.zeros(12)
      
       for i in range(0, self.nsites):    
           # if temperature is a number add the temperature to the 
           # bound region
         
           if np.isfinite(tanom[i]):
               for bound in range(0,12):
              
                   if ((self.boundmin[bound] < self.lat[i]) &
                       (self.lat[i] <= self.boundmax[bound])):
            
                           boundtemp[bound] = (boundtemp[bound] + tanom[i])
                           count_boundtemp[bound] = count_boundtemp[bound] + 1
                           
                   
       # get average
      
       boundtemp = boundtemp / count_boundtemp
       
       print('bound temp',boundtemp)
       print('nbound',count_boundtemp)
       print(self.boundmin)
      
      
       return boundtemp, count_boundtemp
       
        
    
# end of class   
 
######################################################################
def combine_mgca_uk37(temp_uk37, temp_mgca, n_uk37, n_mgca,
                     boundmin_uk37, boundmax_uk37, boundmin_mgca, boundmax_mgca):
    """
    we have lots of arrays which show average temperature (temp_????) in a bounded
    box (boundmin_???? - boundmax_????).  We also have the number of points that make
    up the average temperature.
    
    We would like to average these depending on how many points are in the box
    
    returns: an array which contains the average combined temperature of uk37 and mg/ca
    """
    
    nvals = len(boundmin_uk37)
    temp_combined = np.zeros(nvals)
        
    for i in range(0, nvals):
        j = np.where(boundmin_mgca == boundmin_uk37[i])
        
        print(i, n_uk37[i], n_mgca[i])
        temp_combined[i] = (((np.nan_to_num(temp_uk37[i]) * n_uk37[i]) +
                            (np.nan_to_num(temp_mgca[j]) * n_mgca[j])) / 
                            (n_uk37[i] + n_mgca[j]))
    
    return temp_combined
 
#####################################################
def weightdata(tanom, lowerbounds, upperbounds):
        """
        we have some proxy data.  We would like to weight it to find the
        average value in each region.  
        Currently the proxy data is in latitudinal bands
        we want to find the global average and the (30S-30N) - (60N-75N)
        gradients
        
        input: temperature anomaly data, 
        lowervalue of the latituinal bound
        uppervalue of the latitudinal bound
        
        returns: merid_dy (30S-30N) - (60N-75N) - weighted by ocean area
                 glob_avg (average of (60S-75N) - weighted by ocean area)
        """
       
        # get land sea mask from pi model
        lsmcube = iris.load_cube(FILESTART + 'HadCM3/E280.SST.allmean.nc')
        lsmcube.data = lsmcube.data / lsmcube.data
        # get weights and multiply by lsm
        lsmcube.coord('latitude').guess_bounds()
        lsmcube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(lsmcube)
        lsmcube.data = lsmcube.data * grid_areas
       
        
        # find weights area between each lowerbound and upper bound
        weights = np.zeros(len(tanom))
        for i, lat in enumerate(lsmcube.coord('latitude').points):
            latcube = lsmcube.extract(iris.Constraint(latitude=lat))
            latdata = np.sum(latcube.data)
            for bound in range(0, len(lowerbounds)):
                if lowerbounds[bound] < lat <=upperbounds[bound]:
                   weights[bound] = weights[bound] + latdata
        
        # global average = temperautre in each bound * weights / total of weights

        
        globavg = np.nansum(weights * tanom) / np.nansum(weights)
        print('weights and tanom')
        print('weights',weights)
        print('tanom',tanom)
        print('upperbound',upperbounds)
        
        
        # get average in each bound
        
        tropavg_deep = bound_avg(-15., 15., lowerbounds, upperbounds, weights, tanom)
        tropavg = bound_avg(-30., 30., lowerbounds, upperbounds, weights, tanom)
        tropavg_nh = bound_avg(0., 30.,lowerbounds, upperbounds, weights, tanom)
        highlatavg = bound_avg(60., 75.,lowerbounds, upperbounds, weights, tanom)
        
        # get average polewards of 45degrees
        weightcount_midhighlat = 0.
        midhighlatavg = 0.
        for bound in range(0, len(lowerbounds)):
            # do poleward of 45 deg
            if ((lowerbounds[bound] >=45. or upperbounds[bound] <=-45.)
                 and (np.isfinite(tanom[bound]))):
                midhighlatavg = midhighlatavg + (weights[bound] * tanom[bound])
                weightcount_midhighlat = weightcount_midhighlat + weights[bound]
        
        midhighlatavg = midhighlatavg / weightcount_midhighlat
        
        # get gradients
        merid_dy = tropavg - highlatavg
        
        merid_dy_nh = tropavg_nh - highlatavg
        
        merid_dy_45 = tropavg_deep - midhighlatavg
        
    
        return merid_dy, merid_dy_nh, merid_dy_45, globavg
    
def bound_avg(minval, maxval, lowerbounds, upperbounds, weights, tanom):
    """
    averages the temperature over the range minval, maxval
    returns:  weighted averaged temperautre
    """
    
    avgval = 0.
    weightcount = 0.
      
    for bound in range(0, len(lowerbounds)):
        if lowerbounds[bound] >= minval and upperbounds[bound] <= maxval:
            avgval = avgval + (weights[bound] * tanom[bound]) 
            weightcount = weightcount + weights[bound]
    avgval = avgval / weightcount
    
    return avgval
            
############################################################
def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if field == 'SST' or field == 'NearSurfaceTemperature':
        if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'
                or modeluse == 'CESM1.0.5'):
            cube.units = 'Celsius'
        else:
            cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    if modeluse == 'EC-Earth3.1' and field == 'SST':
        cube.coord('latitude').bounds = None
        cube.coord('longitude').bounds = None

    cube.cell_methods = None

    return cube

def globmean(cube):
    """
    returns the global mean value of a SST cube (single value)
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    return tempcube.data

def getgradient(cube):
    """
    gets the gradient in the cube.  This is the average SST 60-75N
    minus the average SST equatorward of 30N
    input : cube
    output : gradient_et (30N-30S) - (60N-75N)
             gradient_na (30N-30S) - (60N-75N)(290E-5E)
             gradient_na_nh (30N-0S) - (60N-75N)(290E-5E)
    """

    grid_areas = iris.analysis.cartography.area_weights(cube)
    grid_areas_tropics = np.zeros(np.shape(grid_areas))
    grid_areas_deeptropics = np.zeros(np.shape(grid_areas))
    grid_areas_tropics_nh = np.zeros(np.shape(grid_areas))
    grid_areas_et = np.zeros(np.shape(grid_areas)) # 60N-75N
    grid_areas_na = np.zeros(np.shape(grid_areas)) # 60N - 75N 290E-360E
    grid_areas_45 = np.zeros(np.shape(grid_areas))

    nlat = len(cube.coord('latitude').points)
    nlon = len(cube.coord('longitude').points)
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points

    for j in range(0, nlat):
        if ((lats[j] >= 60.) and (lats[j] <= 75)):
            grid_areas_et[j, :] = grid_areas[j, :]
            for i in range(0, nlon):
                if (lons[i] >= 290 or lons[i]<=5.):
                    grid_areas_na[j, i] = grid_areas[j, i]
                else:
                    grid_areas_na[j, i] = 0.0
        else:
            grid_areas_et[j, :] = 0.0
            grid_areas_na[j, :] = 0.0

        if ((lats[j] <= 30.) and (lats[j] >= -30.)):
            grid_areas_tropics[j, :] = grid_areas[j, :]
        else:
            grid_areas_tropics[j, :] = 0.0
            
        if ((lats[j] <= 15.) and (lats[j] >= -15.)):
            grid_areas_deeptropics[j, :] = grid_areas[j, :]
        else:
            grid_areas_deeptropics[j, :] = 0.0
            
        if ((lats[j] >= 45.) or (lats[j] <= -45.)):
            grid_areas_45[j, :] = grid_areas[j, :]
        else:
            grid_areas_45[j, :] = 0.0
            
        if ((lats[j] <= 30.) and (lats[j] >= 0.)):
            grid_areas_tropics_nh[j, :] = grid_areas[j, :]
        else:
            grid_areas_tropics_nh[j, :] = 0.0

    temptrop = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_tropics)
    temptropdeep = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_deeptropics)
    temptrop_nh = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_tropics_nh)
    tempet = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_et)
    temp45 = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_45)
    tempna = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_na)

    gradient_et = temptrop.data - tempet.data 
    gradient_na = temptrop.data - tempna.data 
    gradient_nh = temptrop_nh.data - tempna.data
    gradient_deep_extra = temptropdeep.data - temp45.data
    
    print(np.sum(grid_areas))  
    print(np.sum(grid_areas_tropics)/np.sum(grid_areas))  
    print(np.sum(grid_areas_deeptropics)/np.sum(grid_areas))  
    print(np.sum(grid_areas_et)/np.sum(grid_areas))  
    print(np.sum(grid_areas_na)/np.sum(grid_areas)) 
    print(np.sum(grid_areas_45)/np.sum(grid_areas)) 
   
    
    return gradient_et, gradient_na, gradient_nh, gradient_deep_extra

def get_model_data(modelname):
    """
    1. gets the pliocene and the preindustrial SST data for each file
    2. calculates the global SSTA
    3. calculates the gradient as (SST polewards of 60deg) - (SST equatorward of 30deg)
    4. calculates the mPWP - PI gradient (gradient produced in 3.)

    input modelname
    output modTanom = the global mPWP-PI SSTA
           mod_gradanom = the mPWP (meridional SST gradient) minus the PI (meridional SST gradient)

    """

    #1. get data
    cubepi = get_data(FILESTART + modelname + '/E280.' + FIELDNAME + '.allmean.nc',
                      FIELDNAME, modelname)
    cubeplio = get_data(FILESTART + modelname + '/EOI400.' + FIELDNAME + '.allmean.nc',
                        FIELDNAME, modelname)
    #2 global mean anomaly
    meanpi = globmean(cubepi)
    meanplio = globmean(cubeplio)
    tempSSTA = meanplio - meanpi

    # calculate gradient
    gradpi_et, gradpi_na, gradpi_nh, gradpi_15NS_45NS = getgradient(cubepi)
    gradplio_et, gradplio_na, gradplio_nh, gradplio_15NS_45NS = getgradient(cubeplio)
    gradSSTA_et = gradplio_et - gradpi_et
    gradSSTA_na = gradplio_na - gradpi_na
    gradSSTA_nh = gradplio_nh - gradpi_nh
    gradSSTA_15NS_45NS = gradplio_15NS_45NS - gradpi_15NS_45NS

    return tempSSTA, gradSSTA_et, gradSSTA_na, gradSSTA_nh, gradSSTA_15NS_45NS

def plotdata(global_data, merid_grad_data, Tanom_model, gradanom_model, fileout):
    """
    this will plot the data and the model output
    we are plotting the global temerature anomaly vs the meridional gradient
    temperautre anomaly for both model and data
    """
    
    plt.scatter(global_data, merid_grad_data,
                label='proxy data', color='black',
                marker = 's')
    
    for i, model in enumerate(MODELNAMES):
         # add this to scatterplot
        if i < 6:
            plt.scatter(Tanom_model[i], gradanom_model[i], label=model)
        else:
            plt.scatter(Tanom_model[i], gradanom_model[i], label=model, marker='^')

    plt.xlabel('global mean SSTA')
    plt.ylabel('change in meridional gradient')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.tight_layout()

   
    plt.savefig(fileout + 'pdf')
    plt.savefig(fileout + 'png')
    plt.close()
    
    # write data to textfile
    txtout = open(fileout + 'txt', 'w+')

    txtout.write('model, global_mean, merid_gradient \n')
    txtout.write('proxy_data,' + 
                 np.str(np.around(global_data, 2)) + ',' + 
                 np.str(np.around(merid_grad_data,2)) + '\n')
    for i, model in enumerate(MODELNAMES):
        txtout.write(model + ',' + 
                     np.str(np.around(Tanom_model[i],2)) + ',' + 
                     np.str(np.around(gradanom_model[i],2)) + '\n')
    txtout.write('multimodelmean,' + 
                     np.str(np.around(np.mean(Tanom_model),2)) + ',' + 
                     np.str(np.around(np.mean(gradanom_model),2)) + '\n')
    txtout.close


def main():
    """
    1. Call a program that will get the data to plot
    2. Weight the proxy data by area of each latitude banc
    3. Get the output (global SST, meridional gradient SST anomaly)
       for each of the models
    4. Plot the data on a symplot
    """

    # 1. get data
    obj = GetProxy('t1', 'UK37') # get data for t1 timeslice
    (t1_temp, t1_temp_bs, 
     boundmin, boundmax,
     nval, nval_bs) = obj.get_proxydata()
    
    
    if MG_CA == 'y': # also get Mg/Ca data
        obj = GetProxy('t1', 'MGCA') # get data for t1 timeslice
        (t1_temp_mgca, t1_temp_bs_mgca, 
         boundmin_mgca, boundmax_mgca,
         nval_mgca, nval_bs_mgca)  = obj.get_proxydata()
       
        print('doing t1_comb')
        t1_comb = (combine_mgca_uk37(t1_temp, t1_temp_mgca, nval, nval_mgca,
                                     boundmin, boundmax, boundmin_mgca, boundmax_mgca))
        print('doing t1_comb_bs')
        t1_comb_bs = (combine_mgca_uk37(t1_temp_bs, t1_temp_bs_mgca, nval_bs, nval_bs_mgca,
                                     boundmin, boundmax, boundmin_mgca, boundmax_mgca))
        
        t1_temp = t1_comb
        t1_temp_bs = t1_comb_bs
        print('about to exit')
        
     
    
    # 2. weight proxydata (standard and bayspline)
    
    (merid_grad, merid_grad_nh, 
     merid_grad_15_45, global_ssta) = weightdata(t1_temp, boundmin, boundmax)
    
    (merid_grad_bs, merid_grad_nh_bs, 
     merid_grad_15_45_bs, global_ssta_bs) = weightdata(t1_temp_bs, boundmin, boundmax)
   
    
    # 3. get model results 
    Tanom = np.zeros(len(MODELNAMES))
    gradanom_et = np.zeros(len(MODELNAMES))
    gradanom_na = np.zeros(len(MODELNAMES))
    gradanom_na_nh = np.zeros(len(MODELNAMES))
    gradanom_15_45 = np.zeros(len(MODELNAMES))
    
    for i, model in enumerate(MODELNAMES):
        (Tanom[i], gradanom_et[i], 
         gradanom_na[i], 
         gradanom_na_nh[i],
         gradanom_15_45[i]) = get_model_data(model)
   
    #4. plot standard (gradient 30N-30S - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '.'
    plotdata(global_ssta, merid_grad, Tanom, gradanom_na, fileout)
    
    #plot standard (gradient 30N-0N - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_nh.'
    plotdata(global_ssta, merid_grad_nh, Tanom, gradanom_na_nh, fileout)
    
    #plot Bayspline (gradient 30N-30S - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_bayspline.'
    plotdata(global_ssta_bs, merid_grad_bs, Tanom, gradanom_na, fileout)
    
    
    #plot Bayspline (gradient 30N-0N - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_bayspline_nh.'
    plotdata(global_ssta_bs, merid_grad_nh_bs, Tanom, gradanom_na_nh, fileout)
    
    # plot original (gradient 15N-15S, 45N/S-90N/S)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_15NS_45NS-90NS.'
    plotdata(global_ssta, merid_grad_15_45, Tanom, gradanom_15_45, fileout)
    
    # plot Bayspline (gradient 15N-15S, 45N/S-90N/S)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_15NS_45NS-90NS_bayspline.'
    plotdata(global_ssta_bs, merid_grad_15_45_bs, Tanom, gradanom_15_45, fileout)



    return

    

# variable definition
LINUX_WIN = 'l'
FIELDNAME = 'SST'

if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    FILESTARTP = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/' # where proxy data is
else:
    FILESTART = 'C:/Users/julia/OneDrive/WORK/DATA/regridded/'
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    FILESTARTP = 'C:/Users/julia/OneDrive/WORK/DATA/proxydata/'

UNITS = 'deg C'
TIMEPERIODS = ['pi', 'mPWP']
MODELNAMES = ['CCSM4', 'CCSM4-UoT',
              'CCSM4-Utr',  
              'CESM1.2','CESM2',
              'COSMOS', 'EC-Earth3.3', 
              'GISS2.1G', 'HadCM3',
              'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
              'MIROC4m', 'MRI2.3',
              'NorESM-L', 'NorESM1-F'
             ]


MG_CA = 'y'
if MG_CA == 'y':
    MGCASS = '_mgca'
else:
    MGCASS = ''

main()
::::::::::::::
PlioMIP_new/large_scale_features/plot_individual_models_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models 
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#import Basemap
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys


def getmeanfield(fieldname, period):
    """
    get the mean values from the mean value file 
    
    inputs: fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
            period (probably mPWP PI or anomaly)

    returns the mean value from the multuimodelmean.nc file
    """
    
    meanfile = ('/nfs/hera1/earjcti/regridded/' + fieldname + 
                '_multimodelmean.nc')
    meanfield = fieldname + 'mean_' + period
    
    cube = iris.load_cube(meanfile, meanfield)

    return cube


def getmodelfield(modelname, fieldname, period):
     """
     get the mean values from the model data
     inputs: modelname (ie HadCM3)
             fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
             period (likely EOI400 or E280)
     returns:  a cube contatining the mean data from the model
     
     """
     
     modfile = ('/nfs/hera1/earjcti/regridded/' + modelname + '/' + 
                period + '.' + fieldname + '.allmean.nc')
    
     tempcube = iris.load(modfile)
     cube = tempcube[0]
     cube.units = UNITS
     
     return cube

class Plotalldata:   
    def __init(self):
        self.data = []
        
        
    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
              modelnames : the names of all the models
        """
        self.nmodels = len(modelnames)
        self.filestart = ('/nfs/hera1/earjcti/regridded/allplots/' +
                          fieldname + '/individual')
    
        for i in range(0, self.nmodels):
            
            cubedata = anom_cubes[i].data
            self.latitudes = anom_cubes[i].coord('latitude').points
            lon = anom_cubes[i].coord('longitude').points
            self.datatoplot, self.longitudes = (shiftgrid(180.,
                                                          cubedata,
                                                          lon,
                                                          start=False))
            self.model = modelnames[i]
            self.plotmap(i)
        

        return      
    
    def plotmap(self, i):
        """
        will plot the data in a map format
        
        """
        
        plotpos = np.mod(i, 8) + 1
        plt.subplot(3, 3, plotpos)
        lons, lats = np.meshgrid(self.longitudes, self.latitudes)
        
        map=Basemap(llcrnrlon=-180.0, urcrnrlon=180.0, 
                    llcrnrlat=-90.0, urcrnrlat=90.0, 
                    projection='cyl',resolution='l')
   
        map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines()
    
        V = np.arange(-5.0, 6.0, 1.0)    
        cs = map.contourf(x, y, self.datatoplot, V, cmap='RdBu_r',
                          extend='both')
        plt.title(self.model)
        
        print(i,self.nmodels)
        if plotpos == 8 or (i + 1) == self.nmodels:
            plt.subplot(3, 3, 9)
            plt.gca().set_visible(False)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.set_label(UNITS)
            
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                        + '.eps')
            plt.savefig(fileout, bbox_inches='tight')
            
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                    + '.png')
            
            plt.savefig(fileout, bbox_inches='tight')
        
        
##########################################################
# main program
# set up variable information

fieldname='NearSurfaceTemperature'
filename=' '
linux_win='l'


modelnames=['CESM1.0.5','COSMOS','EC-Earth3.1','GISS','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI-CGCM2.3',
            'NorESM-L','NorESM1-F',
            'UofT',
            ]

#modelnames=['GISS']
UNITS = 'Celsius'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']

# set up cubelists to store data
mpwp_anom_cubes=iris.cube.CubeList([])
pi_anom_cubes=iris.cube.CubeList([])
anom_anom_cubes=iris.cube.CubeList([])


#################################################
# get mean data
mean_plio_cube = getmeanfield(fieldname, 'mPWP')
mean_pi_cube = getmeanfield(fieldname, 'pi')
mean_anom_cube = getmeanfield(fieldname, 'anomaly')


for model in range(0,len(modelnames)):
    model_plio_cube = getmodelfield(modelnames[model], fieldname, 'EOI400')
    model_pi_cube = getmodelfield(modelnames[model], fieldname, 'E280')
    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_anom_cubes.append(model_plio_cube - mean_plio_cube)
    pi_anom_cubes.append(model_pi_cube - mean_pi_cube)
    anom_anom_cubes.append(model_anom_cube - model_pi_cube)
    
##################################################
# plot the cubes for the model anomalies relative to the mean

obj = Plotalldata()
obj.plotdata(fieldname, mpwp_anom_cubes, modelnames)

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/plot_individual_models.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import sys
import numpy as np
#import matplotlib as mp
import matplotlib.pyplot as plt
#from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#import netCDF4
#from netCDF4 import Dataset, MFDataset
import iris
import iris.analysis.cartography
import iris.coord_categorisation

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def getmeanfield(period):
    """
    get the mean values from the mean value file

    inputs: fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
            period (probably mPWP PI or anomaly)

    returns the mean value from the multuimodelmean.nc file
    """

    meanfile = (FILESTART + 'regridded/' + FIELDNAME +
                '_multimodelmean.nc')
    meanfield = FIELDNAME + 'mean_' + period

    cube = iris.load_cube(meanfile, meanfield)


    return cube


def getmodelfield(modelname, period):
    """
    get the mean values from the model data
    inputs: modelname (ie HadCM3)
            period (likely EOI400 or E280)
    returns:  a cube contatining the mean data from the model

    """

    modfile = (FILESTART + 'regridded/' + modelname + '/' +
               period + '.' + FIELDNAME + '.allmean.nc')

    tempcube = iris.load(modfile)
    cube = tempcube[0]
    cube.units = UNITS

    #this will make all the dimensions of all the cubes match.


    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube


class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, timeperiod, anom_cubes):
        self.nmodels = len(MODELNAMES)
        self.filestart = (FILESTART + '/regridded/allplots/' +
                          FIELDNAME + '/' + timeperiod + '_individual')
        self.timeperiod = timeperiod
        self.anom_cubes = anom_cubes

        if (FIELDNAME == 'NearSurfaceTemperature'
            or FIELDNAME == 'SST'):
                self.valmin = -5.
                self.valmax = 6.
                self.diff = 1.
                self.colormap = 'RdBu_r'

        if FIELDNAME == 'TotalPrecipitation':
            self.valmin = -2.
            self.valmax = 2.1
            self.diff = 0.2
            self.colormap = 'RdBu'


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        fig = plt.figure(figsize=(11.0, 8.5))
        for i in range(0, self.nmodels):

            cubedata = self.anom_cubes[i].data
            latitudes = self.anom_cubes[i].coord('latitude').points
            lon = self.anom_cubes[i].coord('longitude').points
            datatoplot, longitudes = (shiftgrid(180., cubedata,
                                                lon, start=False))
            #if (np.mod(i, 8) + 1) == 1:
            #    title_ = (MODELNAMES[i] + ':' +
            #              self.timeperiod + ' (model - MMM)')
            #else:
            #    title_ = (MODELNAMES[i])

            title_ = (MODELNAMES[i])
            self.plotmap(i, title_,
                         datatoplot, longitudes, latitudes, fig)


        return

    def plotmap(self, i, titlename, datatoplot, longitudes, latitudes, fig):
        """
        will plot the data in a map format

        """

        xplot = 4
        yplot = 4


        plotpos = np.mod(i, xplot * yplot) + 1
        plt.subplot(xplot, yplot, plotpos)
        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=-90.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)

        V = np.arange(self.valmin, self.valmax, self.diff)
        cs = map.contourf(x, y, datatoplot, V, cmap=self.colormap,
                          extend='both')
        plt.title(titlename)


        if plotpos == (xplot * yplot) or (i + 1) == self.nmodels:
             # Shrink current axis by 20% and put a legend to the right
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.82, top=0.9,
                                wspace=0.1, hspace=0.0)

            cb_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])
           
            cbar = fig.colorbar(cs, cax=cb_ax, orientation='vertical')
            #cbar = plt.colorbar(fig, orientation='horizontal')
            #fig.colorbar(fix, ax=axs[:, col], shrink=0.6)
            cbar.ax.tick_params(labelsize=15)
            cbar.set_label(UNITS, fontsize=15)
            print('plotted colorbar')
            #plt.show()
            #plt.tight_layout()
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.eps')
            plt.savefig(fileout, bbox_inches='tight')

            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.pdf')

            plt.savefig(fileout, bbox_inches='tight')
            plt.close()


##########################################################
# main program
# set up variable information

#FIELDNAME = 'NearSurfaceTemperature'
#UNITS = 'Celsius'
FIELDNAME = 'SST'
UNITS = 'Celsius'
#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'
#FIELDNAME = 'SST'
LINUX_WIN = 'l'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
             ]

#MODELNAMES = ['NorESM-L']



# set up cubelists to store data
mpwp_anom_cubes = iris.cube.CubeList([])
pi_anom_cubes = iris.cube.CubeList([])
anom_anom_cubes = iris.cube.CubeList([])


#################################################
# get mean data
mean_plio_cube = getmeanfield('mPWP')
mean_pi_cube = getmeanfield('pi')
mean_anom_cube = getmeanfield('anomaly')


for model, modelname in enumerate(MODELNAMES):
    model_plio_cube = getmodelfield(modelname, 'EOI400')
    model_pi_cube = getmodelfield(modelname, 'E280')
    print(modelname)
    if modelname == 'EC-Earth3.1' and FIELDNAME == 'SST':
       model_pi_cube.coord('latitude').bounds = None
       model_pi_cube.coord('longitude').bounds = None

    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_anom_cubes.append(model_plio_cube - mean_plio_cube)
    pi_anom_cubes.append(model_pi_cube - mean_pi_cube)
    anom_anom_cubes.append(model_anom_cube - mean_anom_cube)

##################################################
# plot the cubes for the model anomalies relative to the mean

obj = Plotalldata('mPWP', mpwp_anom_cubes)
obj.plotdata()

obj = Plotalldata('PI', pi_anom_cubes)
obj.plotdata()

obj = Plotalldata('mPWP-PI', anom_anom_cubes)
obj.plotdata()

#
::::::::::::::
PlioMIP_new/large_scale_features/precip_plot_for_IPCC_with_data.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap


def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """
    colors = [(84, 48, 5), (113, 70, 16), (143, 93, 27), (173, 115, 38),
              (195, 137, 60), (206, 160, 97), (216, 182, 135),
              (227, 204, 173), (238, 226, 211), (248, 248, 247),
              (212, 230, 229), (176, 212, 209), (140, 194, 190),
              (103, 176, 170), (67, 158, 150), (44, 135, 127),
              (29, 110, 100), (14, 85, 74), (0, 60, 48)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube


def get_model_data():
    """
    read in precipitation data and return
    """

    (lsm_cube, lsm_plio_cube) = get_lsm()

    precip_cube = iris.load_cube(PRECIP_MMM_FILE, 
                                 'TotalPrecipitationmean_anomaly')
  
   
    return precip_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns precipitation
    """

    dfs = pd.read_excel(PRECIP_DATAFILE)
    sites = []
    lats = []
    lons = []
    precip = []
   
    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        precip_file = dfs.iloc[rl,11]
        if precip_file == '1000**':
            precip_file = 1000.
        print(rl, precip_file)
        if np.isfinite(precip_file):
           sites.append(dfs.iloc[rl, 0])
           lats.append(dfs.iloc[rl, 2])
           lons.append(dfs.iloc[rl, 3])
           precip.append(precip_file / 365.) # mm/year to mm/day

    return lats, lons, precip

def get_cru_precip(lats, lons):
    """
    get's the cru precip at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUPRECIP/' + 
               'E280.TotalPrecipitation.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_precip = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_precip[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_precip[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_precip[i] = np.nanmean(surround)

        # convert from mm/month to mm/day
        cru_precip[i] = cru_precip[i] * 12.0 / 365. 
           
    return cru_precip


   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -1.4
    vmax = 1.4
    incr = 0.1
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    #cbar = plt.colorbar(cs,  orientation= 'horizontal',
    #                    ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar = plt.colorbar(cs,  orientation= 'horizontal')
   
    cbar.set_label('mm/day')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI precipitation anomaly')
    

    # overplot data 
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
 
    plt.scatter(lons, lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Panom.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Panom_.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsm_cube = get_model_data()

    # get land observations and cru precipitation at land points
    
    lats, lons, land_precip = get_land_obs()
    cru_land_precip = get_cru_precip(lats, lons)

    print('land precip obs',land_precip)
    print('cru precip',cru_land_precip)
   
    data_panom = land_precip - cru_land_precip
    print('precip anom',data_panom)
    
  
    plot(model_anom_cube, lsm_cube, lats, lons, data_panom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')

PRECIP_MMM_FILE = FILESTART + 'regridded/TotalPrecipitation_multimodelmean.nc'  
PRECIP_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_HadGEM3_clim.py
::::::::::::::
import iris 
import numpy as np
import numpy.ma as ma

def regrid_HadGEM(field, period):

    """
    creates the netcdf averaged files from HG3
    """

    periodalt = {'E280' : 'pi',
                 'Eoi400' : 'pliocene'}

    if field == 'NearSurfaceTemperature':
        fileend = ('/atmos/clims_hadgem3_' + periodalt.get(period) + 
                    '_airtemp_final.nc')
    if field == 'TotalPrecipitation':
        fileend = ('/atmos/clims_hadgem3_' + periodalt.get(period) + 
                    '_precip_final.nc')
  
    if field == 'SST':
       fileend = ('/ocean/clims_hadgem3_' + periodalt.get(period) + 
                    '_sst')
       if period == 'Eoi400':
           fileend = fileend + '_final.nc'
       else:
           fileend = fileend + '.nc'

    filein = ('/nfs/hera1/pliomip2/data/HadGEM3_new/climatologies/' 
          + period + fileend)

    cube = iris.load_cube(filein)
    cube.long_name = field
    if field == 'NearSurfaceTemperature':
        cube.data = cube.data - 273.15
        cube.units = 'Celsius'
    if field == 'TotalPrecipitation':
        cube.data = cube.data *60.*60.*24.
        cube.units = 'mm/day'
    print(cube)

    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())
    print('cubegrid',cubegrid)
    print('regridded_cube',regridded_cube)
   
    if field == 'SST':
        regridded_data = np.ma.asarray(regridded_cube.data) 
        
        for index, x in np.ndenumerate(regridded_data):
            if not np.isfinite(x):
                regridded_data.mask[index] = True
        
       
  

    fileout = ('/nfs/hera1/earjcti/regridded100/HadGEM3/' + period.upper() 
               + '.' + 
           field + '.mean_month.nc')

    iris.save(regridded_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    avg_cube =  regridded_cube.collapsed(['time'], iris.analysis.MEAN)
   
    fileout = ('/nfs/hera1/earjcti/regridded100/HadGEM3/' 
               + period.upper() + '.' + 
               field + '.allmean.nc')

    iris.save(avg_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    return (regridded_cube, avg_cube)


########################################
def avg_NorESM_SST(model, period):
    """
    averages NorESM that the NorESM group regridded
    """

    filename = ('/nfs/hera1/pliomip2/data/' + model + 
                 '/' + model + '_' + period + '.sst.climo.nc')
    cubeorig = iris.load_cube(filename)
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    cube = cubeorig.regrid(cubegrid, iris.analysis.Linear())


    fileout = ('/nfs/hera1/earjcti/regridded100/'+ model +'/' 
               + period.upper() +  
               '.SST.mean_month.nc')

    iris.save(cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    avg_cube =  cube.collapsed(['time'], iris.analysis.MEAN)

    fileout = ('/nfs/hera1/earjcti/regridded100/' + model + '/' + period.upper() + 
               '.SST.allmean.nc')

    iris.save(avg_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    return cube, avg_cube

################################################
def means_to_txt(modelname, annmeancube, avgcube, field, period):
    """ 
    creates the text file from HadGEM3
    """ 

    textout = ('/nfs/hera1/earjcti/regridded100/' + modelname 
               + '/' + period.upper() +
               '.' + field + '.data.txt')
    file1 =  open(textout, "w")

    # get mean field for cube

    avgcube.coord('latitude').guess_bounds()
    avgcube.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(avgcube)
    tempcube = avgcube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data 


    # get mean for each latitude
    tempcube = avgcube.collapsed(['longitude'], iris.analysis.MEAN)
    
    meanlat = tempcube.data 
    meanlat = np.squeeze(meanlat)
  
    
    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2))+', 0.0\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    annmeancube.coord('latitude').guess_bounds()
    annmeancube.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(annmeancube)
    tempcube = annmeancube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    if field == 'NearSurfaceTemperature':
        meanmon = tempcube.data -273.15
    else:
        meanmon = tempcube.data 

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', 0.0\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(avgcube.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', 0.0\n')

    file1.close()


#############################################

periods = ['Eoi400','E280']
fields = ['NearSurfaceTemperature','SST']
#fields = ['TotalPrecipitation']

for period in periods:
    for field in fields:
        (annmeancube, avgcube) = regrid_HadGEM(field, period)
        means_to_txt('HadGEM3',annmeancube,avgcube, field, period)


######
# also average noresm here because we need to do it somewhe
#models = ['NorESM1-F','NorESM-L']
#for period in periods:
#    for model in models:
#        annmeancube, avgcube = avg_NorESM_SST(model, period)
#        means_to_txt(model, annmeancube, avgcube, 'SST', period)
        
::::::::::::::
PlioMIP_new/large_scale_features/regrid_HCM3_50_year_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will is a bit like the regridding program for PlioMIP.
# However it will regrid non PlioMIP experiments for HadCM3
# and calculate all the means.
# There is also the option to calculate the means without regridding
# 
# Before this program preprocess data using extract_HadCM3_PlioMIP.py

import numpy as np
import iris
from iris.cube import CubeList
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
#from iris.experimental.equalise_cubes import equalise_attributes
import cf_units as unit
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################

def get_hadcm3_cube():
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = CubeList([])
  
    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+fieldnamein + '/' + exptnamein + '.' + fieldnamein + '.' + yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

       # if i ==startyear:
       #     allcubes = iris.cube.CubeList([])
   
        allcubes.append(cubetemp)

    iris.util.equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

  
    return(cube)


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = '100_'
    else:
        regridded = '_'

    if REGRID == 'y':
        regridded = 'regridded' + regridded
   

    # outfile
    if linux_win  == 'l':
        print(regridded,  exptnameout, fieldnameout)
        if REGRID == 'n':
            outstart = (filename + fieldnameout + '/means/' )
        if REGRID == 'y':
            outstart = (filename + fieldnameout + '/regriddedmeans/' )
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    cube100 = get_hadcm3_cube()
    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if REGRID == 'n':
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    if fieldnamein  == 'tas' or fieldnamein  == 'SST':
        regridded_cube.convert_units('Celsius')
        cube.convert_units('Celsius')

  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    print('writing outfile',outfile)
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int64(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()



##########################################################
# main program

filename  =  ' '
linux_win  =  'l'

# this is regridding where all results are in a single file
avg100yr = 'n'

fieldnamein = 'NearSurfaceTemperature'
exptnamein = 'xozzf'
REGRID = 'n'
startyear = 0
endyear = 100
 
 
if linux_win  == 'l':
    filestart = '/nfs/hera1/earjcti/um/' + exptnamein + '/' 
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

fielduse = fieldnamein
filename = filestart
     

fieldnameout = fieldnamein
exptnameout = exptnamein
regrid_data(fieldnamein, exptnamein)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_noresm.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019
"""
The NorESM group have now uploaded averaged data for SST.  We will need to 
get this in the required format for our paper.  
"""


import numpy as np
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import sys


##############################################
def write_avg_to_file(moncube, anncube):
    """
    average the data and write to a file
    """


    textout = FILEDIR + PERIOD + '.SST.data.txt'

    file1 =  open(textout, "w")

    # 1, globalmean

    anncube.coord('latitude').guess_bounds()
    anncube.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(anncube)
    tempcube = anncube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data
    stdevann=-999.999


    # get mean for each latitude
    tempcube = anncube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)
    stdevlat = np.zeros(np.shape(meanlat)) - 1000.



    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    moncube.coord('latitude').guess_bounds()
    moncube.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(moncube)
    tempcube = moncube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data
    stdevmon = np.zeros(12) - 1000.

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(anncube.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    
    
def main():
    """
    reprocesses the NorESM SST data.  
    We have a monthly averaged netcdf file.  
    We need an annual averaged netcdf file + a text file containing all the averages
    """

    filein = FILEDIR + PERIOD + '.SST.mean_month.nc'
    cube = iris.load_cube(filein, 'Ocean surface temperature')
    
    # write mean cube to a file
    meancube = cube.collapsed(['time'],  iris.analysis.MEAN)
    iris.save(meancube, FILEDIR + PERIOD + '.SST.allmean.nc', 
              netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)
    
    # get text data

    write_avg_to_file(cube, meancube)
    print('end of prog')
   
    
##########################################################
# main program


print('start')
LINUX_WIN = 'l'
MODELNAME = 'NorESM1-F'
PERIOD = 'E280' #EOI400 E280

if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'
    
FILEDIR = (FILESTART + 'regridded/' + MODELNAME + '/')
main()
::::::::::::::
PlioMIP_new/large_scale_features/regrid_ocn_100yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
        if exptname  == 'E280':
            lsmfile = lsmstart+modelname+'/lsm.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        for name,  dimension in src.dimensions.iteritems():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.iteritems():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube


def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        print(cube.units)
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                       'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b.e12.B1850.f09_g16.preind.' + 
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

   
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear


        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if modelname  == 'EC-Earth3.1': # all fields in one file
        cube = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI-CGCM2.3'):
        cube = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube = get_miroc_tos()
    elif (modelname  == 'GISS'):
        cube = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        cube = get_cesm12(exptnamein)
    else:
        cube = iris.load_cube(filename)


    ndim = cube.ndim




    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid UofTdata or a field that was originally on a tripolar grid
    print('julia1')
    if ((modelname   == 'UofT-CCSM4')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname  == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'UofT' or modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' or modelname == 'CESM1.2'
        or modelname == 'CCSM4-1deg'):
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            cube.data = cube.data* 60. *60. *24. *1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'UofT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname  == 'UofT':
        # we need to add the missing time coordinate
        points = (np.arange(0, 1200)*30)+15. # go for middle of month
        u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

        regridded_cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    elif (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
          modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate



       # end of Uof T loop

    print('julia2')
    #print(regridded_cube.coord('time'))

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".0806.0905"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'UofT':
        if linux_win  == 'l':
            filename = filestart+modelname+'/'
            filename = filename+'UofT-CCSM4/'+exptnamein+'/'+atm_ocn_ind.get(fieldnamein)+'/'
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '_'+atm_ocn_ind.get(fieldnamein)+'_'+exptnamein+
                      '_'+modelname+'-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI-CGCM2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1':
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        print(fielduse)
        filename = (filestart+'EC-Earth3.1/'
                  +ECearth_EXPT.get(exptnamein)
                  +'.EC-Earth3.1.surface.nc')
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "IPSLCM5A2" # MIROC4m  COSMOS UofT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS
                   # new to this version CCSM4-2deg, CESM1.2
                   # CCSM4-1deg

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr']
#exptnamein = ['Eoi400']

#fieldnamein = ['pr']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['tos']
exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_ocn_50yr_avg_pre_HadGEM.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    
    if fieldnamein  == 'pr':
       cube.data = cube.data * 60. *60. *24. *1000.
       cube.name = 'Total precipitation'
       cube.long_name = 'Total precipitation'
       cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e21.B1850.f09_g17.' +
                           'CMIP6-piControl.001.cam.h0.'+
                           'LANDFRAC.1300.1399.nc')
            else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'
       or modelname == 'CESM2'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4-1deg'
          or modelname == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    else:
        cube100 = iris.load_cube(filename)


  

    ###########################################
    # reduce number of years to 50

    cube = reduce_years(cube100)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4-1deg'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }
    
    CESM2_TIME = {"E280" : ".1300.1399",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        if linux_win  == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        filename = (filename +  fielduse +
                      '_' + atm_ocn_ind.get(fieldnamein) +
                      '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CESM2" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['Eoi400','E280']

fieldnamein = ['tos']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_ocn_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# note a similar program which calculates the means without regridding is
# noregrid_ocn_50yr_avg.py


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    print('julia',fielduse)
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    print(cube.data)
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm_400(fieldname):
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename, fieldname)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    print(allcube)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        print('before',filename)
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F' and exptnamein == 'E400'):
        cube100 = get_noresm_400(fielduse)
    else:
        cube100 = iris.load_cube(filename)

   
     

    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A' or modelname == 'HadGEM3'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-Utr' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                    "E400": "b.e21.B1850.f09_g17.CMIP6-piControl.400.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : ".110001-120012",
                  "E400" : ".0801.0900",
                  "Eoi400" : ".110001.120012"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        if linux_win  == 'l':
#            filename = filestart + 'UofT/'
#            filename = (filename + 'UofT-CCSM4/for_julia/' + 
#                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
            filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        
        
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
        if exptnamein == 'E400':
            filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_TREFHT_PRECT_month.nc')
            fielduse = NorESM_FIELDS.get(fieldnamein + 'E400')
            if fieldnamein == 'tos':
                filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_SST_month.nc')
          
            

    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            if exptnamein == 'Eoi400' or exptnamein == 'E400':
                filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename = [filename1, filename2]
                fielduse = ['PRECC', 'PRECL']
            if exptnamein == 'E280':
                filename = (filestart + 'NCAR/b.e21.B1850.f09_g17.' + 
                            'CMIP6-piControl.001.cam.h0.PRECT.110001-120012.nc')
                fielduse = 'PRECT'
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            print(exptnamein)
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
        if fieldnamein =='totcloud':
            filestart='/nfs/hera1/earjcti/PLIOMIP2/CESM2/clt_Amon_CESM2_'
            fielduse = 'clt'
            if exptnamein == 'Eoi400':
                filename = (filestart + 'midPliocene-eoi400_r1i1p1f1_'+
                            'gn_015101-020012.nc')
            if exptnamein == 'E280':
                filename = (filestart +'piControl_r1i1p1f1_gn_090001-099912.nc')
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "NorESM1-F" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

#fieldnamein = ['tas']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['tos']
#exptnamein = ['Eoi400', 'E280','E560']
exptnamein = ['E400']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
#        sys.exit(0)
        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_ocn_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
        if exptname  == 'E280':
            lsmfile = lsmstart+modelname+'/lsm.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    
    if fieldnamein  == 'pr':
       cube.data = cube.data * 60. *60. *24. *1000.
       cube.name = 'Total precipitation'
       cube.long_name = 'Total precipitation'
       cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        print(cube.units)
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                       'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b.e12.B1850.f09_g16.preind.' + 
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if modelname  == 'EC-Earth3.1': # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    else:
        cube100 = iris.load_cube(filename)


  

    ###########################################
    # reduce number of years to 50

    cube = reduce_years(cube100)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname  == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' or modelname == 'CESM1.2'
        or modelname == 'CCSM4-1deg'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    #print(regridded_cube.coord('time'))

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        if linux_win  == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        filename = (filename +  fielduse +
                      '_' + atm_ocn_ind.get(fieldnamein) +
                      '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1':
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        print(fielduse)
        filename = (filestart+'EC-Earth3.1/'
                  +ECearth_EXPT.get(exptnamein)
                  +'.EC-Earth3.1.surface.nc')
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CCSM4-2deg" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # new to this version CCSM4-2deg, CESM1.2
                   # CCSM4

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['Eoi400','E280']

#fieldnamein = ['pr']
fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_ocn_tripolar.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#Created on June2019 2019


#
# This very simple program will convert data that is on a tripolar grid onto a 
# rectilinear grid.  It will still need to be passed through regrid_ocn in order
# to calculate means etc.


import numpy as np
import cf
import iris
#import cfplot as cfp
#import matplotlib.pyplot as plt
from netCDF4 import Dataset, MFDataset
#import sys
#import os


def reformat_with_iris(exptnamein):
    """
    we are going to load the data as an iris cube and add some
    auxillary coordinates and then write out to a file called temporary.nc
    
    the data is currently in filename
    """
    
    print('reformatting ',filename)
    
    origcube = iris.load_cube(filename, 'Near-Surface Air Temperature')
    latcube = iris.load_cube(filename, 'array of t-grid latitudes')
    loncube = iris.load_cube(filename, 'array of t-grid longitudes')
    

    print(origcube)
   
    # promote the auxillary coordinates to dimension coordinates
    nt, ny, nx = origcube.shape
    origcube.coord('nlat').points=np.arange(0,ny,1)
    origcube.coord('nlat').rename('y')
    origcube.coord('y').var_name='y'
    origcube.coord('y').long_name=None
    origcube.coord('y').units=None
    origcube.coord('nlon').points=np.arange(0,nx,1)
    origcube.coord('nlon').rename('x')
    origcube.coord('x').var_name='x'
    origcube.coord('x').long_name=None
    origcube.coord('x').units=None
    print('j3',origcube)
    iris.util.promote_aux_coord_to_dim_coord(origcube, 'y')
    iris.util.promote_aux_coord_to_dim_coord(origcube, 'x')
    print(origcube.coord('x'))
    
 
    
    
    # add an auxillary coordinate for latitude and longitude these are 
    # 2d coordinates
    loncoord=iris.coords.AuxCoord(loncube.data,standard_name='longitude', 
                                  long_name='Longitude',var_name='nav_lon',
                                  units='degrees_east')
    latcoord=iris.coords.AuxCoord(latcube.data,standard_name='latitude', 
                                  long_name='Latitude',var_name='nav_lat',
                                  units='degrees_north')


    origcube.add_aux_coord(loncoord,[1,2])
    origcube.add_aux_coord(latcoord,[1,2])
    
    print('j6')
    print(origcube)
    print(origcube.coord('latitude'))
    #sys.exit(0)
    
    
    iris.save(origcube, exptnamein + '_temporary.nc', 
              fill_value=2.0E20)
    print('saved')

    


#####################################
def regrid_data(fieldnamein,fieldnameout,exptnamein,exptnameout,filename,modelname,linux_win,fielduse,filenameout):

   
    
    print('moodelname is',modelname)
    print('filename is',filename)
    print('fielduse is',fielduse)
      
    
    if ((modelname=='IPSLCM5A') and exptnamein=='Eoi400'):
       # there is a bit of an error in the file calendar so we will 
       # copy the data to a new file but without the error
       with Dataset(filename) as src, Dataset("temporary.nc", "w",format='NETCDF3_CLASSIC') as dst:
        # copy attributes
            for name in src.ncattrs():
                dst.setncattr(name, src.getncattr(name))
                #print('att',name)   
                # copy dimensions
            for name, dimension in src.dimensions.iteritems():
              
                if name != 'tbnds':   # don't copy across time counter bounds
                   # if fielduse=='SeasurfaceTemp_sst' and name=='y':
                   #     name='latitude'
                   # if fielduse=='SeasurfaceTemp_sst' and name=='x':
                   #     name='longitude'
                    dst.createDimension(name, (len(dimension)))
           
                     
            # copy all file data 
            for name, variable in src.variables.iteritems():
                print('name is',name,variable)
                if name !='time_counter_bnds' and name!='time_centered':
                    x = dst.createVariable(name, variable.datatype, 
                                               variable.dimensions)
                       
                    if name=='time_counter':
                    # convert from seconds to days and start at middle of month
                        dst.variables[name][:] = (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                    else:
                        dst.variables[name][:] = src.variables[name][:]
                    # copy attributes for this variable
                    for ncattr in src.variables[name].ncattrs():
                        attribute=src.variables[name].getncattr(ncattr)
                        #print('j2',name,ncattr,attribute)
                           
                        if ncattr=='calendar' and exptnamein=='Eoi400':
                            dst.variables[name].setncattr(ncattr,'360_day')
                        else:
                            if (ncattr=='units' and name=='time_counter'):
                            # change units from seconds to days
                                dst.variables[name].setncattr(ncattr,attribute.replace('seconds','days'))
                            else:
                                dst.variables[name].setncattr(ncattr,attribute) 
               
      
    
        
       origf=cf.read_field('temporary.nc')
       print('read copied dataset')
    else:
       if (modelname == 'IPSLCM6A'):
           print (filename)
           origf = cf.read_field(filename, select='sea_surface_temperature')
       elif (modelname == 'CESM1.0.5'):
           #origf = cf.read(filename)
           #print(origf)
           #sys.exit(0)
           reformat_with_iris(exptnamein) # the data is not in a very good format
                                # we will reformat and write to temporary.nc
           
           print('about to read temporaray.nc')
           
           origf=cf.read_field(exptnamein + '_temporary.nc', )
           # iterate over auxillary coordinates and see if they are
           # longitud eor latitude
           for aux in origf.auxs():
               print (origf.auxs(aux))
           a=origf.aux('latitude')
           a.units=('degrees_north')
           print(a)
           
           b=origf.aux('longitude')
           b.units=('degrees_east')
           print(b)
           print(origf)
           #sys.exit(0)  
               
           #for key, aux in self.auxs(ndim=2).iteritems():
           #     if aux.Units.islongitude:
           #sys.exit(0)
           print (origf)
           print (origf.data_axes())
           print (origf.coords())
           
         
       elif (modelname == 'HadGEM3'):
           
       else:
           origf=cf.read_field(filename)
        
        
    print (origf)
   
    
    gridf=cf.read_field('one_lev_one_deg.nc')
    #print  gridf
    
    print('about to regrid')
   
    # assume tripoplar grid
    if modelname == 'CESM1.0.5':
       regridf=origf.regrids(gridf,
                          method='bilinear',
                          src_axes={'X': 'ncdim%x','Y':'ncdim%y'},
                          src_cyclic=True)
    else:   
        regridf=origf.regrids(gridf,
                          method='bilinear',
                          src_axes={'X': 'ncdim%x','Y':'ncdim%y'},
                          src_cyclic=True)
    print('regridded')
    
    # see if we can remove auxillary coordinates
    if (modelname=='IPSLCM5A' or modelname=='IPSLCM5A2'):
        regridf.remove_item(description='T',role='a')
   
    
   
    # write to a temporary file so that we can read in as an iris cube and do all our iris analysis 
    # in exactly the same way as before
    
    print(filename)
    print(filenameout)
    cf.write(regridf,filenameout,fmt='NETCDF4_CLASSIC')
 
   
  
    
   
  

#############################################################################
def getnames(modelname,fieldname,exptname): 
    """
    parameters: 
        modelname - the model
        fieldname - the name of the field ie 'tos'
        exptname - the name of the experiment ie [Eoi400]
        
    returns:
        fielduse - the name of the field in the file
        filename - the input file
        filenameout = the output file
    
    this program will get the names of the files and the field for each
    of the models   
    
    """
    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                    "tas" : "NearSurfaceAirTemp",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "SeaSurfaceTemp"
                    }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }
    
    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }
    
    NorESM_FIELDS = {"pr" : "PRECT",
                     "ts" : "TREFHT",
                     "sic" : "SeaIceAreaFraction"
                     }
    
     
    CESM105_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }

    ECearth_EXPT = {"Eoi400" : "mPlio",
                    "E280" : "PI"
                    }
    
    IPSLCM5A_EXPT = {"Eoi400" : "Eoi400",
                     "E280" : "PI"
                     }
    
    IPSLCM5A_TIME = {"Eoi400" : "3581_3680",
                     "E280" : "3600_3699"
                     }
    
    IPSLCM5A21_TIME = {"Eoi400" : "3381_3480",
                     "E280" : "6110_6209",
                     }
    
    IPSLCM6A_TIME = {"Eoi400" : "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                     "E280":"piControl_r1i1p1f1_gn_285001-304912",
                     }
    
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"
                   }

    # get names for each model
   
    if modelname == 'COSMOS':
        if linux_win=='l':
            filename=filestartin+'/AWI/COSMOS/'
            filename=filename+exptname+'/'
        else:
            filenameout=filestartout+'/COSMOS/'
        fielduse=COSMOS_FIELDS.get(fieldname)
        filename=(filename+exptname+'.'+fielduse+
                      '_CMIP6_name_'+fieldname+
                      '_2650-2749_monthly_mean_time_series.nc')
        filenameout=(filestartout+'COSMOS/'+exptname+'.'+fielduse+
                      '_CMIP6_name_'+fieldname+
                      '_2650-2749_monthly_mean_time_series_rectilinear.nc')
   
   
    if modelname=='IPSLCM5A' or modelname=='IPSLCM5A2':
        exptuse=exptname_l.get(exptname)
        if modelname=='IPSLCM5A':
            timeuse=IPSLCM5A_TIME.get(exptname)
        if modelname=='IPSLCM5A2':
            timeuse=IPSLCM5A21_TIME.get(exptname)
        fielduse=IPSLCM5A_FIELDS.get(fieldname)
        filename=(filestartin+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptname)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')
        filenameout=(filestartout+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptname)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        print(filename)
        print(filenameout)
        
        
   
    if modelname=='IPSLCM6A':
        fielduse=MIROC_FIELDS.get(fieldname)
        filename=(filestartin+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptname)+'.nc')
        filenameout=(filestartout+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptname)+'_rectilinear.nc')
        
    if modelname == 'CESM1.0.5':
        print (filestartin)
        print (modelname)
        print (exptname)
        print (CESM105_FIELDS.get(fieldname))
        
        filename = (filestartin + modelname + '/' + exptname + '/' +
                  exptname + '_' + CESM105_FIELDS.get(fieldname) +
                  '.nc')
        fielduse = fieldname
        filenameout = (filestartout + modelname + '/' + exptname + '/' +
                  exptname + '_' + CESM105_FIELDS.get(fieldname) +
                  '.nc')
    
        print(filename)
        print(filenameout)
        #sys.exit(0)

    if modelname == 'HadGEM3':
        ending = 'midPliocene-eoi400_r1i1p1f1_gn_233401-239312.nc'
        filename = (filestartin + 'HadGEM3/tos_Omon_HadGEM3-GC31-LL_' + 
                   ending)
        fielduse = 'sea_surface_temperature'
        filenameout = filestartin + 'HadGEM3/regridded_' + ending
   
    
    return [fielduse,filename,filenameout]


##########################################################
# main program

filename=' '
linux_win='l'
modelname='HadGEM3' # IPSLCM5A, IPSLCM5A2
                   #                      IPSLCM6A
                   # 'CESM.1.0.5 HadGEM3
                   
#modelname = 'IPSLCM5A'

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

# only need fields that are on a tripolar grid like orca
fieldname = {
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein=['ts','pr']
#exptnamein=['Eoi400','E280']

#fieldnamein=['pr']
fieldnamein=['tos'] # ocean tempeature or sst
#exptnamein=['E280','Eoi400']

exptnamein=['E280']
if linux_win=='l':
    filestart='/nfs/hera1/pliomip2/data/'
    if (modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2' 
        or modelname == 'COSMOS' or modelname =='CESM1.0.5'):
        filestartin='/nfs/hera1/pliomip2/data/'
    if modelname=='IPSLCM6A' or modelname == 'HadGEM3':
        filestartin='/nfs/hera1/earjcti/PLIOMIP2/'
    filestartout='/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    
    


for expt in range(0,len(exptnamein)):
    for field in range(0,len(fieldnamein)):

        # call program to get model dependent names
        # fielduse, and  filename 
        fielduse, filename, filenameout = (getnames
                                           (modelname,fieldnamein[field],
                                            exptnamein[expt]))
        
        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])

        print ('filename',filename)
        
        regrid_data(fieldnamein[field],fieldnameout,exptnamein[expt],exptnameout,
                    filename,modelname,linux_win,fielduse,filenameout)

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_timeseries.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 21 2020

#
# This program will produce a regridded 1X1degree timeseries of a given field.  
# We will remove the annual cycle in order to look for interannual variability 
# etc.  


import numpy as np
from netCDF4 import Dataset
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname, lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse == "sst":
        if exptname == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'

        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data = np.ma.array(cube.data, mask=mymask)
    return cube

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u = unit.Unit('days since 0800-01-01 00:00:00',
                      calendar=unit.CALENDAR_360_DAY)
        if model == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model == 'HadCM3' and fielduse == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model == 'HadCM3' and fielduse == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube = cube_temp.extract(iris.Constraint(surface=0.))

    return cube

def get_noresm_ocn(exptnamein, fieldnamein):
    """
    get noresm ocean things one file per month

    """
    print(modelname, exptnamein)
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    fileend = '_remap.nc'
   
    
    if exptnamein == 'Eoi400' and modelname == 'NorESM1-F': 
        startyear = 2400
        endyear = 2500
    if exptnamein == 'E280' and modelname == 'NorESM1-F': 
        startyear = 1900
        endyear = 2000
    if exptnamein == 'E280' and modelname == 'NorESM-L': 
        startyear = 2100
        endyear = 2200
    if exptnamein == 'Eoi400' and modelname == 'NorESM-L': 
        startyear = 1100
        endyear = 1200
  
    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        for i, mon in enumerate(months):
            file = filename + np.str(year) + '-' + mon + fileend
            cubetemp = iris.load_cube(file, 'Ocean surface temperature')
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
    print(cube)
  
  
    return cube

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src, Dataset("temporary.nc", "w", format='NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name, src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name, dimension in src.dimensions.items():

            if name != 'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name, (len(dimension)))

        # copy all file data
        for name, variable in src.variables.items():
            print('name is', name, variable)
            if name not in ('time_counter_bnds', 'time_centered'):
                x = dst.createVariable(name, variable.datatype,
                                       variable.dimensions)
                if name == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:] = ((src.variables[name][:] / (60.*60.*24))
                                              -(src.variables[name][0] / (60.*60.*24)) + 15.)
                else:
                    dst.variables[name][:] = src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr == 'calendar' and exptname == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr == 'units' and name == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein in ('ts', 'tas'):
            cube.convert_units('Celsius')

        if exptname == 'Eoi400':
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_360_DAY)
        else:
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return cube

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return cube

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return cube

def get_HadGEM3():
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    # eoi400
    #startyear = 2334
    #endyear = 2434

    #e280
    startyear=1950
    endyear = 2050
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        #if year < 2394: 
        #   extra = 'v963'
        #else:
        #    extra = 'x150'
        #e280
        extra='q637'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    return cube



def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """

    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]

    # put units as celcius if required
    if fielduse == 'tas':
        cube.units = 'Celsius'

    cube2 = iris.util.new_axis(cube, 'time')

    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """

    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

    cube.add_dim_coord(iris.coords.DimCoord(points,
                                            standard_name='time', long_name='time',
                                            var_name='time',
                                            units=u,
                                            bounds=None,
                                            coord_system=None, circular=False), 0)

    if fieldnamein == 'pr':
        cube.data = cube.data * 60. *60. *24. *1000.
        cube.name = 'Total precipitation'
        cube.long_name = 'Total precipitation'
        cube.units = 'mm/day'


    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]

    return singlecube

def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """

    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
        cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
        cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
        if fieldnameout == 'TotalPrecipitation':
            cube = cube1 + cube2
        if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
            cube1.convert_units('Celsius')
            cubedata = np.where((cube2.data > 0.01),
                                -1.8, cube1.data)
            cube = cube1.copy(data=cubedata)
    else:
        cube = get_cesm12_singlecube(filename, fielduse)

    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b.e21.B1850.f09_g17.' +
                           'PMIP4-midPliocene-eoi400.001.'+
                           'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' +
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280':
            #if modelname == 'CESM2':
            #    filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
           #                'b.e21.B1850.f09_g17.' +
           #                'CMIP6-piControl.001.cam.h0.'+
           #                'LANDFRAC.1300.1399.nc')
           # else:
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                       'b.e12.B1850.f09_g16.preind.' +
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data = np.ma.array(cube.data, mask=mymask)


    return cube



######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube

    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years


    """

    if modelname in ('CCSM4', 'CESM1.2', 'CESM2'):
        print('CCSM',cube.coord('month').points)
        months = cube.coord('month').points
        months = months -1
        for i, month in enumerate(months):
            if month == 0: months[i] = 12
        print(months)
        cube.coord('month').points = months
 
    else:
        startyear = (cube.coord('year').points[0])
        endyear = (cube.coord('year').points[-1])
        # count the number of months that have the same year as the first index
        nstart = 0
        nend = 0
        for i in range(0, 12):
            if cube.coord('year').points[i] == startyear:
                nstart = nstart+1
        for i in range(-13, 0):
            if cube.coord('year').points[i] == endyear:
                nend = nend+1
        if nend != 12 or nstart != 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
            if nend + nstart == 12:
                for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                    cube.coord('year').points[i] = startyear


                else:

                    print('you have a partial year somewhere')
                    print('correct input data to provide full years')
                    print(nend, nstart)
                    sys.exit(0)


    return cube

######################################################
def cube_avg(cube):
    """
    Extract monthly averaged data from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months

    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)

    meanmonthcube.long_name = fieldnameout

    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')

    return meanmonthcube


def remove_ann_cycle(cube, mon_avg_cube):
    """
    removes the annual cycle stored in mon_avg_cube from cube
    """

    cubedata = cube.data
    for i, monthno in enumerate(cube.coord('month').points):
        mon_avg_data = (mon_avg_cube.extract(iris.Constraint(month=monthno))).data
        cubedata[i, :, :] = cubedata[i, :, :] - mon_avg_data


    timeseries_cube = cube.copy(data=cubedata)

    return timeseries_cube


##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

    # outfile
    if linux_win == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/' + modelname +
                    '/timeseries/' + exptnameout + '.' + fieldnameout + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
                    + modelname + '\\timeseries\\' + exptnameout 
                    + '.' + fieldnameout + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


    #####################################
    # get all data in a single cube
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3'): # all fields in one file
        cube = get_ecearth_cube(exptnamein, lsmstart)
    elif modelname in ('HadCM3', 'MRI2.3'):
        cube = get_hadcm3_cube(modelname)
    elif modelname in ('IPSLCM5A', 'IPSLCM5A2') and fieldnamein != 'tos':
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif modelname in ('NorESM1-F', 'NorESM-L') and fieldnamein == 'tos':
        cube = get_noresm_ocn(exptnamein, fieldnamein)
    elif modelname == 'IPSLCM6A':
        cube = get_ipslcm6()
    elif modelname in ('MIROC4m', 'tos'):
        cube = get_miroc_tos()
    elif modelname == 'GISS2.1G':
        cube = get_giss()
    elif modelname == 'CCSM4-Utr':
        cube = get_ccsm4_2deg()
    elif modelname in ('CESM1.2', 'CCSM4', 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'HadGEM3'):
        cube = get_HadGEM3()  
    else:
        cube = iris.load_cube(filename)

  
    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid

    if ((modelname == 'CCSM4-UoT')
            or (modelname == 'IPSLCM5A' and fieldnamein == 'tos')
            or (modelname == 'IPSLCM5A2' and fieldnamein == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A',
                     'IPSLCM5A2', 'IPSLCM6A', 'CCSM4-Utr', 'GISS2.1G'):
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if modelname in ('NorESM1-F', 'NorESM-L', 'CESM1.2', 'CESM2', 'CCSM4'):

       # if precipitation is in m/s convert to mm/day
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. * 60. * 24. * 1000.
            cube.data = cube.data * 60. * 60. * 24. * 1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if modelname in ('CCSM4-UoT', 'NorESM1-F', 'NorESM-L', 'IPSLCM6A',
                     'EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A', 'IPSLCM5A2',
                     'HadCM3', 'GISS2.1G'):
         # convert to celcius
        if fieldnamein in ('tas', 'tos'):
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname in ('COSMOS', 'MIROC4m', 'IPSLCM6A', 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate


    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube, 'time', name='month')
    iris.coord_categorisation.add_year(regridded_cube, 'time', name='year')


     # correct the start month if required
    regridded_cube = correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_cube = cube_avg(regridded_cube)



    # remove annual cyble
    anom_cube = remove_ann_cycle(regridded_cube, mean_mon_cube)

    # to check we have removed the average properly get the monthly
    # average of the anomaly cube it should be zero

    new_mean_mon_cube = cube_avg(anom_cube)
    qplt.contourf(new_mean_mon_cube[2, :, :], levels=np.arange(-0.01, 0.011, 0.001), extend='both')
    plt.show()



    # write the cubes out to a file

    outfile = outstart+'timeseries_no_ann_cycle.nc'
    iris.save(anom_cube, outfile, netcdf_format='NETCDF3_CLASSIC', fill_value=2.0E20)




#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                     }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }

    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }

    NorESM_FIELDS = {"pr" : "PRECT",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "sst"
                    }

    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                      }

    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA = {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                    "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }

    CESM2_EXTRA = {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                   "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                   }

    CCSM4_EXTRA = {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                   "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                   }

    ECearth_EXPT = {"Eoi400": "mPlio",
                    "E280":"PI"
                   }

    CESM12_EXPT = {"Eoi400": "PlioMIP2",
                   "E280":"PI"
                   }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
                     "E280":"PI"
                    }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }

    CESM2_TIME = {"E280" : ".110001-120012",
                  "Eoi400" : ".1101.1200"
                 }

    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
                     "E280":"3600_3699"
                    }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
                       "E280":"6110_6209",
                      }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
                     "E280":"piControl_r1i1p1f1_gr_285001-304912",
                     }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                         "E280":"piControl_r1i1p1f1_gn_285001-304912",
                        }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
                  "E280":"piControl_r1i1p1f1_gn_490101-495012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
                 }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
                       "E280":"piControl_r1i1p1f1_gr1_150101-160012",
                       "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                      }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
                  "E280":"piControl_r1i1p1f1_gn_495101-500012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                  }
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"}
    cosmos_version = {"tas": "",
                      "pr": "",
                      "tos":"_remapbil"}

    # get names for each model
    if modelname == 'MIROC4m':
        filename = filestart + modelname + '/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename + fielduse + '/MIROC4m_'+exptnamein
                    + '_' + atm_ocn_ind.get(fieldnamein) + '_' + fielduse + '.nc')
    if modelname == 'COSMOS':
        if linux_win == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename + exptnamein + '.' + fielduse +
                    '_CMIP6_name_' + fieldnamein +
                    '_2650-2749_monthly_mean_time_series' +
                    cosmos_version.get(fieldnamein) + '.nc')
    if modelname == 'CCSM4-UoT':
        if linux_win == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' +
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart + 'UofT-CCSM4\\' + exptnamein + '\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)

        filename = (filename + fielduse +
                    '_' + atm_ocn_ind.get(fieldnamein) +
                    '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart + 'LEEDS/HadCM3/' + exptuse + '/' + fielduse + '/'
                    + exptuse + '.' + fielduse + '.')
    if modelname == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                    +exptuse+'.'+fielduse+'.')
    if modelname == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname
                    + '_'
                    + ECearth_EXPT.get(exptnamein)
                    + fileend)
    if modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse 
                        + '_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse + '_monthly_TS.nc')

    if modelname == 'NorESM1-F' or modelname == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/sst_regrid_1degree/'
                    + modelname + '_' + exptnamein + '.' + fielduse + '.')
    if modelname == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' + fielduse +
                        '_Omon_IPSL-CM6A-LR_' + IPSLCM6A_TIME_ALT.get(exptnamein) 
                        + '_rectilinear.nc')
        else:
            filename = (filestart + modelname + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) + '_IPSL-CM6A-LR_' 
                        + IPSLCM6A_TIME.get(exptnamein) + '.nc')
    if modelname == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart + modelname + '/' + exptuse + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME1.get(exptnamein)
                        + '.nc')
        filename.append(filestart + modelname + '/' + exptuse + '/' 
                        + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME2.get(exptnamein) + '.nc')

    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein

    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                      CESM2_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECL' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']

        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'TS' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'ICEFRAC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']

        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' +
                      CCSM4_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)


    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if exptnamein == 'Eoi400':
            filename = filestart + 'Eoi400/ocean/sst_sal_temp/new_nemo_b'
        if exptnamein == 'E280':
            filename = filestart + 'E280/ocean/sst_sal_temp/new_nemo_b'
       
        fielduse = 'sea_surface_temperature'

    print(fielduse, filename)
    retdata = [fielduse, filename]
    return retdata


##########################################################
# main program

filename =  ' '
linux_win =  'l'
modelname = "HadGEM3" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"
            }

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"
            }

fieldname = {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['E280']

fieldnamein = ['tos']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if (modelname in ('IPSLCM5A', 'IPSLCM5A2', 'CCSM4-Utr')
            and fieldnamein[field] == 'tos'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if modelname in ('IPSLCM6A', 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]


        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/regrid_winds_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on june 24 2020; copied from regrid_ocn_50yr_avg.py


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 50 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#



import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)

    print(allcubes)
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()

 
    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 
                                                 'longitude',
                                                 'air_pressure'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot():
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    print(filename)
    cubes = iris.load(filename)
    cube = cubes[0]
    pressures = cubes[1].data
    
    print(cube)
    print(pressures)
    
    
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)

    cube.add_dim_coord(iris.coords.DimCoord(pressures,
                standard_name = 'air_pressure',  long_name = 'pressure',
                var_name = 'pressure',
                units = 'Pa',
                bounds = None,
                coord_system = None,  circular = False), 1)


    return cube

    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
        if allcube[i].var_name == "lev":
            cubelev = allcube[i]

    if exptnamein == 'E280' or MODELNAME == 'CCSM4' or MODELNAME == 'CESM1.2':
        pressures = cubelev.data * 100.
        print(pressures)
        cube.add_dim_coord(iris.coords.DimCoord(pressures,
                standard_name = 'air_pressure',  long_name = 'pressure',
                var_name = 'pressure',
                units = 'Pa',
                bounds = None,
                coord_system = None,  circular = False), 1)

    if exptname == 'E280' and MODELNAME == 'CESM1.2':
        points = (np.arange(0, 1200)*30)+15. # go for middle of month
        u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of
        cube.coord('time').points = points
        cube.coord('time').units = u
        iris.util.promote_aux_coord_to_dim_coord(cube, 'time')
       
         #cube.add_dim_coord(iris.coords.DimCoord(points,
         #       standard_name = 'time',  long_name = 'time',
         #       var_name = 'time',
         #       units = u,
         #       bounds = None,
         #       coord_system = None,  circular = False), 0)

    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube, exptnamein):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (MODELNAME  == 'NorESM1-F' or MODELNAME  == 'NorESM-L'
       or MODELNAME == 'CESM1.2' or MODELNAME == 'CCSM4'
       or (MODELNAME == 'CESM2' and exptnamein == 'E280')):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = FIELDNAMEOUT
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = FIELDNAMEOUT

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=FIELDNAMEOUT

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=FIELDNAMEOUT
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=FIELDNAMEOUT

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(exptnamein, filename, fielduse, constraint):
    """
    regrid the data
    """


    print('moodelname is', MODELNAME)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if LINUX_WIN  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+MODELNAME+'/'+ EXPTNAMEOUT +'.'+
        FIELDNAMEOUT + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +MODELNAME+'\\' + EXPTNAMEOUT + '.' + FIELDNAMEOUT + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (MODELNAME  == 'EC-Earth3.1' or
       MODELNAME == 'EC-Earth3.3'): # all fields in one file
        cube100 = iris.load_cube(filename, fielduse)
    elif (MODELNAME  == 'HadCM3' or MODELNAME  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(MODELNAME)
    elif ((MODELNAME  == 'IPSLCM5A' or MODELNAME  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (MODELNAME  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (MODELNAME  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (MODELNAME  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (MODELNAME  == 'CESM1.2' 
          or MODELNAME == 'CCSM4'
          or MODELNAME == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (MODELNAME == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot()
    else:
        cube100 = iris.load_cube(filename)

    print(cube100)

    cube_level = cube100.extract(constraint)
   
    ###########################################
    # reduce number of years to 50

    print(cube_level)
    #sys.exit(0)
    cube = reduce_years(cube_level)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((MODELNAME   == 'CCSM4-UoT')
        or (MODELNAME  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (MODELNAME  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if MODELNAME  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (MODELNAME  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'
   
        
    if (MODELNAME  == 'COSMOS' or MODELNAME  == 'MIROC4m' or
        MODELNAME  == 'IPSLCM6A' or 
        MODELNAME  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube, exptnamein)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg ' + FIELDNAMEIN)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    
    COSMOS_FIELDS  = {"ua" : "u-velocity", "va" : "v-velocity"}

    ECearth_FIELDS  = {"ua" : "U component of wind", 
                       "va" : "V component of wind" }

    HadCM3_FIELDS  = {"ua" : "U COMPNT OF WIND ON PRESSURE LEVELS", 
                      "va" : "V COMPNT OF WIND ON PRESSURE LEVELS" }

    IPSLCM5A_FIELDS  = { }

    NorESM_FIELDS = {"ua" : "U", "va" : "V"}
    
    CCSM42_FIELDS = {"ua" : "U", "va": "V" }
    
    CESM12_FIELDS = {"ua": "U", "va": "V"}
    
    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b.e12.B1850.f09_g16.preind.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0707.0806",
                   "Eoi400" : ".1101.1200"
                   }
    
    CESM2_TIME = {"E280" : ".1201.1300",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    
    # get names for each model
    if MODELNAME   ==  'MIROC4m':
        filename = FILESTART + MODELNAME + '/'
        fielduse = FIELDNAMEIN
        filename = (filename + fielduse + '/MIROC4m_' + 
                    exptnamein + '_Amon_' + fielduse + '.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
   

    if MODELNAME   ==  'COSMOS':
        if LINUX_WIN  == 'l':
            filename = FILESTART + 'AWI/COSMOS/'
            filename = filename + exptnamein + '/'
        else:
            filename = FILESTART + '/COSMOS/'
        fielduse = COSMOS_FIELDS.get(FIELDNAMEIN)
        filename = (filename + exptnamein + '.' + FIELDNAMEIN +
                    '_2650-2749_monthly_mean_time_series.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
   

    if MODELNAME   ==  'CCSM4-UoT':
        if LINUX_WIN  == 'l':
            filename = FILESTART + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/Amon/')
        else:
            filename = FILESTART+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = FIELDNAMEIN
        
        filename = (filename +  fielduse +
                      '_Amon_' + exptnamein + '_UofT-CCSM4_gr.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
        

    if MODELNAME  == 'HadCM3':
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = HadCM3_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + 'LEEDS/HadCM3/' + exptuse
                    + '/' + FIELDNAMEIN + '/'
                    + exptuse + '.' + FIELDNAMEIN + '.')
        constraint = iris.Constraint(p = LEVEL)

    if MODELNAME  == 'MRI2.3':
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = FIELDNAMEIN + str(np.int(LEVEL))
        
        filename = (FILESTART + 'MRI-CGCM2.3/' + fielduse + 
                    '/' + exptuse + '.' + fielduse + '.')
        constraint = None
      
    if MODELNAME  == 'EC-Earth3.1' or MODELNAME == 'EC-Earth3.3':
        fileend = '_uv.nc'
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = ECearth_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + MODELNAME + '/'
                    + MODELNAME 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
        constraint = iris.Constraint(air_pressure = LEVEL * 100)
   

    if MODELNAME  == 'IPSLCM5A' or MODELNAME  == 'IPSLCM5A2':
        exptuse = EXPTNAME_L.get(exptnamein)
        if MODELNAME  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if MODELNAME  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART+MODELNAME+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +IPSLCM5A_FIELD.get(fielduse)+'_'+timeuse+'_monthly_TS.nc')
        constraint = None

    if MODELNAME  == 'NorESM1-F' or MODELNAME  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + MODELNAME + '/' + MODELNAME + '_' 
                    + exptnamein + '_' + fielduse + '.nc')
        constraint = iris.Constraint(pressure = LEVEL)

    if MODELNAME  == 'IPSLCM6A':
        fielduse = FIELDNAMEIN
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/' + MODELNAME 
                    + '/' + fielduse + '_' + 'Amon_IPSL-CM6A-LR_'
                   + IPSLCM6A_TIME.get(exptnamein)+'.nc')
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
    if MODELNAME  == 'GISS2.1G':
        fielduse = FIELDNAMEIN
        exptuse = EXPTNAME_L.get(exptnamein)
        filename = []
        filename.append('/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
                        + exptuse + '/' + fielduse + '_Amon'
                        + '_GISS-E2-1-G_' 
                        + GISS_TIME1.get(exptnamein) + '.nc')
        filename.append('/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
                        + exptuse + '/' + fielduse + '_Amon'
                        + '_GISS-E2-1-G_' 
                        + GISS_TIME2.get(exptnamein) + '.nc')
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
       

    if MODELNAME == 'CCSM4-Utr':
        filename=(FILESTART + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(FIELDNAMEIN) +
                  '.nc')
        fielduse = FIELDNAMEIN
        
    if MODELNAME == 'CESM1.2':
        filename=(FILESTART + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CESM12_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
        

    
    if MODELNAME == 'CESM2':
        print(exptnamein)
        print(CESM2_TIME.get(exptnamein))
        print(CESM2_EXTRA.get(exptnamein))
        print(CESM12_FIELDS.get(FIELDNAMEIN))

        filename=(FILESTART + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CESM2_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
               
            
    if MODELNAME == 'CCSM4':
        filename = (FILESTART + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
        
    print(fielduse,filename,constraint)
    retdata = [fielduse, filename, constraint]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "IPSLCM5A" # MIROC4m  COSMOS CCSM4UoT -EC-Earth3.3
                   # HadCM3 MRI
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

EXPTNAME  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

EXPTNAME_L  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}



# this is regridding where all results are in a single file
FIELDNAMEIN = 'va'
LEVEL = 850.
EXPTNAMEIN = ['Eoi400', 'E280', 'E400']
#EXPTNAMEIN = ['E400']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for exptname in EXPTNAMEIN:


    # call program to get model dependent names
    # fielduse,  and  filename
    retdata = getnames(exptname)

    fielduse = retdata[0]
    filename = retdata[1]
    lev_constraint = retdata[2]
    print(fielduse, filename, lev_constraint)
    #sys.exit(0)

    FIELDNAMEOUT = FIELDNAMEIN + '_' + np.str(LEVEL)
    EXPTNAMEOUT = EXPTNAME.get(exptname)

    regrid_data(exptname, filename, fielduse, lev_constraint)

::::::::::::::
PlioMIP_new/large_scale_features/sea_SAT_relationships.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
#Created on 05/03/2020


#@author: earjcti

This program will plot the land temperature relationships
requested by IPCC in particular

- Land vs sea: I suggest reporting both (1) all sea vs GMAT, and 
(2) sea from 60S - 60N vs GMST. 
These metrics are also needed to help inform GMAT reconstructions from the proxy data, 
which are primarily marine-based.


"""

import warnings
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
from scipy import stats

warnings.filterwarnings("ignore")

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match.
    """

    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube

def get_lsm(filein):
    """
    returns a  numpy array of land points and one of sea points
    """
    tempcube = iris.load_cube(filein)
    cubegrid = iris.load_cube('one_lev_one_deg.nc')
    lsmcube = tempcube.regrid(cubegrid, iris.analysis.Linear())
    landpoints = lsmcube.data
    seapoints = (lsmcube.data - 1.0) * (-1.0)

    return landpoints, seapoints

def get_mean_data(model, expt, field):
    """
    gets the cube of mean data for a single model

    Parameters
    ----------
    model : the name of the model we are interested in
    expt : whether it is the experiment or the control

    Returns
    -------
    a cube with the mean data from this file
    grid_areas = the size of the grid for averaging
    """

    filename = (FILESTART + 'regridded/' + model
                + '/' + expt + '.' + field + '.allmean.nc')

    print(field, filename)
   
    cube = iris.load_cube(filename)
    cube2 = resort_coords(cube)

    cube2.coord('latitude').guess_bounds()
    cube2.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube2)

    return cube2, grid_areas


def get_region(latmin, latmax, cube, mask, grid_areas):
    """
    Gets the average temeprature over land within the
    bounded range

    Parameters
    ----------
    latmin, latmax : the range of latitudes we are extracting data from
    cube : the cube containing average temperatures t
    mask : numpy array containing the mask we want.  Could be a land mask
           a sea mask or ones everywhere (ie all points)
    grid_areas : the size of each gridpoint for weighting

    Returns
    -------
    region_avg : scalar containing the average temperature over the
                     required region

    """

    grid_areas_mask = grid_areas * mask
    grid_areas_band = np.zeros(grid_areas.shape)
    lats = cube.coord('latitude').points

    for j, lat in enumerate(lats):
        if latmin <= lat <= latmax:
            grid_areas_band[j, :] = grid_areas_mask[j, :]

    region_avg = cube.collapsed(['longitude', 'latitude'],
                                iris.analysis.MEAN,
                                weights=grid_areas_band)

    return region_avg.data

def scatter_sea_vs_global(allanom, seaanom, plottype, txtfile):
    """
    plot the global temperature anomaly vs the 
    ocean temperature anomaly for theregion on one plot
    also outputs the regression equation

    Parameters
    ----------
    allanom : temperature anomaly from the globe
    seaanom : temperature anomaly from the sea
    plot type: '' - global or latitude range

    Returns
    -------
    None.

    """

    titlename = 'MPWP - PI: global vs ocean ' + plottype + ' temperature '
    ax = plt.subplot(1, 1, 1)

    for i, model in enumerate(MODELNAMES):
        if i % 4 == 0: # i divides 4 with no remainder
            ax.scatter(seaanom[i], allanom[i], label = model) 
        elif i % 4 == 1 :
            ax.scatter(seaanom[i], allanom[i], label = model, marker='^') 
        elif i % 4 == 2 :
            ax.scatter(seaanom[i], allanom[i], label = model, marker='<') 
        else:
            ax.scatter(seaanom[i], allanom[i], label = model, marker='v') 
    #plt.title(titlename)
    plt.xlabel('ocean temperature ' + plottype + ' anomaly (' + UNITS + ')')
    plt.ylabel('global SAT ' + ' anomaly (' + UNITS + ')')


    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout = (FILEOUTSTART + '/sea_SAT'  + plottype + '_anomaly.eps')
    plt.savefig(fileout)
    fileout = (FILEOUTSTART + '/sea_SAT'  + plottype + '_anomaly.pdf')
    #plt.show()
    plt.savefig(fileout)
    plt.close()


    # print out the regression equation and the data to a file
    print(plottype)
    print('=======')
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(seaanom, allanom)
    print('GMSAT = ' + np.str(np.around(slope, 3)) + 
          'x OSAT + ' + np.str(np.around(intercept, 3)))
    print('pvalue = ' + np.str(np.around(p_value, 3)) + 
          ' rvalue = ' + np.str(np.round(r_value, 3)) + 
          ' rsq = ' + np.str(np.round(r_value * r_value, 3)))
    print('   ')

    # print out the values to a file
    
    txtfile.write("modelname, global mean SAT, oceanSAT " + plottype + '\n')
    for i, model in enumerate(MODELNAMES):
        txtfile.write((model + ',' + np.str(np.around(allanom[i],3)) + 
                       ',' + np.str(np.around(seaanom[i],3)) + '\n'))
    
      

#####################################
def main():
    """
    Tha main control of the program to plot the
    polar amplification by temperature by latitude band

    """

    if LINUX_WIN == 'w': 
        exptlsm = (FILESTART + 'regridded/PlioMIP2_Boundary_conds' +
                   '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'regridded/PlioMIP2_Boundary_conds' +
                   '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
    else:
        exptlsm = (FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh' +
                   '/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'PlioMIP2_Boundary_conds/Modern_std/' +
                   'Modern_std/Modern_std_LSM_v1.0.nc')


    ########################################################
    # setup: get the lsm for the land sea contrast plot

    exptland, exptsea = get_lsm(exptlsm)
    cntlland, cntlsea = get_lsm(cntllsm)
    fullmask = np.ones(np.shape(exptsea))
    

    #########################################################
    # need to get data from annual mean plot

    sea_anomaly = np.zeros((len(MODELNAMES)))
    sea_6060_anomaly = np.zeros((len(MODELNAMES))) # from 60N-60S
    all_anomaly = np.zeros((len(MODELNAMES)))
    
    for modelno, modeluse in enumerate(MODELNAMES):

        # get mean data
        (exptcube_SAT, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME, FIELD_SAT)
        (cntlcube_SAT, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME, FIELD_SAT)
        (exptcube_SST, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME, FIELD_SST)
        (cntlcube_SST, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME, FIELD_SST)
        
        # get all data
        expt_data = get_region(-90.0, 90.0, exptcube_SAT, 
                               fullmask, grid_areas_expt)
        cntl_data = get_region(-90.0, 90.0, cntlcube_SAT, 
                               fullmask, grid_areas_cntl)
        print(expt_data, cntl_data)
        all_anomaly[modelno] = expt_data - cntl_data
        

        # get sea anomaly
        expt_data = get_region(-90.0, 90.0, exptcube_SST, 
                               exptsea, grid_areas_expt)
        cntl_data = get_region(-90.0, 90.0, cntlcube_SST, 
                               cntlsea, grid_areas_cntl)
        sea_anomaly[modelno] = expt_data - cntl_data

        # get sea anomaly from 60N-60S
        expt_data = get_region(-60.0, 60.0, exptcube_SST, 
                               exptsea, grid_areas_expt)
        cntl_data = get_region(-60.0, 60.0, cntlcube_SST, 
                               cntlsea, grid_areas_cntl)
        sea_6060_anomaly[modelno] = expt_data - cntl_data

   # plot everything on one plot and write results to a text file
    filetext = open((DATAOUTSTART + '/data_for_sea_SAT_anomaly.txt'), "w+")
   
    scatter_sea_vs_global(all_anomaly, sea_anomaly, '', filetext)
    scatter_sea_vs_global(all_anomaly, sea_6060_anomaly, '_60N-60S_', filetext)
    
    filetext.close  

##########################################################
# DEFINITIONS

LINUX_WIN = 'l'
FIELD_SAT = 'NearSurfaceTemperature'
FIELD_SST = 'SST'
UNITS = 'degC'
EXPTNAME = 'EOI400'
CNTLNAME = 'E280'

MODELNAMES=['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]


if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    FILEOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD_SAT + '\\')
    DATAOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD_SAT + '\\')
else:
    FILESTART = '/nfs/hera1/earjcti/'
    FILEOUTSTART = '/nfs/hera1/earjcti/regridded/allplots/' + FIELD_SAT + '/'
    DATAOUTSTART = '/nfs/hera1/earjcti/regridded/alldata/'

FILEOUT = FILESTART + 'regridded/alldata/data_for_sea_SAT_relationships.txt'


main()
::::::::::::::
PlioMIP_new/large_scale_features/statistical_significance_of_changes.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sep 21 2019

@author: earjcti

This will plot the MMM precipitation and temperature anomalies.  It will hatch the
statistically robust changes based on the followin method (Mba et al 2018)

1. Roughly 80% of models must agree on the direction of the change
   (if we have 13 models then 10 must agree)
2. The ((ensemble mean change) / (the ensemble standard deviation))>1.
   (note I am not sure whether to get the ensemble standard deviation from
    pi or the mPWP maybe I will try both)

"""

import sys
import iris
import iris.quickplot as qplt
import iris.plot as iplt
#from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
import netCDF4


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if field == 'SST' or field == 'NearSurfaceTemperature':
        if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
            cube.units = 'Celsius'
        else:
            cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    if modeluse == 'EC-Earth3.1' and field == 'SST':
        cube.coord('latitude').bounds = None
        cube.coord('longitude').bounds = None

    cube.cell_methods = None

    return cube



def get_ind_model_anomaly():
    """
    gets the anomaly data for all the models puts them into a list of cubes for
    returning to the calling program
    """

    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        cubepi = get_data(FILESTART + model + '/E280.' + FIELDNAME + '.allmean.nc',
                          FIELDNAME, model)
        cubeplio = get_data(FILESTART + model + '/EOI400.' + FIELDNAME + '.allmean.nc',
                            FIELDNAME, model)
        cubediff = cubeplio - cubepi
        cubelist.append(cubediff)


    return cubelist

def check_sign(cubelist):
    """
    we are checking that a number (NSIGN) of cubes has the same sign in a field
    for example if the field is temperature we may want to see that 10/13 models all show the
    same sign of temperature change

    input a list of cubes from models
    output a numpy array which is 1 where at least 'NSIGN' models have the same sign
           and 0 when they don't
    """

    cube = cubelist[0]
    ydim, xdim = np.shape(cube.data)
    posarr = np.zeros((ydim, xdim))
    negarr = np.zeros((ydim, xdim))

    for cubeno, cube in enumerate(cubelist):
        cubedata = cube.data
        temparr = np.ma.where(cubedata > 0, 1, 0) # temporary array
        posarr = posarr + temparr
        temparr = np.ma.where(cubedata < 0, 1, 0)
        negarr = negarr + temparr

    # if there are more than NSIGN elements in posarr or negarr than the same sign arr is set
    sign_arr = np.ma.where(posarr >= NSIGN, 1, 0)
    temparr = np.ma.where(negarr >= NSIGN, 1, 0)
    sign_arr = sign_arr + temparr
    newcube = cube.copy(data=sign_arr)

    return newcube

def check_large_anomaly(ratiocube):
    """
    if ratio is greater than 1 set to 1, otherwise set to zero
    """
    temparr = np.ma.where(ratiocube.data > 1, 1, 0)
    newcube = ratiocube.copy(data=temparr)

    return newcube


def main():
    """
    1. get the change in the field from all the models
    2. find out where > 70% of the models agree in the sign of the change (region1)
    3. get mean change in the field
    4. get the standard deviation for the control climate
    5. find out where mean change / standard deviation > 1 (region2)
    6. plot, hatching where region 1 and region2 are satisfied
    """

    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }

    modelcubelist = get_ind_model_anomaly()
    cube_sign = check_sign(modelcubelist) # checks where a given number of them are the correct sign
    meancube = iris.load_cube(FILESTART + FIELDNAME + '_multimodelmean.nc',
                              FIELDNAME + 'mean_anomaly')  # get mean
    stdevcube = iris.load_cube(FILESTART + FIELDNAME + '_multimodelmean.nc',
                               FIELDNAME + 'std_pi') # get standard deviation
    cube_mean_std_sign = check_large_anomaly(meancube / stdevcube)

    #qplt.contourf(cube_sign)
    ax = plt.axes(projection = ccrs.PlateCarree())
    if FIELDNAME == 'TotalPrecipitation':
       
        qplt.contourf(meancube, np.arange(-1.4, 1.6, 0.2), cmap='RdBu', extend='both')
        plt.figtext(0.02, 0.97,'d)',
                   horizontalalignment='left',
                   verticalalignment='top',
                   fontsize=20)
    else:
        qplt.contourf(meancube, np.arange(0, 5.5, 0.5), cmap='Reds', extend='both')
    iplt.contourf(cube_sign, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(cube_mean_std_sign, 1, hatches=[None, 3 * '\\\''], colors='none')
    plt.gca().coastlines()
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    plt.title(namefield.get(FIELDNAME) +' anomaly: multimodel mean')

    plt.savefig(OUTSTART + FIELDNAME + 'robust_anomalies.eps')
    plt.savefig(OUTSTART + FIELDNAME + 'robust_anomalies.pdf')
    plt.close()

    OUTNC = FILESTART + 'dummy.nc'
    if FIELDNAME == 'NearSurfaceTemperature':
        OUTNC = FILESTART + 'alldata/data_for_fig2.nc'
    if FIELDNAME == 'TotalPrecipitation':
        OUTNC = FILESTART + 'alldata/data_for_fig5d.nc'
        
    print(OUTNC)
    
    cubelist = iris.cube.CubeList([meancube, cube_sign, cube_mean_std_sign])
    iris.save(cubelist, OUTNC)        
    


    return



# variable definition
LINUX_WIN = 'l'

#FIELDNAME = 'NearSurfaceTemperature'
FIELDNAME = 'SST'
UNITS = 'deg C'

#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'


if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    OUTSTART = ' '


MODELNAMES = ['CCSM4-Utr', 'COSMOS', 'CESM1.2', 'CESM2','CCSM4',
              'EC-Earth3.3', 'GISS2.1G', 'HadCM3',
              'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
              'MIROC4m', 'MRI2.3',
              'NorESM-L', 'NorESM1-F',
              'CCSM4-UoT'
             ]
#ODELNAMES = ['EC-Earth3.1']
NSIGN = np.floor(len(MODELNAMES) * 0.8) # *0.8 is 80%

main()
::::::::::::::
PlioMIP_new/large_scale_features/temperature_diagnostics.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#import Basemap
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match. 
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
       
    return cube  

def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
            
    # find line which contains 'modelnameglobal[mean_ocean_eoi400'
    # which is the start of the land sea contrast
    string = 'modelnameglobal[mean_ocean_eoi400'
    land_amplification = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, mean_ocean_eoi400, meanocean_e280, meanocean_anom, 
        mean_land_eoi400, mean_land_e280, mean_land_anom) = line.split(',')
        amp = np.float(mean_land_anom) / np.float(meanocean_anom)
        if model != 'MEAN':
            land_amplification.append(amp)
            
    land_amp_arr = np.asarray(land_amplification, dtype=float)
    
    # find line which contains 'modelname20N-20S[mean_ocean_eoi400'
    # which is the start of the land sea contrast over the tropics
    string = 'modelname20N-20S[mean_ocean_eoi400'
    index=0
    land_amplification_20 = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
     
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, mean_ocean_eoi400, meanocean_e280, meanocean_anom, 
        mean_land_eoi400, mean_land_e280, mean_land_anom) = line.split(',')
        amp = np.float(mean_land_anom) / np.float(meanocean_anom)
        if model != 'MEAN':
            land_amplification_20.append(amp)
            
    land_amp20_arr = np.asarray(land_amplification_20, dtype=float)
    
    # find line which contains '45N-90N_anom'
    # which is the start of the fields averaged over certain regions
    string = '45N-90N_anom'
    index=0
    NH_SH_ratio45 = []
    PA_NH_60 = []
    PA_SH_60 = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
     
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, anom_45_90N, anom_45_90S, 
         anom_60_90N, anom_60_90S) = line.split(',')
        NH_SH_ratio = np.float(anom_45_90N) / np.float(anom_45_90S)
      
        if model != 'MEAN':
            NH_SH_ratio45.append(NH_SH_ratio)
            PA_NH_60.append(anom_60_90N)
            PA_SH_60.append(anom_60_90S)
   
    NH_SH_ratio45_arr = np.asarray(NH_SH_ratio45, dtype=float)
    PA_NH_60_arr = np.asarray(PA_NH_60, dtype = float)
    PA_SH_60_arr = np.asarray(PA_SH_60, dtype = float)
    
        
    return (land_amp_arr, land_amp20_arr, NH_SH_ratio45_arr, 
            PA_NH_60_arr, PA_SH_60_arr)

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\'+field+'\\'
        exptlsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
        cntllsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
    else:
        fileoutstart='/nfs/hera1/earjcti/regridded/allplots/'+field+'/'
        dataoutstart='/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm=FILESTART+'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
        cntllsm=FILESTART+'PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
     
 
    ########################################################
    # setup: get the lsm for the land sea contrast plot
    
    tempcube=iris.load_cube(exptlsm)
    cubegrid=iris.load_cube('one_lev_one_deg.nc')
    exptlsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
    exptsea=(exptlsmcube.data - 1.0)*(-1.0)
    
   
    tempcube=iris.load_cube(cntllsm)
    cntllsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
    cntlsea=(cntllsmcube.data - 1.0)*(-1.0)
    
 
    #########################################################
    # need to get data from annual mean plot
    
    nh_anomaly=np.zeros(len(modelnames))
    sh_anomaly=np.zeros(len(modelnames))
    nh_anomaly_extratropics=np.zeros(len(modelnames))
    sh_anomaly_extratropics=np.zeros(len(modelnames))
    nh_anomaly_polar=np.zeros(len(modelnames))
    sh_anomaly_polar=np.zeros(len(modelnames))
    all_anomaly=np.zeros(len(modelnames))
    land_anomaly=np.zeros(len(modelnames))
    sea_anomaly=np.zeros(len(modelnames))
    land_anomaly_tropics=np.zeros(len(modelnames))
    sea_anomaly_tropics=np.zeros(len(modelnames))
    
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+'regridded/'+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+'regridded/'+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        cube=iris.load_cube(exptfile)
        exptcube = resort_coords(cube)
        cube=iris.load_cube(cntlfile)
        cntlcube = resort_coords(cube)
    
        #######################################
        # compare NH vs SH temperature
        exptcube.coord('latitude').guess_bounds()
        exptcube.coord('longitude').guess_bounds()
        cntlcube.coord('latitude').guess_bounds()
        cntlcube.coord('longitude').guess_bounds()
        grid_areas_expt = iris.analysis.cartography.area_weights(exptcube)
        grid_areas_cntl = iris.analysis.cartography.area_weights(cntlcube)
        
        # exptcube
        nlat=len(exptcube.coord('latitude').points)
        lats=exptcube.coord('latitude').points
        
        grid_areas_nh=np.zeros(grid_areas_expt.shape)
        grid_areas_sh=np.zeros(grid_areas_expt.shape)
        grid_areas_nh_extratropics=np.zeros(grid_areas_expt.shape)
        grid_areas_sh_extratropics=np.zeros(grid_areas_expt.shape)
        grid_areas_nh_polar=np.zeros(grid_areas_expt.shape)
        grid_areas_sh_polar=np.zeros(grid_areas_expt.shape)
        polarval=60.0
        for j in range(0,nlat):
            if lats[j] <0:
                grid_areas_sh[j,:]=grid_areas_expt[j,:]
            else:
                grid_areas_nh[j,:]=grid_areas_expt[j,:]
            if lats[j] < -45.0:
                grid_areas_sh_extratropics[j,:]=grid_areas_expt[j,:]
            if lats[j] > 45.0:
                grid_areas_nh_extratropics[j,:]=grid_areas_expt[j,:]
            if lats[j] <= -1.0*polarval:
                grid_areas_sh_polar[j,:]=grid_areas_expt[j,:]
            if lats[j] >= polarval:
                grid_areas_nh_polar[j,:]=grid_areas_expt[j,:]
           
        expt_nh = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh)
        expt_sh = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh)
        expt_anom = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN,weights=grid_areas_expt)
        expt_nh_extratropics = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_extratropics)
        expt_sh_extratropics = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_extratropics)
        expt_nh_polar = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_polar)
        expt_sh_polar = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_polar)
        
        
        
        #cntlcube
        nlat=len(cntlcube.coord('latitude').points)
        lats=cntlcube.coord('latitude').points
        grid_areas_nh=np.zeros(grid_areas_cntl.shape)
        grid_areas_sh=np.zeros(grid_areas_cntl.shape)
        for j in range(0,nlat):
            if lats[j] <0:
                grid_areas_sh[j,:]=grid_areas_cntl[j,:]
            else:
                grid_areas_nh[j,:]=grid_areas_cntl[j,:]
           
        cntl_nh = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh)
        cntl_sh = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh)
        cntl_anom = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN,weights=grid_areas_cntl)
        cntl_nh_extratropics = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_extratropics)
        cntl_sh_extratropics = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_extratropics)
        cntl_nh_polar = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_polar)
        cntl_sh_polar = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_polar)
        
        
        nh_anomaly[modelno]=expt_nh.data-cntl_nh.data
        sh_anomaly[modelno]=expt_sh.data-cntl_sh.data
        nh_anomaly_extratropics[modelno]=expt_nh_extratropics.data-cntl_nh_extratropics.data
        sh_anomaly_extratropics[modelno]=expt_sh_extratropics.data-cntl_sh_extratropics.data
        nh_anomaly_polar[modelno]=expt_nh_polar.data-cntl_nh_polar.data
        sh_anomaly_polar[modelno]=expt_sh_polar.data-cntl_sh_polar.data
        all_anomaly[modelno]=expt_anom.data-cntl_anom.data
      
        
        #######################################
        # compare land with sea (globally and for tropics)
        
        
        # expt
        # first check grid
        for i in range(0,len(exptcube.coord('latitude').points)):
            if exptcube.coord('latitude').points[i] !=exptlsmcube.coord('latitude').points[i]:
                print('differences in lsm and gridded data',i)
                sys.exit(0)
       
    
        for i in range(0,len(exptcube.coord('longitude').points)):    
            if exptcube.coord('longitude').points[i] != exptlsmcube.coord('longitude').points[i]:
                print('differences in lsm and gridded data',i,exptcube.coord('longitude').points[i],
                  exptlsmcube.coord('longitude').points[i])
                sys.exit(0)
            
        grid_areas_land=grid_areas_expt * exptlsmcube.data  
        grid_areas_sea=grid_areas_expt * exptsea
        
        expt_land = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_land)
        expt_sea = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sea)
        
        # get grid areas and experiment means for tropics
        nlat=len(exptcube.coord('latitude').points)
        lats=exptcube.coord('latitude').points
        grid_areas_land_tropics=np.zeros(grid_areas_land.shape)
        grid_areas_sea_tropics=np.zeros(grid_areas_sea.shape)
        for j in range(0,nlat):
            if ((lats[j] < 20.) and (lats[j] > -20.)):
                grid_areas_land_tropics[j,:]=grid_areas_land[j,:]
                grid_areas_sea_tropics[j,:]=grid_areas_sea[j,:]
    
        expt_land_tropics = exptcube.collapsed(['longitude', 'latitude'],
                                               iris.analysis.MEAN, weights=grid_areas_land_tropics)
        expt_sea_tropics = exptcube.collapsed(['longitude', 'latitude'], 
                                              iris.analysis.MEAN, weights=grid_areas_sea_tropics)
    
        # cntl
        # first check grid
        for i in range(0,len(cntlcube.coord('latitude').points)):
            if cntlcube.coord('latitude').points[i] !=cntllsmcube.coord('latitude').points[i]:
                print('differences in lsm and gridded data',i)
                sys.exit(0)
       
    
        for i in range(0,len(cntlcube.coord('longitude').points)):    
            if cntlcube.coord('longitude').points[i] != cntllsmcube.coord('longitude').points[i]:
                print('differences in lsm and gridded data',i,cntlcube.coord('longitude').points[i],
                  cntllsmcube.coord('longitude').points[i])
                sys.exit(0)
            
        grid_areas_land=grid_areas_cntl * cntllsmcube.data  
        grid_areas_sea=grid_areas_cntl * cntlsea
    
        cntl_land = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_land)
        cntl_sea = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sea)
    
        # get grid areas and experiment means for tropics
        nlat=len(cntlcube.coord('latitude').points)
        lats=cntlcube.coord('latitude').points
        grid_areas_land_tropics=np.zeros(grid_areas_land.shape)
        grid_areas_sea_tropics=np.zeros(grid_areas_sea.shape)
        for j in range(0,nlat):
            if ((lats[j] < 20.) and (lats[j] > -20.)):
                grid_areas_land_tropics[j,:]=grid_areas_land[j,:]
                grid_areas_sea_tropics[j,:]=grid_areas_sea[j,:]
    
        cntl_land_tropics = cntlcube.collapsed(['longitude', 'latitude'],
                                               iris.analysis.MEAN, weights=grid_areas_land_tropics)
        cntl_sea_tropics = cntlcube.collapsed(['longitude', 'latitude'], 
                                              iris.analysis.MEAN, weights=grid_areas_sea_tropics)
    
    
        land_anomaly[modelno]=expt_land.data-cntl_land.data
        sea_anomaly[modelno]=expt_sea.data-cntl_sea.data
    
        land_anomaly_tropics[modelno]=expt_land_tropics.data-cntl_land_tropics.data
        sea_anomaly_tropics[modelno]=expt_sea_tropics.data-cntl_sea_tropics.data
    
    # get data from PlioMIP1 if required
    if PLIOMIP1 == 'y':
        (pliomip1_landsea_amp, 
         pliomip1_tropics_landsea_amp,
         pliomip1_NH_SH_ratio45,
         pliomip1_PA_NH,
         pliomip1_PA_SH) = get_pliomip1_data(field)
   
    
    # plot NH SH contrast
    ax=plt.subplot(1,1,1)
    ax.plot(modelnames,nh_anomaly,'x',label='NH anomaly')
    ax.plot(modelnames,sh_anomaly,'x',label='SH anomaly')
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    plt.xticks(rotation='90')
    plt.title('mPWP - PI anomaly for each hemisphere')
    fileout=fileoutstart+'/hemisphere_difference.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/hemisphere_difference.pdf'
    plt.savefig(fileout)
    plt.close()
    
    
    # plot NH SH contrast for extratropics
    ax=plt.subplot(2,1,1)
    ax.plot(modelnames,nh_anomaly_extratropics,'x',label='>45N anom')
    ax.plot(modelnames,sh_anomaly_extratropics,'x',label='<45S anom')
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    #plt.xticks(rotation='45')
    ax.tick_params(axis='x',labelbottom='False')
    plt.title('Plio_core - PI_Ctl; extratropical NH/SH anomaly')
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,nh_anomaly_extratropics/sh_anomaly_extratropics
            ,'x',label='>45N/<45S')
    ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
    
    if PLIOMIP1 == 'y':
       for mod_p1 in pliomip1_NH_SH_ratio45:
            ax.axhline(y=mod_p1, color='grey', alpha=0.4)
    
    
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.4*box.height), box.width * 0.8, (0.9*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel('factor')
    plt.xticks(rotation='90')
    #plt.title('mPWP - PI anomaly')
    
    fileout=fileoutstart+'/hemisphere_difference_extratropics.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/hemisphere_difference_extratropics.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out hemisphere difference extratropics
    
    txtout = open(FILEOUT, "w+") 
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3c \n')
        
        writedata = ("model_name, nh_anom_et, sh_anom_et \n")
    
        txtout.write(writedata)
        for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(nh_anomaly_extratropics[i],2)) + ',' + 
                         np.str(np.around(sh_anomaly_extratropics[i],2)) + '\n')
            txtout.write(writedata)
   
     # plot polar amplification 
    ax=plt.subplot(1,1,1)
    labelname='>'+np.str(np.int(polarval))+'N amplification'
    ax.plot(modelnames,nh_anomaly_polar/all_anomaly,'x',label=labelname)
    labelname='<'+np.str(np.int(polarval))+'S amplification'
    ax.plot(modelnames,sh_anomaly_polar/all_anomaly,'x',label=labelname)
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.7, (0.7*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel('factor')
    plt.xticks(rotation='90')
    ax.axhline(y=1.0, xmin=0.0, xmax=len(modelnames), color='r')
    #ax.tick_params(axis='x',labelbottom='False')
    plt.title('Plio_Core - PI_Ctl; polar amplification factor')
    plt.figtext(0.02, 0.97,'d)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    print('NH polar amplification', np.mean(nh_anomaly_polar) / np.mean(all_anomaly))
    print('SH polar amplification', np.mean(sh_anomaly_polar) / np.mean(all_anomaly))
    print('total polar amplification', np.mean(nh_anomaly_polar + sh_anomaly_polar) / (2.0 * np.mean(all_anomaly)))
    print('all polar amplification', (nh_anomaly_polar + sh_anomaly_polar) / (2.0 * all_anomaly))
    nh_amps = [x1 / x2 for (x1, x2) in zip(nh_anomaly_polar, all_anomaly)] 
    sh_amps = [x1 / x2 for (x1, x2) in zip(sh_anomaly_polar, all_anomaly)] 
    print('nh amps unsorted', nh_amps)
    print('nh amps',np.sort(nh_amps))
    print('nh median', np.median(nh_amps))
    print('sh median', np.median(sh_amps))
    print('nh percentiles 10/50/90',np.percentile(nh_amps, 10),
          np.percentile(nh_amps,50), np.percentile(nh_amps,90))
    print('sh percentiles 10/50/90',np.percentile(sh_amps, 10),
          np.percentile(sh_amps,50), np.percentile(sh_amps,90))
   
   
    
    fileout=fileoutstart+'/polar_amplification_'+np.str(np.int(polarval))+'.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/polar_amplification_'+np.str(np.int(polarval))+'.pdf'
    plt.savefig(fileout)
    plt.close()
    
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3d \n')
        
        writedata = ("model_name, nh_anom_polar, sh_anom_polar, global_anom \n")
    
        txtout.write(writedata)
        for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(nh_anomaly_polar[i],2)) + ',' + 
                         np.str(np.around(sh_anomaly_polar[i],2)) + ',' + 
                         np.str(np.around(all_anomaly[i],2)) + 
                         '\n')
            txtout.write(writedata)
    
    
    # plot land sea contrast
    if field != 'TotalPrecipitation':
        ax=plt.subplot(2, 1, 1)
    else:
        ax = plt.subplot(1, 1, 1)
    ax.plot(modelnames,land_anomaly,'x',label='Land anomaly')
    ax.plot(modelnames,sea_anomaly,'x',label='Sea anomaly')
    box = ax.get_position()
    if field != 'TotalPrecipitation':
        ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
        ax.tick_params(axis='x',labelbottom='False')
        plt.figtext(0.02, 0.97,'b)',
                       horizontalalignment='left',
                       verticalalignment='top',
                       fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    plt.ylabel(units)
   
    #plt.xticks(rotation='45')
    plt.title('Plio_Core - PI_Ctrl global land-sea anomaly')
    
    
    # plot land sea contrast tropics]
    
    ax2=plt.subplot(2,1,2)
    ax2.plot(modelnames,land_anomaly_tropics,'x',label='Land anomaly')
    ax2.plot(modelnames,sea_anomaly_tropics,'x',label='Sea anomaly')
    box = ax2.get_position()
    ax2.set_position([box.x0, box.y0+(0.4*box.height), box.width * 0.8, (0.9*box.height)])
    ax2.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    plt.xticks(rotation='90')
    plt.title('Plio_Core - PI_Ctrl 20N-20S land-sea anomaly')
    
    print('sea anomaly tropics',np.mean(sea_anomaly_tropics))

    fileout=fileoutstart+'/land_sea_contrast.pdf'
    plt.savefig(fileout)
    fileout=fileoutstart+'/land_sea_contrast.eps'
    plt.savefig(fileout)
    plt.close()
    
    # write out
    
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3b \n')
    else:
        txtout.write('data for 6b \n' )
        
    writedata = ("model_name, land_anomaly, sea_anomaly, " + 
                 "tropical_land_anomaly, tropical_sea_anomaly\n")
    
    
    txtout.write(writedata)
    for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(land_anomaly[i],2)) + ',' + 
                         np.str(np.around(sea_anomaly[i],2)) + ',' + 
                         np.str(np.around(land_anomaly_tropics[i],2)) + ',' + 
                         np.str(np.around(sea_anomaly_tropics[i],2)) + '\n')
            print(writedata)
            txtout.write(writedata)
        
    txtout.close
    print('mean land anomaly', np.mean(land_anomaly))
    print('mean sea anomaly', np.mean(sea_anomaly))
    
    ######################################
    # plot land sea contrast as a factor
    factor_land=land_anomaly / sea_anomaly
    factor_land_tropics=land_anomaly_tropics / sea_anomaly_tropics
   
    if field != 'NearSurfaceTemperature':
        ax=plt.subplot(2,1,1)
    else:
        ax = plt.subplot(1, 1, 1)
    ax.plot(modelnames,factor_land,'x')
    ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
    if PLIOMIP1 == 'y':
        for mod_amp in pliomip1_landsea_amp:
            ax.axhline(y=mod_amp, color='grey', alpha=0.4)
           
    box = ax.get_position()
    if field != 'NearSurfaceTemperature':
        ax.set_position([box.x0 + (0.1 * box.width),
                         box.y0+(0.3*box.height), 
                         box.width * 0.8, (0.8*box.height)])
        ax.tick_params(axis='x',labelbottom='False')
    else:
        ax.set_position([box.x0, box.y0+(0.3*box.height), 
                         box.width, (0.7*box.height)])
    plt.figtext(0.02, 0.97,'b)',
                horizontalalignment='left',
                verticalalignment='top',
                fontsize=20)
    plt.xticks(rotation='90')
    plt.ylabel('land amplification')
   
    #plt.xticks(rotation='45')
    plt.title('mPWP - PI; land_anomaly / sea anomaly')
    
    if field != 'NearSurfaceTemperature':
        ax=plt.subplot(2,1,2)
        ax.plot(modelnames,factor_land_tropics,'x')
        ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
        if PLIOMIP1 == 'y':
            for mod_amp in pliomip1_tropics_landsea_amp:
                ax.axhline(y=mod_amp, color='grey', alpha=0.4)
        
        box = ax.get_position()
        ax.set_position([box.x0 + (0.1 * box.width), 
                         box.y0+(0.3*box.height), 
                        box.width * 0.8, (0.9*box.height)])
        plt.ylabel('land amplification')
        plt.xticks(rotation='90')
        plt.title('mPWP - PI (20N-20S); land_anomaly / sea anomaly')
        plt.figtext(0.02, 0.97,'b)',
                   horizontalalignment='left',
                   verticalalignment='top',
                   fontsize=20)
    fileout=fileoutstart+'/land_sea_amplification.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/land_sea_amplification.pdf'
    plt.savefig(fileout)
    plt.close()
   
    
   
   
    
   
    
    
    
        
################################################################
def plotmap(modelnames,field,exptname,cntlname,linux_win,units):  
    # this subprogram will 
    # 1. read in the data from each map.  
    # 2. It will normalise the data by subtracting the mean and dividing by the spatial standard deviation
    # 3. it will then plot the field change by number of standard deviations above and below the m2an

    normalized_cubes=iris.cube.CubeList([])
    standard_dev=[]
    if linux_win=='w':
        FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
        fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\'+field+'\\'
        exptlsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        FILESTART='/nfs/hera1/earjcti/'
        fileoutstart='/nfs/hera1/earjcti/regridded/allplots/'+field+'/'
        dataoutstart='/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm=FILESTART+'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
      
    
    lsmcube=iris.load_cube(exptlsm)
        
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        print(modeluse)
        exptfile=FILESTART+'regridded/'+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+'regridded/'+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        cube=iris.load_cube(exptfile)
        exptcube = resort_coords(cube)
        exptcube.data=exptcube.data.astype('float32') # change to float32 for concatentation later
       
       
        for coord in exptcube.coords():
            if coord.standard_name !='longitude' and coord.standard_name !='latitude':
                exptcube.remove_coord(coord)
                
        cube=iris.load_cube(cntlfile)
        cntlcube = resort_coords(cube)
        cntlcube.data=cntlcube.data.astype('float32')
        
        for coord in cntlcube.coords():
            if coord.standard_name !='longitude' and coord.standard_name !='latitude':
                cntlcube.remove_coord(coord)
    
        
        diffcube=exptcube-cntlcube
       
        #print(exptcube.coord('surface'))
      
        #######################################
        # get mean and spatial standard deviation
        diffcube.coord('latitude').guess_bounds()
        diffcube.coord('longitude').guess_bounds()
       
        grid_areas_diff = iris.analysis.cartography.area_weights(diffcube)
       
        diffmean = diffcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_diff)
        diffsd = diffcube.collapsed(['longitude', 'latitude'], iris.analysis.STD_DEV)
        
        standard_dev.append(diffsd.data)
       
        ########################################
        # normalise the cube by dividing by the mean and std dev
        # and plot
        
        normcube=(diffcube-diffmean)/diffsd
        print(normcube)
      
        
        V=np.arange(-4,4.5,0.5)
        mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
        newcolors=mycmap(np.linspace(0,1,len(V+2)))
        white=([1,1,1,1])
        print((len(V)/2)-1,(len(V)/2)+2)
        print((np.ceil(len(V)/2)-1),np.floor((len(V)/2)+2))
        
        newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
        
        mycmap=ListedColormap(newcolors)
           
        #Draw the contour w
       
        qplt.contourf(normcube, V,extend='both',cmap=mycmap)
        #qplt.contourf(normcube, V,extend='both',cmap='RdBu_r')
        
       
        #sys.exit()
        qplt.contour(lsmcube,1,colors='black')
        plt.title(modeluse+': '+field+'\n No of stddev from mean'
                  +'('+(np.str(np.round(diffsd.data,1)))+units+')')

        if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/'+modeluse+'.eps'
            plt.savefig(fileout)
            
        fileout=fileoutstart+'Map_normalised//'+modeluse+'.pdf'
        plt.savefig(fileout)
        plt.close()
        
        #normcube.Coord(1, standard_name='model', long_name='model', var_name='model', units='1', 
       #                       bounds=None, attributes=None, coord_system=None)
        tempcube=iris.util.new_axis(normcube)
        tempcube.add_dim_coord(iris.coords.DimCoord(modelno, 
                standard_name='model_level_number', long_name='model', 
                var_name='model', 
                units=None,
                bounds=None,
                coord_system=None, circular=False),0)
        # tempcube needs to be dtype=float64
        
        
      
        normalized_cubes.append(tempcube)
        
    
    ################################################
    # END OF MODEL LOOP
    equalise_attributes(normalized_cubes)
   
    print(normalized_cubes)
    print(normalized_cubes[0])
    print(normalized_cubes[1])
    print(normalized_cubes[0].dtype)
    print(normalized_cubes[1].dtype)
    allnormcube=normalized_cubes.concatenate_cube()
    
    meancube=allnormcube.collapsed(['model_level_number'], iris.analysis.MEAN)
    maxcube=allnormcube.collapsed(['model_level_number'], iris.analysis.MAX)
    mincube=allnormcube.collapsed(['model_level_number'], iris.analysis.MIN)
    mediancube=allnormcube.collapsed(['model_level_number'], iris.analysis.MEDIAN)
    
    ###########################
    # plot the mean value
    
    V=np.arange(-2.5,2.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    print(standard_dev)
    print(np.amin(standard_dev))
    print(np.amax(standard_dev))
    print(minval,maxval)
    #sys.exit(0)
    plt.title(field+'\n Mean No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/meandiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//meandiff.pdf'
    plt.savefig(fileout)
    plt.close()
    plt.show()
        
    
    ###########################
    # plot the median value
    
        
    qplt.contourf(mediancube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Median No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/mediandiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//mediandiff.pdf'
    plt.savefig(fileout)
    plt.close()
    plt.show()
        
    
    ###########################
    # plot the maximum value
    
    V=np.arange(-4.5,4.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(maxcube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Maximum No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/maxdiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//maxdiff.pdf'
    plt.savefig(fileout)
    plt.close()
   
    
    ###########################
    # plot the minimum value
    
    V=np.arange(-2.5,2.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(mincube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Minimum No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/mindiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//mindiff.pdf'
    plt.savefig(fileout)
    plt.close()
   

        
         


##########################################################
# main program

filename=' '
linux_win='l'
if linux_win=='w':
    FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART='/nfs/hera1/earjcti/'
        

modelnames=['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]


#modelnames=['HadCM3','NorESM-L']
fieldnames=['NearSurfaceTemperature']
#units=['degC']
#
#fieldnames=['TotalPrecipitation']
units=['mm/day']
exptname='EOI400'
cntlname='E280'
PLIOMIP1 = 'n'

for field in range(0,len(fieldnames)):
    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'regridded/alldata/data_for_6b.txt'
        
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'regridded/alldata/data_for_3b_3c_3d.txt'
       
    
    # will plot NH vs SHPlio_enh_LSM_v1.0Plio_enh_LSM_v1.0
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])
     
    # plotmap will show where the field is larger or smaller than the mean  
    plotmap(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)
::::::::::::::
PlioMIP_new/large_scale_features/temperature_gradients.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the SST zonal and meridional gradients acropss the atlantic and 
the pacific

"""

import sys
import iris
import iris.quickplot as qplt
#from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    print(modeluse)
    if modeluse == 'MMM':
        print(filereq,field)
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def get_within_region(cube, region):
    """
    This routine will mask out all the regions that are not 
    input: a temperature cube, a region name
    output: a new cube with all data not within the region masked out.
    """


    if region == 'TROPATL':
        latmin = -20.
        latmax = 20.
        lonmin = 300.
        lonmax = 360.
        
    if region == 'TROPPAC':
        latmin = -20.
        latmax = 20.
        lonmin = 140.
        lonmax = 260.
        
    if region == 'MERIDATL':
        latmin = -70.
        latmax = 70.
        lonmin = 290.
        lonmax = 360.
        
    if region == 'MERIDPAC':
        latmin = -70.
        latmax = 60.
        lonmin = 150.
        lonmax = 260.
        
        
    cubedata = cube.data
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points
    

    for j in range(0, len(lats)):
        cubedata[j, :] = np.ma.masked_where(lons < lonmin, 
                                            cubedata[j, :])
        cubedata[j, :] = np.ma.masked_where(lons > lonmax, 
                                            cubedata[j, :])
  
        
    for i in range(0, len(lons)):
        cubedata[:, i] = np.ma.masked_where(lats < latmin, 
                                            cubedata[:, i])
        cubedata[:, i] = np.ma.masked_where(lats > latmax, 
                                            cubedata[:, i])
        
    newcube = cube.copy(data=cubedata)
    
    #if region == 'MERIDPAC':
    #plot
    #    contour = qplt.contourf(newcube)
    #    plt.gca().coastlines()
    #    plt.show()
    #    sys.exit(0)

    return [newcube, lonmin, lonmax, latmin, latmax]



class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    it can either plot the zonal mean or the meridional mean
    """
    def __init__(self, region, timeperiod, lonmin, lonmax, latmin, latmax):

        """
        inputs are:
                    
        """

        fullregname  =  {
                         "TROPATL" : "Atlantic ",
                         "TROPPAC" : "Pacific ",
                         "MERIDATL" : "Atlantic ",
                         "MERIDPAC": "Pacific "}
    
        colorreq = {
                    "mPWP" : "red",
                    "pi" : "blue",
                    "mPWP-pi" : "green"}
        
        hatchreq = {
                    "mPWP" : "",
                    "pi" : "",
                    "mPWP-pi" : ""}
        
        self.region = region
        self.timeperiod = timeperiod
        self.regiontitle = fullregname.get(region)
        self.colorreq = colorreq.get(timeperiod)
        self.hatchreq = hatchreq.get(timeperiod)


        if region == 'TROPATL':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(latmin)) + 
            #                    '-' + np.str(np.int(latmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 10.
            else:
                self.valmin = 20.
                self.valmax = 30.
                
        if region == 'TROPPAC':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(latmin)) + '-' 
            #                    + np.str(np.int(latmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 10.
            else:
                self.valmin = 20.
                self.valmax = 32.
                
        if region == 'MERIDATL':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(lonmin)) + 
            #                    '-' + np.str(np.int(lonmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = -0.
                self.valmax = 20.
            else:
                self.valmin = -10.
                self.valmax = 30.
                
        if region == 'MERIDPAC':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(lonmin)) + '-' 
            #                    + np.str(np.int(lonmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 20.
            else:
                self.valmin = -5.
                self.valmax = 35.
            



    def plotzm(self, cube, cubemin, cubemax, ax, colorname):
        """
        plot the zonal mean
        """
    

        cube_zm = cube.collapsed('latitude', iris.analysis.MEAN)
        cube_zm_min = cubemin.collapsed('latitude', iris.analysis.MEAN)
        cube_zm_max = cubemax.collapsed('latitude', iris.analysis.MEAN)
        lons = cube_zm.coord('longitude').points
        
        #cube_zm_var = cubevar.collapsed('latitude', iris.analysis.MEAN)
        #zm_std = np.sqrt(cube_zm_var.data)
        #cube_zm_2sigma = cube_zm_var.copy(data=zm_std * 2)
        ax.plot(lons, cube_zm.data, label=self.timeperiod, color=self.colorreq)
        ax.fill_between(lons, cube_zm_min.data, cube_zm_max.data, alpha=0.2, 
                        color=self.colorreq,
                        hatch=self.hatchreq)
        plt.title('20N-20S Mean SST:' +  self.regiontitle)
        ax.set_xlabel('longitude')
        ax.set_ylabel('deg C', color=colorname)
        ax.set_ylim(self.valmin, self.valmax)
        ax.tick_params(axis='y', labelcolor=colorname)
        
    
        return

    def plotmm(self, cube, cubemin, cubemax, ax, colorname, fig):
        """
        plot the meridional mean from the cube
        """

        cube_mm = cube.collapsed('longitude', iris.analysis.MEAN)
        cube_mm_min = cubemin.collapsed('longitude', iris.analysis.MEAN)
        cube_mm_max = cubemax.collapsed('longitude', iris.analysis.MEAN)
        lats = cube_mm.coord('latitude').points
        
        for i, lat in enumerate(lats):
            print(lat, cube_mm.data[i], cube_mm_min.data[i], cube_mm_max.data[i])
        ax.plot(cube_mm.data, lats, label=self.timeperiod, color=self.colorreq)
        ax.fill_betweenx(lats, cube_mm_min.data, cube_mm_max.data, alpha=0.2, 
                        color=self.colorreq,
                        hatch=self.hatchreq)
        fig.suptitle('Zonal Mean SST: ' +  self.regiontitle)
        ax.set_xlabel('deg C', color=colorname)
        ax.set_ylabel('latitude')
        ax.set_xlim(self.valmin, self.valmax)
        ax.tick_params(axis='x', labelcolor=colorname)
   
    
        return


# emd of class

def main_time(timeperiod, region):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = (FILESTART + FIELDNAME + '_multimodelmean.nc')

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')
    
    max_cube = get_data(filename, FIELDNAME + 'max_' + timeperiod, 
                         'MMM')
    
    min_cube = get_data(filename, FIELDNAME + 'min_' + timeperiod, 
                         'MMM')
    
    if timeperiod == 'anomaly':
        sd_cube = get_data(filename, 'SSTanomaly_multimodel_stddev', 
                         'MMM')
    else:
        sd_cube = get_data(filename, FIELDNAME + 'std_' + timeperiod, 
                         'MMM')
    variance_cube = np.square(sd_cube)

    # get the mean value within the region and the standard deviation within the 
    # region
    regioncube, lonmin, lonmax, latmin, latmax = get_within_region(mean_cube, region)
    regioncubemax, lonmin, lonmax, latmin, latmax = get_within_region(max_cube, region)
    regioncubemin, lonmin, lonmax, latmin, latmax = get_within_region(min_cube, region)
    regionvariance, lonmin, lonmax, latmin, latmax = get_within_region(variance_cube, region)


    return regioncube, regionvariance, regioncubemin, regioncubemax, lonmin, lonmax, latmin, latmax

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    cubelist = iris.cube.CubeList([])
    
    figno =  {"MERIDATL": "a)",
            "MERIDPAC": "b)",
            "TROPATL": "c)",
            "TROPPAC": "d)"
    }
    
    for j, region in enumerate(REGIONNAMES):
        
        
        
        # get cubes for each timeperiod and plot
        fig, ax1 = plt.subplots() 
        for i in range(0, len(TIMEPERIODS)):
            (cube_time_region, cube_region_variance, 
             cube_region_min, cube_region_max,
             lonmin, lonmax, latmin, latmax)  = main_time(TIMEPERIODS[i], region)
            
            cubelist.append(cube_time_region)
            cubelist.append(cube_region_min)
            cubelist.append(cube_region_max)
       
            plobj = Plotalldata(region, TIMEPERIODS[i], lonmin, lonmax, latmin, latmax)
            if region[0:3] == 'TRO': 
                plobj.plotzm(cube_time_region, cube_region_min,
                             cube_region_max,
                             ax1, 'black')
                outname = OUTSTART + 'zonalmean' + region
                
    
            
                
            if region[0:3] == 'MER':             
                plobj.plotmm(cube_time_region, cube_region_min,
                             cube_region_max,
                             ax1,'black', fig)
                outname = OUTSTART + 'mm_' + region
        
       
        box = ax1.get_position()
        #print(box)
        #sys.exit(0)
        #ax1.set_position([box.x0+(box.height*0.2), box.y0, 1.0, 1.0])
        # plot zonal or meridional mean for the difference
        
        (cubediff_region, cubediff_region_variance, 
         cubediff_region_min, cubediff_region_max,
             lonmin, lonmax, latmin, latmax)  = main_time('anomaly', region)
        print(np.mean(cubediff_region_max.data))
       
        
        plobjdiff = Plotalldata(region, TIMEPERIODS[1] + '-' + TIMEPERIODS[0],
                                lonmin, lonmax, latmin, latmax)
       
        if region[0:3] == 'TRO':             
           ax2 = ax1.twinx() 
           
           plobjdiff.plotzm(cubediff_region, cubediff_region_min,
                            cubediff_region_max, ax2, 'green') 
           
           print('julia',np.mean(cubediff_region.data), 
                 np.mean(cubediff_region_min.data),
                 np.mean(cubediff_region_max.data)
                 )
           #sys.exit(0)
           
           
        if region[0:3] == 'MER':             
            ax2 = ax1.twiny() 
            ax2.xaxis.set_ticks_position("bottom")
            ax2.xaxis.set_label_position("bottom")
            ax2.spines["bottom"].set_position(("axes", -0.2))
            plobjdiff.plotmm(cubediff_region, cubediff_region_min,
                             cubediff_region_max, ax2, 'green', fig) 
           
         # plot the legend and close the plot 
        
        #fig.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        #fig.legend()
        plt.figtext(0.02, 0.97,figno.get(region),
                    horizontalalignment='left',
                    verticalalignment='top',
                    fontsize=20)
        #plt.subplots_adjust(top=0.85)
        fig.tight_layout(rect=[0, 0.03, 1, 0.95])
        #plt.show()
        #sys.exit(0)
        plt.savefig(outname + '.eps')
        plt.savefig(outname + '.pdf')
       
        iris.save(cubelist, OUTWRITE, netcdf_format='NETCDF3_CLASSIC', fill_value=1.0E20)        
    
       
    return    
        
        

# variable definition
LINUX_WIN = 'l'
FIELDNAME = 'SST'

if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    OUTWRITE = FILESTART + 'alldata/data_for_fig4.nc'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    OUTSTART = FILESTART + 'allplots\\' + FIELDNAME + '\\'
    OUTWRITE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_fig4.nc'
    

REGIONNAMES = ['TROPATL', 'TROPPAC', 'MERIDATL', 'MERIDPAC']
#REGIONNAMES = ['TROPATL']

UNITS = 'deg C'
#TIMEPERIODS = ['pi']
TIMEPERIODS = ['pi', 'mPWP']


main()
::::::::::::::
PlioMIP_new/large_scale_features/temperature_isotherms_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the multimodel mean near surface air temperature with
+/-2 degree isotherms marked out.  It is to understand which regions
are likely to support thermofrost

"""

import sys
import iris
from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def plotmean_newaxis(cube, modelno_):
    """
    this will add a new axis to the cube which contains the model number
    this is needed for concatenation
    """

    tempcube_ = iris.util.new_axis(cube)
    tempcube_.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                 standard_name='model_level_number',
                                                 long_name='model',
                                                 var_name='model',
                                                 units=None,
                                                 bounds=None,
                                                 coord_system=None,
                                                 circular=False), 0)
    return tempcube_

def get_within_pm2deg(cube):
    """
    This routine will mask out all the regions that are not within
    + or - 2degrees
    input: a temperature cube
    output: a new cube with all data not within +/-deg masked out.
    """

    cubedata = cube.data

    newdata = np.ma.masked_outside(cubedata, -2.0, 2.0, copy=True)

    newcube = cube.copy(data=newdata)

    return newcube

class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, cube, timeperiod_, fieldname, modelname,
                 outname):

        """
        inputs are:
                    modelname - like 'HadCM3' or 'MMM' multimodelmean
                    cube - a single cube containing the data
        """


        self.titlename = timeperiod_ + ' ' + modelname
        self.cube = cube
        self.outname = outname

        if fieldname == 'NearSurfaceTemperature':
            self.valmin = -30.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'RdBu_r'
            self.use_cbar = 'y'
            
        if fieldname == 'MMM':
            self.valmin = 0.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'gist_ncar_r'
            self.use_cbar = 'n'

        if fieldname == 'TEMP +/- 2DEG':
            self.valmin = -10.
            self.valmax = -5.0
            self.diff = 1.0
            self.colormap = 'jet'
            self.use_cbar = 'n'



    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        cubedata = self.cube.data
        latitudes = self.cube.coord('latitude').points
        lon = self.cube.coord('longitude').points
        datatoplot, longitudes = (shiftgrid(180., cubedata,
                                            lon, start=False))


        self.plotmap(datatoplot, longitudes, latitudes)


        return

    def plotmap(self, datatoplot, longitudes, latitudes):
        """
        will plot the data in a map format

        """

        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=0.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        lonmap, latmap = map(lons, lats)
        map.drawcoastlines()

        values = np.arange(self.valmin, self.valmax, self.diff)
        contourplot = map.contourf(lonmap, latmap, datatoplot, values,
                                   cmap=self.colormap,
                                   extend='both')
        plt.title(self.titlename)

        if self.use_cbar == 'y':
            cbar = plt.colorbar(contourplot, orientation='horizontal')
            cbar.set_label(UNITS, size=10)
            cbar.ax.tick_params(labelsize=8, labelrotation=60)


        fileout = (self.outname + '.eps')
        plt.savefig(fileout, bbox_inches='tight')

        fileout = (self.outname + '.png')#

        plt.savefig(fileout, bbox_inches='tight')
        plt.close()

def main_model_ind():
    """
    get the data from the individual models
    plot the multimodel mean and the individual models on the same 
    figure
    """
    
    # get a mean value of all the cubes which are within +/- 2deg
    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + model + '/' + exptname.get(timeperiod) +
                    '.' + FIELDNAME + '.allmean.nc')

        mod_cube = get_data(filename, FIELDNAME, model)
        mod_cube_2deg = get_within_pm2deg(mod_cube)

        if i == 0:
            tempcube_orig = mod_cube_2deg.copy(data=mod_cube_2deg.data)
        else:
            tempcube_orig.data = mod_cube_2deg.data
        tempcube = plotmean_newaxis(tempcube_orig, i)
        cubelist.append(tempcube)


    twodeg_cube = cubelist.concatenate_cube()
    any_in_range_cube = twodeg_cube.collapsed(['model_level_number'],
                                              iris.analysis.MEAN)


# now we want to plot in blue if multimodel mean is in the range and
# in red if only some of the models are in range

    data_mmm = permafrost_mean_cube.data
    data_any = any_in_range_cube.data
    ysize, xsize = np.shape(data_mmm)
    data_both = np.zeros(np.shape(data_mmm))
    for i in range(0, xsize):
        for j in range(0, ysize):
            if np.ma.is_masked(data_mmm[j, i]):
                data_both[j, i] = data_any[j, i]
            else:
                data_both[j, i] = -100.

    all_mean_cube = any_in_range_cube.copy(data=data_both)

    plioobj = Plotalldata(all_mean_cube, timeperiod, 'TEMP +/- 2DEG',
                          '+/- 2deg: any (red) MMM (blue)',
                          OUTSTART + timeperiod +'_within_2deg')
    plioobj.plotdata()
    
    
    
def main_time(timeperiod):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    toplot: y if we want to plot, n if we don't
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = ('/nfs/hera1/earjcti/regridded/'
                + FIELDNAME + '_multimodelmean.nc')

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')

    permafrost_mean_cube = get_within_pm2deg(mean_cube)

# now we need to get all the models and see if any of the values
# are within +/-degC
    if len(MODELNAMES) > 1:
        main_model_ind()


    return permafrost_mean_cube

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    if len(TIMEPERIODS) ==1 : # just do main_timeperiod
        dummy = main_time(TIMEPERIODS[0])
        
    if len(TIMEPERIODS) == 2:  # plotting two timeperiods on same figure
        cube1 = main_time(TIMEPERIODS[0])
        cube2 = main_time(TIMEPERIODS[1])
        
        data1 = cube1.data
        data2 = cube2.data
        ysize, xsize = np.shape(data1)
        databoth = np.zeros((ysize, xsize))
      
        for i in range(0, xsize):
            for j in range(0, ysize):
                if np.ma.is_masked(data1[j, i]):
                    pass
                else:
                    data1[j, i] = 10
                    databoth[j, i] = databoth[j, i] + data1[j, i]
                if np.ma.is_masked(data2[j, i]):
                    pass
                else:
                    data2[j, i] = 20
                    databoth[j, i] = databoth[j, i] + data2[j, i]
       
        databoth = np.where(databoth == 0, np.nan, databoth) 
        print(databoth[:,0])

        cubeboth = cube1.copy(data=databoth)
        
        plotobj = Plotalldata(cubeboth, '', 'MMM',
                              '+/- 2deg: mPWP (green), PI (orange), both (blue)',
                              OUTSTART + 'MMM_within_2deg')
        plotobj.plotdata()
        plt.show()
        
        

# variable definition
FILESTART = ('/nfs/hera1/earjcti/regridded/')
FIELDNAME = 'NearSurfaceTemperature'
OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
UNITS = 'deg C'
#TIMEPERIODS = ['pi']
#TIMEPERIODS = ['mPWP']
TIMEPERIODS = ['pi','mPWP']

if len(TIMEPERIODS) > 1:
    MODELNAMES = []
else:
    MODELNAMES = ['CESM1.0.5', 'COSMOS', 'EC-Earth3.1', 'GISS', 'HadCM3',
                  'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                  'MIROC4m', 'MRI-CGCM2.3',
                  'NorESM-L', 'NorESM1-F',
                  'UofT'
                  ]

main()
::::::::::::::
PlioMIP_new/large_scale_features/temperature_isotherms.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the multimodel mean near surface air temperature with
isotherms (at a given level) marked out.  It is to understand which regions
are likely to support thermofrost

"""

import sys
import os
import iris
import numpy as np
import matplotlib.pyplot as plt

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def plotmean_newaxis(cube, modelno_):
    """
    this will add a new axis to the cube which contains the model number
    this is needed for concatenation
    """

    tempcube_ = iris.util.new_axis(cube)
    tempcube_.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                 standard_name='model_level_number',
                                                 long_name='model',
                                                 var_name='model',
                                                 units=None,
                                                 bounds=None,
                                                 coord_system=None,
                                                 circular=False), 0)
    return tempcube_

class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, cubelist, timeperiod_, fieldname, modelname,
                 outname):

        """
        inputs are:
                    modelname - like 'HadCM3' or 'MMM' multimodelmean
                    cube - a single cube containing the data
        """


        self.titlename = timeperiod_ + ' ' + modelname
        self.cubelist = cubelist
        self.outname = outname

        if fieldname == 'NearSurfaceTemperature':
            self.valmin = -30.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'RdBu_r'
            self.use_cbar = 'y'
            
        if fieldname == 'MMM':
            self.valmin = 0.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'gist_ncar_r'
            self.use_cbar = 'n'

     


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """

        cube = self.cubelist[0]
        latitudes = cube.coord('latitude').points
        lon = cube.coord('longitude').points

        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 30., 180.)
        plt.savefig(self.outname + '_Asia.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_Asia.png', bbox_inches='tight')
        plt.close()
        
        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 180., 360.)
        plt.savefig(self.outname + '_America.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_America.png', bbox_inches='tight')
        plt.close()
        
        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 0., 360.)
        plt.savefig(self.outname + '_Globe.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_Globe.png', bbox_inches='tight')
        plt.close()


        return

    def plotmap(self, longitudes, latitudes, left, right):
        """
        will plot the data in a map format

        """

        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=left, urcrnrlon=right,
                      llcrnrlat=0.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        lonmap, latmap = map(lons, lats)
        map.drawcoastlines()

        for i, cube in enumerate(self.cubelist):
            cubedata = cube.data
            contourplot = map.contour(lonmap, latmap, cube.data, 
                                      ISOTHERM_NEEDED,
                                   colors=COLORUSE[i], linewidths=1.5,
                                   linestyles='solid')

        plt.title(self.titlename)
    



def main_model_ind():
    """
    get the data from the individual models
    plot the multimodel mean and the individual models on the same 
    figure
    """
    
    # get a mean value of all the cubes which are within +/- 2deg
    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + model + '/' + exptname.get(timeperiod) +
                    '.' + FIELDNAME + '.allmean.nc')

        mod_cube = get_data(filename, FIELDNAME, model)
        mod_cube_2deg = get_within_pm2deg(mod_cube)

        if i == 0:
            tempcube_orig = mod_cube_2deg.copy(data=mod_cube_2deg.data)
        else:
            tempcube_orig.data = mod_cube_2deg.data
        tempcube = plotmean_newaxis(tempcube_orig, i)
        cubelist.append(tempcube)


    twodeg_cube = cubelist.concatenate_cube()
    any_in_range_cube = twodeg_cube.collapsed(['model_level_number'],
                                              iris.analysis.MEAN)


# now we want to plot in blue if multimodel mean is in the range and
# in red if only some of the models are in range

    data_mmm = permafrost_mean_cube.data
    data_any = any_in_range_cube.data
    ysize, xsize = np.shape(data_mmm)
    data_both = np.zeros(np.shape(data_mmm))
    for i in range(0, xsize):
        for j in range(0, ysize):
            if np.ma.is_masked(data_mmm[j, i]):
                data_both[j, i] = data_any[j, i]
            else:
                data_both[j, i] = -100.

    all_mean_cube = any_in_range_cube.copy(data=data_both)

    plioobj = Plotalldata(all_mean_cube, timeperiod, 'TEMP +/- 2DEG',
                          '+/- 2deg: any (red) MMM (blue)',
                          OUTSTART + timeperiod +'_within_2deg')
    plioobj.plotdata()
    
    
    
def main_time(timeperiod):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    toplot: y if we want to plot, n if we don't
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = (FILESTART  + FIELDNAME + '_multimodelmean.nc')
    print(filename)

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')

# now we need to get all the models and see if any of the values
# are within +/-degC
    #if len(MODELNAMES) > 1:
    #    main_model_ind()


    return mean_cube

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    cubelist_times = iris.cube.CubeList([])
    if len(TIMEPERIODS) ==1 : # just do main_timeperiod
        cube = main_time(TIMEPERIODS[0])
        cubelist_times.append(cube)
        
    if len(TIMEPERIODS) == 2:  # plotting two timeperiods on same figure
    
        cube1 = main_time(TIMEPERIODS[0])
        cubelist_times.append(cube1)
        cube2 = main_time(TIMEPERIODS[1])
        cubelist_times.append(cube2)
        
        data1 = cube1.data
        data2 = cube2.data
        
        
       
    # plot isotherm contour
        title = np.str(ISOTHERM_NEEDED) + 'deg: mPWP (red), PI (blue)'
        plotobj = Plotalldata(cubelist_times, '', 'MMM',
                          title,
                          OUTSTART + 'MMM_' + np.str(ISOTHERM_NEEDED) + 'deg')
        plotobj.plotdata()
       
        

# variable definition
LINUX_WIN = 'l'
ISOTHERM_NEEDED = 0.0

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/regridded/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    
FIELDNAME = 'NearSurfaceTemperature'
OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
UNITS = 'deg C'
#TIMEPERIODS = ['pi']
#TIMEPERIODS = ['mPWP']
TIMEPERIODS = ['pi','mPWP']
COLORUSE = ['blue','red']

#if len(TIMEPERIODS) > 1:
#    MODELNAMES = []
#else:
#    MODELNAMES = ['CESM1.0.5', 'COSMOS', 'EC-Earth3.1', 'GISS', 'HadCM3',
#                  'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
#                  'MIROC4m', 'MRI-CGCM2.3',
#                  'NorESM-L', 'NorESM1-F',
#                  'UofT'
#                  ]

main()
::::::::::::::
PlioMIP_new/model_differences/annmean_temp_euclidean_dist.py
::::::::::::::
#!/usr/bin/env python3.7
#NAME
#    ANNMEAN_TEMP_EUCLIDEAN_DIST
#PURPOSE
#    This program will compare all the models to see how similar they
#    are on the annual mean temperature
#
#    It will do this by obtaining a number of the euclidean distance between 
#    them
#
# Julia 11/1/2018



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
import iris






################################
# main program

# annual mean

MODELSET = ['CCSM4-UoT','CCSM4-Utr','CCSM4','CESM1.2','CESM2','COSMOS','EC-Earth3.3','GISS2.1G','HadCM3','HadCM3_new','HadGEM3','IPSLCM5A','IPSLCM5A2','IPSLCM6A','MIROC4m','MRI2.3','NorESM1-F','NorESM-L']

FIELD = 'NearSurfaceTemperature'
EXPT = 'E280'
CNTL = 'E280'
ANOMALY = 'N'

nmods = len(MODELSET)
eucldistarr = np.zeros((nmods,nmods))

for i,model1 in enumerate(MODELSET):
    cube1 = iris.load_cube('/nfs/hera1/earjcti/regridded/' + model1 + '/' + 
                           EXPT + '.' + FIELD + '.allmean.nc')
    if ANOMALY == 'Y':
        cubec = iris.load_cube('/nfs/hera1/earjcti/regridded/' + model1 + '/' + 
                           CNTL + '.' + FIELD + '.allmean.nc')
        cube1 = cube1 - cubec
        
    if model1 == 'HadCM3_new' and ANOMALY == 'N':
        cube1 = cube1 - 273.15
 
    for j,model2 in enumerate(MODELSET):
        cube2 = iris.load_cube('/nfs/hera1/earjcti/regridded/'+ model2 + '/' + 
                           EXPT + '.' + FIELD + '.allmean.nc')
        
        if ANOMALY == 'Y':
            cubec = iris.load_cube('/nfs/hera1/earjcti/regridded/' + 
                                   model2 + '/' + 
                                   CNTL + '.' + FIELD + '.allmean.nc')
            cube2 = cube2 - cubec
        if model2 == 'HadCM3_new' and ANOMALY == 'N':
            cube2 = cube2 - 273.15
 
 

        cubediff = cube2.data - cube1.data

        eucldistarr[i,j] = np.sqrt(np.mean(np.square(cubediff)))
        print(model1,model2,eucldistarr[i,j])


fig = plt.figure(figsize=[12.0,12.0])
cs = plt.pcolormesh(MODELSET,MODELSET,eucldistarr,cmap='rainbow')
cbar = plt.colorbar(cs,orientation='horizontal')
cbar.set_label('euclidian distance')
plt.xticks(rotation=90)

if ANOMALY == 'Y':
    titlename = EXPT + '-' + CNTL + '_' + FIELD + '_annmean'
else:
    titlename = EXPT + '_' + FIELD + '_annmean'

plt.title(titlename)
plt.savefig('/nfs/hera1/earjcti/regridded/allplots/Euclidean_distance/' + titlename + '.eps')
plt.savefig('/nfs/hera1/earjcti/regridded/allplots/Euclidean_distance/' + titlename + '.png')
::::::::::::::
PlioMIP_new/PlioMIP2_collaborations/A_Haywood_PAGES_article.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created June 2021
# based on DMC_for_IPCC_with_land.
# altered to create the figure for Alans Pages article


#@author: earjcti
#
# This program plot a figure for Alan's pages article  This includes
# a) MPWP - PI SAT anomaly  PlioMIP2 (MMM)
# b) MPWP - PI SAT anomaly  PlioMIP1 (MMM)
# c) b-a
# d) MPWP - PI precip anomaly  PlioMIP2 (MMM)
# e) MPWP - PI precip anomaly  PlioMIP1 (MMM)
# f) e-d


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
from iris.experimental.equalise_cubes import equalise_attributes
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def customise_cmap3():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """
    colors = [(84, 48, 5), (113, 70, 16), (143, 93, 27), (173, 115, 38),
              (195, 137, 60), (206, 160, 97), (216, 182, 135),
              (227, 204, 173), (238, 226, 211), (248, 248, 247),
              (212, 230, 229), (176, 212, 209), (140, 194, 190),
              (103, 176, 170), (67, 158, 150), (44, 135, 127),
              (29, 110, 100), (14, 85, 74), (0, 60, 48)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
    precfile = '/nfs/hera1/earjcti/regridded/TotalPrecipitation_multimodelmean.nc'
    precip_cube = iris.load_cube(precfile, 'TotalPrecipitationmean_anomaly')
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube, precip_cube


def average_cubes(cubelist, name, units):
    """
    finds the average of the cube
    """
    newcubelist = iris.cube.CubeList([])

    for i, cube in enumerate(cubelist):
        # makes cube ready for concatenation
        print(cube.var_name)
  
        cube.long_name = name
        cube.short_name = name
        cube.var_name = name
        cube.standard_name = None
        cube.attributes = None
        cube.units = units
        cube = iris.util.squeeze(cube)
        cube.cell_methods=None
        cube.data = cube.data.astype('float32')
        for coord in cube.coords():
            coodname = coord.standard_name
            coord.points = coord.points.astype('float32')
            print(coord)
            if coodname !='latitude' and coodname !='longitude':
                if coodname==None:
                    if coord.long_name==None:
                        cube.remove_coord(coord.var_name)
                    else:
                        cube.remove_coord(coord.long_name)
                else:
                    cube.remove_coord(coodname)
        # add new axis for merging
        tempcube=iris.util.new_axis(cube)
        tempcube.add_dim_coord(iris.coords.DimCoord(i, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
    
        newcubelist.append(tempcube)




    iris.experimental.equalise_cubes.equalise_attributes(newcubelist)
    allcubes = newcubelist.concatenate_cube()
    cubeavg = allcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
   
    return cubeavg

def get_model_p1():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
 
    modelnames = ['COSMOS', 'Had', 'CCSM', 'IPSL', 'MIROC', 'MRI', 'NOR']
    filename = '/nfs/hera1/earjcti/PLIOMIP/PlioMIP1_regridded.nc'
    cubeall = iris.load(filename)
    plio_sat_cubes = iris.cube.CubeList([])
    pi_sat_cubes = iris.cube.CubeList([])
    plio_sst_cubes = iris.cube.CubeList([])
    pi_sst_cubes = iris.cube.CubeList([])
    plio_precip_cubes = iris.cube.CubeList([])
    pi_precip_cubes = iris.cube.CubeList([])
       
    for model in modelnames:
        
        sat_pi = model + '_ctrl_sat'
        sat_plio = model + '_plio_sat'
        sst_pi = model + '_ctrl_sst'
        sst_plio = model + '_plio_sst'
        precip_pi = model + '_ctrl_precip'
        precip_plio = model + '_plio_precip'
    
        for cubetemp in cubeall:
            var = cubetemp.var_name
            if sat_pi.lower() in var.lower():
                pi_sat_cubes.append(cubetemp)
            if sat_plio.lower() in var.lower():
                plio_sat_cubes.append(cubetemp)
            if sst_pi.lower() in var.lower():
                pi_sst_cubes.append(cubetemp)
            if sst_plio.lower() in var.lower():
                plio_sst_cubes.append(cubetemp)
            if precip_pi.lower() in var.lower():
                pi_precip_cubes.append(cubetemp)
            if precip_plio.lower() in var.lower():
                plio_precip_cubes.append(cubetemp)



    plio_avgsatcube = average_cubes(plio_sat_cubes, 'sat', 'degC')
    pi_avgsatcube = average_cubes(pi_sat_cubes, 'sat', 'degC')   
    plio_avgsstcube = average_cubes(plio_sst_cubes, 'sst', 'degC')
    pi_avgsstcube = average_cubes(pi_sst_cubes, 'sat', 'degC')   
    plio_avgprecipcube = average_cubes(plio_precip_cubes, 'precip', 'mm')
    pi_avgprecipcube = average_cubes(pi_precip_cubes, 'precip', 'mm')   
    anom_sat_cube = plio_avgsatcube - pi_avgsatcube
    anom_sst_cube = plio_avgsstcube - pi_avgsstcube
    anom_precip_cube = plio_avgprecipcube - pi_avgprecipcube
   
    # get land sea mask

    lsminit = iris.load_cube('/nfs/hera1/earjcti/PRISM/PLIOMIP/exp2_preferred/land_fraction_v1.1.nc')
    lsm_newdata = np.where(lsminit.data > 0.0, 1.0, 0.0)
    lsm_cube = lsminit.copy(data=lsm_newdata)

    Tanom_data =  ((anom_sat_cube.data * lsm_cube.data) - 
                  (anom_sst_cube.data * (lsm_cube.data - 1.0)))
    Tanom_data.mask = False
    Tanom_cube = anom_sat_cube.copy(data=Tanom_data)

        
    return Tanom_cube, anom_precip_cube, lsm_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = P2_DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = P2_DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = P2_DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = P2_DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        data_tanom = self.sitetemp_bs - self.temppi
        
        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons


  
def main_p2_sat():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    """

    # get model data
    model_anom_cube, lsmplio_cube, model_anom_precip_cube = get_model_data()
   
    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
   
    # get ocean observations
    obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
    lats, lons, data_tanom, Npoints = obj.get_proxydata() 
    obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
    lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
    
    for i in range(0, Npoints_UK37):
        lats.append(lats_UK37[i])
        lons.append(lons_UK37[i])
        data_tanom.append(data_tanom_UK37[i])

  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        

    return [model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom, 
            land_lats, land_lons, land_tanom, model_anom_precip_cube]


def get_p1_land():
    """
    gets the land data from p1
    """

    lons = []
    lats = []
    temps = []

    filename = '/nfs/hera1/earjcti/PRISM/prism3_pliocene/SAT_2012_07.txt'
    f1 = open(filename)
    lines = f1.readlines()
    for i in range(1, len(lines)):
        line = lines[i]
        lon, lat, temp, dummy = line.split()
        lons.append(np.float(lon))
        lats.append(np.float(lat))
        temps.append(np.float(temp))

    return lons, lats, temps

def get_p1_ocn():
    """
    gets the ocean data from p1
    """

    lons = []
    lats = []
    temps = []

    filename = '/nfs/hera1/earjcti/PRISM/prism3_pliocene/SST_2012_07_reformat.txt'
    f1 = open(filename)
    lines = f1.readlines()
    for i in range(1, len(lines)):
        line = lines[i]
        print(line)
        print(line.split())
        name, lat, lon, modern, prism, anomaly = line.split()
        lons.append(np.float(lon))
        lats.append(np.float(lat))
        temps.append(np.float(anomaly))

    return lons, lats, temps

def main_p1_sat():
    """
    gets the pliomip1 data
    calling structure
    a) get's model data
    b) get's proxy data
    """

    # get model data
    (model_satanom_cube, model_precipanom_cube, lsm_cube) = get_model_p1()
   
    # get data from p1
    (land_lon, land_lat, land_data) = get_p1_land()
    (ocn_lon, ocn_lat, ocn_data) = get_p1_ocn()
  
   
    
    return [model_satanom_cube, lsm_cube, land_lon, land_lat, land_data,
            ocn_lon, ocn_lat, ocn_data, model_precipanom_cube]

########################################################

def plot(ax, i, model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data, plot_ocean, plot_land):

    """
    plots the model anomaly with the data anomaly on top
    """
   
    plt.subplot(2,3,i)
    # plot model
    if i ==1 or i==2:  # SAT plot
        vmin = -10.0
        vmax = 10.0
        incr = 1.0
        ticks = [-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]
        mycmap = customise_cmap2()
        if i ==1:
            title = 'a) PlioMIP2 '
        if i==2:
            title = 'b) PlioMIP1'
        units = 'deg C'


    if i ==3: #SAT anom
        vmin = -3.0
        vmax = 3.0
        incr = 0.5
       # ticks = [-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5]
        ticks = [-3, -2, -1, 0,  1,  2, 3]
        mycmap = customise_cmap2()
        title = 'c) PlioMIP2 - PlioMIP1'
        units = 'deg C'



    if i ==4 or i==5:  # precip plot
        vmin = -2.0
        vmax = 2.0
        incr = 0.2
        ticks = [-1.8, -1.2, -0.6, 0, 0.6,  1.2, 1.8]
        mycmap = customise_cmap3()
        if i ==4:
            title = 'd) PlioMIP2 '
        if i==5:
            title = 'e) PlioMIP1'
        units = 'mm / day'



    if i ==6:  # precip anom
        vmin = -2.0
        vmax = 2.0
        incr = 0.2
        ticks = [-1.8, -1.2, -0.6, 0, 0.6,  1.2, 1.8]
        mycmap = customise_cmap3()
        title = 'f) PlioMIP2 - PlioMIP1'
        units = 'mm / day'




    V = np.arange(vmin, vmax + incr, incr)
   
    # turn the iris cube data structure into numpy arrays
    gridlons_d = model_cube.coord('longitude').points
    gridlats_d = model_cube.coord('latitude').points
    moddata = model_cube.data


    print(model_cube)
    print(moddata.shape, gridlons_d.shape,  gridlats_d.shape)
    #cs1 = ax.contourf(gridlons_d, gridlats_d, moddata, levels=V,  extend='both',
    #                   cmap=mycmap)
   # 
    #plt.title('3.205Ma - PI temperature anomaly')
 
    cs = iplt.contourf(model_cube, V, extend='both', cmap=mycmap)
    print('j1',gridlons_d.shape, gridlats_d.shape, mask_cube.data.shape)
    plt.contour(mask_cube.coord('longitude').points, 
                mask_cube.coord('latitude').points,mask_cube.data, colors='black', linewidths=0.1)
  
    plt.title(title, fontsize=9)
    cbar =plt.colorbar(cs,  orientation= 'horizontal', ticks = ticks)
    cbar.set_label(units, fontsize=7)
    cbar.ax.tick_params(labelsize=7) 
 

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    if plot_ocean:
        plt.scatter(lons, lats, c='black',  marker='o', s=30, transform=ccrs.Geodetic())

        plt.scatter(lons, lats, c=data,  marker='o', s=15,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    if plot_land:
        plt.scatter(land_lons, land_lats, c='black',  
                    marker='o', s=30, transform=ccrs.Geodetic())#

        plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=15,
                    norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
  
###################################################################

def main():
    """
    calling structure
    a) get's sat (model and data) from pliomip2
    b) get's sat (model and data) from pliomip1
    c) get's total precip (model and data) from pliomip2
    d) get's total precip (model and data) from pliomip1
    d) plot
    """
    
    fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(7.09, 6.0),
                      subplot_kw={'projection': ccrs.Robinson(central_longitude=0)})
    fig.set_dpi(300.0)

    fig.text(0.02, 0.86, 'Temperature anomaly:', fontsize = 12)
   
    fig.text(0.02, 0.41, 'Precipitation anomaly:', fontsize = 12)
   

    # process pliomip2 temperature
    (model_satanom_p2_cube, lsmplio_p2_cube, lats_p2, 
     lons_shift_p2, p2_data_tanom, p2_land_lats,#
     p2_land_lons, p2_land_tanom, model_precipanom_p2_cube) = main_p2_sat()
   
    plot(ax[0,0], 1, model_satanom_p2_cube, lsmplio_p2_cube, lats_p2, lons_shift_p2,
         p2_data_tanom, p2_land_lats, p2_land_lons, p2_land_tanom,True,True)


    # process pliomip1 temperature
    (model_satanom_p1_cube, lsmplio_p1_cube, land_lat_p1, 
     land_lon_p1, land_data_p1, ocean_lon_p1, ocean_lat_p1, sst_p1,
     model_precipanom_p1_cube) = main_p1_sat()
    dummy=-999.
    plot(ax[0,1], 2, model_satanom_p1_cube, lsmplio_p1_cube, ocean_lat_p1, ocean_lon_p1,
         sst_p1, land_lat_p1, land_lon_p1, land_data_p1, True, True)

  
    # plot p2 - p1
    p1_regrid = model_satanom_p1_cube.regrid(model_satanom_p2_cube, iris.analysis.Linear())
    p2_p1_anom = model_satanom_p2_cube.data - p1_regrid.data
    p2_p1_cube = model_satanom_p2_cube.copy(data=p2_p1_anom)

    plot(ax[0,2],3, p2_p1_cube, lsmplio_p2_cube, dummy, dummy, dummy, dummy, dummy, dummy,
         False, False)



    # plot precipitation
    plot(ax[1,0],4, model_precipanom_p2_cube, lsmplio_p2_cube,
         dummy, dummy, dummy, dummy, dummy, dummy, False, False)
  
    plot(ax[1,1],5, model_precipanom_p1_cube, lsmplio_p1_cube,
         dummy, dummy, dummy, dummy, dummy, dummy, False, False)

    # plot p2 - p1
    p1_regrid = model_precipanom_p1_cube.regrid(model_precipanom_p2_cube, iris.analysis.Linear())
    p2_p1_anom = model_precipanom_p2_cube.data - p1_regrid.data
    p2_p1_cube = model_precipanom_p2_cube.copy(data=p2_p1_anom)

    plot(ax[1,2],6, p2_p1_cube, lsmplio_p2_cube, dummy, dummy, dummy, dummy, dummy, dummy,
         False, False)

    #plt.tight_layout(w_pad=3)
    plt.tight_layout()
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/Collaborators/A_Haywood_PAGES.png')
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/Collaborators/A_Haywood_PAGES.eps')
    plt.close()
   

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


P2_DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
P2_OUTSS = 'McClymont_Bayspline'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/PlioMIP2_collaborations/J_Hall_extract_data_locations.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#Updated for JH on 17th May 2021
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
# This program has been ammended from 
#PlioMIP2/large_scale_features/extract_data_locations.py
#
#

import pandas as pd
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Jonathan':
            self.filenamein = filename + 'metadata_JH_based_on_pliovar.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.sitecolumn = 1
            self.outend = 'modeloutput_JH.xls'
            self.sitesreq = ['ODP662','ODP999','DSDP606','DSDP607','U1313',
                             'U1308','DSDP552','ODP982','ODP642']
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'test_localities.xls'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            if pliomip1 == 'y':
                self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/pliomip1_'+self.outend
            else:
                self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend
        


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []
        sitelist = self.sitesreq

        df = pd.read_csv(self.filenamein, delimiter=',')
        for site in sitelist:
            print('site is',site)
            dfrow = df.loc[lambda df: df['name'] == site]
            print(dfrow)
            lat = dfrow.iloc[:, self.latcolumn]
            lon = dfrow.iloc[:, self.loncolumn]
            latlist.append(pd.DataFrame.to_numpy(lat)[0])
            lonarr = pd.DataFrame.to_numpy(lon)[0]
            lonlist_alt.append(lonarr)
            if lonarr < 0.:# longitude goes from 0-360 in models
                lonlist.append(lonarr+360.)
            else:
                lonlist.append(lonarr)
            lonlist.append
           

        for i, site in enumerate(sitelist):
            print(site, lonlist[i], lonlist_alt[i], latlist[i])

        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt, sitelist
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period

        if test == 'y':
            self.modelnames = ['NorESM1-F', 'HadCM3']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3','HadGEM3',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]

        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            print(self.latlist[i], self.lonlist[i])
            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            
            data_slice_data = data_slice.data
            if -100. < data_slice_data < 100.: 
                model_data[i] = data_slice_data
            else:
                model_data[i] = float('NaN')
        
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.SST.allmean.nc')

            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################

class Getmodeldata_p1:
    # get all of the data from the pliomip1 models at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period

        if test == 'y':
            self.modelnames = ['NOR']
        else:
            self.modelnames = ['COSMOS', 'Had', 'CCSM',
                               'IPSL', 'MIROC', 'MRI', 'NOR']

        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


    def extract_obs(self, filenameuse):
        """
        will extract the data at each point from NOAAERSST5 or HadISST
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            
            data_slice_data = data_slice.data
            if -100. < data_slice_data < 100.: 
                model_data[i] = data_slice_data
            else:
                model_data[i] = float('NaN')
        
         
        return model_data

    def extract_model_points_p1(self, model):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """
        filename = '/nfs/hera1/earjcti/PLIOMIP/PlioMIP1_regridded.nc'
        print(self.period, model)
        if self.period == 'E280':
            field = model + '_ctrl_sst'
        if self.period == 'EOI400':
            field = model + '_plio_sst'

        cubeall = iris.load(filename)
        for cube_temp in cubeall:
            var = cube_temp.var_name
            if field.lower() in var.lower():
               cube=cube_temp

        print(field, cube)
       
        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
       
        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            if self.lonlist[i] > 180.:
                lonreq = self.lonlist[i] - 360.
            else:
                lonreq = self.lonlist[i]
            
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-lonreq)).argmin()
            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            
            if model == 'NOR':
                data_slice_data = data_slice.data - 273.15
            else:
                data_slice_data = data_slice.data

            if -100. < data_slice_data < 100.: 
                model_data[i] = data_slice_data
            else:
                model_data[i] = float('NaN')
       
        return model_data



   

    def extract_all_p1(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
       
        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitevals = np.zeros((nmodels, npoints)) # data at point

        for modno, modelname in enumerate(self.modelnames):
           if modelname == 'HadISST' or modelname == 'NOAAERSST5':
              filename = (self.modelstart + modelname + '/' +
                          self.period + '.SST.allmean.nc')

              sitevals[modno,:] = self.extract_obs(filename)
           else:
               # get model data
               sitevals[modno,:] = self.extract_model_points_p1(modelname)

        return [self.modelnames,sitevals]
# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    plt.show()

def model_correct():
    # calculate eoi400 - e280 + noaa_ersstv5 and eoi400-e280 + hadisst
    noaaix = modelnames_e280.index('NOAAERSST5')
    hadix = modelnames_e280.index('HadISST')

    ny, nx = eoi400.shape
    eoi400_corr_hadiss = eoi400 - e280[0:ny,:] + e280[hadix, :]
    eoi400_corr_noaa = eoi400 - e280[0:ny,:] + e280[noaaix, :]
   
    print(eoi400_corr_noaa.shape, eoi400.shape)
    return eoi400_corr_hadiss, eoi400_corr_noaa

def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite, sitename):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'site')
    sheet.write(0,1,'lat')
    sheet.write(0,2,'lon')
    for model in range(0,nmodels):
        sheet.write(0,3+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,sitename[i],style)
        sheet.write(i+1,1,latlist[i],style)
        sheet.write(i+1,2,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,3+model,datawrite[model,i],style)

    sheet.write(0,3+nmodels,'MMM')
    sheet.write(0,4+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,3+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,4+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,5+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,5+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, sitename):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400_corr_NOAA', modelnames_eoi400, lonlist_alt, latlist, eoi400_corr_noaa, sitename)
    write_sheet(wb,style, 'EOI400_RAW', modelnames_eoi400, lonlist_alt, latlist, eoi400, sitename)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280, sitename)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels], sitename)

    # add_sheet near Eoi400 E280 and difference
    # JULIA NOTE:  I HAVE GOT RID OF ALL THESE FOR JH BECAUSE ALL HIS POINTS
    # ARE DEFINATELY OCEAN.  
    #write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near, sitename)
    #write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near, sitename)
    #write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
    #            eoi400_near[0:nmodels]-e280_near[0:nmodels], sitename)

    # add sheet for how far away we need to look for data
    #write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance, sitename)
    #write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance, sitename)

    # add sheet for how many gridboxes we are averaging over
    #write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg, sitename)
    #write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg, sitename)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


def plot_by_lat(SST, lat, outmid):
# do a temperature by latitude plot for all of the data
    print(modelnames_eoi400)
    print(SST.shape, np.mean(SST, axis=0))
    plt.scatter(lat, np.mean(SST, axis=0),label='multimodel mean')
    for i, model in enumerate(modelnames_eoi400):
        if i < 8:
            plt.scatter(lat, SST[i, :], s=5, marker='^', label=model)
        else:
            plt.scatter(lat, SST[i, :], s=5, marker='v', label=model)
   
    plt.errorbar(np.asarray(lat)-1.0, np.mean(SST, axis=0), yerr=np.std(SST, axis=0))
    plt.title(outmid)
    plt.xlabel('latitude')
    plt.ylabel('temp deg C')
    plt.legend(ncol=2, prop={'size':6})
    

    outstart = '/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/'
    outstart = outstart + 'Collaborators/Jonathan_Hall/'
    if pliomip1 == 'y':
        outstart + outstart + 'PlioMIP1_'
    plt.savefig(outstart + outmid + '.png')
    plt.savefig(outstart + outmid + '.eps')
    plt.close()
   
#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Jonathan' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'
pliomip1 = 'y'

indata = Getinitialdata(linuxwin,datafile)
(modelstart, outputfile, longitudes, 
 latitudes, longitudes_alt, sitenames) = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points

######################################
# setup a map and plot the points

#if linuxwin == 'l':
#    plotpoints(longitudes_alt,latitudes,np.zeros(npoints))


    

##############################
# get the SST data from IRIS cubes

if pliomip1 =='y':
    modeldata=Getmodeldata_p1(testdata,modelstart,fieldnames,
                               latitudes,longitudes,'EOI400')
    (modelnames_eoi400,eoi400)=modeldata.extract_all_p1()
    modeldata=Getmodeldata_p1(testdata,modelstart,fieldnames,
                               latitudes,longitudes,'E280')
    (modelnames_e280,e280)=modeldata.extract_all_p1()

else:  #get pliomip2 data
#eoi400
    modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
    (modelnames_eoi400,eoi400,eoi400_near,
     eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


    modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
    (modelnames_e280,e280,e280_near,
     e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

#get eoi400 corrected (eoi400 -e280 + e280noaa)
(eoi400_corr_hadisst, eoi400_corr_noaa) = model_correct() 

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt, sitenames)

#####################################
# plot figures
if pliomip1 == 'y':
    extra = 'PLIOMIP1_'
else:
    extra = ''
plot_by_lat(eoi400, latitudes, extra + 'JH_raw_data')
plot_by_lat(eoi400_corr_hadisst, latitudes, extra + 'JH_corr_hadisst')
plot_by_lat(eoi400_corr_noaa, latitudes,extra + 'JH_corr_noaa')

::::::::::::::
PlioMIP_new/vegetation_data_analysis/assess_CO2.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.gridspec as gridspec
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array


def get_single_model(model, latreq, lonreq, exptid):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + exptid + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_mean_array = np.zeros(nsites)
    plio_min_array = np.zeros(nsites)
    plio_max_array = np.zeros(nsites)
   
    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_array = plio_cube.data[:, lat_ix, lon_ix]
        plio_min_array[i] = np.min(plio_array)
        plio_max_array[i] = np.max(plio_array)
        plio_mean_array[i] = np.mean(plio_array)

   
    return plio_mean_array, plio_max_array, plio_min_array

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []
    temp_uncert = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temp_uncert.append(dfs.iloc[rl,10])
            temps.append(temp)

    print(temp_uncert)
    for i, temp in enumerate(temp_uncert):
        if i > 0:
            temp2 = temp[2:]
        else:
            temp2=0.0
        print(temp, temp2)
        temp_uncert[i]=np.float(temp2)
     
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
        label = ''.join([c for c in site if c.isupper()])
        if lats[i] < 0:
            latstr = np.str(np.int(np.round(lats[i] * -1.0, 0))) + deg +  'S'
        else:
            latstr = np.str(np.int(np.around(lats[i], 0))) + deg + 'N'
        if lons[i] < 0:
            lonstr = np.str(np.int(np.round(lons[i] * -1.0, 0))) + deg +  'W'
        else:
            lonstr = np.str(np.int(np.around(lons[i], 0))) + deg + 'E'
        
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return lats, lons, temps, temp_modern, temp_uncert, labels

 

def plot_figure(plio_temp_obs, plio_model_400, plio_model_450, labels, ax, fig):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    #ax1 = ax.axes(frameon=False)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
    #ax.axes.get_yaxis().set_visible(False)
    #ax.get_xaxis().tick_bottom()
   

    yarray = np.arange(1, len(plio_temp_obs) + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
   
    # plot individual models for pliocene
    model_400_anom = np.zeros(np.shape(plio_model_400))
    model_450_anom = np.zeros(np.shape(plio_model_450))
    colors = ['black','green','orange']
    for i, model in enumerate(MODELNAMES):
        model_400_anom[:, i] = plio_model_400[:, i] - plio_temp_obs  
        model_450_anom[:, i] = plio_model_450[:, i] - plio_temp_obs  
        plt.scatter(model_400_anom[:, i], yarray, marker = 'o', 
                        color = colors[i], s=30)
        plt.scatter(model_450_anom[:, i], yarray-0.2, marker = '^',
                       color = colors[i], s=30)
       
  
    # add site labels
    plt.text(-5.0, yarray[7], labels[7], ha='right')
    for j in range(0, 7):
        plt.text(np.min(model_400_anom[j, :]) - 1.0,
                 yarray[j], labels[j], ha='right')

    plt.xlabel('Temperature difference from observations (deg C)')
    #fig.legend(loc = 'center left')
    plt.title('a) Annual Mean Temperature', loc='left')

  
    plt.xlim(-25, 7.5)
    plt.ylim(9, 0)
   
def get_land_warm_cold():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle
    sitedata.append(['Meighen Island', 80, 261, 19.6, 20.5,
                     11.5, 13.5, -11.6, -11.4,
                     -33.0, -18.5])
    sitedata.append(['Beaver Pond', 79, 278, 18.4, 20.9,
                     np.nan, np.nan, -12.2, -11.5, np.nan, np.nan])
    sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
                    np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
    sitedata.append(['Lost Chicken Mine', 64, 142, 12.0, 12.0, 
                     13.5, 16.0, -2.0, -2.0, -27.75, -19.25])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     np.nan, np.nan, -1.67, 1.07, np.nan, np.nan])
                     

    sites = []
    lats = []
    lons = []
    WMMT_veg_min = []
    WMMT_veg_max = []
    WMMT_beetle_min = []
    WMMT_beetle_max = []
    CMMT_veg_min = []
    CMMT_veg_max = []
    CMMT_beetle_min = []
    CMMT_beetle_max = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_veg_min.append(info[3])
        WMMT_veg_max.append(info[4])
        WMMT_beetle_min.append(info[5])
        WMMT_beetle_max.append(info[6])
        CMMT_veg_min.append(info[7])
        CMMT_veg_max.append(info[8])
        CMMT_beetle_min.append(info[9])
        CMMT_beetle_max.append(info[10])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return  (labels, lats, lons, np.asarray(WMMT_veg_min),
             np.asarray(WMMT_veg_max), np.asarray(WMMT_beetle_min),
             np.asarray(WMMT_beetle_max), np.asarray(CMMT_veg_min),
             np.asarray(CMMT_veg_max), np.asarray( CMMT_beetle_min),
             np.asarray(CMMT_beetle_max))


def plot_seas_fig(veg_temp, beetle_temp, plio_model_400,
                     plio_model_450, labels, ax, fig, wc_ind):
    """
    this subroutine tries to plot the figure for the paper which shows how 
    different values of CO2 affect the seasonal anomaly
    """

    nmods = len(veg_temp)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
   
    yarray = np.arange(1, nmods + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
    plt.hlines(y=8.95, xmin=-25, xmax=7.5, linewidth=0.5)
   
    # plot individual models for pliocene
    model_400_anom = np.zeros(np.shape(plio_model_400))
    model_450_anom = np.zeros(np.shape(plio_model_450))
    model_450_beetle_anom = np.zeros(np.shape(plio_model_450))
    colors = ['black','green','orange']
    for i, model in enumerate(MODELNAMES):
       
        model_400_anom[:, i] = plio_model_400[:, i] - veg_temp 
        model_450_anom[:, i] = plio_model_450[:, i] - veg_temp  
        model_450_beetle_anom[:, i] = plio_model_450[:, i] - beetle_temp
        ax.scatter(model_400_anom[:, i], yarray - 0.2, marker = 'o', 
                        color = colors[i], s=30, label=model + ' 400ppmv - vegdata')
        ax.scatter(model_450_anom[:, i], yarray, marker = '^',
                       color = colors[i], s=30, label = model + ' 450ppmv - vegdata')
        ax.scatter(model_450_beetle_anom[:, i], yarray + 0.2, marker = 's',
                    color = colors[i], s=30, label= model + ' 450ppmv - beetledata')
   
    # add site labels
    if wc_ind == 'c':
        for j in [0, 1, 3, 4]:
            plt.text(0.5,
                     yarray[j], labels[j], ha='left')
        plt.title('c) Cold Month Temperature', loc='left')
        print(labels[j], model_400_anom[j, :], model_450_anom[j, :], model_450_anom[j, :] -  model_400_anom[j, :], MODELNAMES)

    else:
        for j in range(0, 5):
            plt.text(-20, yarray[j], labels[j], ha='right')
        plt.title('b) Warm Month Temperature', loc='left')



        
    if wc_ind == 'c':
        plt.xlabel('Temperature difference from observations (deg C)')
        handles, labs = fig.gca().get_legend_handles_labels()
        order = [0, 3, 6, 1, 4, 7, 2, 5, 8, ]
        fig.legend([handles[i] for i in order], 
                   [labs[i] for i in order],
                   loc = 'center left')
       # fig.legend(loc='center left')
  
    plt.xlim(-35, 15)
    plt.ylim(nmods + 1, 0)
   
  

  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

   
    # get land observations and cru temperature at land points
    
    (land_lats, land_lons, land_temp, 
     modern_temp, plio_unc, land_labels)= get_land_obs()
   
    
    # get ind models data
    all_models_plio_400 = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_450 = np.zeros((len(land_lons), len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind400, ind400WMT, 
         ind400CMT) = get_single_model(model, land_lats, land_lons, 'EOI400')
        all_models_plio_400[:, i] = ind400
        
        (ind450, ind450WMT,
         ind450CMT) = get_single_model(model, land_lats, land_lons, 'EOI450')
        all_models_plio_450[:, i] = ind450
        

  
    # get warm month and cold month temperatures from data
    (sites, land_lats, land_lons, WMMT_veg_min, WMMT_veg_max, WMMT_beetle_min,
     WMMT_beetle_max, CMMT_veg_min, CMMT_veg_max, CMMT_beetle_min,
     CMMT_beetle_max) =  get_land_warm_cold()

    # get warm month and cold month temperatures from model
    allmod_plio_400_wmt = np.zeros((len(land_lons), len(MODELNAMES)))
    allmod_plio_450_wmt = np.zeros((len(land_lons), len(MODELNAMES)))
    allmod_plio_400_cmt = np.zeros((len(land_lons), len(MODELNAMES)))
    allmod_plio_450_cmt = np.zeros((len(land_lons), len(MODELNAMES)))
   
    for i, model in enumerate(MODELNAMES):
        (ind400, ind400WMT, 
         ind400CMT) = get_single_model(model, land_lats, land_lons, 'EOI400')
        allmod_plio_400_wmt[:, i] = ind400WMT
        allmod_plio_400_cmt[:, i] = ind400CMT
        
        (ind450, ind450WMT,
         ind450CMT) = get_single_model(model, land_lats, land_lons, 'EOI450')
        allmod_plio_450_wmt[:, i] = ind450WMT
        allmod_plio_450_cmt[:, i] = ind450CMT
  

    # plot figure annual mean
    fig1 = plt.figure(figsize=[12.0, 12.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=2, ncols=2)

    ax1 = fig1.add_subplot(gs[:,0])
    plot_figure(land_temp,  all_models_plio_400, all_models_plio_450, 
                land_labels, ax1, fig1) 

  
    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_veg_min + WMMT_veg_max) / 2.0, 
                  (WMMT_beetle_min + WMMT_beetle_max) / 2.0,
                  allmod_plio_400_wmt, allmod_plio_450_wmt, sites, 
                  ax2, fig1, 'w')

    ax3 = fig1.add_subplot(gs[1,1])
    plot_seas_fig((CMMT_veg_min + CMMT_veg_max) / 2.0, 
                  (CMMT_beetle_min + CMMT_beetle_max) / 2.0,
                  allmod_plio_400_cmt, allmod_plio_450_cmt, sites, ax3, 
                  fig1, 'c')

  
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/CO2_uncertainty.eps')
    plt.savefig(fileout)


##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

MODELNAMES = ['COSMOS', 'HadCM3', 'MIROC4m']

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/assess_different_orbits_old.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.gridspec as gridspec
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs

import sys




def get_single_model(expt, latreq, lonreq):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    
    nsites = len(latreq)
    months = ['January','February','March','April','May','June','July','August','September','October','November','December']
    monthalt = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    seas_field = np.zeros((len(months), nsites))

    plio_mean_array = np.zeros(nsites)
    plio_min_array = np.zeros(nsites)
    plio_max_array = np.zeros(nsites)
   
  
 
    # can change this to calendar corrected files
    for j, month in enumerate(months):
        if expt in ('xozzb', 'xozzc','xozzd','xozzf','xibol'):
            filename = ('/nfs/hera1/earjcti/um/' + expt +
                        '/database_averages/' + expt + 
                        '_Monthly_Average_' + month 
                        + '_a@pd_Temperature.nc')
            cubetemp = iris.load_cube(filename)
      
        else:
            filename = ('/nfs/hera1/earjcti/um/'+ expt + 
                        '/cal_cor/' + expt + 'a@pa_avg'+ monthalt[j] + '.nc')
            cube2 = iris.load_cube(filename, 
                                      'SURFACE TEMPERATURE AFTER TIMESTEP')
            cubetemp = cube2.collapsed(['t'], iris.analysis.MEAN)
        cube = iris.util.squeeze(cubetemp)
       
        for i in range(0,nsites):
            # modellon is whole numbers from 0-360
            # lat is half numbers from -89.5 to 89.5

            modlon = np.around(lonreq[i])
            if modlon < 0: modlon = modlon + 360.

            lat_ix = ((np.abs(cube.coord('latitude').points 
                              - latreq[i])).argmin())
            lon_ix = ((np.abs(cube.coord('longitude').points 
                         - modlon)).argmin())

            seas_field[j, i] = cube.data[lat_ix, lon_ix]
    
    for i in range(0, nsites):
        plio_min_array[i] = np.min(seas_field[:, i]) - 273.15
        plio_max_array[i] = np.max(seas_field[:, i]) - 273.15
        plio_mean_array[i] = np.mean(seas_field[:, i]) - 273.15

   
    return plio_mean_array, plio_max_array, plio_min_array

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []
    temp_uncert = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temp_uncert.append(dfs.iloc[rl,10])
            temps.append(temp)

    print(temp_uncert)
    for i, temp in enumerate(temp_uncert):
        if i > 0:
            temp2 = temp[2:]
        else:
            temp2=0.0
        print(temp, temp2)
        temp_uncert[i]=np.float(temp2)
     
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
        label = ''.join([c for c in site if c.isupper()])
        if lats[i] < 0:
            latstr = np.str(np.int(np.round(lats[i] * -1.0, 0))) + deg +  'S'
        else:
            latstr = np.str(np.int(np.around(lats[i], 0))) + deg + 'N'
        if lons[i] < 0:
            lonstr = np.str(np.int(np.round(lons[i] * -1.0, 0))) + deg +  'W'
        else:
            lonstr = np.str(np.int(np.around(lons[i], 0))) + deg + 'E'
        
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return lats, lons, temps, temp_modern, temp_uncert, labels

 

def plot_figure(plio_temp_obs, plio_model, labels, ax, fig):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    #ax1 = ax.axes(frameon=False)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
    #ax.axes.get_yaxis().set_visible(False)
    #ax.get_xaxis().tick_bottom()
   

    yarray = np.arange(1, len(plio_temp_obs) + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
   
    # plot individual models for pliocene
    model_anom = np.zeros(np.shape(plio_model))
    colors = ['blue','orange','green', 'red', 'black','purple']
  
    for i, model in enumerate(EXPTNAMES):
        model_anom[:, i] = plio_model[:, i] - plio_temp_obs  
        plt.scatter(model_anom[:, i], yarray - (0.01 * i), marker = 'o', 
                    color=colors[i],s=30)
       
  
    # add site labels
    plt.text(-5.0, yarray[7], labels[7], ha='right')
    for j in range(0, 7):
        plt.text(np.min(model_anom[j, :]) - 1.0,
                 yarray[j], labels[j], ha='right')

    plt.xlabel('Temperature difference from observations (deg C)')
    #fig.legend(loc = 'center left')
    plt.title('a) Annual Mean Temperature', loc='left')

    
    plt.xlim(-25, 7.5)
    plt.ylim(9, 0)
   
def get_land_warm_cold():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle
    sitedata.append(['Meighen Island', 80, 261, 19.6, 20.5,
                     11.5, 13.5, -11.6, -11.4,
                     -33.0, -18.5])
    sitedata.append(['Beaver Pond', 79, 278, 18.4, 20.9,
                     np.nan, np.nan, -12.2, -11.5, np.nan, np.nan])
    sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
                    np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
    sitedata.append(['Lost Chicken Mine', 64, 142, 12.0, 12.0, 
                     13.5, 16.0, -2.0, -2.0, -27.75, -19.25])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     np.nan, np.nan, -1.67, 1.07, np.nan, np.nan])
                     

    sites = []
    lats = []
    lons = []
    WMMT_veg_min = []
    WMMT_veg_max = []
    WMMT_beetle_min = []
    WMMT_beetle_max = []
    CMMT_veg_min = []
    CMMT_veg_max = []
    CMMT_beetle_min = []
    CMMT_beetle_max = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_veg_min.append(info[3])
        WMMT_veg_max.append(info[4])
        WMMT_beetle_min.append(info[5])
        WMMT_beetle_max.append(info[6])
        CMMT_veg_min.append(info[7])
        CMMT_veg_max.append(info[8])
        CMMT_beetle_min.append(info[9])
        CMMT_beetle_max.append(info[10])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return  (labels, lats, lons, np.asarray(WMMT_veg_min),
             np.asarray(WMMT_veg_max), np.asarray(WMMT_beetle_min),
             np.asarray(WMMT_beetle_max), np.asarray(CMMT_veg_min),
             np.asarray(CMMT_veg_max), np.asarray( CMMT_beetle_min),
             np.asarray(CMMT_beetle_max))


def plot_seas_fig(veg_temp, beetle_temp, plio_model,
                     labels, ax, fig, wc_ind):
    """
    this subroutine tries to plot the figure for the paper which shows how 
    different values of orbit affect the seasonal anomaly
    """
    print(plio_model[4,:],labels[4],wc_ind)

    nmods = len(veg_temp)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
   
    yarray = np.arange(1, nmods + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
    plt.hlines(y=8.95, xmin=-25, xmax=7.5, linewidth=0.5)
   
    # plot individual models for pliocene
    model_anom = np.zeros(np.shape(plio_model))
    model_beetle_anom = np.zeros(np.shape(plio_model))
    colors = ['blue','green','orange', 'red', 'black','purple']
    for i, model in enumerate(EXPTNAMES):
       
        model_anom[:, i] = plio_model[:, i] - veg_temp 
        model_beetle_anom[:, i] = plio_model[:, i] - beetle_temp
        if wc_ind == 'c':
            ax.scatter(model_anom[:, i], yarray - 0.2 - (0.01 * i) , 
                       marker = 'o', 
                       color = colors[i],
                       s=30, label=PERIOD.get(model))
        else:
            ax.scatter(model_anom[:, i], yarray - 0.2- (0.01 * i),
                       marker = 'o', 
                       color = colors[i], s=30)
        ax.scatter(model_beetle_anom[:, i], yarray + 0.2- (0.01 * i),
                   marker = 's',
                   color = colors[i], 
                   s=30)
   
    # add site labels
    if wc_ind == 'c':
        for j in [0, 1, 3, 4]:
            plt.text(0.5,
                     yarray[j], labels[j], ha='left')
        plt.title('c) Cold Month Temperature', loc='left')
        print(labels[j], model_anom[j, :],  EXPTNAMES)


    else:
        for j in range(0, 5):
            plt.text(-20, yarray[j], labels[j], ha='right')
        plt.title('b) Warm Month Temperature', loc='left')

        
    


        
    if wc_ind == 'c':
        plt.xlabel('Temperature difference from observations (deg C)')
        #handles, labs = fig.gca().get_legend_handles_labels()
        #order = [0, 3, 6, 1, 4, 7, 2, 5, 8, ]
        #fig.legend([handles[i] for i in order], 
        #           [labs[i] for i in order],
        #           loc = 'center left')
        fig.legend(loc='center left')
  
    #plt.xlim(-35, 15)
    plt.ylim(nmods + 1, 0)
   
  

  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

   
    # get land observations and cru temperature at land points
    
    (land_lats, land_lons, land_temp, 
     modern_temp, plio_unc, land_labels)= get_land_obs()
   
    
    # get ind models data
    all_models_plio = np.zeros((len(land_lons), len(EXPTNAMES)))
    for i, model in enumerate(EXPTNAMES):
        (ind, indWMT, 
         indCMT) = get_single_model(model, land_lats, land_lons)
        all_models_plio[:, i] = ind
        
        

  
    # get warm month and cold month temperatures from data
    (sites, land_lats, land_lons, WMMT_veg_min, WMMT_veg_max, WMMT_beetle_min,
     WMMT_beetle_max, CMMT_veg_min, CMMT_veg_max, CMMT_beetle_min,
     CMMT_beetle_max) =  get_land_warm_cold()

    # get warm month and cold month temperatures from model
    allmod_plio_wmt = np.zeros((len(land_lons), len(EXPTNAMES)))
    allmod_plio_cmt = np.zeros((len(land_lons), len(EXPTNAMES)))
   
    for i, model in enumerate(EXPTNAMES):
        (ind, indWMT, 
         indCMT) = get_single_model(model, land_lats, land_lons)
        allmod_plio_wmt[:, i] = indWMT
        allmod_plio_cmt[:, i] = indCMT
        
     

    # plot figure annual mean
    fig1 = plt.figure(figsize=[12.0, 12.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=2, ncols=2)

    ax1 = fig1.add_subplot(gs[:,0])
    plot_figure(land_temp,  all_models_plio, 
                land_labels, ax1, fig1) 

   
    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_veg_min + WMMT_veg_max) / 2.0, 
                  (WMMT_beetle_min + WMMT_beetle_max) / 2.0,
                  allmod_plio_wmt, sites, 
                  ax2, fig1, 'w')

    ax3 = fig1.add_subplot(gs[1,1])
    plot_seas_fig((CMMT_veg_min + CMMT_veg_max) / 2.0, 
                  (CMMT_beetle_min + CMMT_beetle_max) / 2.0,
                  allmod_plio_cmt, sites, ax3, 
                  fig1, 'c')

  
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_uncertainty.eps')
    plt.savefig(fileout)


##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

EXPTNAMES = ['xibol', 'xozzf', 'xozzd', 'xozze', 'xozzc']

#EXPTNAMES = ['xogzc', 'xogzb']
PERIOD = {'xiboi' : 'pi (old)',
          'xibol' : 'Km5c(old)',
          'xozzc' : 'K1 (3.0560)',
          'xozzd' : 'G17 (2.950)',
          'xozze' : 'KM3 (3.155)',
          'xozzf' : '    (3.053)'}


LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/assess_different_orbits.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.gridspec as gridspec
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs

import sys




def get_single_model(expt, latreq, lonreq):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    
    nsites = len(latreq)
    months = ['January','February','March','April','May','June','July','August','September','October','November','December']
    monthalt = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    seas_field = np.zeros((len(months), nsites))

    plio_mean_array = np.zeros(nsites)
    plio_min_array = np.zeros(nsites)
    plio_max_array = np.zeros(nsites)
   
  
 
    # can change this to calendar corrected files
    for j, month in enumerate(months):
        if expt in ('xibol', 'xozzb', 'xozzd'):
            filename = ('/nfs/hera1/earjcti/um/' + expt +
                        '/database_averages/' + expt + 
                        '_Monthly_Average_' + month 
                        + '_a@pd_Temperature.nc')
            cubetemp = iris.load_cube(filename)
      
        else:
            filename = ('/nfs/hera1/earjcti/um/'+ expt + 
                        '/cal_cor/' + expt + 'a@pa_avg'+ monthalt[j] + '.nc')
            cube2 = iris.load_cube(filename, 
                                      'SURFACE TEMPERATURE AFTER TIMESTEP')
            cubetemp = cube2.collapsed(['t'], iris.analysis.MEAN)
        cube = iris.util.squeeze(cubetemp)
       
        for i in range(0,nsites):
            # modellon is whole numbers from 0-360
            # lat is half numbers from -89.5 to 89.5

            modlon = np.around(lonreq[i])
            if modlon < 0: modlon = modlon + 360.

            lat_ix = ((np.abs(cube.coord('latitude').points 
                              - latreq[i])).argmin())
            lon_ix = ((np.abs(cube.coord('longitude').points 
                         - modlon)).argmin())

            seas_field[j, i] = cube.data[lat_ix, lon_ix]
    
    for i in range(0, nsites):
        plio_min_array[i] = np.min(seas_field[:, i]) - 273.15
        plio_max_array[i] = np.max(seas_field[:, i]) - 273.15
        plio_mean_array[i] = np.mean(seas_field[:, i]) - 273.15

   
    return plio_mean_array, plio_max_array, plio_min_array

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []
    temp_uncert = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temp_uncert.append(dfs.iloc[rl,10])
            temps.append(temp)

    print(temp_uncert)
    for i, temp in enumerate(temp_uncert):
        if i > 0:
            temp2 = temp[2:]
        else:
            temp2=0.0
        print(temp, temp2)
        temp_uncert[i]=np.float(temp2)
     
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
        label = ''.join([c for c in site if c.isupper()])
        if lats[i] < 0:
            latstr = np.str(np.int(np.round(lats[i] * -1.0, 0))) + deg +  'S'
        else:
            latstr = np.str(np.int(np.around(lats[i], 0))) + deg + 'N'
        if lons[i] < 0:
            lonstr = np.str(np.int(np.round(lons[i] * -1.0, 0))) + deg +  'W'
        else:
            lonstr = np.str(np.int(np.around(lons[i], 0))) + deg + 'E'
        
            label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return lats, lons, temps, temp_modern, temp_uncert, labels

 

def plot_figure(plio_temp_obs, plio_model, labels, ax, fig):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    #ax1 = ax.axes(frameon=False)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
    #ax.axes.get_yaxis().set_visible(False)
    #ax.get_xaxis().tick_bottom()
   

    yarray = np.arange(1, len(plio_temp_obs) + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
   
    # plot individual models for pliocene
    model_anom = np.zeros(np.shape(plio_model))
    colors = ['blue','orange','green', 'red', 'black','purple']
  
    for i, model in enumerate(EXPTNAMES):
        model_anom[:, i] = plio_model[:, i] - plio_temp_obs  
        plt.scatter(model_anom[:, i], yarray - (0.01 * i), marker = 'o', 
                    color=colors[i],s=30)
       
  
    # add site labels
    print(EXPTNAMES)
    plt.text(-5.0, yarray[7], labels[7], ha='right')
    for j in range(0, 7):
        plt.text(np.min(model_anom[j, :]) - 1.0,
                 yarray[j], labels[j], ha='right')
        print(labels[j], model_anom[j, :])

    plt.xlabel('Temperature difference from observations (deg C)')
    #fig.legend(loc = 'center left')
    plt.title('a) Annual Mean Temperature', loc='left')
    #sys.exit(0)
    
    plt.xlim(-25, 7.5)
    plt.ylim(9, 0)
   
def get_land_warm_cold():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle
    sitedata.append(['Near Meighen Island', 77.5, 261, 19.6, 20.5,
                     11.5, 13.5, -11.6, -11.4,
                     -33.0, -18.5])
    sitedata.append(['Beaver Pond', 79, 278, 18.4, 20.9,
                     np.nan, np.nan, -12.2, -11.5, np.nan, np.nan])
    sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
                    np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
    sitedata.append(['Lost Chicken Mine', 64, 142, 12.0, 12.0, 
                     13.5, 16.0, -2.0, -2.0, -27.75, -19.25])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     np.nan, np.nan, -1.67, 1.07, np.nan, np.nan])
                     

    sites = []
    lats = []
    lons = []
    WMMT_veg_min = []
    WMMT_veg_max = []
    WMMT_beetle_min = []
    WMMT_beetle_max = []
    CMMT_veg_min = []
    CMMT_veg_max = []
    CMMT_beetle_min = []
    CMMT_beetle_max = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_veg_min.append(info[3])
        WMMT_veg_max.append(info[4])
        WMMT_beetle_min.append(info[5])
        WMMT_beetle_max.append(info[6])
        CMMT_veg_min.append(info[7])
        CMMT_veg_max.append(info[8])
        CMMT_beetle_min.append(info[9])
        CMMT_beetle_max.append(info[10])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return  (labels, lats, lons, np.asarray(WMMT_veg_min),
             np.asarray(WMMT_veg_max), np.asarray(WMMT_beetle_min),
             np.asarray(WMMT_beetle_max), np.asarray(CMMT_veg_min),
             np.asarray(CMMT_veg_max), np.asarray( CMMT_beetle_min),
             np.asarray(CMMT_beetle_max))


def plot_seas_fig(veg_temp, beetle_temp, plio_model,
                     labels, ax, fig, wc_ind, beetle_ind):
    """
    this subroutine tries to plot the figure for the paper which shows how 
    different values of orbit affect the seasonal anomaly
    """

    OFFSET = {'xiboi' : 0.0,
          'xibol' : 0.0,
          'xozzb' : 0.2,
          'xozzc' : 0.0,
          'xozzd' : 0.0,
          'xozze' : 0.0,
          'xozzf' : 0.0}
    SIZE = {'xiboi' : 30,
          'xibol' : 30,
          'xozzb' : 50,
          'xozzc' : 30,
          'xozzd' : 30,
          'xozze' : 30,
          'xozzf' : 30}
    SYMBOL = {'xiboi' : 'o',
          'xibol' : 'o',
          'xozzb' : 's',
          'xozzc' : 'v',
          'xozzd' : '^',
          'xozze' : '^',
          'xozzf' : 'v'}


    print(plio_model[4,:],labels[4],wc_ind)

    nmods = len(veg_temp)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
   
    yarray = np.arange(1, nmods + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
    plt.hlines(y=8.95, xmin=-25, xmax=7.5, linewidth=0.5)
   
    # plot individual models for pliocene
    model_anom = np.zeros(np.shape(plio_model))
    model_beetle_anom = np.zeros(np.shape(plio_model))
    colors = ['blue','green','orange', 'red', 'black','purple']
    for i, model in enumerate(EXPTNAMES):
       
        model_anom[:, i] = plio_model[:, i] - veg_temp 
        model_beetle_anom[:, i] = plio_model[:, i] - beetle_temp
        if wc_ind == 'c':
            ax.scatter(model_anom[:, i], yarray - OFFSET.get(model) , 
                       marker = SYMBOL.get(model), 
                       color = colors[i],
                       s=SIZE.get(model), label=PERIOD.get(model))
        else:
            ax.scatter(model_anom[:, i], yarray - OFFSET.get(model),
                       marker = SYMBOL.get(model), 
                       color = colors[i], s=SIZE.get(model))
        if beetle_ind == 'y':
            ax.scatter(model_beetle_anom[:, i], yarray + 0.2- (0.01 * i),
                       marker = 's',
                       color = colors[i], 
                       s=30)
   
    # add site labels
    if wc_ind == 'c':
        for j in [0, 1, 3, 4]:
            plt.text(0.5,
                     yarray[j], labels[j], ha='left')
        plt.title('a) Cold Month Temperature', loc='left')
        print(labels[j], model_anom[j, :],  EXPTNAMES)


    else:
        for j in range(0, 5):
            plt.text(-20, yarray[j], labels[j], ha='right')
        plt.title('b) Warm Month Temperature', loc='left')

        
    


    plt.xlabel('Temperature difference from observations (deg C)')
          
    if wc_ind == 'c':
        #handles, labs = fig.gca().get_legend_handles_labels()
        #order = [0, 3, 6, 1, 4, 7, 2, 5, 8, ]
        #fig.legend([handles[i] for i in order], 
        #           [labs[i] for i in order],
        #           loc = 'center left')
        fig.legend(loc='best')
  
    plt.xlim(-45, 30)
    plt.ylim(nmods + 1, 0)
   
   
  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

   
    # get land observations and cru temperature at land points
    
    (land_lats, land_lons, land_temp, 
     modern_temp, plio_unc, land_labels)= get_land_obs()
   
    
    # get ind models data
    all_models_plio = np.zeros((len(land_lons), len(EXPTNAMES)))
    for i, model in enumerate(EXPTNAMES):
        (ind, indWMT, 
         indCMT) = get_single_model(model, land_lats, land_lons)
        all_models_plio[:, i] = ind
        
        

  
    # get warm month and cold month temperatures from data
    (sites, land_lats, land_lons, WMMT_veg_min, WMMT_veg_max, WMMT_beetle_min,
     WMMT_beetle_max, CMMT_veg_min, CMMT_veg_max, CMMT_beetle_min,
     CMMT_beetle_max) =  get_land_warm_cold()

    # get warm month and cold month temperatures from model
    allmod_plio_wmt = np.zeros((len(land_lons), len(EXPTNAMES)))
    allmod_plio_cmt = np.zeros((len(land_lons), len(EXPTNAMES)))
   
    for i, model in enumerate(EXPTNAMES):
        (ind, indWMT, 
         indCMT) = get_single_model(model, land_lats, land_lons)
        allmod_plio_wmt[:, i] = indWMT
        allmod_plio_cmt[:, i] = indCMT
        
     

    # plot figure annual mean, warm month and cold month
    fig1 = plt.figure(figsize=[12.0, 12.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=2, ncols=2)

    ax1 = fig1.add_subplot(gs[:,0])
    plot_figure(land_temp,  all_models_plio, 
                land_labels, ax1, fig1) 

   
    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_veg_min + WMMT_veg_max) / 2.0, 
                  (WMMT_beetle_min + WMMT_beetle_max) / 2.0,
                  allmod_plio_wmt, sites, 
                  ax2, fig1, 'w','y')

    ax3 = fig1.add_subplot(gs[1,1])
    plot_seas_fig((CMMT_veg_min + CMMT_veg_max) / 2.0, 
                  (CMMT_beetle_min + CMMT_beetle_max) / 2.0,
                  allmod_plio_cmt, sites, ax3, 
                  fig1, 'c','y')

  
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_uncertainty.eps')
    plt.savefig(fileout)


     # plot figure  cold month and warm month
    fig1 = plt.figure(figsize=[12.0, 8.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=1, ncols=2)

   
  
    ax1 = fig1.add_subplot(gs[0,0])
    plot_seas_fig((CMMT_veg_min + CMMT_veg_max) / 2.0, 
                  (CMMT_beetle_min + CMMT_beetle_max) / 2.0,
                  allmod_plio_cmt, sites, ax1, 
                  fig1, 'c','n')

    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_veg_min + WMMT_veg_max) / 2.0, 
                  (WMMT_beetle_min + WMMT_beetle_max) / 2.0,
                  allmod_plio_wmt, sites, 
                  ax2, fig1, 'w','n')

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_uncertainty_WMMT_CMMT.eps')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_uncertainty_WMMT_CMMT.png')
    plt.savefig(fileout)


##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

EXPTNAMES = ['xozzd', 'xozze', 'xozzb', 'xozzc', 'xozzf']

#EXPTNAMES = ['xogzc', 'xogzb']
PERIOD = {'xiboi' : 'pi (old)',
          'xibol' : 'KM5c',
          'xozzb' : 'KM5c',
          'xozzc' : 'K1 (3.0560Ma)',
          'xozzd' : 'G17 (2.950Ma)',
          'xozze' : 'KM3 (3.155Ma)',
          'xozzf' : '3.053Ma'}


LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/assess_diff_orb_plus_models.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.gridspec as gridspec
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import netCDF4

import sys


###########################
def check_lsm(lsm_lons, lsm_lats, lsm_data, latrq, lonrq):
    """
    if our model is a sea point then set index to nan
    """

    lat_ix = (np.abs(lsm_lats - latrq)).argmin()
    lon_ix = (np.abs(lsm_lons - lonrq)).argmin()
       
    if lsm_data[lat_ix, lon_ix] <  0.5:
        # check to south, north, east, west
#        print(lsm_data[lat_ix - 1, lon_ix],lsm_data[lat_ix + 1, lon_ix],lsm_data[lat_ix, lon_ix-1],lsm_data[lat_ix, lon_ix+1], lsm_data[lat_ix - 1, lon_ix - 1],lsm_data[lat_ix + 1, lon_ix+1 ],lsm_data[lat_ix + 1, lon_ix-1],lsm_data[lat_ix-1, lon_ix+1])
#        if lsm_data[lat_ix - 1, lon_ix] >  0.5:
#            lat_ix = lat_ix -1
#        elif lsm_data[lat_ix + 1, lon_ix] >  0.5:
#            lat_ix = lat_ix + 1
#        elif lsm_data[lat_ix, lon_ix - 1] > 0.5:
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix, lon_ix + 1] > 0.5:
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix + 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix + 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix - 1
       # elif lsm_data[lat_ix, lon_ix - 2] > 0.5:
       #     lon_ix = lon_ix - 2
       # elif lsm_data[lat_ix, lon_ix + 2] > 0.5:
       #     lon_ix = lon_ix + 2
       # elif lsm_data[lat_ix, lon_ix - 3] > 0.5:
       #     lon_ix = lon_ix - 3
       # elif lsm_data[lat_ix, lon_ix + 3] > 0.5:
       #     lon_ix = lon_ix + 3
       # elif lsm_data[lat_ix - 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix -2
       # elif lsm_data[lat_ix + 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix + 2
      
       
  #      else:
            lat_ix = np.nan
            lon_ix = np.nan
            
       # print('new',lsm_data[lat_ix, lon_ix], lat_ix, lon_ix, latrq, lonrq, lsm_lons[lon_ix], lsm_lats[lat_ix])
       # sys.exit(0)

            
        

    return lat_ix, lon_ix

def get_lsm_names(model, period):
    """
    gets the names for each of the land sea masks
    period 0 =e280, period 1 = eoi400
    """
    if model == 'CESM2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'COSMOS':
        lsm = ["/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/E280_et_al/E280.slf.atm.nc", "/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/Eoi400_et_al/Eoi400.slf.atm.nc"]
        fieldlsm = "SLF"

    if model == 'EC-Earth3.3':
        lsm =  [DATABASE + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc',
                DATABASE + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc']
        fieldlsm = 'Land/sea mask'

    if model == 'CESM1.2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if model   ==  'MIROC4m':
        lsm = [DATABASE + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc', 
               DATABASE + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc']
        fieldlsm = "sftlf"

    if model  == 'HadCM3':
        lsm = [DATABASE+'LEEDS/HadCM3/e280/qrparm.mask.nc',
               DATABASE+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc']
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if model == 'CCSM4':
        lsm = [DATABASE + 'NCAR/b40.B1850.f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc', DATABASE + 'NCAR/b40.B1850.f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'CCSM4-Utr':
        lsm = [DATABASE + 'Utrecht/CESM1.0.5/E280/land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC_control_r1i1p1f1_gn.nc', DATABASE + 'Utrecht/CESM1.0.5/Eoi400/land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_f19g16_NESSC_control_r1i1p1f1_gn.nc']
        fieldlsm = 'LANDMASK[D=1]'
  
    if model == 'CCSM4-UoT':
        start = DATABASE + 'UofT/UofT-CCSM4/'
        lsm = [start + 'for_julia/E_mask.nc', start + 'for_julia/Eoi_mask.nc']
        fieldlsm = 'gridbox land fraction'
      
    if model == 'NorESM-L':
       lsm = [DATABASE + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc',
              DATABASE + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc']
       fieldlsm = 'Fraction of sfc area covered by land'


    if model  == 'MRI2.3':
        lsm = [DATABASE + 'MRI-CGCM2.3/sftlf.nc', 
               DATABASE + 'MRI-CGCM2.3/sftlf.nc']
        fieldlsm = 'landsea mask [0 - 1]'


    if model  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        lsm = [start + 'e280/NASA-GISS_PIctrl_all_fland.nc',
               start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc']
        fieldlsm = 'fland'

    if model == 'NorESM1-F':
        lsm = [DATABASE + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc',
               DATABASE + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc']
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if model == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        lsm = [start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc',
              start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc']
        fieldlsm = 'land_area_fraction'

    if model == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if model == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',
               start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if model == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        lsm = [start + 'hadgem3.mask.nc', start + 'hadgem3.mask.nc']
        fieldlsm = 'land_binary_mask'
            
            
    return lsm[period], fieldlsm



def get_land_sea_mask(model, period):
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d


    lsm, fieldlsm = get_lsm_names(model,period)

    ############################################
    if model == 'IPSLCM5A' or model == 'IPSLCM5A2':
        lsm_cube = get_ipsl_lsm(lsm, fieldlsm)
    elif model == 'HadGEM3':
        f = netCDF4.Dataset(lsm, "r")
        print(f.variables['longitude'])
        longitude = iris.coords.DimCoord(f.variables['longitude'], 
                             standard_name = 'longitude', units='degrees')
        latitude = iris.coords.DimCoord(f.variables['latitude'], 
                             standard_name = 'latitude', units='degrees')
        lsm_cube = iris.cube.Cube(np.squeeze(f.variables['lsm'][:]),
                             long_name='lsm', var_name='lsm', units=None, 
                             attributes=None, cell_methods=None, 
                             dim_coords_and_dims=[(latitude,0), (longitude,1)])
     
    else:
        lsm_cube = iris.util.squeeze(iris.load_cube(lsm, fieldlsm))
     
    lsm_cube2 = change_to_2d(lsm_cube)
   
   
    if model == 'IPSLCM6A':
        lsm_cube2.data = lsm_cube2.data / 100.0
       

    # regrid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsm_cube3 = lsm_cube2.regrid(cubegrid, iris.analysis.Linear())
   

    lsm_cube3.var_name = 'land_mask'
    lsm_cube3.long_name = 'land_mask'
    
    return lsm_cube3


def get_single_orbit(expt, latreq, lonreq):
    """
    obtain the modelled temperature at each orbit return the temperatures
    at the list of sites
    """
    
    nsites = len(latreq)
    months = ['January','February','March','April','May','June','July','August','September','October','November','December']
    monthalt = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    seas_field = np.zeros((len(months), nsites))

    plio_mean_array = np.zeros(nsites)
    plio_min_array = np.zeros(nsites)
    plio_max_array = np.zeros(nsites)
   
  
 
    # can change this to calendar corrected files
    for j, month in enumerate(months):
        if expt in ('xibol', 'xozzb', 'xozzd', 'xozzc'):
            filename = ('/nfs/hera1/earjcti/um/' + expt +
                        '/database_averages/' + expt + 
                        '_Monthly_Average_' + month 
                        + '_a@pd_Temperature.nc')
            cubetemp = iris.load_cube(filename)
      
        else:
            filename = ('/nfs/hera1/earjcti/um/'+ expt + 
                        '/cal_cor/' + expt + 'a@pa_avg'+ monthalt[j] + '.nc')
            cube2 = iris.load_cube(filename, 
                                      'SURFACE TEMPERATURE AFTER TIMESTEP')
            cubetemp = cube2.collapsed(['t'], iris.analysis.MEAN)
        cube = iris.util.squeeze(cubetemp)
       
        for i in range(0,nsites):
            # modellon is whole numbers from 0-360
            # lat is half numbers from -89.5 to 89.5

            modlon = np.around(lonreq[i])
            if modlon < 0: modlon = modlon + 360.

            lat_ix = ((np.abs(cube.coord('latitude').points 
                              - latreq[i])).argmin())
            lon_ix = ((np.abs(cube.coord('longitude').points 
                         - modlon)).argmin())

            seas_field[j, i] = cube.data[lat_ix, lon_ix]
    
    for i in range(0, nsites):
        plio_min_array[i] = np.min(seas_field[:, i]) - 273.15
        plio_max_array[i] = np.max(seas_field[:, i]) - 273.15
        plio_mean_array[i] = np.mean(seas_field[:, i]) - 273.15
        if i == 1:
            print(expt, i,seas_field[:,i]-273.15,plio_min_array[i],
                  plio_max_array[i],plio_mean_array[i])
        
   
    return plio_mean_array, plio_max_array, plio_min_array

def get_single_model(model, latreq, lonreq, period):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    # get lsm
    if period == 'E280':
        lsm_cube  = get_land_sea_mask(model, 0)
    if period == 'EOI400':
        lsm_cube  = get_land_sea_mask(model, 1)

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + period + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_minval_array = np.zeros(nsites)
    plio_maxval_array = np.zeros(nsites)
    plio_MAT_array = np.zeros(nsites)

    plio_cube_lats = plio_cube.coord('latitude').points
    plio_cube_lons = plio_cube.coord('longitude').points

    lsm_cube_lats = lsm_cube.coord('latitude').points
    lsm_cube_lons = lsm_cube.coord('longitude').points
    lsm_cube_data = lsm_cube.data
   
  
    if plio_cube_lats.any() != lsm_cube_lats.any():
        print('data cube does not match lsm lat')
    if plio_cube_lats.any() != lsm_cube_lons.any():
        print('data cube does not match lsm lat')

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        (lat_ix, 
         lon_ix) = check_lsm(lsm_cube_lons, lsm_cube_lats, 
                                    lsm_cube_data, latreq[i], modlon)

        if np.isfinite(lat_ix):
            plio_array = plio_cube.data[:, lat_ix, lon_ix]
        else:
            plio_array = np.zeros(12)
            plio_array[:] = np.nan
        plio_minval_array[i] = np.min(plio_array)
        plio_maxval_array[i] = np.max(plio_array)
        plio_MAT_array[i] = np.mean(plio_array)
      
    return plio_MAT_array, plio_minval_array, plio_maxval_array


 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []
    temp_uncert = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        print(dfs.iloc[rl,0])
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temp_uncert.append(dfs.iloc[rl,10])
            temps.append(temp)

    for i, temp in enumerate(temp_uncert):
        if i > 0:
            temp2 = temp[2:]
        else:
            temp2=0.0
        temp_uncert[i]=np.float(temp2)
     
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
        label = ''.join([c for c in site if c.isupper()])
        if lats[i] < 0:
            latstr = np.str(np.int(np.round(lats[i] * -1.0, 0))) + deg +  'S'
        else:
            latstr = np.str(np.int(np.around(lats[i], 0))) + deg + 'N'
        if lons[i] < 0:
            lonstr = np.str(np.int(np.round(lons[i] * -1.0, 0))) + deg +  'W'
        else:
            lonstr = np.str(np.int(np.around(lons[i], 0))) + deg + 'E'
        
            label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return lats, lons, temps, temp_modern, temp_uncert, labels

 

def plot_figure(plio_temp_obs, plio_model_orb, labels, ax, fig):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    #ax1 = ax.axes(frameon=False)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
    #ax.axes.get_yaxis().set_visible(False)
    #ax.get_xaxis().tick_bottom()
   

    yarray = np.arange(1, len(plio_temp_obs) + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
   
    # plot individual models for pliocene
    model_anom = np.zeros(np.shape(plio_model_orb))
    colors = ['orange','blue','green', 'red', 'black','purple']
  
    for i, model in enumerate(EXPTNAMES):
        model_anom[:, i] = plio_model_orb[:, i] - plio_temp_obs  
        plt.scatter(model_anom[:, i], yarray - (0.01 * i), marker = 'o', 
                    color=colors[i],s=30)
       
  
    # add site labels
    plt.text(-5.0, yarray[7], labels[7], ha='right')
    for j in range(0, 7):
        plt.text(np.min(model_anom[j, :]) - 1.0,
                 yarray[j], labels[j], ha='right')

    plt.xlabel('Temperature difference from observations (deg C)')
    #fig.legend(loc = 'center left')
    plt.title('a) Annual Mean Temperature', loc='left')
    #sys.exit(0)
    
    plt.xlim(-25, 7.5)
    plt.ylim(9, 0)
   
def get_land_km5c_lp():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min MAT veg, max MAT veg
    # modern obs WMMT, modern obs CMMT
    # reference and date

    # lake baikal is from what ulrich sent me.
    # Lake E from Brigette-greeme mean july temp of +8 and average winter lows of 35degC

    sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
                    -36.8, -30.4, -11.85, -8.0,
                     8.0, np.nan,'CMMT 3.199Ma - 3.209Ma; Pavel Tarasov (pers. comm) \n WMMT Brigham-Grette et al. 2013'])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     -1.67, 1.07, 6.65,8.65,
                     15.3, -17.4,'Km5c - unpublished (Method of Klage et al 2020)'])
  
    
    sitedata.append(['Mirny', 55, 82, 18.8, 24.6, -0.3, 0.7,9.9,12.5,
                     np.nan, np.nan,'Popova et al 2012'])
    sitedata.append(['Merkutlinskiy', 56, 72, 17.3, 23.8, -3.8, 6.2,7.3,16.2,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Kabinet', 55, 80, 21.6, 24.4, -4.4, 4.6,6.6,7.3,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Delyankir', 63, 133, 18.9, 24.9, -6.9, 1.3,6.9,7.8,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Chernoluche', 55, 73, 19.6, 20.3, -5.9, 0.7,5.4,7.3,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Blizkiy', 64, 162, 15.6, 23.3, -12.8, 5.2,-0.6,11.1,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['42km', 55, 80, 21.6, 23.3, -4.4, 0.7,6.6,11.1,
                     np.nan, np.nan,' --"--'])
  
    sitedata.append(['Lost Chicken Mine', 64, 218, 12.0, 12.0, 
                     -2.0, -2.0, 2.5,2.5,15.3, -25.1, '2.9 +/- 0.4Ma: Ager et al. 1994'])


        
    sites = []
    lats = []
    lons = []
    WMMT_data_min = []
    WMMT_data_max = []
    WMMT_modern_obs = []
    CMMT_data_min = []
    CMMT_data_max = []
    MAT_data_min = []
    MAT_data_max = []
    CMMT_modern_obs = []
    refs = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_data_min.append(info[3])
        WMMT_data_max.append(info[4])
        CMMT_data_min.append(info[5])
        CMMT_data_max.append(info[6])
        MAT_data_min.append(info[7])
        MAT_data_max.append(info[8])
        WMMT_modern_obs.append(info[9])
        CMMT_modern_obs.append(info[10])
        refs.append(info[11])
     
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + ' (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)

  
    return (labels, lats, lons, np.asarray(WMMT_data_min), 
            np.asarray(WMMT_data_max),  np.asarray(CMMT_data_min), 
            np.asarray(CMMT_data_max), 
            np.asarray(MAT_data_max), np.asarray(MAT_data_min),
            np.asarray(WMMT_modern_obs),
            np.asarray(CMMT_modern_obs),refs)

   


def plot_seas_fig(veg_temp, plio_model_orb, plio_model_p2,
                     labels, ax, fig, wc_ind):
    """
    this subroutine tries to plot the figure for the paper which shows how 
    different values of orbit affect the seasonal anomaly
    """

    OFFSET = {'xiboi' : 0.0,
          'xibol' : 0.0,
          'xozzb' : 0.2,
          'xozzc' : 0.0,
          'xozzd' : 0.0,
          'xozze' : 0.0,
          'xozzf' : 0.0}
    SIZE = {'xiboi' : 30,
          'xibol' : 30,
          'xozzb' : 50,
          'xozzc' : 30,
          'xozzd' : 30,
          'xozze' : 30,
          'xozzf' : 30}
    SYMBOL = {'xiboi' : 'o',
          'xibol' : 'o',
          'xozzb' : 's',
          'xozzc' : 'v',
          'xozzd' : '^',
          'xozze' : '^',
          'xozzf' : 'v'}


 
    nmods = len(veg_temp)
    ax.spines['top'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_yaxis().set_ticks([])
   
    yarray = np.arange(1, nmods + 1, 1)

 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=11, linewidth=0.5)
   # plt.hlines(y=8.95, xmin=-25, xmax=7.5, linewidth=0.5)
   
    # plot individual models for pliocene
    model_anom_orb = np.zeros(np.shape(plio_model_orb))
    model_anom_p2 = np.zeros(np.shape(plio_model_p2))
    colors = ['orange','green','blue', 'red', 'black','purple']
    for i, model in enumerate(EXPTNAMES):
        model_anom_orb[:, i] = plio_model_orb[:, i] - veg_temp 
        if wc_ind == 'c' or wc_ind =='m':
            ax.scatter(model_anom_orb[:, i], yarray - OFFSET.get(model) , 
                       marker = SYMBOL.get(model), 
                       color = colors[i],
                       s=SIZE.get(model), label=PERIOD.get(model))
        else:
            ax.scatter(model_anom_orb[:, i], yarray - OFFSET.get(model),
                       marker = SYMBOL.get(model), 
                       color = colors[i], s=SIZE.get(model))
  

    for i, model in enumerate(MODELNAMES):
        model_anom_p2[:, i] = plio_model_p2[:, i] - veg_temp 
        if i == 0 :
            ax.scatter(model_anom_p2[:, i], yarray+0.2,
                       marker = 'o', edgecolors='black', linewidths=0.5, 
                       color = 'tab:red', s=15, label='KM5c (other models)')
        else:
            ax.scatter(model_anom_p2[:, i], yarray+0.2,
                       marker = 'o', edgecolors='black', linewidths=0.5,
                       color = 'tab:red', s=15)
      
    # add site labels
    if wc_ind == 'c':
        plt.title('a) Cold Month Temperature', loc='left')
    if wc_ind == 'w':
        for j in range(0, nmods):
            plt.text(-60, yarray[j], labels[j], ha='left')
        plt.title('b) Warm Month Temperature', loc='left')
    if wc_ind == 'm':
        for j in range(0, nmods):
            plt.text(11, yarray[j], labels[j], ha='left',fontsize=8)
        plt.title('Annual mean Temperature', loc='left')
        fig.subplots_adjust(bottom=0.25)


      
        
    


    plt.xlabel('Temperature difference from palaeodata (deg C)')
          
    if wc_ind == 'c':
        #handles, labs = fig.gca().get_legend_handles_labels()
        #order = [0, 3, 6, 1, 4, 7, 2, 5, 8, ]
        #fig.legend([handles[i] for i in order], 
        #           [labs[i] for i in order],
        #           loc = 'center left')
        fig.legend(loc='best')
  
    if wc_ind == 'm':
        plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.3), ncol=3)
  
    if wc_ind == 'm':
        plt.xlim(-18, 20)
    else:
        plt.xlim(-40, 30)
    
    plt.ylim(nmods + 1, 0)
   
   
  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

   
    # get land observations and cru temperature at land points
    
    (land_lats, land_lons, land_temp, 
     modern_temp, plio_unc, land_labels)= get_land_obs()
   


    # get different orbit from HadCM3
    all_orbits_plio = np.zeros((len(land_lons), len(EXPTNAMES)))
   
    for i, model in enumerate(EXPTNAMES):
        (ind, indWMT, 
         indCMT) = get_single_orbit(model, land_lats, land_lons)
        all_orbits_plio[:, i] = ind
       
        
        
    
  
    # get warm month and cold month temperatures from data
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, MAT_data_min, MAT_data_max,
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_km5c_lp()


     # get warm month and cold month temperatures from model
    allorb_plio_wmt = np.zeros((len(land_lons), len(EXPTNAMES)))
    allorb_plio_cmt = np.zeros((len(land_lons), len(EXPTNAMES)))
    allorb_plio_mat = np.zeros((len(land_lons), len(EXPTNAMES)))
   
    for i, model in enumerate(EXPTNAMES):
        (ind, indWMT, 
         indCMT,) = get_single_orbit(model, land_lats, land_lons)
        allorb_plio_wmt[:, i] = indWMT
        allorb_plio_cmt[:, i] = indCMT
        allorb_plio_mat[:, i] = ind
        print(model, indWMT[1], indCMT[1], ind[1])
    
  
    # get output from all models in the Pliomip2 ensemble
    all_models_plio_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_MAT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_MAT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_CMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_WMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_MAT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind_MAT, ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'EOI400')
        all_models_plio_WMT[:, i] = ind_WMT
        all_models_plio_CMT[:, i] = ind_CMT
        all_models_plio_MAT[:, i] = ind_MAT

        (ind_MAT, ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'E280')
        all_models_pi_WMT[:, i] = ind_WMT
        all_models_pi_CMT[:, i] = ind_CMT
        all_models_pi_MAT[:, i] = ind_MAT

        all_models_CMMT_anom[:, i] = (all_models_plio_CMT[:, i] -
                                      all_models_pi_CMT[:, i])

        all_models_WMMT_anom[:, i] = (all_models_plio_WMT[:, i] -
                                      all_models_pi_WMT[:, i])

        all_models_MAT_anom[:, i] = (all_models_plio_MAT[:, i] -
                                      all_models_pi_MAT[:, i])
  

    mmm_WMT = np.nanmean(all_models_plio_WMT, axis=1)
    mmm_CMT = np.nanmean(all_models_plio_CMT, axis=1)
    mmm_MAT = np.nanmean(all_models_plio_MAT, axis=1)
   
    mmm_WMT_pi = np.nanmean(all_models_pi_WMT, axis=1)
    mmm_CMT_pi = np.nanmean(all_models_pi_CMT, axis=1)
    mmm_MAT_pi = np.nanmean(all_models_pi_MAT, axis=1)


    # plot figure annual mean, warm month and cold month
    fig1 = plt.figure(figsize=[12.0, 12.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=2, ncols=2)

    ax1 = fig1.add_subplot(gs[:,0])
    print('j1')
    plot_figure(land_temp,  all_orbits_plio, 
                land_labels, ax1, fig1) 

    print('j2', np.shape(allorb_plio_wmt), np.shape(WMMT_data_min))
   
    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_data_min + WMMT_data_max) / 2.0, 
                  allorb_plio_wmt, all_models_plio_WMT, sites, 
                  ax2, fig1, 'w')

    print('j3')
    ax3 = fig1.add_subplot(gs[1,1])
    plot_seas_fig((CMMT_data_min + CMMT_data_max) / 2.0, 
                  allorb_plio_cmt, all_models_plio_CMT, sites, ax3, 
                  fig1, 'c')

    print('j4')
  
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_plus_mod_uncert.eps')
    plt.savefig(fileout)


     # plot figure  cold month and warm month
    fig1 = plt.figure(figsize=[12.0, 8.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=1, ncols=2)

   
  
    ax1 = fig1.add_subplot(gs[0,0])
    plot_seas_fig((CMMT_data_min + CMMT_data_max) / 2.0, 
                  allorb_plio_cmt, all_models_plio_CMT, sites, ax1, 
                  fig1, 'c')

    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_data_min + WMMT_data_max) / 2.0, 
                  allorb_plio_wmt, all_models_plio_WMT, sites, 
                  ax2, fig1, 'w')

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_plus_mod_uncert_WMMT_CMMT.eps')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orb_plus_mod_uncert_WMMT_CMMT.png')
    plt.savefig(fileout)


   # alternative plot.  Where the individual model is the
   # plio_model - pi_model + mmm_pi
    fig1 = plt.figure(figsize=[12.0, 8.0], constrained_layout=True)
    gs = gridspec.GridSpec(nrows=1, ncols=2)

  
    ax1 = fig1.add_subplot(gs[0,0])
    plot_seas_fig((CMMT_data_min + CMMT_data_max) / 2.0, 
                  allorb_plio_cmt,  
                  np.add(all_models_CMMT_anom,
                       np.transpose(np.tile(mmm_CMT_pi, (len(MODELNAMES),1)))),
                  sites, ax1, 
                  fig1, 'c')

    ax2 = fig1.add_subplot(gs[0,1])
    plot_seas_fig((WMMT_data_min + WMMT_data_max) / 2.0, 
                  allorb_plio_wmt, 
                  np.add(all_models_WMMT_anom,
                       np.transpose(np.tile(mmm_WMT_pi, (len(MODELNAMES),1)))),
                  sites,ax2, fig1, 'w')

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_plus_mod_uncert_WMMT_CMMT_alternative.eps')
    plt.savefig(fileout)


   # alternative plot - annual mean.  Where the individual model is the
   # plio_model - pi_model + mmm_pi
    fig1 = plt.figure(figsize=[12.0, 8.0])
    ax=fig1.add_subplot(1,1,1)
  
  
    plot_seas_fig((MAT_data_min + MAT_data_max) / 2.0, 
                  allorb_plio_mat,  
                  np.add(all_models_MAT_anom,
                       np.transpose(np.tile(mmm_MAT_pi, (len(MODELNAMES),1)))),
                  sites, ax, 
                  fig1,'m')

   
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/orbital_plus_mod_uncert_annmean_alternative.eps')
    plt.savefig(fileout)
  

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'


EXPTNAMES = ['xozzb', 'xozzd', 'xozzf', 'xozzc', 'xozze']
#EXPTNAMES = ['xibol', 'xozzb','xozzd', 'xozzf', 'xozzc', 'xozze']

PERIOD = {'xiboi' : 'pi (old)',
          'xibol' : 'KM5c',
          'xozzb' : 'KM5c',
          'xozzc' : 'K1 (3.0560Ma)',
          'xozzd' : 'G17 (2.950Ma)',
          'xozze' : 'KM3 (3.155Ma)',
          'xozzf' : '3.053Ma'}


MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 
#              'GISS2.1G', 
              'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
#             ,  'MRI2.3'
              ]
#MODELNAMES = ['HadCM3']

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/basic_dmc_plot.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array


def get_single_model(model, latreq, lonreq):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/EOI400.NearSurfaceTemperature.allmean.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_array = np.zeros(nsites)
   
    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_array[i] = plio_cube.data[lat_ix, lon_ix]
   
    return plio_array

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []
    temp_uncert = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temp_uncert.append(dfs.iloc[rl,10])
            temps.append(temp)

    print(temp_uncert)
    for i, temp in enumerate(temp_uncert):
        if i > 0:
            temp2 = temp[2:]
        else:
            temp2=0.0
        print(temp, temp2)
        temp_uncert[i]=np.float(temp2)
     
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
        label = ''.join([c for c in site if c.isupper()])
        if lats[i] < 0:
            latstr = np.str(np.int(np.round(lats[i] * -1.0, 0))) + deg +  'S'
        else:
            latstr = np.str(np.int(np.around(lats[i], 0))) + deg + 'N'
        if lons[i] < 0:
            lonstr = np.str(np.int(np.round(lons[i] * -1.0, 0))) + deg +  'W'
        else:
            lonstr = np.str(np.int(np.around(lons[i], 0))) + deg + 'E'
        
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return lats, lons, temps, temp_modern, temp_uncert, labels

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def plot_figure(plio_temp_obs, plio_temp_unc, pi_temp_obs, plio_mmm, pi_mmm,
                crutemp, plio_ind_models, labels):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    fig1 = plt.figure(figsize=[6.0, 8.0])
    ax1 = plt.axes(frameon=False)
    #ax1.get_xaxis().tick_bottom()
    ax1.axes.get_yaxis().set_visible(False)

    yarray = np.arange(1, len(plio_temp_obs) + 1, 1)
 
   # try plotting model data anomaly
    plt.vlines(x=0, ymin=-0, ymax=9, linewidth=0.5)
    plt.hlines(y=8.95, xmin=-25, xmax=7.5, linewidth=0.5)
    plt.scatter(plio_mmm - plio_temp_obs, yarray-0.2, color='black', s=50)
    plt.scatter(plio_mmm - plio_temp_obs, yarray-0.2, color='red', 
                s=25, label='mPWP MMM')
    plt.scatter(pi_mmm - pi_temp_obs, yarray, color='blue', label='PI MMM')

   
    # plot individual models for pliocene
    model_anom = np.zeros(np.shape(plio_ind_models))
    for i in range(0, len(MODELNAMES)):
        model_anom[:, i] = plio_ind_models[:, i] - plio_temp_obs  
        if i == 0:
            plt.scatter(model_anom[:, i], yarray-0.2, color='red', marker = 'x',
                    s=10, label='mPWP models')
        else:
            plt.scatter(model_anom[:, i], yarray-0.2, color='red', marker = 'x',
                    s=10)
    
    # add uncertainty on modern due to difference with CRU and uncert on plio data
    cruanom = crutemp - pi_temp_obs   
    for j in range(0,len(pi_temp_obs)):
    #    valmin = np.min([0, cruanom[j]])
    #    valmax = np.max([0, cruanom[j]]) 
        if j == 0:
            #plt.hlines(y = j+1, xmin = valmin, xmax=valmax, linestyle='dotted',
            #           color='blue', label='PI data \n uncertainty')
            plt.hlines(y = j+0.9, xmin = plio_temp_unc[j] * -1.0, 
                       xmax=plio_temp_unc[j], color='red', linestyle='dotted',
                       label='mPWP data \n uncertainty') 
        else:
            #plt.hlines(y = j+1, xmin = valmin, xmax=valmax, linestyle='dotted',
            #           color='blue')
            plt.hlines(y = j+0.9, xmin = plio_temp_unc[j] * -1.0, 
                   xmax=plio_temp_unc[j], color='red', linestyle='dotted') 
       

    # plot MMM again so it is on the top of the figure
    plt.scatter(plio_mmm - plio_temp_obs, yarray-0.2, color='black', s=50)
    plt.scatter(plio_mmm - plio_temp_obs, yarray-0.2, color='red', 
                s=25)
    plt.scatter(pi_mmm - pi_temp_obs, yarray, color='blue')
    # plot cruanom
    plt.scatter(cruanom,yarray+0.2,color='blue',marker='^',label='CRU 1901-1930')

 
    # add site labels
    plt.text(-5.0, yarray[7], labels[7], ha='right')
    for j in range(0, 7):
        plt.text(np.min(model_anom[j, :]) - 1.0,
                 yarray[j], labels[j], ha='right')

    plt.xlabel('Difference between modelled/reanalysis \n and observed/reconstructed temperatures (deg C)')
    plt.legend(loc = 'lower left')
  
    plt.xlim(-25, 7.5)
    plt.ylim(9, 0)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/basic_dmc_plot.eps')
    plt.savefig(fileout)
  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

   
    # get land observations and cru temperature at land points
    
    (land_lats, land_lons, land_temp, 
     modern_temp, plio_unc, land_labels)= get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    
    # get model data
    MMM_plio, MMM_pi = get_MMM_data(land_lats, land_lons)

    # get ind models data
    all_models_plio = np.zeros((len(land_lons), len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        ind = get_single_model(model, land_lats, land_lons)
        all_models_plio[:, i] = ind

    # plot data
    plot_figure(land_temp, plio_unc, modern_temp, MMM_plio,
                MMM_pi, cru_land_temp, all_models_plio, land_labels) 


##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

MODELNAMES = [
              'CESM2', 'HadGEM3',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'MRI2.3', 'NorESM1-F'
              ]

NSAT_MMM_FILE = (FILESTART + 
                 'regridded100/NearSurfaceTemperature_multimodelmean.nc')

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/extract_CRU.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# 
#  This program will extract the cru temperatures for all of the sites  
#1
#
#
#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

 
def get_land_obs(SITE_REQUIRED):
    """
    reads in the spredsheet from ulrich and returns 
    the field, latitude and longitude from the required site. 
    """

    if SITE_REQUIRED == 'Meighen Island':
        fielddata = 10.0
        lat = 80.0
        lon = -99.0

    if SITE_REQUIRED == 'Beaver Pond':
        fielddata = 1.0
        lat = 79.0
        lon = -82.0
   
    if SITE_REQUIRED == 'Flyes Leaf Bed':
        fielddata = 1.0
        lat = 79.0
        lon = -83.0

    if SITE_REQUIRED == 'Lake Elgygytgyn':
        fielddata = 16.0
        lat = 67.0
        lon = -172.0
    
    if SITE_REQUIRED == 'Alpes-Maritimes N':
        lat = 44.0
        lon = -7.19
        fielddata = 1.0
   
    
    if SITE_REQUIRED == 'Alpes-Maritimes S':
        lat = 43.5
        lon = -7.0
        fielddata = 1.0
   
    dfs = pd.read_excel(LAND_DATAFILE)
    found = 'n'
    
    colreq = 9
    
    
    for index, row in dfs.iterrows():
        if row[0] == SITE_REQUIRED:
            found = 'y'
            fielddata = row[colreq]
            lat = row[2]
            lon = row[3]
            pass
            
    
    if found == 'n' and SITE_REQUIRED != 'Meighen Island':
        print('couldnot find site in Ulrichs file')
   #     sys.exit(0)
        
    
   
    return fielddata, lat, lon





  
def main():
    """
    calling structure
    a) put cru data into an iris cube
    b) get lat and long from from Ulrichs spreadsheet
    c) get cru temperature at nearest lat and long
    c) printout
    """
    
    cru_cube = iris.load_cube(FILENAME, 'near-surface temperature')

 
    for site in SITES_REQUIRED:
        #    # get data from Ulrichs spreadsheet
    
        proxy_temperature, proxy_lat, proxy_lon = get_land_obs(site)
    
       
        lat_ix = (np.abs(cru_cube.coord('latitude').points - proxy_lat)).argmin()
        lon_ix = (np.abs(cru_cube.coord('longitude').points - proxy_lon)).argmin()
    
        crutemp = cru_cube.data[lat_ix, lon_ix]
   
        print(site, proxy_lat, proxy_lon, 
              cru_cube.coord('latitude').points[lat_ix],
              cru_cube.coord('longitude').points[lon_ix], crutemp)
 #########################################################
# main program


LINUX_WIN = 'l'
EXPTNAME = 'EOI400'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/' 
    FILENAME = (FILESTART + 'regridded/CRUTEMP/' + 
                'E280.NearSurfaceTemperature.allmean.nc')
    LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')
    PLOTTYPE = '_' + EXPTNAME + '.eps'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    LAND_DATAFILE = (FILESTART + '/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')
    PLOTTYPE = '_' + EXPTNAME + '.png'
 


SITES_REQUIRED = ['Lake Baikal', 'Lake Elgygytgyn', 'Meighen Island',
                  'Beaver Pond',
                  'Flyes Leaf Bed', 'Lost Chicken Mine',
                  'James Bay Lowland',
                  'Pula Maar',
                  'Alpes-Maritimes',
                  'Tarragona',
                  'Rio Maior',
                  'Yallalie, Perth',
                  'Alpes-Maritimes N', 'Alpes-Maritimes S']

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/extract_data_at_site.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on October 2021
# 
#  This program will extract the temperature cycle at a given site
#1
#
#
#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

 
def get_land_obs(SITE_REQUIRED):
    """
    reads in the spredsheet from ulrich and returns 
    the field, latitude and longitude from the required site. 
    """

    if SITE_REQUIRED == 'Meighen Island':
        fielddata = 10.0
        lat = 80.0
        lon = -99.0

    if SITE_REQUIRED == 'Beaver Pond':
        fielddata = 1.0
        lat = 79.0
        lon = -82.0
   
    if SITE_REQUIRED == 'Flyes Leaf Bed':
        fielddata = 1.0
        lat = 79.0
        lon = -83.0

    if SITE_REQUIRED == 'Lake Elgygytgyn':
        fielddata = 16.0
        lat = 67.0
        lon = -172.0
    
    if SITE_REQUIRED == 'Alpes-Maritimes N':
        lat = 44.0
        lon = -7.19
        fielddata = 1.0
   
    
    if SITE_REQUIRED == 'Alpes-Maritimes S':
        lat = 43.5
        lon = -7.0
        fielddata = 1.0
   
   
    return  lat, lon





  
def main():
    """
    get temperatures from mmm
    get temperatures from individual models
    
    write out annual temperatures
    write out monthly temperatures
    write out seasonal temperatures
    """
    
    proxy_lat, proxy_lon = get_land_obs(SITE_REQUIRED)
    if proxy_lon < 0: proxy_lon = proxy_lon + 360.
    
    field = 'NearSurfaceTemperaturemean_plio'
    mmm_cube = iris.load_cube(NSAT_MMM_FILE,field)
  
    lat_ix = (np.abs(mmm_cube.coord('latitude').points - proxy_lat)).argmin()
    lon_ix = (np.abs(mmm_cube.coord('longitude').points - proxy_lon)).argmin()
    
    mmmtemp = mmm_cube.data[:, lat_ix, lon_ix]
    mmmtemp_avg = np.mean(mmmtemp)
    mmmtemp_djf = (mmmtemp[0] + mmmtemp[1] + mmmtemp[11]) / 3.0
    mmmtemp_mam = (mmmtemp[2] + mmmtemp[3] + mmmtemp[4]) / 3.0
    mmmtemp_jja = (mmmtemp[5] + mmmtemp[6] + mmmtemp[7]) / 3.0
    mmmtemp_son = (mmmtemp[8] + mmmtemp[9] + mmmtemp[10]) / 3.0
   
    print(proxy_lat, proxy_lon, 
          mmm_cube.coord('latitude').points[lat_ix],
          mmm_cube.coord('longitude').points[lon_ix])

    print('mean annual temp = ',mmmtemp_avg)
    print('mean annual djf = ',mmmtemp_djf)
    print('mean annual mam = ',mmmtemp_mam)
    print('mean annual jja = ',mmmtemp_jja)
    print('mean annual son = ',mmmtemp_son)

    allmodel_summer = []
    allmodel_mean = []
    for model in MODELNAMES:
        filename = (FILESTART + 'regridded100/' + model +  
                    '/EOI400.NearSurfaceTemperature.mean_month.nc')
        model_cube = iris.load_cube(filename)
  
        lat_ix = (np.abs(model_cube.coord('latitude').points - proxy_lat)).argmin()
        lon_ix = (np.abs(model_cube.coord('longitude').points - proxy_lon)).argmin()
        modeltemp = model_cube.data[:, lat_ix, lon_ix]
  
    
        modeltemp_jja = (modeltemp[5] + modeltemp[6] + modeltemp[7]) / 3.0
        modeltemp_mean = np.mean(modeltemp)
        allmodel_summer.append(modeltemp_jja)
        allmodel_mean.append(modeltemp_mean)
      #  print('jja temp for ',model,' = ',modeltemp_jja)
        print('ann temp for ',model,' = ',modeltemp_mean)
  
   # print('these models median jja',np.median(np.asarray(allmodel_summer)))
   # print('these models 20th percentile jja',np.percentile(np.asarray(allmodel_summer),20))
   # print('these models 80th percentile jja',np.percentile(np.asarray(allmodel_summer),90))
  
    print('these models median ann',np.median(np.asarray(allmodel_mean)))
    print('these models 20th percentile ann',np.percentile(np.asarray(allmodel_mean),20))
    print('these models 80th percentile ann',np.percentile(np.asarray(allmodel_mean),80))

 #########################################################
# main program


LINUX_WIN = 'l'
EXPTNAME = 'EOI400'

LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'

MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 
               'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
            ,  'MRI2.3'
              ]


NSAT_MMM_FILE = (FILESTART + 'regridded100/' + 
                 'NearSurfaceTemperature_multimodelmean_month.nc')


SITE_REQUIRED= 'Beaver Pond'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/Land_DMC_growing_season_analysis_2.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# 
#  This program produces additional of figures to assess
#  the DMC in various ways.  
#
#  The ultimate aim is to do a better DMC by comparing the model
#  to the times when the proxy represents.  
#
#
#
#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

class GetMeteodata:
    """
    This class is everything to do with 
    getting the meteorological data.

    """
    def __init__(self):
        
        # sel.nearsites = [sitename, sitelat, sitelon]
        if SITE_REQUIRED == 'Lake Baikal':
            self.nearsites = [['Krasnojarsk', 56.0, 92.3],
                              ['Nizneudinsk', 54.9, 99.0],
                              ['Zigalovo', 54.8, 105.2],
                              ['Nizneangarsk', 55.8, 109.6],
                              ['Kalakan', 55.1, 116.8]
                              ]
            self.nearest = 'Nizneangarsk'
            
        if SITE_REQUIRED == 'Lake Baikal 19501970':
            self.nearsites = [['Zigalovo_1950_1970', 54.8, 105.2],
                              ['Nizneangarsk_1950_1970', 55.8, 109.6],
                              ['Kalakan_1950_1970', 55.1, 116.8]
                              ]
            self.nearest = 'Nizneangarsk_1950_1970'
            
    def get_meteoanncycle(self):
        
        anncycle = {
            "Krasnojarsk" : [-16.7575, -14.9292, -7.16471, 1.79832,    
                             9.57983, 16.3615, 19.0419, 15.8119,
                             9.34746, 1.61453, -8.17542, -15.3414], 
            "Nizneudinsk": [-21.3235, -19.0240, -9.20600, 0.672549,
                            7.95306, 14.5939, 17.0184, 14.1551,
                            7.68367, -0.253061, -10.9122, -19.0490],  
            "Nizneangarsk": [-22.2907, -21.3667,  -12.8852, -3.03148,
                             4.41731, 11.7189, 15.5906, 14.5269, 
                             7.85741, -1.11481, -11.3630, -17.8389],
            "Kalakan" : [-34.8708, -27.9969, -16.5879, -3.28750, 
                         6.24923, 13.7091, 16.6369, 13.5492,  
                         5.68438,  -5.90909, -22.3375, -32.9889],
            "Zigalovo" : [-27.9258, -24.3152, -12.6881, -0.409091,  
                          7.56562, 14.8478,  17.5433, 14.2940, 
                          6.71912, -2.14118, -15.0515, -24.7530],
            "Kalakan_1950_1970" : [-35.7381, -29.6000,   -17.4048,
                                   -4.49500,   5.98571,   13.4190, 
                                   16.4429,    13.5048,  5.23333,
                                    -6.30952,   -23.4200, -33.8190],     
            "Nizneangarsk_1950_1970" :[-22.3842,  -21.9474,  -13.5684,
                                       -3.92632,   3.93684,  10.6421,
                                        14.9789, 14.2000,  7.21000,
                                        -0.935000,  -12.5700,  -18.4150],
            "Zigalovo_1950_1970" : [-28.3476,  -25.6952,  -14.0571, 
                                    -1.68571,  7.35714,  14.5143,
                                    17.5952,  14.4571,  6.22857,
                                     -2.30476, -16.6238,  -25.5000]
                }
        
        monthnames = ['ja','fb','mr','ar','my','jn',
                       'jl','ag','sp','ot','nv','dc']
        allseascyc = []
        allseascyc_anom = []
         
        fig = plt.figure(figsize=(11.0, 8.5))
        ax=plt.subplot(2,1,1)
         
        for siteinfo in self.nearsites:
            sitename = siteinfo[0]
            sitelon = np.str(np.round(siteinfo[2])) + ' deg E'
            site_anncycle = anncycle.get(sitename)
            print(sitename, np.mean(site_anncycle))
            allseascyc.append(site_anncycle)
            if sitename == self.nearest:
                ann_cycle_nearest = site_anncycle
            ax.plot(monthnames, site_anncycle,label=sitename + ' ' + sitelon)
            if (sitename[0:7]) == 'Zigalov':
                ann_cycle_zigalov = np.array(site_anncycle)
                lon_zigalov = siteinfo[2]
            if (sitename[0:7]) == 'Kalakan':
                ann_cycle_kalakan = np.array(site_anncycle)
                lon_kalakan = siteinfo[2]
                anom = ((ann_cycle_zigalov - ann_cycle_kalakan) 
                                         * (109.0 - lon_kalakan)
                                         / (lon_zigalov - lon_kalakan))
                est_ann_cycle_Baikal = ann_cycle_kalakan + anom
                print('est cycle',est_ann_cycle_Baikal)
                print('est mean baikal',np.mean(est_ann_cycle_Baikal))
               
        print('anomalous annual cycle', est_ann_cycle_Baikal - ann_cycle_nearest)   
    
      
       
        box = ax.get_position()
        ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
        ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        plt.title('annual cycle for sites near ' + SITE_REQUIRED)
        
        ax=plt.subplot(2,1,2)
        for siteinfo in self.nearsites:
            sitename = siteinfo[0]
            sitelon = np.str(np.round(siteinfo[2])) + ' deg E'

            anncyc_anom = np.array(anncycle.get(sitename)) - np.array(ann_cycle_nearest)
            allseascyc_anom.append(anncyc_anom)
            if sitename != self.nearest:
                ax.plot(monthnames, anncyc_anom, label= sitename + ' ' + sitelon)
              
        ax.axhline(y=0)
        box = ax.get_position()
        ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
        ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        plt.title('annual cycle anomaly from ' + self.nearest)
        
        
        plt.savefig(FILESTART + '/Growing_seas/Meteorological_data/seas_cyc_near_' + SITE_REQUIRED + PLOTTYPE)
        plt.close()
        
        return (self.nearsites, allseascyc, allseascyc_anom,
                est_ann_cycle_Baikal)
        





def get_model_data(period, modelname, lat, lon):
    """
    gets the model data for each month of the year for the site.
    """
    
    lonreq=lon
    if lonreq < 0.:
        lonreq=lonreq + 360.
   
    filename = (FILESTART + '/regridded/' + modelname + '/'+ period + '.'
                + FIELD + '.mean_month.nc')
    
    
    fieldname = FIELD
   
    if modelname == 'GISS2.1G' or modelname == 'IPSLCM6A':
        if fieldname == 'NearSurfaceTemperature':
            fieldname = 'air_temperature'
        if fieldname == 'TotalPrecipitation':
            fieldname = 'precipitation_flux'
    
  
    field_cube = iris.load_cube(filename,
                              fieldname)
    
    
    lat_ix = (np.abs(field_cube.coord('latitude').points - lat)).argmin()
    lon_ix = (np.abs(field_cube.coord('longitude').points - lonreq)).argmin()
    
    seas_field = field_cube.data[:, lat_ix, lon_ix]
   
   
    return seas_field.data



def plot_seascyc_vs_data(proxyT, models_Tseas, siteinfo, period):
    """
    plots the temperature from the data vs the modelled seasonal cycle.

    """
    
    ylab = {'NearSurfaceTemperature': 'degC',
            'TotalPrecipitation': 'mm/day'}
    
    ax = plt.subplot(111)
    
    print(siteinfo[0])
    for i, mod_data in enumerate(models_Tseas):
        if i > 6:
            ax.plot(mod_data, linestyle='dashed', label=MODELNAMES[i])
        else:
            ax.plot(mod_data, label=MODELNAMES[i])
        MAT = np.mean(mod_data)
        ax.plot([-1.0,0], [MAT, MAT], color='blue')
        print(MODELNAMES[i],
              np.around(np.sqrt(((mod_data - proxyT) ** 2).mean())))
        
        
       # print('mean=',np.around(np.mean(mod_data),2), 
       #       ' median=', np.around(np.median(mod_data),2),
       #       ' avg wam/cold=', np.around((np.max(mod_data) + np.min(mod_data)) / 2.0, 2))
        
    
    ax.plot(proxyT, label=siteinfo[0], color='black', linewidth=2)
    title = (siteinfo[0] + ' lat=' + np.str(np.around(siteinfo[1])), 
        ' lon='+np.str(np.around(siteinfo[2])))
    plt.title(title)
    plt.xlabel('month')
    plt.ylabel(ylab.get(FIELD))
    
    
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    plt.savefig(FILESTART + '/Growing_seas/Meteorological_data/seas_cyc_' + siteinfo[0] +  '_' + period + PLOTTYPE)
    plt.close()


def plot_model_near_site(lons, all_month_pi, all_month_plio, monthname):
    """
    plots the modelled temperature at the site and for some nearby
    gridboxes
    lons = list of longitudes
    all_seascyc = seasonal cycle at (list of longs (list of models))
    """
    
    fig = plt.figure(figsize=(11.0, 11.0))
    fig.suptitle(monthname + 'temperature gradient: pi (blue), plio (red)', fontsize=20)
    fig.text(0, 0.5, 'temperature deg C', rotation=90, fontsize=20, verticalalignment = 'center')
    fig.text(0.6, 0.0, 'longitude deg E', fontsize=20, horizontalalignment = 'center')
    for i, model in enumerate(MODELNAMES):
        ax = plt.subplot(4,4,i+1)
        plt.title(model)
        monthtemp = all_month_pi[:, i] - all_month_pi[0, i]
        monthtemp_plio = all_month_plio[:, i] - all_month_plio[0, i]
        ax.plot(lons, monthtemp, color='blue')
        ax.plot(lons, monthtemp_plio, color='red')
    
       
    plt.subplots_adjust(left=0.05, bottom=0.05, right=1.00, top=0.9,
                        wspace=0.3, hspace=0.3)
       
    plt.savefig(FILESTART + '/Growing_seas/' + monthname + '_near_' + SITE_REQUIRED + PLOTTYPE, bbox_inches='tight')
    plt.close()
        




  
def main():
    """
    calling structure
    a) get modern meteorological data seasonal cycle
    b) get modern data at the location of interest
    """

#    #########################################
#    # get data for modern sites.  
    
    siteinfo = GetMeteodata() # get data for t1 timeslice
    (sitenames, seascyc, seascyc_anom,
     est_ann_cycle_Baikal_nolake)  = siteinfo.get_meteoanncycle()
       
    sitenames.append(['Estimated Lake Baikal (without lake)', 
                      55.0, 108.0])
    seascyc.append(est_ann_cycle_Baikal_nolake)
    
    #################################################
    # get PI model data for each month of the year.
    for siteno, sitedetails in enumerate(sitenames):
        sitelat = sitedetails[1]
        sitelon = sitedetails[2]
        allmodel_TseascycPI = []
   
        for i, model in enumerate(MODELNAMES):
            T_seascyc = get_model_data('E280', model, sitelat, sitelon)
            allmodel_TseascycPI.append(T_seascyc)
      
        plot_seascyc_vs_data(seascyc[siteno], allmodel_TseascycPI,
                          sitedetails, 'E280')
        
    #############################################
    # look and see if the modelled seasonal cycle
    # at the location is different from nearby gridboxes
        
    lons_list = []
    PI_Jan = []
    Plio_Jan = []
    PI_Jul = []
    Plio_Jul = []
   
    for lon in range(np.int(np.floor(sitelon)) - 5, 
                     np.int(np.ceil(sitelon)) + 5):
        lons_list.append(lon)
        allmodel_Jan = []
        allmodel_Jan_plio = []
        allmodel_Jul = []
        allmodel_Jul_plio = []
        for model in MODELNAMES:
            T_seascyc = get_model_data('E280', model, sitelat, lon)
            T_seascyc_plio = get_model_data('EOI400', model, sitelat, lon)
            allmodel_Jan.append(T_seascyc[0])
            allmodel_Jan_plio.append(T_seascyc_plio[0])
            allmodel_Jul.append(T_seascyc[6])
            allmodel_Jul_plio.append(T_seascyc_plio[6])
            
            
        PI_Jan.append(allmodel_Jan)
        Plio_Jan.append(allmodel_Jan_plio)
        PI_Jul.append(allmodel_Jul)
        Plio_Jul.append(allmodel_Jul_plio)
        
    plot_model_near_site(lons_list, np.asarray(PI_Jan),
                         np.asarray(Plio_Jan),
                         'January')
    plot_model_near_site(lons_list, np.asarray(PI_Jul),
                         np.asarray(Plio_Jul),
                         'July')
            
  
    
 
       
##########################################################
# main program


LINUX_WIN = 'w'


if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/'   
    LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')
    PLOTTYPE = '.eps'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    LAND_DATAFILE = (FILESTART + '/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')
    PLOTTYPE = '.png'
 

MODELNAMES = ['CESM2', 
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]

#MODELNAMES = ['CESM2','IPSLCM6A']

ABSOLUTE_ANOMALY_IND = 'absolute' # do we want to compare the absolute value or the anomaly value.
FIELD = 'NearSurfaceTemperature'
#FIELD = 'TotalPrecipitation'

SITE_REQUIRED = 'Lake Baikal'
#SITE_REQUIRED = 'Lake Baikal 19501970'
#SITE_REQUIRED = 'Lake Elgygytgyn'
#SITE_REQUIRED = 'Meighen Island'
#SITE_REQUIRED = 'Lost Chicken Mine'
#SITE_REQUIRED = 'James Bay Lowland'
#SITE_REQUIRED = 'Pula Maar'
#SITE_REQUIRED = 'Alpes-Maritimes'
#SITE_REQUIRED = 'Tarragona'
#SITE_REQUIRED = 'Rio Maior'
#SITE_REQUIRED = 'Yallalie, Perth'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/Land_DMC_growing_season.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# 
#  This program will look at the temperature data provided by Ulrich.
#  It will assess whether the temperature agrees better with a particular
#  time of year. 
#
#  The ultimate aim is to do a better DMC by comparing the model
#  to the times when the proxy represents.  
#
#
#
#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns 
    the field, latitude and longitude from the required site. 
    """

    if SITE_REQUIRED == 'Meighen Island':
        fielddata = 3.6
        lat = 77.5
        lon = -99.0

    if SITE_REQUIRED == 'Beaver Pond':
        fielddata = 1.1
        lat = 79.0
        lon = -82.0
   
    if SITE_REQUIRED == 'Flyes Leaf Bed':
        fielddata = 1.0
        lat = 79.0
        lon = -83.0

    if SITE_REQUIRED == 'Lake Elgygytgyn':
        fielddata = 16.0
        lat = 67.0
        lon = -172.0
    dfs = pd.read_excel(LAND_DATAFILE)
    found = 'n'
    
    if FIELD == 'NearSurfaceTemperature':
        colreq = 9
    if FIELD == 'TotalPrecipitation':
        colreq = 11
   
    
    for index, row in dfs.iterrows():
        if row[0] == SITE_REQUIRED:
            found = 'y'
            fielddata = row[colreq]
            lat = row[2]
            lon = row[3]
            pass
            
    
    if found == 'n' and SITE_REQUIRED != 'Meighen Island':
        print('couldnot find site in Ulrichs file')
   #     sys.exit(0)
        
    if FIELD == 'TotalPrecipitation':
       fielddata = fielddata / 365.
    
   
    return fielddata, lat, lon




def get_model_data(period, modelname, lat, lon, field):
    """
    gets the model data for each month of the year for the site.
    """
    
    fielduse = {'T' : 'NearSurfaceTemperature',
                'P' : 'TotalPrecipitation'}
    lonreq=lon
    if lonreq < 0.:
        lonreq=lonreq + 360.
    filename = (FILESTART + '/regridded100/' + modelname + '/'+ period + '.'
                + fielduse.get(field) + '.mean_month.nc')
    
    fieldname = fielduse.get(field)
   
    if modelname == 'GISS2.1G' or modelname == 'IPSLCM6A':
        if fieldname == 'NearSurfaceTemperature':
            fieldname = 'air_temperature'
        if fieldname == 'TotalPrecipitation':
            fieldname = 'precipitation_flux'
    if modelname == 'HadGEM3_TEMPORARY':
        if fieldname == 'NearSurfaceTemperature':
            fieldname = 'temp'
    
  
    field_cube = iris.load_cube(filename,
                              fieldname)
    
    
    lat_ix = (np.abs(field_cube.coord('latitude').points - lat)).argmin()
    lon_ix = (np.abs(field_cube.coord('longitude').points - lonreq)).argmin()
    
    seas_field = field_cube.data[:, lat_ix, lon_ix]
   
   
    return seas_field.data



def plot_seascyc_vs_proxy(proxyT, models_Tseas, lat, lon, period):
    """
    plots the temperature from the data vs the modelled seasonal cycle.

    """
    
    ylab = {'NearSurfaceTemperature': 'degC',
            'TotalPrecipitation': 'mm/day'}
    
    ax = plt.subplot(111)
    for i, mod_data in enumerate(models_Tseas):
        if i > 6:
            ax.plot(mod_data, linestyle='dashed', label=MODELNAMES[i])
        else:
            ax.plot(mod_data, label=MODELNAMES[i])
        MAT = np.mean(mod_data)
        ax.plot([-1.0,0], [MAT, MAT], color='blue')
        
        
        
    if proxyT > -100:
        ax.axhline(y=proxyT)
    title = SITE_REQUIRED + ' lat=' + np.str(np.around(lat)), ' lon='+np.str(np.around(lon))
    plt.title(title)
    plt.xlabel('month')
    plt.ylabel(ylab.get(FIELD))
    
    
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   

    if period == 'anom':
        periodwrite = EXPTNAME + '-' + PI_EXPT
    else:
        periodwrite = period
        
    plt.savefig(FILEOUTSTART + 'seas_cyc_' + FIELD + '_' + periodwrite + '_' + SITE_REQUIRED.replace(' ','_') + PLOTTYPE)
    plt.close()
   
    

def get_lsm_for_cube(cube):
    """
    get's the PI land sea mask and checks it is on the same grid as cube

    """
    LSMfile = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
    lsm_pi_cube = iris.load_cube(LSMfile)
    if LINUX_WIN == 'l':
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    else:
        cubegrid = iris.load_cube('C:\\Users\\julia\\OneDrive\\WORK\\PROGRAMS\\one_lev_one_deg.nc')

    regrid_lsm_cube = lsm_pi_cube.regrid(cubegrid, iris.analysis.Linear())
    
    for i, lsm in enumerate(regrid_lsm_cube.coord('longitude').points):
        if lsm != cube.coord('longitude').points[i]:
            print('lsm lon error')
            print(lsm, cube.coord('longitude').points[i])
            sys.exit(0)
       
    for j, lsm in enumerate(regrid_lsm_cube.coord('latitude').points):
         if lsm != cube.coord('latitude').points[j]:
            print('lsm lat error')
            print(lsm, cube.coord('latitude').points[i])
            sys.exit(0)
            
   
        
    return regrid_lsm_cube
        
#####################################################        
  
def locate_PI_with_plioJulyT(model, temp_req, lonreq, proxy_lat):
    """
    this subroutine will find locations in the preindustrial MAT file 
    which have July temperature of temp_req
    The site must be land and within 30deg of lonreq
    temperature

    """
    
    def get_near_tempreq(cube, errormargin, mask, lsm):
        
        indexes= []
        cube_loc = cube.copy(data = np.zeros(np.shape(cube.data)))
        for index, temp in np.ndenumerate(cube.data):
            if (temp_req - errormargin < temp < temp_req + errormargin):
                if mask.data[index] == 1.0:
                    cube_loc.data[index] = 1.0
                    indexes.append(index)
                   
        
    
        return indexes
    
    def get_mean_min(): 
        """
        gets the july temperature and the january temperature
        from the pi cube for each index
        """
        jan_temps = []
        mean_rain = []
        lons = []
        lats = []
        
        for ix in indexes:
            jan = cube.data[0, ix[0], ix[1]]
            mean = cubeprecip_annmean.data[ix[0], ix[1]]
            jan_temps.append(jan)
            mean_rain.append(mean)
            lons.append(cubeprecip_annmean.coord('longitude').points[ix[1]])
            lats.append(cubeprecip_annmean.coord('latitude').points[ix[0]])
            
        
        return jan_temps, mean_rain, lons, lats
    
    # get data and LSM 
    if model == 'MMM':
        filename = (FILESTART + 'regridded100/' + 
                    'NearSurfaceTemperature_multimodelmean_month.nc')
        cube = iris.load_cube(filename, 'NearSurfaceTemperaturemean_pi')
        filename = (FILESTART + 'regridded100/' + 
                    'TotalPrecipitation_multimodelmean_month.nc')
        cubeprecip = iris.load_cube(filename, 'TotalPrecipitationmean_pi')
    else:
        filename = (FILESTART + 'regridded100/' + model + 
                    '/E280.NearSurfaceTemperature.mean_month.nc')
        if model == 'GISS2.1G' or model == 'IPSLCM6A':
            fieldname = 'air_temperature'
        else:
            fieldname = 'NearSurfaceTemperature'    
        cube = iris.load_cube(filename, fieldname)
        filename = (FILESTART + 'regridded100/' + model + 
                    '/E280.TotalPrecipitation.mean_month.nc')
        if model == 'GISS2.1G' or model == 'IPSLCM6A':
            fieldname = 'precipitation_flux'
        else:
            fieldname = 'TotalPrecipitation'    
        cubeprecip = iris.load_cube(filename, fieldname)
        
 
    if model == 'HadGEM3' or model == 'MMM':
        cube_julymean =  cube.extract(iris.Constraint(time=7))
    else:
        cube_julymean =  cube.extract(iris.Constraint(month=7))
    cubeprecip_annmean =  cubeprecip.collapsed(['time'], iris.analysis.MEAN)
    
    lsm = get_lsm_for_cube(cube)
    mask = get_lsm_for_cube(cube)
    for i, lon in enumerate(lsm.coord('longitude').points):
        if lon < lonreq - 10.:
            mask.data[:,i] = 0
        if lon > lonreq + 10.:
            mask.data[:,i] = 0
    for j, lat in enumerate(lsm.coord('latitude').points):
        if lat < proxy_lat - 50.:
            mask.data[j,:] = 0
       
    
    indexes = get_near_tempreq(cube_julymean, 2.0, mask, lsm)
    mintemps, meanrain,  lons, lats  = get_mean_min()
    
    return mintemps, meanrain, lons, lats


def locate_PI_with_plioMAT(model, temp_req, lonreq, proxy_lat):
    """
    this subroutine will find locations in the preindustrial MAT file 
    which have MAT of temp_req
    The site must be land and within 45deg of lonreq
    temperature

    """
    
    def get_near_tempreq(cube, tempreq, errormargin, mask, lsm):
        
        indexes= []
        cube_loc = cube.copy(data = np.zeros(np.shape(cube.data)))
        for index, temp in np.ndenumerate(cube.data):
            if (tempreq - errormargin < temp < tempreq + errormargin):
                if mask.data[index] == 1.0:
                    cube_loc.data[index] = 1.0
                    indexes.append(index)
                  
        #qplt.contourf(cube_loc)  
        #iplt.contour(lsm, colors='black', linewidths=0.1)
        #plt.scatter(lonreq, proxy_lat, color='red',transform=ccrs.Geodetic())
        #plt.show()
        
    
        return indexes
    
    def get_max_min():
        """
        gets the july temperature and the january temperature
        from the pi cube for each index
        """
        jan_temps = []
        july_temps = []
        range_temps = []
       
        for ix in indexes:
            jan = cube.data[0, ix[0], ix[1]]
            jul = cube.data[6, ix[0], ix[1]]
            jan_temps.append(jan)
            july_temps.append(jul)
            range_temps.append(jul - jan)
            
           
      
        return jan_temps, july_temps, range_temps
    
    # get data and LSM 
    filename = FILESTART + 'regridded100/' + model + '/E280.NearSurfaceTemperature.mean_month.nc'
    if model == 'GISS2.1G' or model == 'IPSLCM6A':
        fieldname = 'air_temperature'
    else:
        fieldname = 'NearSurfaceTemperature'
        
    cube = iris.load_cube(filename, fieldname)
    cube_annmean =  cube.collapsed(['time'],
                                    iris.analysis.MEAN)
    
    lsm = get_lsm_for_cube(cube)
    mask = get_lsm_for_cube(cube)
    for i, lon in enumerate(lsm.coord('longitude').points):
        if lon < lonreq - 30.:
            mask.data[:,i] = 0
        if lon > lonreq + 30.:
            mask.data[:,i] = 0
    for j, lat in enumerate(lsm.coord('latitude').points):
        if lat < 0.:
            mask.data[j,:] = 0
       
    
   
    indexes = get_near_tempreq(cube_annmean, temp_req, 1.0, mask, lsm)
    maxtemps, mintemps, temprange = get_max_min()
    
    
    return maxtemps, mintemps, temprange
  
def plot_scatter_temperatures(temp_xaxis, temp_yaxis, modelname, i,
                              temp_xplio, temp_yplio, titlename,
                              yaxistitle, xaxistitle,
                              fileout):
    """
    does a scatter plot of the temperatures on a multi subplot figure
    temp_xaxis and temp_yaxis is the preindustrial temperatures that 
    will be plotted in blue
    temp_xplio and temp_yplio is the pliocene temperature at the site that 
    will be plotted in red

    """
    
    if i == 0:
        fig = plt.figure(figsize=(11.0, 11.0))
        fig.suptitle(titlename, fontsize=20)
        fig.text(0, 0.5, yaxistitle, rotation=90, fontsize=20, verticalalignment = 'center')
        fig.text(0.6, 0.0, xaxistitle, fontsize=20, horizontalalignment = 'center')
    
    if modelname == 'MMM':
        xplot = 1
        yplot = 1
        plotpos = 1
    else:
        xplot = 4
        yplot = 5
        plotpos = np.mod(i, xplot * yplot) + 1

    plt.subplot(xplot, yplot, plotpos)
    plt.scatter(temp_xaxis, temp_yaxis)
    plt.scatter(temp_xplio, temp_yplio, color='red')
    plt.title(modelname)
        
    if (plotpos == (xplot * yplot)
        or i == len(MODELNAMES) - 1
        or modelname == 'MMM'):
       
        plt.subplots_adjust(left=0.05, bottom=0.05, right=1.00, top=0.9,
                            wspace=0.3, hspace=0.3)
        plt.savefig(fileout, bbox_inches='tight')
        plt.close()
      
def plot_locations_match(modellons, modellats, proxylon, proxylat, 
                         modeltemps, fileout, modelnames, jandata,
                         janplio):
    """
    this will plot the locations where the model has pi data the same as the 
    pliocene site
    """
    def get_minmax(proxydata, modeldata):
        """
        gets the minimum or maximum of the model data or the proxy data
        """
        minval = proxydata
        maxval = proxydata
        for lons in modeldata:
            minval = np.min([minval, np.min(lons)])
            maxval = np.max([maxval, np.max(lons)])

        return minval, maxval
      
        
    if len(modelnames) > 1:
        fig = plt.figure(figsize=(11.0, 11.0))
        fig.suptitle(SITE_REQUIRED + 'All sites have same july temperature: pliocene (red) matches pi (blue)', fontsize=15)
      
    lonmin, lonmax = get_minmax(proxylon, modellons)
    latmin, latmax = get_minmax(proxylat, modellats)


    for i, model in enumerate(modelnames):
        lons = modellons[i]
        lats = modellats[i]
       
        left = False
        right = False
        top = False
        bottom = False

        if np.mod(i, 4) == 0:
           left = True
        if np.mod(i, 4) == 3:
           tight = True
        if i > 12:
           bottom = True

        if len(modelnames) == 1:
            plt.subplot(1,1,1)
        else:
            plt.subplot(4,5,i+1)
        #m=Basemap(llcrnrlon=lonmin-5.,urcrnrlon=lonmax+5.,
        #          llcrnrlat=latmin -5.,
        #          urcrnrlat=latmax+5.,projection='cyl',resolution='c')
        #m.drawmapboundary
        #m.drawcoastlines()
        #parallels=np.arange(-90.,90.,20.)
        #m.drawparallels(parallels,labels=[left,right,top,bottom],fontsize=10) # labels right
        #meridians=np.arange(-180.,180.,20.)
        #m.drawmeridians(meridians,labels=[left,right,top,bottom],fontsize=10)
    
        #ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
        ax = plt.axes(projection=ccrs.PlateCarree())
        ax.set_extent([lonmin-5., lonmax + 5., latmin-5., latmax+5.])
        ax.coastlines()
        gl = ax.gridlines(draw_labels=True)
        gl.xlabels_top = False
        gl.left_labels = False

       
        if model == 'MMM':
            V = np.arange(np.min(jandata), np.max(jandata), 2)
            #V = np.arange(-40, 10, 5)
            #mycmap = 'gist_earth'
            mycmap = plt.get_cmap('gist_earth')
            #rgb_cm = cmap.colors  # returns array-like color
            norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
  
            print(jandata)
            scatter = plt.scatter(lons, lats, c=jandata,  marker='o', s=20,
                               norm = norm , cmap=mycmap, 
                                transform=ccrs.Geodetic())

           # plt.scatter(proxylon, proxylat, c=janplio,marker='s', s=50,
           #                    norm = norm , cmap=mycmap, 
           #                     transform=ccrs.Geodetic())
            print(lons,lats)
            print(proxylon, proxylat)
            plt.text(proxylon, proxylat, 'x', fontsize=30, 
                                transform=ccrs.Geodetic())
            
            plt.text(proxylon+2.0, proxylat - 0.5, SITE_REQUIRED, 
                                transform=ccrs.Geodetic())
           
            cbar = plt.colorbar(scatter, orientation = 'horizontal')
            cbar.set_label('cold month temperature (degC)')
        else:
            plt.scatter(proxylon, proxylat, c='red',marker='s', s=50)
            plt.scatter(lons, lats, c='blue',  marker='o', s=10)
       


        if model == 'MMM':
            plt.title('PI Locations with WMMT the same as' + 
                      ' mPWP ' + SITE_REQUIRED)
        else:
            plt.title(model + ' ' + np.str(np.around(modeltemps[i])))


    plt.subplots_adjust(left=0.05, bottom=0.05, right=1.00, top=0.9,
                            wspace=0.3, hspace=0.3)
    plt.savefig(fileout, bbox_inches='tight')
    plt.close()

  
def main():
    """
    calling structure
    a) get data from Ulrichs spreadsheet
    b) get's proxy data
    c) plots model data with proxy data on top
    """
 
#    #########################################
#    # get data from Ulrichs spreadsheet
    
    proxy_temperature, proxy_lat, proxy_lon = get_land_obs()
    
    #################################################
    # get Pliocene model data for each month of the year.
    allmodel_Tseascyc = []
    allmodel_Pseascyc = []
    for model in MODELNAMES:
        T_seascyc = get_model_data(EXPTNAME, model, proxy_lat, proxy_lon,'T')
        P_seascyc = get_model_data(EXPTNAME, model, proxy_lat, proxy_lon,'P')
        allmodel_Tseascyc.append(T_seascyc)
        allmodel_Pseascyc.append(P_seascyc)
       

        
    plot_seascyc_vs_proxy(proxy_temperature, allmodel_Tseascyc,
                          proxy_lat, proxy_lon,EXPTNAME)
        
    
    #################################################
    # get PI model data for each month of the year.
    allmodel_TseascycPI = []
    allmodel_Tseascycanom = []
    for i, model in enumerate(MODELNAMES):
        T_seascyc = get_model_data(PI_EXPT, model, proxy_lat, proxy_lon,'T')
        allmodel_TseascycPI.append(T_seascyc)
       
        allmodel_Tseascycanom.append(allmodel_Tseascyc[i] - T_seascyc)
    
   
       
    plot_seascyc_vs_proxy(-3.61, allmodel_TseascycPI,
                          proxy_lat, proxy_lon, PI_EXPT)
    plot_seascyc_vs_proxy(-999.999, allmodel_Tseascycanom,
                          proxy_lat, proxy_lon, 'anom')
    
    ##############################################################
    # find the PI locations which have the same MAT as the
    # Pliocene temperature for our site.  
    
    titlename = ('Red - Pliocene Jan/july temperatures at ' + SITE_REQUIRED +  
                 ', \n Blue - Preindustrial Jan/July temperatures for preindustrial sites which have the' + 
                 ' same MAT as ' + SITE_REQUIRED)
    fileout = (FILEOUTSTART + SITE_REQUIRED.replace(' ','_')  + 
               '_Jan_July_T_for_this_MAT' + EXPTNAME + PLOTTYPE)
    if proxy_lon < 0: proxy_lon = proxy_lon + 360.
    for i, model in enumerate(MODELNAMES):
        mean_plio_temp = np.mean(allmodel_Tseascyc[i])
        

        janT, julyT, T_seascyc_amp = locate_PI_with_plioMAT(model, mean_plio_temp, proxy_lon, proxy_lat)

        plot_scatter_temperatures(janT, julyT, model, i,
                                  allmodel_Tseascyc[i][0],
                                  allmodel_Tseascyc[i][6],
                                  titlename, 'July temperature',
                                  'January temperature', fileout)
        
    
    ##############################################################
    # find the PI locations which have the same July as the
    # Pliocene temperature for our site.  
    
    titlename = ('Red - Pliocene Jan/MAT temperatures at ' + SITE_REQUIRED +  
                 ', \n Blue - Preindustrial Jan/MAT for PI sites which have the' + 
                 ' same July temperature as ' + SITE_REQUIRED)
    fileout = (FILEOUTSTART + SITE_REQUIRED.replace(' ','_')  + 
               '_JanT_MAT_for_this_JulyT' + EXPTNAME + PLOTTYPE)
    modellons = []
    modellats = []
    julytemp = []
    modelpi_janT = []
    for i, model in enumerate(MODELNAMES):
        july_plio_temp = allmodel_Tseascyc[i][6]
        mean_plio_precip = np.mean(allmodel_Pseascyc[i])
        print(allmodel_Pseascyc[i])
     
        julytemp.append(july_plio_temp)
        
        janT, MAP, lons, lats = locate_PI_with_plioJulyT(model, july_plio_temp, proxy_lon, proxy_lat)
        modellons.append(lons)
        modellats.append(lats)
        modelpi_janT.append(janT)
       
        plot_scatter_temperatures(janT, MAP, model, i,
                                  allmodel_Tseascyc[i][0],
                                  mean_plio_precip,
                                  titlename, 'MAP',
                                  'January temperature', fileout)

    plt.close()
    # plot the locations where pi is same as plio.
    fileout =  FILEOUTSTART + SITE_REQUIRED.replace(' ','_') + 'locations_with_same_julyT' + EXPTNAME + PLOTTYPE
       
    plot_locations_match(modellons, modellats, proxy_lon, proxy_lat, julytemp, 
                         fileout, MODELNAMES, modelpi_janT,'0')
                        

    ###################################################################
    #  CHECK WHERE THE JULY TEMPERATURES IN PLIOCENE MATCH THE PI IN
    #  THE MULTIMODELMEAN

    MAPplio = np.mean(allmodel_Pseascyc)
    print(MAPplio)
#    sys.exit(0)
    janplio = np.mean(allmodel_Tseascyc, axis=0)[0]
    julplio = np.mean(allmodel_Tseascyc, axis=0)[6]
    julplio = 19.0
    (janTpi, MAPpi, 
     lonspi, latspi) = locate_PI_with_plioJulyT('MMM', julplio, 
                                                proxy_lon, proxy_lat)

    fileout = (FILEOUTSTART + SITE_REQUIRED.replace(' ','_')  + 
               '_MMM_analogies_' + EXPTNAME + PLOTTYPE)

    plot_scatter_temperatures(janTpi, MAPpi, 'MMM', 0,
                               janplio, MAPplio, 
                               titlename, 'MAP (mm/day)',
                               'January temperature', fileout)
    plt.close()
    fileout =  (FILEOUTSTART + SITE_REQUIRED.replace(' ','_') + 
                 'locations_with_MMM_julyT' + EXPTNAME + PLOTTYPE)
  
    plot_locations_match([lonspi], [latspi], proxy_lon, 
                          proxy_lat, [julplio], fileout, ['MMM'],
                         janTpi, janplio)
  #                       proxy_lat, [19.0], fileout, ['MMM'])
 




    #########################
    # some printouts for adding to paper

    
    allmeans = np.zeros(len(MODELNAMES))
    coldmeans = np.zeros(len(MODELNAMES))
    warmmeans = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        print('Jan, Jul mean', model, allmodel_Tseascyc[i][0],
              allmodel_Tseascyc[i][6], np.mean(allmodel_Tseascyc[i][:]))
        allmeans[i] = np.mean(allmodel_Tseascyc[i][:])
        coldmeans[i] = np.min(allmodel_Tseascyc[i][:])
        warmmeans[i] = np.max(allmodel_Tseascyc[i][:])
  
    print(SITE_REQUIRED, 'multmodelmean mean stdev',np.mean(allmodel_Tseascyc), np.mean(allmeans), np.std(allmeans))
    print(np.shape(allmodel_Tseascyc))
    print('mean annual cycle pliocene', np.mean(allmodel_Tseascyc, axis=0))
    print('mean annual cycle pi', np.mean(allmodel_TseascycPI, axis=0))
    print('standard deviation pliocene', np.std(allmodel_Tseascyc, axis=0))
    #print('max annual cycle pliocene', np.max(allmodel_Tseascyc, axis=0))
    #print('min annual cycle pliocene', np.min(allmodel_Tseascyc, axis=0))

    datameanmin = 6.65
    datameanmax = 7.65

    datacoldmin = -9.47
    datacoldmax = -6.73

    datawarmmin = 15.3
    datawarmmax = 20.5

    correctcold = 0
    correctwarm = 0
    correctmean = 0
    for i, model in enumerate(MODELNAMES):
        if datameanmin - 2.0  < allmeans[i] < datameanmax + 2.0:
            correctmean = correctmean + 1
        if datacoldmin - 2.0 < coldmeans[i] < datacoldmax + 2.0:
            correctcold = correctcold + 1
        if datawarmmin - 2.0 < warmmeans[i] < datawarmmax + 2.0:
            correctwarm = correctwarm + 1
            print('correctwarm',model)

    print('correct models', correctmean, correctwarm, correctcold)
    print('mean annual cycle pliocene', np.mean(allmodel_Tseascyc))
    print('mean annual cycle pi', np.mean(allmodel_TseascycPI))

    print('got to end of program')
##########################################################
# main program


LINUX_WIN = 'l'
EXPTNAME = 'EOI400'
PI_EXPT = 'E280'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/' 
    FILEOUTSTART = '/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/'
    LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')
    PLOTTYPE =  '.eps'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    LAND_DATAFILE = (FILESTART + '/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')
    PLOTTYPE = '.png'
 

MODELNAMES = [
              'CESM2', 'HadGEM3',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'MRI2.3', 'NorESM1-F'
              ]

#MODELNAMES = [
#              'CESM2', 'HadGEM3',
#              'IPSLCM6A', 'CCSM4',
#              'COSMOS', 
#              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
#              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
#              'CCSM4-Utr', 'CCSM4-UoT', 
#              'NorESM-L',  'NorESM1-F'
#              ]

#MODELNAMES = ['COSMOS', 'HadCM3', 'MIROC4m']
#
ABSOLUTE_ANOMALY_IND = 'absolute' # do we want to compare the absolute value or the anomaly value
FIELD = 'NearSurfaceTemperature'
#FIELD = 'TotalPrecipitation'

#SITE_REQUIRED = 'Lake Baikal'
#SITE_REQUIRED = 'Lake Elgygytgyn'
SITE_REQUIRED = 'Meighen Island'
#SITE_REQUIRED = 'Beaver Pond'
#SITE_REQUIRED = 'Flyes Leaf Bed'
#ITE_REQUIRED = 'Lost Chicken Mine'
#SITE_REQUIRED = 'James Bay Lowland'
#SITE_REQUIRED = 'Pula Maar'
#SITE_REQUIRED = 'Alpes-Maritimes'
#SITE_REQUIRED = 'Tarragona'
#SITE_REQUIRED = 'Rio Maior'
#SITE_REQUIRED = 'Yallalie, Perth'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/map_seas_cyc.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on December 2020
# 
#  This program will plot a map showing the amplitude of the seasonal cycle 
#  over Land for the pliocene and the preindustrial
#
#
#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.gridspec as gridspec
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

 




def mask_ocean_for_cube(cube, lsmfile):
    """
    get's the land sea mask and sets the cube value to missing where the  
    lsm is 0

    """
    lsm_cube = iris.load_cube(lsmfile)
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    
    regrid_lsm_cube = lsm_cube.regrid(cubegrid, iris.analysis.Linear())
    
    for i, lsm in enumerate(regrid_lsm_cube.coord('longitude').points):
        if lsm != cube.coord('longitude').points[i]:
            print('lsm lon error')
            print(lsm, cube.coord('longitude').points[i])
            sys.exit(0)
       
    for j, lsm in enumerate(regrid_lsm_cube.coord('latitude').points):
         if lsm != cube.coord('latitude').points[j]:
            print('lsm lat error')
            print(lsm, cube.coord('latitude').points[i])
            sys.exit(0)
    
    masked_cube = iris.util.mask_cube(cube, np.where(regrid_lsm_cube.data == 0))
    
    print(masked_cube.data)
        
    return masked_cube
        
  

def get_max_seascyc(fieldname):
    """
    gets the warm month temperature minus the cold month temperature
    """

    temp_cube = iris.load_cube(FILENAME, fieldname)
    warm_cube = temp_cube.collapsed(['time'], iris.analysis.MAX)
    cold_cube = temp_cube.collapsed(['time'], iris.analysis.MIN)

    ann_mean_cube = temp_cube.collapsed(['time'], iris.analysis.MEAN)
    seas_cyc_cube = warm_cube - cold_cube


    return seas_cyc_cube, ann_mean_cube, warm_cube, cold_cube

def plotcube(cube, period, outname, V, ax):
    """
    plots the cube to a file
    """

    ax.coastlines()
    ax.set_extent([-180, 180, 30, 80], ccrs.PlateCarree())
   
    cs = iplt.contourf(cube, V, extend = 'both')
    plt.title(period + ': ' + outname)
    if period == 'mPWP':
        cbar = plt.colorbar(cs, orientation = 'horizontal')
        cbar.set_label('degC')
        
   

def plot_locations2(match_cube, alt_seasoncube, precip_cube,
                    matchno, subscript):
    """
    we have a cube (match_cube) where the temperature has to match the matchno.
    This could be the warm_cube matching 20degC, or the cold_cube matching -10degC
    We find all the locations in match_cube which are close to match number
    We then plot all the locations (and their temperatures in the alt_seasoncube).  The alternative-season_cube could be either the warmcube or the cold cube
    """  

    lons = {'Lake Baikal' : 108., 'Lake Baikal M' : 108.,
            'Lost Chicken Mine' : -142., 'Lost Chicken Mine M' : -142.,
            'Lost Chicken Mine B' : -142.}
    lats = {'Lake Baikal' : 56., 'Lake Baikal M' : 56.,
            'Lost Chicken Mine' : 64., 'Lost Chicken Mine M' : 64.,
            'Lost Chicken Mine B' : 64.}
  
    
    
    lon = lons.get(SITE)
    lat = lats.get(SITE)
    lonalt = lon
    if lonalt > 180: lonalt = lon - 360.
   

    treqmin = matchno - 1.0
    treqmax = matchno + 1.0
    
  
    cube2 = match_cube.copy()
    cube3 = alt_seasoncube.copy()
    for index, point in np.ndenumerate(cube2.data):
        if treqmax < point  or treqmin > point:
            cube2.data[index] = 0
    for i, loncube in enumerate(cube2.coord('longitude').points):
        if lon  < loncube  or loncube < lon - 50:
            cube2.data[:, i] = 0
    for j, latcube in enumerate(cube2.coord('latitude').points):
        if 85 < latcube  or latcube < lat - 30:
            cube2.data[j, :] = 0
       
    masked_cube = iris.util.mask_cube(cube2, 
                                      np.where(cube2.data == 0))   
    masked_alt = iris.util.mask_cube(cube3, 
                                      np.ma.where(masked_cube.data.mask == 1))
            
    
    # plot locations and winter temperatures.
    
    ax1 = plt.subplot(221, projection = ccrs.PlateCarree())
    ax1.coastlines()
    ax1.set_extent([np.max([-180., lonalt - 50.]), lonalt + 50., lat-30., 85.], ccrs.PlateCarree())
    cs=iplt.contourf(masked_alt, extend='both')
    cbar = plt.colorbar(cs, orientation = 'horizontal')
    plt.text(lonalt, lat, 'x', fontsize=20)
    plt.text(lon+5.0, lat, SITE)        
    plt.title(subscript)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/' + SITE.replace(' ','_') + '_' + subscript + '.eps')
    
    # plot an histogram of the temperatures
    allvals = []
    precipvals = []
    for index, point in np.ndenumerate(masked_alt.data):
        val = masked_alt.data[index]
        if np.isfinite(val):
            allvals.append(val)
            precipvals.append(precip_cube.data[index] * 360.)
    ax2 = plt.subplot(222)
    num_bins = 10
    n, bins, patches = plt.hist(allvals, num_bins, alpha=0.5)
   
    # plot a scatter plot of the cold month temperatures vs the precip
    ax3 = plt.subplot(223)
    plt.scatter(allvals,precipvals)
    plt.xlabel('cold month temperature')
    plt.ylabel('precipitation')
   
    plt.tight_layout()
    plt.savefig(fileout)
    plt.close()



def plot_locations_bothmatch(warm_cube, cold_cube):
    """
    Is there a location where the warm temperature matches the warm cube
    and the cold temperature matches the cold cube
    """  

    lons = {'Lake Baikal' : 108., 'Lake Baikal M' : 108.,
            'Lost Chicken Mine' : -142., 'Lost Chicken Mine M' : -142.,
            'Lost Chicken Mine B' : -142.}
    lats = {'Lake Baikal' : 56., 'Lake Baikal M' : 56.,
            'Lost Chicken Mine' : 64., 'Lost Chicken Mine M' : 64.,
            'Lost Chicken Mine B' : 64.}
  
    
    lon = lons.get(SITE)
    lat = lats.get(SITE)
    lonalt = lon
    if lonalt > 180: lonalt = lon - 360.
   
    precision = 3.0
    locs_lon = []
    locs_lat = []
    warm_temp = []
    cold_temp = []
    
    cube2 = warm_cube.copy()
    data2 = cube2.data
    cube3 = cold_cube.copy()
    data3 = cube3.data
    for index, point_warm in np.ndenumerate(data2):
        point_cold = data3[index]
        
        if ((WARM_TEMP_REQ + precision  > point_warm 
        and WARM_TEMP_REQ - precision  < point_warm
        and COLD_TEMP_REQ + precision > point_cold
        and COLD_TEMP_REQ - precision < point_cold) and not data2.mask[index]):
            locs_lon.append(cube2.coord('longitude').points[index[1]])
            locs_lat.append(cube2.coord('latitude').points[index[0]])
            warm_temp.append(point_warm)
            cold_temp.append(point_cold)
        else:
            cube2.data[index] = 0
    
    for i, loncube in enumerate(cube2.coord('longitude').points):
        if lon  < loncube  or loncube < lon - 50:
            cube2.data[:, i] = 0
    for j, latcube in enumerate(cube2.coord('latitude').points):
        if 85 < latcube  or latcube < lat - 30:
            cube2.data[j, :] = 0
       
    for i, lon in enumerate(locs_lon):
        print(lon, locs_lat[i], warm_temp[i], cold_temp[i])
    print('MEAN', np.mean(np.asarray(warm_temp)), np.mean(np.asarray(cold_temp)), np.mean(np.asarray(warm_temp) - np.asarray(cold_temp)))
  
    # plot locations and winter temperatures.
    
    ax1 = plt.subplot(111, projection = ccrs.PlateCarree())
    ax1.coastlines()
    #ax1.set_extent([np.max([-180., lonalt - 50.]), lonalt + 50., lat-30., 85.], ccrs.PlateCarree())
    ax1.set_extent([-180, 180, -90, 90], ccrs.PlateCarree())
    #cs=iplt.contourf(masked_alt, extend='both')
    plt.scatter(locs_lon, locs_lat)
    #cbar = plt.colorbar(cs, orientation = 'horizontal')
    plt.text(lonalt, lat, 'x', fontsize=20)
    plt.text(lonalt-20.0, lat-5, SITE)        
   # plt.title(subscript)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/' + SITE.replace(' ','_') + '_bothmatch.eps')
    
    plt.tight_layout()
    plt.savefig(fileout)
    plt.close()

  
                                      

def plot_locations(cube, warm_cube, cold_cube):
    """
    we have a value for the required annual cycle.  We will plot all locations
    which have this annual cycle
    """  

    lons = {'Lake Baikal' : 108., 'Lake Baikal M' : 108.,
            'Lost Chicken Mine' : -142., 'Lost Chicken Mine M' : -142.,
            'Lost Chicken Mine B' : -142.}
    lats = {'Lake Baikal' : 56., 'Lake Baikal M' : 56.,
            'Lost Chicken Mine' : 64., 'Lost Chicken Mine M' : 64.,
            'Lost Chicken Mine B' : 64.}
    treqmin = ANN_CYC_REQ - 1.0
    treqmax = ANN_CYC_REQ + 1.0
    
  
    cube2 = cube.copy()
    for index, point in np.ndenumerate(cube2.data):
        if treqmax < point  or treqmin > point:
            cube2.data[index] = 0
           
    masked_cube = iris.util.mask_cube(cube2, 
                                      np.where(cube2.data == 0))
    masked_warm = iris.util.mask_cube(warm_cube, 
                                      np.ma.where(cube2.data.mask == 1))
    masked_cold = iris.util.mask_cube(cold_cube, 
                                      np.ma.where(cube2.data.mask == 1))
    
    # plot
    lon = lons.get(SITE)
    lat = lats.get(SITE)

    ax1 = plt.subplot(221, projection = ccrs.PlateCarree())
    ax1.coastlines()
    ax1.set_extent([np.max([lon - 50., -180.]), lon + 50., lat-50., 85.], ccrs.PlateCarree())
    #V = np.arange(13, 28, 3)
    cs=iplt.contourf(masked_warm)
    cbar = plt.colorbar(cs, orientation = 'horizontal')
    plt.text(lon, lat, 'x', fontsize=20)
    plt.text(lon+5.0, lat, SITE)
   
    ax2 = plt.subplot(222, projection = ccrs.PlateCarree())
    ax2.coastlines()
    ax2.set_extent([lon - 50, lon + 50, lat-50, 85], ccrs.PlateCarree())
    #V = np.arange(-15, 10, 5)
    cs=iplt.contourf(masked_cold)
    cbar = plt.colorbar(cs, orientation = 'horizontal')
    plt.text(lon, lat, 'x', fontsize=20)
    plt.text(lon+5.0, lat, SITE)
           
    plt.title('Annual cycle amplitude ~ ' + np.str(np.int(ANN_CYC_REQ)) + 'degC')
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/' + SITE.replace(' ','_') + '_same_seas_cyc.eps')

    plt.show()
    plt.close()
                                      

  
def main():
    """
    calling structure
    1. read in data
    2. get amplitude(warm month T - cold month T) and annual mean temperature 
    3. filter out land sea mask
    4. plot amplitude and annmeantemp
    """

    # get pliocene stuff
    (plio_amplitude_cube,
     plio_mean_cube,
     plio_warm_cube,
     plio_cold_cube)= get_max_seascyc('NearSurfaceTemperaturemean_plio')
    lsmfile = BCSTART + '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    plio_landamp_cube  = mask_ocean_for_cube(plio_amplitude_cube, lsmfile)
    plio_landmean_cube = mask_ocean_for_cube(plio_mean_cube, lsmfile)
   
    # get pi stuff
    (pi_amplitude_cube,
     pi_mean_cube,
     pi_warm_cube,
     pi_cold_cube)= get_max_seascyc('NearSurfaceTemperaturemean_pi')
    pi_precip_cube = iris.load_cube(PRECIPFILE, 'TotalPrecipitationmean_pi')
    lsmfile = BCSTART + '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
    pi_landamp_cube= mask_ocean_for_cube(pi_amplitude_cube, lsmfile)
    pi_landmean_cube = mask_ocean_for_cube(pi_mean_cube, lsmfile)
    pi_warm_cube = mask_ocean_for_cube(pi_warm_cube, lsmfile)
    pi_cold_cube = mask_ocean_for_cube(pi_cold_cube, lsmfile)
   

    #fig1 = plt.figure(figsize=[8.0, 4.0], constrained_layout=True)
   
    fig1 = plt.figure(figsize=[8.0, 4.0])
    gs = gridspec.GridSpec(nrows=2, ncols=2)

    ax1 = fig1.add_subplot(gs[0,0], projection=ccrs.PlateCarree())
    plotcube(pi_landmean_cube, 'PI', 'mean', np.arange(-20, 20, 5), ax1)
    ax2 = fig1.add_subplot(gs[0,1], projection=ccrs.PlateCarree())
    plotcube(pi_landamp_cube, 'PI','amplitude', np.arange(20, 50, 5), ax2)
    ax3 = fig1.add_subplot(gs[1,0], projection=ccrs.PlateCarree())
    plotcube(plio_landmean_cube, 'mPWP', 'mean', np.arange(-20, 20, 5), ax3)
    ax4 = fig1.add_subplot(gs[1,1], projection=ccrs.PlateCarree()) 
    plotcube(plio_landamp_cube, 'mPWP', 'amplitude', np.arange(20, 50, 5), ax4)
  
   
    
    plt.tight_layout()
    #plt.subplots_adjust(bottom=0.1, top=0.9, hspace=None)
    
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/map_seas_cyc.eps')
   
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/map_seas_cyc.png')
   
    plt.savefig(fileout)
    plt.close()
  


    # anomalies
    #plotcube(plio_landamp_cube - pi_landamp_cube, 'mPWP-PI','amplitude',
    #         np.arange(-5, 5, 1))
    #plotcube(plio_landmean_cube - pi_landmean_cube, 'mPWP-PI','mean',
    #         np.arange(-5, 5, 1))

    # plot all pi locations which have an annual cycle within 1 deg of 
    # ann_cyc_req

    # note that I think this is a bit misleading.
    #  all it shows are that these temperatures can exist in the preindustrial
    plot_locations_bothmatch(pi_warm_cube, pi_cold_cube)

    plot_locations2(pi_warm_cube, pi_cold_cube, pi_precip_cube,
                    WARM_TEMP_REQ, 'warm_match')
    plot_locations2(pi_cold_cube, pi_warm_cube, pi_precip_cube,
                    COLD_TEMP_REQ, 'cold_match')
    plot_locations(pi_landamp_cube, pi_warm_cube, pi_cold_cube)
      

##########################################################
# main program

LINUX_WIN = 'l'
EXPTNAME = 'EOI400'
PI_EXPT = 'E280'

FILENAME= ('/nfs/hera1/earjcti/regridded100/' + 
                    'NearSurfaceTemperature_multimodelmean_month.nc')
PRECIPFILE  = ('/nfs/hera1/earjcti/regridded100/' + 
                    'TotalPrecipitation_multimodelmean.nc')

BCSTART = '/nfs/hera1/earjcti/regridded/PlioMIP2_Boundary_conds/'

ANN_CYC = {'Lake Baikal' : 17.0, 'Lake Baikal M': 46.0,
           'Lost Chicken Mine' : 14.0, 'Lost Chicken Mine M' : 35.0,
           'Lost Chicken Mine B' : 39.0}
#WARM_TEMP = {'Lake Baikal' : 22.4, 'Lost Chicken Mine' : 15.4, 
#             'James Bay Lowland' : 22.6}
WARM_TEMP = {'Lake Baikal' : 16.4, 'Lake Baikal M' : 20.0, 
             'Lost Chicken Mine' : 12.0, 'Lost Chicken Mine M': 15.4,
             'Lost Chicken Mine B' : 15.0}
COLD_TEMP = {'Lake Baikal' : -0.3, 'Lake Baikal M' : -15.0,
             'Lost Chicken Mine' : -2.0, 'Lost Chicken Mine M': -20,
             'Lost Chicken Mine B' : -24.0}

SITE = 'Lake Baikal M'  #'James Bay Lowland' #'Lost Chicken Mine' #'Lake Baikal' # Lost Chicken Mine B (beetle) Lake Baikal M (model

ANN_CYC_REQ = ANN_CYC.get(SITE)
WARM_TEMP_REQ = WARM_TEMP.get(SITE)
COLD_TEMP_REQ = COLD_TEMP.get(SITE)
main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/map_sites.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on December 2020
# 
#  This program will plot a map showing the amplitude of the seasonal cycle 
#  over Land for the pliocene and the preindustrial
#
#
#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.gridspec as gridspec
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

 



    

  
def main():
    """
    plot map with all sites on.
    """

    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, 
    sitedata.append(['MI', 77.5, 261])
    sitedata.append(['BP', 79, 278])
    sitedata.append(['LE', 67, 172])
    sitedata.append(['LCM', 64, -142])
    sitedata.append(['Lake Baikal', 56, 108])
             

    ax = plt.axes(projection=ccrs.PlateCarree())
    ax.coastlines()
    ax.set_global()
    for data in sitedata:
        plt.plot(data[2],data[1], color='red',marker='o', 
                    linewidth=2, transform = ccrs.Geodetic())
  
    
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/map_sites.eps')
   
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/map_sites.png')
   
    plt.savefig(fileout)
    plt.close()
  

##########################################################
# main program

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/plot_biome4_output.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created February 2021 by Julia

This program will plot the biomes in a nice way

; tropical evergreen broadleaf forest                = 1
; tropical semi-evergreen broadleaf forest           = 2
; tropical deciduous broadleaf forest & woodland     = 3
; temperate deciduous broadleaf forest               = 4
; temperate evergreen needleleaf forest              = 5
; warm-temperate evergreen broadleaf & mixed forest  = 6
; cool mixed forest                                  = 7
; cool evergreen needleleaf forest                   = 8
; cool-temperate evergreen needleleaf & mixed forest = 9
; cold evergreen needleleaf forest                   = 10
; cold deciduous forest                              = 11
; tropical savanna                                   = 12
; tropical xerophytic shrubland                      = 13
; temperate xerophytic shrubland                     = 14
; temperate sclerophyll woodland and shrubland       = 15
; temperate deciduous broadleaf savanna              = 16
; temperate evergreen needleleaf open woodland       = 17
; cold parkland                                      = 18
; tropical grassland                                 = 19
; temperate grassland                                = 20
; desert                                             = 21
; graminoid and forb tundra                          = 22
; low and high shrub tundra                          = 23
; erect dwarf-shrub tundra                           = 24
; prostrate dwarf-shrub tundra                       = 25
; cushion-forb tundra                                = 26
; barren                                             = 27
; ice                                                = 28


"""
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import iris
import iris.plot as iplt
import iris.quickplot as qplt
import cartopy.crs as ccrs
import sys

def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def get_biome_details():
    """
    gets the biome names and colormap.  These are hard coded
    """
    
    names = ["Tropical evergreen forest",    
             "Tropical semi-deciduous forest",
             "Tropical decididous forest",
             "Temperate deciduous forest",
             "Temperate conifer forest",
             "warm mixed forest",
             "cool mixed forest",
             "cool conifer forest",  
             "cool-temperate mixed forest",
             "Evergreen taiga/montane forest",                  
             "Deciduous taiga/montane forest",                           
             "Tropical savanna",                            
             "Tropical xerophytic shrubland",                  
             "Temperate xerophytic shrubland",                   
             "Temperate sclerophyll woodland",     
             "Temperate broadleaf savanna",              
             "Open conifer woodland",     
             "Boreal parkland",                                  
             "Tropical grassland",                              
             "Temperate grassland",                              
             "Desert",                                           
             "Steppe tundra",                         
             "Shrub tundra",                         
             "Dwarf-shrub tundra",                          
             "Prostrate shrub tundra",                      
             "Cushion-forb tundra",                               
             "Barren",                                           
             "Land ice"] 

    colors = [ #( 1.000, 1.000, 1.000), 
            # ( 0.000, 0.000, 0.000 ),
             ( 0.110, 0.333, 0.063 ),
             ( 0.396, 0.573, 0.031 ),
             ( 0.682, 0.490, 0.125 ),
             ( 0.333, 0.922, 0.286 ),
             ( 0.094, 0.510, 0.443 ),
             ( 0.000, 0.000, 0.396 ),
             ( 0.792, 1.000, 0.792 ),
             ( 0.000, 0.604, 0.094 ),
             ( 0.443, 0.141, 0.208 ),
             ( 0.000, 0.125, 0.792 ),
             ( 0.396, 0.698, 1.000 ),
             ( 0.729, 1.000, 0.208 ),
             ( 1.000, 0.729, 0.604 ),
             ( 1.000, 0.875, 0.792 ),
             ( 0.557, 0.635, 0.157 ),
             ( 0.459, 1.000, 0.208 ),
             ( 1.000, 0.604, 0.875 ),
             ( 0.396, 0.490, 1.000 ),
             ( 1.000, 0.729, 0.208 ),
             ( 1.000, 0.875, 0.604 ),
             ( 0.969, 1.000, 0.792 ),
             ( 0.906, 0.906, 0.094 ),
             ( 0.396, 1.000, 0.604 ),
             ( 0.475, 0.525, 0.286 ),
             ( 0.824, 0.620, 0.588 ),
             ( 0.604, 0.396, 1.000 ),
             ( 0.729, 0.714, 0.667 ),
             ( 0.714, 0.824, 0.875 ) 
             # this last one was commented out
             #,( 0.700, 0.700, 0.700 )
             ]

    #cmap = mpl.colors.LinearSegmentedColormap.from_list(
    #    'Custom cmap', cmaplist, cmap.N)
    
   # cmap = mpl.colors.ListedColormap(['red',    'green',  'blue', 
   #                                   'cyan', 'red',   'green',  'blue', 
   #                                   'cyan',   'red',    'green',    'blue', 
   #                                   'cyan', 'red',  'green',  'blue', 
   #                                   'cyan',    'red',  'green', 'blue', 
   #                                   'cyan',   'red',  'green',  'blue', 
   #                                   'cyan',  'red',  'green',  'blue', 
   #                                   'cyan'    ])
    cmap = mpl.colors.ListedColormap(colors)
    bounds = np.linspace(0.5, 27.5, 28)
    print(bounds)
    #sys.exit(0)
    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)
    return names, colors, cmap, norm, bounds

def get_data(colors):
    """
    reads in the data and puts it in a array (M, N, 3) with the
    third dimensio being the colors
    """
    cube = iris.load_cube(FILENAME, 'biome')
    cubedata = cube.data
    ny, nx = np.shape(cubedata)
    colordata = np.zeros((ny, nx, 3))

    print(np.shape(cubedata))
    print(np.shape(colordata))

    for index, point in np.ndenumerate(cubedata):
        if point >= 0:
            colordata[index[0], index[1], :] = colors[point-1]
        else:
            colordata[index[0], index[1], :] = (1, 1, 1)

    return cube, colordata
      
def plot_biomes(names, rgbdata, biomecube, cmap, norm, V,):
    """
    plots the biomes
    """
    fig = plt.figure(figsize=(11.0, 11.0))
    ax = plt.axes(projection = ccrs.PlateCarree())
    #ax.set_extent([-180, 180, -65, 90])
    ax.coastlines()

    # turn the iris Cube data structure into numpy arrays
    gridlons = biomecube.coord('longitude').contiguous_bounds()
    gridlats = biomecube.coord('latitude').contiguous_bounds()
    biomedata = biomecube.data

    print(gridlons)
    print(gridlats)
    print(np.shape(biomedata))
    cs = plt.pcolormesh(gridlons, gridlats, biomedata,
                   cmap=cmap, norm=norm)
    plt.title(MODELNAME + ': ' + ABS_ANOM)

    cbar = plt.colorbar(cs, ticks=V+0.5, )
    cbar.ax.set_yticklabels(names)
    cbar.ax.tick_params (labelsize=6)
    cbar.ax.invert_yaxis()
    #ax.imshow(rgbdata, transform=ccrs.PlateCarree())
    if MODELNAME == 'BIOME4':
        midfname = MODELNAME + '/'
    elif MODELNAME == 'BIOME4_NEWPARAMETERS':
        midfname = 'BIOME4/' + MODELNAME + '/'
    else:
        midfname =  MODELNAME + '/biome4/'
    fileout = (FILESTART + midfname + '_biome4out_' + ABS_ANOM + '.eps')
    print(fileout)
    plt.savefig(fileout)
    print(fileout)
    fileout = (FILESTART + midfname + '_biome4out_' + ABS_ANOM + '.png')
    plt.savefig(fileout)
    sys.exit(0)

def get_prism3_biomes():
    """
    get the biomes from prism3 as a cube
    """
    fname = ('/nfs/hera1/earjcti/PRISM/prism3_pliocene/exp2_alternate/' + 
             'biome_veg_v1.2.nc')
    cube = iris.load_cube(fname)

    # change zeros to nan
    cubedata = cube.data
    cubedata2 = np.where(cubedata != 0, cubedata, np.nan)
    cube_prism3 = cube.copy(data = cubedata2)
   
    fname = ('/nfs/hera1/earjcti/PRISM/prism3_pliocene/BAS_Observ_BIOME.nc')
    cube = iris.load_cube(fname)

    # change zeros to nan
    cubedata = cube.data
    cubedata2 = np.where(cubedata != 0, cubedata, np.nan)
    cube_pi = cube.copy(data = cubedata2)
    

    return cube_prism3, cube_pi

def plot_biomes_with_obs(names, rgbdata, modelcube, datacube, cmap, norm, V,
                         model_ind):
    """
    plots the biomes and the observations on the same figure
    """
    fig,ax = plt.subplots(ncols=1,nrows=2,figsize=(11,8),
                      subplot_kw={'projection': ccrs.PlateCarree()})
    ax1 = ax[0]
    ax2 = ax[1]

    ax1.set_extent([-180, 180, -65, 90])
    ax1.coastlines()

    # do observations
    # turn the iris cube data structure into numpy arrays
    gridlons_d = datacube.coord('longitude').contiguous_bounds()
    gridlats_d = datacube.coord('latitude').contiguous_bounds()
    data = datacube.data

    cs = ax1.pcolormesh(gridlons_d, gridlats_d, data,
                   cmap=cmap, norm=norm)
    if model_ind == 'y':
        ax1.title.set_text('reconstructed biomes for mPWP')
    else:
        ax1.title.set_text('biomes for preindustrial')
 

    # do biome cube
    ax2.set_extent([-180, 180, -65, 90])
    ax2.coastlines()

    # turn the iris Cube data structure into numpy arrays
    gridlons = modelcube.coord('longitude').contiguous_bounds()
    gridlats = modelcube.coord('latitude').contiguous_bounds()
    modeldata = modelcube.data

    cs = ax2.pcolormesh(gridlons, gridlats, modeldata,
                   cmap=cmap, norm=norm)
    if model_ind == 'y':
        if MODELNAME == 'BIOME4':
            ax2.title.set_text('simulated biomes from PlioMIP2 multimodel mean')
        else:
            ax2.title.set_text(MODELNAME + ': ' + ABS_ANOM)
    else:
        ax2.title.set_text('reconstructed biomes for mPWP')


    # adjust plots so we have room for colorbar
    fig.subplots_adjust(bottom=0.05, top = 0.95, left = 0.05, right = 0.70)

    # do colorbar
    cbar_ax = fig.add_axes([0.75, 0.05, 0.05, 0.9])
    cbar = fig.colorbar(cs, cax= cbar_ax, ticks=V+0.5, )
    cbar.ax.set_yticklabels(names)
    cbar.ax.tick_params (labelsize=9)
    cbar.ax.invert_yaxis()
 
    if model_ind == 'y':
        if MODELNAME == 'BIOME4':
            midfname = MODELNAME + '/'
        else:
            midfname =  MODELNAME + '/biome4/'
        fileout = (FILESTART + midfname + '_biome4out_' + ABS_ANOM + '_data.eps')
        plt.savefig(fileout)
        print(fileout)
        fileout = (FILESTART + midfname + '_biome4out_' + ABS_ANOM + '_data.png')
        plt.savefig(fileout)
    else:
        fileout = (FILESTART +  'BIOME4/' + 'pi_vs_plio_obs.eps')
        plt.savefig(fileout)
        print(fileout)
        fileout = (FILESTART +  'BIOME4/' + 'pi_vs_plio_obs.png')
        plt.savefig(fileout)

def get_temp_ranges(biome_prism3_cube, plio_mmmT_cube, i):
        """
        get's the minimum and maximum temperature in this cube for Jan/Apr/Jul/Oct
        """

        biome_mean_temps = np.ma.masked_where(np.repeat(biome_prism3_cube.data[np.newaxis,: :],12, axis=0) != i, plio_mmmT_cube.data)
        biome_mean_temp_cube = plio_mmmT_cube.copy(data=biome_mean_temps)
        biome_NHtemp_cube = biome_mean_temp_cube.extract(iris.Constraint(latitude = lambda cell: cell >= 0))

        plio_temp = []
           
        # min and maximum temperatures are
        month1= biome_NHtemp_cube.extract(iris.Constraint(time=1))
        plio_temp.append([np.float(month1.collapsed(['latitude','longitude'], iris.analysis.MEAN).data), np.float(month1.collapsed(['latitude','longitude'], iris.analysis.MAX).data)])
            
        month4 = biome_NHtemp_cube.extract(iris.Constraint(time=4))
        plio_temp.append([np.float(month4.collapsed(['latitude','longitude'], iris.analysis.MEAN).data), np.float(month4.collapsed(['latitude','longitude'], iris.analysis.MAX).data)])

        month7= biome_NHtemp_cube.extract(iris.Constraint(time=7))
        plio_temp.append([np.float(month7.collapsed(['latitude','longitude'], iris.analysis.MEAN).data), np.float(month7.collapsed(['latitude','longitude'], iris.analysis.MAX).data)])
            
        month10 = biome_NHtemp_cube.extract(iris.Constraint(time=10))
        plio_temp.append([np.float(month10.collapsed(['latitude','longitude'], iris.analysis.MEAN).data), np.float(month10.collapsed(['latitude','longitude'], iris.analysis.MAX).data)])

        return np.asarray(plio_temp)
           
def check_seasonal_for_each_biome(biome_pi_cube_lowres, biome_prism3_cube_lowres,
                                  biome_names):
    """
    1. for pliocene find out which biomes are present polewards of 50N
    2. for each of these biomes find the temperature of these biomes in the
       NH for the Pliocene and the PI throughout the annual cycle
    3. Extract Jan/Apr/July/Oct temperature
    4.
    """
  
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
      
    biome_pi_cube = biome_pi_cube_lowres.regrid(cubegrid,iris.analysis.Nearest())
    biome_prism3_cube = biome_prism3_cube_lowres.regrid(cubegrid,iris.analysis.Nearest())

   
    # 1. find out which biomes are present in pliocene polewards of 50N
    present=np.zeros(28,dtype='bool')
    biome_data = biome_prism3_cube.data
    for j,lat in enumerate(biome_prism3_cube.coord('latitude').points):
        if lat >= 50.0:
            for i, lon in enumerate(biome_prism3_cube.coord('latitude').points):
                if np.isfinite(biome_data[j,i]):
                    print(biome_data[j,i])
                    present[np.int(biome_data[j,i])] = True

    # find mean temperature of each biome
    plio_mmmT_cube = iris.load_cube('/nfs/hera1/earjcti/regridded/NearSurfaceTemperature_multimodelmean_month.nc','NearSurfaceTemperaturemean_plio') # load cube
    pi_mmmT_cube = iris.load_cube('/nfs/hera1/earjcti/regridded/NearSurfaceTemperature_multimodelmean_month.nc','NearSurfaceTemperaturemean_pi') # load cube

    allnames = []
    plio_all_range = []
    pi_all_range = []

    # get the ranges of each temperature for months Jan/Apr/July/Oct 
    # where biomes exist
    for i, reqd in enumerate(present):
        if reqd:
           
            allnames.append(biome_names[i-1])
            plio_temps_range = get_temp_ranges(biome_prism3_cube, plio_mmmT_cube, i)
            pi_temps_range = get_temp_ranges(biome_pi_cube, pi_mmmT_cube,  i)

            # note this is a list of arrays
            # each biome corresponding to names, within this each 'month', within this min and max temp
            plio_all_range.append(plio_temps_range)
            pi_all_range.append(pi_temps_range)
            
 
    plio_range = np.asarray(plio_all_range)
    pi_range = np.asarray(pi_all_range)
 

    # plot
    
    fig1 = plt.figure(figsize=[16.0, 20.0])
  

    nbiomes =  len(allnames)
    yarray = np.arange(0,nbiomes,1)
    print(np.shape(plio_range))
#    plt.hlines(y=yarray[0:8], xmin=plio_range[0:8,0,0], xmax=plio_range[0:8,0,1], color='tab:blue', label='January Plio')
#    plt.hlines(y=yarray[0:8]+0.2, xmin=plio_range[0:8,1,0], xmax=plio_range[0:8,1,1], color='tab:green', label='April Plio')
#    plt.hlines(y=yarray[0:8]+0.4, xmin=plio_range[0:8,2,0], xmax=plio_range[0:8,2,1], color='tab:red', label='July Plio')
   
#    plt.hlines(y=yarray[0:8]+0.1, xmin=pi_range[0:8,0,0], xmax=pi_range[0:8,0,1], color='tab:blue', label='January Pi', linestyle='dashed')
#    plt.hlines(y=yarray[0:8]+0.3, xmin=pi_range[0:8,1,0], xmax=pi_range[0:8,1,1], color='tab:green', label='April Pi', linestyle='dashed')
#    plt.hlines(y=yarray[0:8]+0.5, xmin=pi_range[0:8,2,0], xmax=pi_range[0:8,2,1], color='tab:red', label='July Pi', linestyle='dashed')
 
    plt.scatter(plio_range[0:8, 0, 0],yarray[0:8], color='tab:blue', label='January Plio')
    plt.errorbar(plio_range[0:8, 0, 0],yarray[0:8],xerr = plio_range[0:8, 0, 1], color='tab:blue')
    plt.scatter(plio_range[0:8, 1, 0],yarray[0:8], color='tab:green', label='apr Plio')
    plt.scatter(plio_range[0:8, 2, 0],yarray[0:8], color='tab:red', label='Jul Plio')
    plt.scatter(pi_range[0:8, 0, 0],yarray[0:8], color='tab:blue', marker='^',label='Jan Pi')
    plt.scatter(pi_range[0:8, 1, 0],yarray[0:8], color='tab:green', marker='^', label='apr Pi')
    plt.scatter(pi_range[0:8, 2, 0],yarray[0:8], color='tab:red', marker='^',label='jul pi')
  
    for j in range(0, 8):  # just do first 8 biomes, only interested in forest tyeps
        plt.text(-40, yarray[j], allnames[j])
    plt.legend()
    plt.show()

    
            
           
        
    
    

def main():
    """
    driver to get biomes
    """

    print('j1')
    biome_names, colors, cmap, norm, bounds= get_biome_details()
    print('j2')

    cube_model, rgbdata = get_data(colors)
    print('j3')
 
    plot_biomes(biome_names, rgbdata, cube_model, cmap, norm, bounds)
    #print('j4')

    biome_prism3_cube, biome4_pi_cube = get_prism3_biomes()
    plot_biomes_with_obs(biome_names, rgbdata, cube_model, biome_prism3_cube, cmap, norm, bounds,'y')
    #print('j5')
    plot_biomes_with_obs(biome_names, rgbdata, biome_prism3_cube, biome4_pi_cube, cmap, norm, bounds,'n')

    print('j6')
    # look at each biome in turn and find out the Jan / Apr / July/ Oct 
    # temperatures for the PI and the Pliocene
    check_seasonal_for_each_biome(biome4_pi_cube, biome_prism3_cube,
                                  biome_names)

    print('prog finished')
    
 

FILESTART = '/nfs/hera1/earjcti/regridded/'
MODELNAME = 'IPSLCM6A_origgrid' # BIOME4 -  MMM
#MODELNAME = 'xozzb'
ABS_ANOM = 'Anom'
#FILESTART = '/nfs/see-fs-02_users/earjcti/BIOME4/biome4_pliomip2/'
#MODELNAME = ''
#ABS_ANOM = ''
if MODELNAME == 'BIOME4':
    midfname = MODELNAME
elif MODELNAME == 'BIOME4_NEWPARAMETERS':
    midfname = 'BIOME4/BIOME4_NEWPARAMETERS/'
else:
    midfname =  MODELNAME + '/biome4/' 
   
if ABS_ANOM == 'Anom':
    FILENAME = FILESTART + midfname + '/biome4out_anomaly.nc'
if ABS_ANOM == 'Abs':
    FILENAME = FILESTART + midfname + '/biome4out_absolute.nc'
if ABS_ANOM == '':
    FILENAME = FILESTART + midfname + 'biome4out.nc'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/plot_biome_dmc_by_site.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created February 2021 by Julia

This program will do a DMC of biomes by site at high latitudes.

(I am currently thinking polar stereographic plot, I am also thinking to plot data on one figure and another figure showing only those sites where the model is different).  

; tropical evergreen broadleaf forest                = 1
; tropical semi-evergreen broadleaf forest           = 2
; tropical deciduous broadleaf forest & woodland     = 3
; temperate deciduous broadleaf forest               = 4
; temperate evergreen needleleaf forest              = 5
; warm-temperate evergreen broadleaf & mixed forest  = 6
; cool mixed forest                                  = 7
; cool evergreen needleleaf forest                   = 8
; cool-temperate evergreen needleleaf & mixed forest = 9
; cold evergreen needleleaf forest                   = 10
; cold deciduous forest                              = 11
; tropical savanna                                   = 12
; tropical xerophytic shrubland                      = 13
; temperate xerophytic shrubland                     = 14
; temperate sclerophyll woodland and shrubland       = 15
; temperate deciduous broadleaf savanna              = 16
; temperate evergreen needleleaf open woodland       = 17
; cold parkland                                      = 18
; tropical grassland                                 = 19
; temperate grassland                                = 20
; desert                                             = 21
; graminoid and forb tundra                          = 22
; low and high shrub tundra                          = 23
; erect dwarf-shrub tundra                           = 24
; prostrate dwarf-shrub tundra                       = 25
; cushion-forb tundra                                = 26
; barren                                             = 27
; ice                                                = 28


"""
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
from matplotlib.markers import MarkerStyle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import iris
import iris.plot as iplt
import cartopy.crs as ccrs
import matplotlib.path as mpath
import sys
import math

def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def get_biome_details():
    """
    gets the biome names and colormap.  These are hard coded
    """
    
    names = ["Tropical evergreen forest",    
             "Tropical semi-deciduous forest",
             "Tropical decididous forest",
             "Temperate deciduous forest",
             "Temperate conifer forest",
             "warm mixed forest",
             "cool mixed forest",
             "cool conifer forest",  
             "cool-temperate mixed forest",
             "Evergreen taiga/montane forest",                  
             "Deciduous taiga/montane forest",                           
             "Tropical savanna",                            
             "Tropical xerophytic shrubland",                  
             "Temperate xerophytic shrubland",                   
             "Temperate sclerophyll woodland",     
             "Temperate broadleaf savanna",              
             "Open conifer woodland",     
             "Boreal parkland",                                  
             "Tropical grassland",                              
             "Temperate grassland",                              
             "Desert",                                           
             "Steppe tundra",                         
             "Shrub tundra",                         
             "Dwarf-shrub tundra",                          
             "Prostrate shrub tundra",                      
             "Cushion-forb tundra",                               
             "Barren",                                           
             "Land ice"] 

    names_needed = [
             "Temperate deciduous forest",
             "Temperate conifer forest",
             "warm mixed forest",
             "cool mixed forest",
             "cool conifer forest",  
             "cool-temperate mixed forest",
             "Evergreen taiga/montane forest",                  
             "Deciduous taiga/montane forest",                           
             "Tropical savanna",                            
             "Tropical xerophytic shrubland",                  
             "Temperate xerophytic shrubland",                   
             "Temperate sclerophyll woodland",     
             "Temperate broadleaf savanna",              
             "Open conifer woodland",     
             "Boreal parkland",                                  
             "Tropical grassland",                              
             "Temperate grassland",                              
             "Desert",                                           
             "Steppe tundra",                         
             "Shrub tundra",                         
             "Dwarf-shrub tundra",                          
             "Prostrate shrub tundra",                      
             "Cushion-forb tundra",                               
             "Barren",                                           
             "Land ice"]

    colors = [ #( 1.000, 1.000, 1.000), 
            # ( 0.000, 0.000, 0.000 ),
             ( 0.110, 0.333, 0.063 ),
             ( 0.396, 0.573, 0.031 ),
             ( 0.682, 0.490, 0.125 ),
             ( 0.333, 0.922, 0.286 ),
             ( 0.094, 0.510, 0.443 ),
             ( 0.000, 0.000, 0.396 ),
             ( 0.792, 1.000, 0.792 ),
             ( 0.000, 0.604, 0.094 ),
             ( 0.443, 0.141, 0.208 ),
             ( 0.000, 0.125, 0.792 ),
             ( 0.396, 0.698, 1.000 ),
             ( 0.729, 1.000, 0.208 ),
             ( 1.000, 0.729, 0.604 ),
             ( 1.000, 0.875, 0.792 ),
             ( 0.557, 0.635, 0.157 ),
             ( 0.459, 1.000, 0.208 ),
             ( 1.000, 0.604, 0.875 ),
             ( 0.396, 0.490, 1.000 ),
             ( 1.000, 0.729, 0.208 ),
             ( 1.000, 0.875, 0.604 ),
             ( 0.969, 1.000, 0.792 ),
             ( 0.906, 0.906, 0.094 ),
             ( 0.396, 1.000, 0.604 ),
             ( 0.475, 0.525, 0.286 ),
             ( 0.824, 0.620, 0.588 ),
             ( 0.604, 0.396, 1.000 ),
             ( 0.729, 0.714, 0.667 ),
             ( 0.714, 0.824, 0.875 ), 
             #( 0.700, 0.700, 0.700 )
             ]

    #cmap = mpl.colors.LinearSegmentedColormap.from_list(
    #    'Custom cmap', cmaplist, cmap.N)
    
   # cmap = mpl.colors.ListedColormap(['red',    'green',  'blue', 
   #                                   'cyan', 'red',   'green',  'blue', 
   #                                   'cyan',   'red',    'green',    'blue', 
   #                                   'cyan', 'red',  'green',  'blue', 
   #                                   'cyan',    'red',  'green', 'blue', 
   #                                   'cyan',   'red',  'green',  'blue', 
   #                                   'cyan',  'red',  'green',  'blue', 
   #                                   'cyan'    ])
    cmap = mpl.colors.ListedColormap(colors)
    bounds = np.linspace(0.5, 28.5, 29)
   
    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)
    return names, colors, cmap, norm, bounds

def get_site_data():
    """
    This will get the site data from ulrichs spreadsheet
    """

    filein = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/tab_GIS_all_Pliocene_Final.xlsx'
    dataframe = pd.read_excel(filein)
    lats_full = dataframe.loc[:, 'latitude'].to_numpy()
    lons_full = dataframe.loc[:, 'longitude'].to_numpy()
    biome_full = dataframe.loc[:, 'Raster_ID'].to_numpy()
   
    lat = []
    lon = []
    biome = []
    for i, latreq in enumerate(lats_full):
        if latreq > LAT_LIMIT:
            lat.append(latreq)
            lon.append(lons_full[i])
            biome.append(biome_full[i])

    return np.asarray(lat), np.asarray(lon), np.asarray(biome)

def get_MMM_output(latreq, lonreq, biomedata):
    """
    reads in the data and puts it in a array (M, N, 3) with the
    third dimensio being the colors
    """
    cube = iris.load_cube(FILENAME, 'biome')
    cube_lats = cube.coord('latitude').points
    cube_lons = cube.coord('longitude').points
    model_data = []
    model_diff = []

    for i, lat in enumerate(latreq):
        lon = lonreq[i]
        if lon < 0.:
            lon = lon + 360.

        latix = np.abs(cube_lats - lat).argmin()
        lonix = np.abs(cube_lons - lon).argmin()

       # print(i, lat)
        if lat >  LAT_LIMIT and math.isfinite(cube.data[latix,lonix]):
            model_data.append([latreq[i], lonreq[i], cube.data[latix,lonix], 
                                 biomedata[i]])

            if cube.data[latix,lonix] != biomedata[i]:
                model_diff.append([latreq[i], lonreq[i],cube.data[latix,lonix], 
                                   biomedata[i]])
                print('diff',latreq[i],lonreq[i],cube.data[latix,lonix],biomedata[i])

    print('model diff',np.shape(np.asarray(model_diff)),
              ' model data', np.shape(np.asarray(model_data)))
    sys.exit(0)
  
    return cube, np.asarray(model_data), np.asarray(model_diff)
      
def plot_data(mycmap, names, model_results, model_diff,
              model_cube):
    """
    plots the biomes
    model_results is lat, lon, model_data, biome_data
    """
    # set polar stereographic axis
    fig = plt.figure(figsize=(11.0, 11.0))
    ax1 = fig.add_subplot(2,2,1, projection = ccrs.NorthPolarStereo())
    ax1.set_extent([-180, 180, 50, 90], ccrs.PlateCarree())
    ax1.coastlines()
    ax1.gridlines()

    # Compute a circle in axes coordinates, which we can use as a boundary
    # for the map. We can pan/zoom as much as we like - the boundary will be
    # permanently circular.
    theta = np.linspace(0, 2*np.pi, 100)
    center, radius = [0.5, 0.5], 0.5
    verts = np.vstack([np.sin(theta), np.cos(theta)]).T
    circle = mpath.Path(verts * radius + center)

    ax1.set_boundary(circle, transform=ax1.transAxes)
    
   
    # plot the data (confusingly this is stored in model_results[:,3}
    V = np.arange(1,29,1)
    print(V)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
  
    cs = plt.scatter(model_results[:,1], model_results[:,0], 
                     c=model_results[:,3],  marker='o', s=100,
                     norm = norm , cmap=mycmap, edgecolors='black',
                     transform=ccrs.Geodetic())

    plt.title('a) Biome reconstruction')

    # plot the model results
    ax2 = fig.add_subplot(2,2,3, projection = ccrs.NorthPolarStereo())
    ax2.set_extent([-180, 180, 50, 90], ccrs.PlateCarree())
    ax2.coastlines()
    ax2.gridlines()
    ax2.set_boundary(circle, transform=ax2.transAxes)
    cs = plt.scatter(model_results[:,1], model_results[:,0], 
                     c=model_results[:,2],  marker='o', s=100,
                     norm = norm , edgecolors='black',cmap=mycmap, 
                     transform=ccrs.Geodetic())

    plt.title('c) Modelled Biome at sites')

    # plot all of the modelled biomes
    ax3 = fig.add_subplot(2,2,2, projection = ccrs.NorthPolarStereo())
    ax3.set_extent([-180, 180, 50, 90], ccrs.PlateCarree())
    ax3.coastlines()
    ax3.gridlines()
    ax3.set_boundary(circle, transform=ax3.transAxes)
    # turn the iris Cube data structure into numpy arrays
    gridlons = model_cube.coord('longitude').contiguous_bounds()
    gridlats = model_cube.coord('latitude').contiguous_bounds()
    modeldata = model_cube.data
    print(modeldata)

    cs = plt.pcolormesh(gridlons, gridlats, modeldata,
                   cmap=mycmap, norm=norm, 
                     transform=ccrs.PlateCarree())
    plt.title('b) Modelled biomes')



    # plot the modelled biome where it is different from data
    #model diff is[lat, lon, model_data[i], biome_data[i]]

    ax4 = fig.add_subplot(2,2,4, projection = ccrs.NorthPolarStereo())
    ax4.set_extent([-180, 180, 50, 90], ccrs.PlateCarree())
    ax4.coastlines()
    ax4.gridlines()
    ax4.set_boundary(circle, transform=ax4.transAxes)
    cs = plt.scatter(model_diff[:,1], model_diff[:,0], c=model_diff[:,2], 
                     marker=MarkerStyle('o', fillstyle='left'),
                     edgecolors='black',
                     s=150, 
                     norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
    cs = plt.scatter(model_diff[:,1], model_diff[:,0], c=model_diff[:,3], 
                     marker=MarkerStyle('o', fillstyle='right'),
                     edgecolors='black',
                     s=150, 
                     norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

    plt.title('d) Model-data mismatch at sites')



    # adjust plots so we have room for colorbar
    fig.subplots_adjust(bottom=0.05, top = 0.95, left = 0.05, right = 0.70)

    # do colorbar
    cbar_ax = fig.add_axes([0.75, 0.05, 0.05, 0.9])
    cbar = fig.colorbar(cs, cax= cbar_ax, ticks=V+0.5, )
    cbar.ax.set_yticklabels(names)
    cbar.ax.tick_params (labelsize=9)
    cbar.ax.invert_yaxis()
   
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/biome_dmc_by_site_'+ ABS_ANOM + '.eps')
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/biome_dmc_by_site_'+ ABS_ANOM + '.png')
    plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/biome_dmc_by_site_'+ ABS_ANOM + '.pdf')
    plt.close()

def main():
    """
    driver to get biomes
    """

    print('j1')
    biome_names, colors, cmap, norm, bounds= get_biome_details()
    print('j2')

    lats_data, lons_data, biomes_data = get_site_data()
    
    (model_cube, biomes_model, 
    biomes_diff) = get_MMM_output(lats_data, lons_data, biomes_data)

    # plot data and model results
    plot_data(cmap, biome_names, 
              biomes_model, biomes_diff, model_cube)
   

    print('prog finished')
    
 

FILESTART = '/nfs/hera1/earjcti/regridded/'
MODELNAME = 'BIOME4' # this will be MMM
ABS_ANOM = 'Anom'
LAT_LIMIT = 55  # only deal with stuff polewards of 55N
#FILESTART = '/nfs/see-fs-02_users/earjcti/BIOME4/biome4_pliomip2/'
#MODELNAME = ''
#ABS_ANOM = ''
if MODELNAME == 'BIOME4':
    midfname = MODELNAME
else:
    midfname =  MODELNAME + '/biome4/' 
   
if ABS_ANOM == 'Anom':
    FILENAME = FILESTART + midfname + '/biome4out_anomaly.nc'
if ABS_ANOM == 'Abs':
    FILENAME = FILESTART + midfname + '/biome4out_absolute.nc'
if ABS_ANOM == '':
    FILENAME = FILESTART + midfname + 'biome4out.nc'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/plot_toa_insolation_by_lat_and_month.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created Sept 2021 by Julia

This program will plot the TOA insolation for the modern.

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import matplotlib.ticker as ticker
import netCDF4

import sys

FILESTART = '/nfs/hera1/earjcti/um/tenvo/pd/tenvoa@pdt39'
MONTHS = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
FIELD = 'INCOMING SW RAD FLUX (TOA): ALL TSS'
NMONTHS = len(MONTHS)

for i, month in enumerate(MONTHS):
    cube = iris.load_cube(FILESTART + month + '.nc', 
                          'INCOMING SW RAD FLUX (TOA): ALL TSS')
    cube_lat_globe = iris.util.squeeze(cube.collapsed('longitude', iris.analysis.MEAN))
    cube_lat_mean = cube_lat_globe[0:15, :]
    cube_insol_55N = cube_lat_globe.extract(iris.Constraint(latitude=55.0))
    lats = cube_lat_mean.coord('latitude').points
    nlats = len(lats)
    if i == 0:
        toa_incom = np.zeros((nlats, NMONTHS))
        toa_55 = np.zeros(NMONTHS)

    toa_incom[:,i] = cube_lat_mean.data
    toa_55[i] = cube_insol_55N.data
   

# fraction of maximum insolation that occurs in that month
toa_frac = np.zeros((nlats, NMONTHS))  
monthly_mean = np.mean(toa_incom,axis=0)
for j, lat in enumerate(lats):
    toa_frac[j, :] = toa_incom[j, :] / np.sum(toa_incom[j, :])
for i, month in enumerate(MONTHS):
   # toa_frac[:, i] = toa_incom[:, i] / monthly_mean[i]
    toa_frac[:, i] = toa_incom[:, i] / toa_55[i]


print('np.mean axis 0',np.mean(toa_incom,axis=0))
print('np.mean axis 1',np.mean(toa_incom,axis=1))
print(toa_incom[:,0])

# plot
ax = plt.subplot(111)
V = np.arange(0,2.2,0.2)
#im = plt.pcolormesh(MONTHS, lats+1.25, toa_frac, vmin=0, vmax=2 ,cmap='RdYlBu_r')
im = plt.imshow(toa_frac, vmin=0, vmax=1.2, cmap='PuBuGn',aspect='auto')
cbar = plt.colorbar(im,orientation='horizontal')
plt.title('Fraction of 55N insolation occuring at each latitude')
plt.ylabel('latitude')
plt.xlabel('month')
ax.set_xticks(np.arange(0,12,1))
ax.set_xticklabels(MONTHS)
ax.set_yticks(np.arange(0,16,2))
ax.set_yticklabels(['90N','85N','80N','75N','70N','65N','60N','55N'])
#plt.xticks(MONTHS)
#ax.xaxis.set_major_locator(ticker.IndexLocator(base=1.0,offset=0.5))
#ax.yaxis.set_major_locator(ticker.IndexLocator(base=5.0,offset=1.25))
#plt.yticks(lats[:-1]+0.5)
plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/NH_insolation.eps')
plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/NH_insolation.png')
plt.savefig('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/vegetation/NH_insolation.pdf')
plt.close()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/seasonal_dmc_alternative_sites.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import netCDF4

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array

def get_lsm_names(model, period):
    """
    gets the names for each of the land sea masks
    period 0 =e280, period 1 = eoi400
    """
    if model == 'CESM2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'COSMOS':
        lsm = ["/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/E280_et_al/E280.slf.atm.nc", "/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/Eoi400_et_al/Eoi400.slf.atm.nc"]
        fieldlsm = "SLF"

    if model == 'EC-Earth3.3':
        lsm =  [DATABASE + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc',
                DATABASE + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc']
        fieldlsm = 'Land/sea mask'

    if model == 'CESM1.2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if model   ==  'MIROC4m':
        lsm = [DATABASE + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc', 
               DATABASE + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc']
        fieldlsm = "sftlf"

    if model  == 'HadCM3':
        lsm = [DATABASE+'LEEDS/HadCM3/e280/qrparm.mask.nc',
               DATABASE+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc']
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if model == 'CCSM4':
        lsm = [DATABASE + 'NCAR/b40.B1850.f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc', DATABASE + 'NCAR/b40.B1850.f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'CCSM4-Utr':
        lsm = [DATABASE + 'Utrecht/CESM1.0.5/E280/land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC_control_r1i1p1f1_gn.nc', DATABASE + 'Utrecht/CESM1.0.5/Eoi400/land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_f19g16_NESSC_control_r1i1p1f1_gn.nc']
        fieldlsm = 'LANDMASK[D=1]'
  
    if model == 'CCSM4-UoT':
        start = DATABASE + 'UofT/UofT-CCSM4/'
        lsm = [start + 'for_julia/E_mask.nc', start + 'for_julia/Eoi_mask.nc']
        fieldlsm = 'gridbox land fraction'
      
    if model == 'NorESM-L':
       lsm = [DATABASE + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc',
              DATABASE + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc']
       fieldlsm = 'Fraction of sfc area covered by land'


    if model  == 'MRI2.3':
        lsm = [DATABASE + 'MRI-CGCM2.3/sftlf.nc', 
               DATABASE + 'MRI-CGCM2.3/sftlf.nc']
        fieldlsm = 'landsea mask [0 - 1]'


    if model  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        lsm = [start + 'e280/NASA-GISS_PIctrl_all_fland.nc',
               start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc']
        fieldlsm = 'fland'

    if model == 'NorESM1-F':
        lsm = [DATABASE + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc',
               DATABASE + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc']
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if model == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        lsm = [start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc',
              start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc']
        fieldlsm = 'land_area_fraction'

    if model == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if model == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',
               start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if model == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        lsm = [start + 'hadgem3.mask.nc', start + 'hadgem3.mask.nc']
        fieldlsm = 'land_binary_mask'
            
            
    return lsm[period], fieldlsm



###########################
def get_land_sea_mask(model, period):
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d


    lsm, fieldlsm = get_lsm_names(model,period)

    ############################################
    if model == 'IPSLCM5A' or model == 'IPSLCM5A2':
        lsm_cube = get_ipsl_lsm(lsm, fieldlsm)
    elif model == 'HadGEM3':
        f = netCDF4.Dataset(lsm, "r")
        print(f.variables['longitude'])
        longitude = iris.coords.DimCoord(f.variables['longitude'], 
                             standard_name = 'longitude', units='degrees')
        latitude = iris.coords.DimCoord(f.variables['latitude'], 
                             standard_name = 'latitude', units='degrees')
        lsm_cube = iris.cube.Cube(np.squeeze(f.variables['lsm'][:]),
                             long_name='lsm', var_name='lsm', units=None, 
                             attributes=None, cell_methods=None, 
                             dim_coords_and_dims=[(latitude,0), (longitude,1)])
     
    else:
        lsm_cube = iris.util.squeeze(iris.load_cube(lsm, fieldlsm))
     
    lsm_cube2 = change_to_2d(lsm_cube)
   
   
    if model == 'IPSLCM6A':
        lsm_cube2.data = lsm_cube2.data / 100.0
       

    # regrid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsm_cube3 = lsm_cube2.regrid(cubegrid, iris.analysis.Linear())
   

    lsm_cube3.var_name = 'land_mask'
    lsm_cube3.long_name = 'land_mask'
    
    return lsm_cube3

def check_lsm(lsm_lons, lsm_lats, lsm_data, latrq, lonrq):
    """
    if our model is a sea point then set index to nan
    """

    lat_ix = (np.abs(lsm_lats - latrq)).argmin()
    lon_ix = (np.abs(lsm_lons - lonrq)).argmin()
       
    if lsm_data[lat_ix, lon_ix] <  0.5:
        # check to south, north, east, west
#        print(lsm_data[lat_ix - 1, lon_ix],lsm_data[lat_ix + 1, lon_ix],lsm_data[lat_ix, lon_ix-1],lsm_data[lat_ix, lon_ix+1], lsm_data[lat_ix - 1, lon_ix - 1],lsm_data[lat_ix + 1, lon_ix+1 ],lsm_data[lat_ix + 1, lon_ix-1],lsm_data[lat_ix-1, lon_ix+1])
#        if lsm_data[lat_ix - 1, lon_ix] >  0.5:
#            lat_ix = lat_ix -1
#        elif lsm_data[lat_ix + 1, lon_ix] >  0.5:
#            lat_ix = lat_ix + 1
#        elif lsm_data[lat_ix, lon_ix - 1] > 0.5:
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix, lon_ix + 1] > 0.5:
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix + 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix + 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix - 1
       # elif lsm_data[lat_ix, lon_ix - 2] > 0.5:
       #     lon_ix = lon_ix - 2
       # elif lsm_data[lat_ix, lon_ix + 2] > 0.5:
       #     lon_ix = lon_ix + 2
       # elif lsm_data[lat_ix, lon_ix - 3] > 0.5:
       #     lon_ix = lon_ix - 3
       # elif lsm_data[lat_ix, lon_ix + 3] > 0.5:
       #     lon_ix = lon_ix + 3
       # elif lsm_data[lat_ix - 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix -2
       # elif lsm_data[lat_ix + 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix + 2
      
       
  #      else:
            lat_ix = np.nan
            lon_ix = np.nan
            
       # print('new',lsm_data[lat_ix, lon_ix], lat_ix, lon_ix, latrq, lonrq, lsm_lons[lon_ix], lsm_lats[lat_ix])
       # sys.exit(0)

            
        

    return lat_ix, lon_ix

def get_single_model(model, latreq, lonreq, period):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    # get lsm
    if period == 'E280':
        lsm_cube  = get_land_sea_mask(model, 0)
    if period == 'EOI400':
        lsm_cube  = get_land_sea_mask(model, 1)

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + period + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_minval_array = np.zeros(nsites)
    plio_maxval_array = np.zeros(nsites)

    plio_cube_lats = plio_cube.coord('latitude').points
    plio_cube_lons = plio_cube.coord('longitude').points

    lsm_cube_lats = lsm_cube.coord('latitude').points
    lsm_cube_lons = lsm_cube.coord('longitude').points
    lsm_cube_data = lsm_cube.data
   
  
    if plio_cube_lats.any() != lsm_cube_lats.any():
        print('data cube does not match lsm lat')
    if plio_cube_lats.any() != lsm_cube_lons.any():
        print('data cube does not match lsm lat')

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        (lat_ix, 
         lon_ix) = check_lsm(lsm_cube_lons, lsm_cube_lats, 
                                    lsm_cube_data, latreq[i], modlon)

        if np.isfinite(lat_ix):
            plio_array = plio_cube.data[:, lat_ix, lon_ix]
        else:
            plio_array = np.zeros(12)
            plio_array[:] = np.nan
        plio_minval_array[i] = np.min(plio_array)
        plio_maxval_array[i] = np.max(plio_array)
   
    return plio_minval_array, plio_maxval_array

 
def get_land_obs():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT CA, max WMMT CA,
    # min CMMT CA, max CMMT CA

    # LP is late pliocene, EP is early pliocene
    # these are late pliocene
    # these are from popova et al 2012 using coexistence approach.
    sitedata.append(['Mirny (LP)', 55, 82, 18.8, 24.6, -0.3, 0.7])
    sitedata.append(['Merkutlinskiy (LP)', 56, 72, 17.3, 23.8, -3.8, 6.2])
    sitedata.append(['Kabinet (LP)', 55, 80, 21.6, 24.4, -4.4, 4.6])
    sitedata.append(['Delyankir (LP)', 63, 133, 18.9, 24.9, -6.9, 1.3])
    sitedata.append(['Chernoluche (LP)', 55, 73, 19.6, 20.3, -5.9, 0.7])
    sitedata.append(['Blizkiy (LP)', 64, 162, 15.6, 23.3, -12.8, 5.2])
    sitedata.append(['42km (LP)', 55, 80, 21.6, 23.3, -4.4, 0.7])
    sitedata.append(['Tnekveem (EP)', 66, 177, 18.9, 25.6, -11.8, 5.8])
    sitedata.append(['Hydzhak (EP)', 63, 147, 18.8, 24.9, -8.7, 1.3])

    # lake baikal is from demske 2002.
    sitedata.append(['Lake Baikal (3.57-3.15Ma)', 56, 108, 13.0, 24.0, -15, 5])
    
   

   
    sites = []
    lats = []
    lons = []
    WMMT_veg_min = []
    WMMT_veg_max = []
    CMMT_veg_min = []
    CMMT_veg_max = []
    
    for site in sitedata:
        sites.append(site[0])
        lats.append(site[1])
        lons.append(site[2])
        WMMT_veg_min.append(site[3])
        WMMT_veg_max.append(site[4])
        CMMT_veg_min.append(site[5])
        CMMT_veg_max.append(site[6])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return  (labels, lats, lons, np.asarray(WMMT_veg_min),
             np.asarray(WMMT_veg_max), np.asarray(CMMT_veg_min),
             np.asarray(CMMT_veg_max))


def plot_figure(WMMT_veg_min, WMMT_veg_max, CMMT_veg_min, CMMT_veg_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                labels):

    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    fig1 = plt.figure(figsize=[10.0, 10])
    ax1 = plt.axes(frameon=False)
    ax1.get_xaxis().tick_top()
    ax1.axes.get_yaxis().set_visible(False)

    nsites=len(WMMT_veg_min)
    yarray = np.arange(1, nsites+1, 1)
 
    # plot warm and cold month temperature
    ax1.hlines(y=yarray, xmin=WMMT_veg_min, xmax= WMMT_veg_max,color='red')
    ax1.hlines(y=yarray, xmin=CMMT_veg_min, xmax= CMMT_veg_max,color='blue')
    plt.scatter((WMMT_veg_max + WMMT_veg_min) / 2.0, 
                yarray, color='red', marker='^',
                label='CA WMMT')
    plt.scatter((CMMT_veg_max + CMMT_veg_min) / 2.0, 
                 yarray, color='blue', marker='^',
                label =  'CA CMMT')
  
       
    # try plotting axis
  
    for j in range(0, nsites):
        plt.text(-35.0, yarray[j], labels[j], ha='right')

     

    
    plt.scatter(mmm_WMT, yarray + 0.2, color='black', s=50)
    plt.scatter(mmm_WMT, yarray + 0.2, color='red', 
                s=25, label='MMM WMMT')
    plt.scatter(mmm_CMT, yarray + 0.2, color='black', s=50)
    plt.scatter(mmm_CMT, yarray + 0.2, color='blue', 
                s=25, label='MMM CMMT')
  
    plt.ylim(12, -0.5)

    # plot individual models for pliocene
    for i in range(0, len(MODELNAMES)):
        if i == 0:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='red', 
                        marker = 'x', s=10, label='models WMMT')
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='blue', 
                        marker = 'x', s=10, label='models CMMT')
        else:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='red', 
                        marker = 'x',s=10)
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='blue', 
                        marker = 'x', s=10)
        
    
    
    plt.legend(loc='lower center', ncol=3)

    fig1.suptitle('a) Pliocene DMC', x=0.1, ha='left', fontsize=16)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/seasonal_dmc_alternative_sites.eps')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/seasonal_dmc_alternative_sites.png')
    plt.savefig(fileout)
    plt.close()



   

  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    d) plots change in seasonal cycle at MI, BP and LB
    """

   
    # get land observations and cru temperature at land points 
    (sites, land_lats, land_lons, WMMT_veg_min, WMMT_veg_max, 
     CMMT_veg_min, CMMT_veg_max) =  get_land_obs()
    
    
    
    # get ind models data
    print(land_lons)
    all_models_plio_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_seas_cyc_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_CMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_WMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'EOI400')
        all_models_plio_WMT[:, i] = ind_WMT
        all_models_plio_CMT[:, i] = ind_CMT

        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'E280')
        all_models_pi_WMT[:, i] = ind_WMT
        all_models_pi_CMT[:, i] = ind_CMT

        all_models_seas_cyc_anom[:, i] = (
            (all_models_plio_WMT[:, i] - all_models_plio_CMT[:, i]) -
            (all_models_pi_WMT[:, i] - all_models_pi_CMT[:, i]))
        
        all_models_CMMT_anom[:, i] = (all_models_plio_CMT[:, i] -
                                      all_models_pi_CMT[:, i])

        all_models_WMMT_anom[:, i] = (all_models_plio_WMT[:, i] -
                                      all_models_pi_WMT[:, i])



    mmm_WMT = np.nanmean(all_models_plio_WMT, axis=1)
    mmm_CMT = np.nanmean(all_models_plio_CMT, axis=1)



    # plot data
  
    plot_figure(WMMT_veg_min, WMMT_veg_max, 
                CMMT_veg_min, CMMT_veg_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites)
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'

MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
             ,  'MRI2.3'
              ]

#MODELNAMES = ['CESM2']

NSAT_MMM_FILE = (FILESTART + 
                 'regridded100/NearSurfaceTemperature_multimodelmean.nc')

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/seasonal_dmc_plot.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import netCDF4

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array

def get_lsm_names(model, period):
    """
    gets the names for each of the land sea masks
    period 0 =e280, period 1 = eoi400
    """
    if model == 'CESM2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'COSMOS':
        lsm = ["/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/E280_et_al/E280.slf.atm.nc", "/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/Eoi400_et_al/Eoi400.slf.atm.nc"]
        fieldlsm = "SLF"

    if model == 'EC-Earth3.3':
        lsm =  [DATABASE + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc',
                DATABASE + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc']
        fieldlsm = 'Land/sea mask'

    if model == 'CESM1.2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if model   ==  'MIROC4m':
        lsm = [DATABASE + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc', 
               DATABASE + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc']
        fieldlsm = "sftlf"

    if model  == 'HadCM3':
        lsm = [DATABASE+'LEEDS/HadCM3/e280/qrparm.mask.nc',
               DATABASE+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc']
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if model == 'CCSM4':
        lsm = [DATABASE + 'NCAR/b40.B1850.f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc', DATABASE + 'NCAR/b40.B1850.f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'CCSM4-Utr':
        lsm = [DATABASE + 'Utrecht/CESM1.0.5/E280/land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC_control_r1i1p1f1_gn.nc', DATABASE + 'Utrecht/CESM1.0.5/Eoi400/land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_f19g16_NESSC_control_r1i1p1f1_gn.nc']
        fieldlsm = 'LANDMASK[D=1]'
  
    if model == 'CCSM4-UoT':
        start = DATABASE + 'UofT/UofT-CCSM4/'
        lsm = [start + 'for_julia/E_mask.nc', start + 'for_julia/Eoi_mask.nc']
        fieldlsm = 'gridbox land fraction'
      
    if model == 'NorESM-L':
       lsm = [DATABASE + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc',
              DATABASE + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc']
       fieldlsm = 'Fraction of sfc area covered by land'


    if model  == 'MRI2.3':
        lsm = [DATABASE + 'MRI-CGCM2.3/sftlf.nc', 
               DATABASE + 'MRI-CGCM2.3/sftlf.nc']
        fieldlsm = 'landsea mask [0 - 1]'


    if model  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        lsm = [start + 'e280/NASA-GISS_PIctrl_all_fland.nc',
               start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc']
        fieldlsm = 'fland'

    if model == 'NorESM1-F':
        lsm = [DATABASE + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc',
               DATABASE + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc']
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if model == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        lsm = [start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc',
              start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc']
        fieldlsm = 'land_area_fraction'

    if model == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if model == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',
               start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if model == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        lsm = [start + 'hadgem3.mask.nc', start + 'hadgem3.mask.nc']
        fieldlsm = 'land_binary_mask'
            
            
    return lsm[period], fieldlsm


def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.mean_month.nc')
    cube = iris.load_cube(crufile)
    
    
    cru_min_temp = np.zeros(len(lats))
    cru_max_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        if lons[i] > 180.:
            lon = lons[i]-360.
        else:
            lon = lons[i]
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lon)).argmin()
        
   
        cru_temp = cube.data[:, lat_ix, lon_ix]
        if np.isfinite(cru_temp[0]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[:, lat_ix + 1, lon_ix],
                        cube.data[:, lat_ix - 1, lon_ix],
                        cube.data[:, lat_ix, lon_ix + 1],
                        cube.data[:, lat_ix, lon_ix -1],
                        ]
            cru_temp = np.ma.mean(surround, axis=0)
            cru_max_temp[i] = np.max(cru_temp)
        cru_min_temp[i] = np.min(cru_temp)
      
     
    return cru_min_temp, cru_max_temp

###########################
def get_land_sea_mask(model, period):
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d


    lsm, fieldlsm = get_lsm_names(model,period)

    ############################################
    if model == 'IPSLCM5A' or model == 'IPSLCM5A2':
        lsm_cube = get_ipsl_lsm(lsm, fieldlsm)
    elif model == 'HadGEM3':
        f = netCDF4.Dataset(lsm, "r")
        print(f.variables['longitude'])
        longitude = iris.coords.DimCoord(f.variables['longitude'], 
                             standard_name = 'longitude', units='degrees')
        latitude = iris.coords.DimCoord(f.variables['latitude'], 
                             standard_name = 'latitude', units='degrees')
        lsm_cube = iris.cube.Cube(np.squeeze(f.variables['lsm'][:]),
                             long_name='lsm', var_name='lsm', units=None, 
                             attributes=None, cell_methods=None, 
                             dim_coords_and_dims=[(latitude,0), (longitude,1)])
     
    else:
        lsm_cube = iris.util.squeeze(iris.load_cube(lsm, fieldlsm))
     
    lsm_cube2 = change_to_2d(lsm_cube)
   
   
    if model == 'IPSLCM6A':
        lsm_cube2.data = lsm_cube2.data / 100.0
       

    # regrid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsm_cube3 = lsm_cube2.regrid(cubegrid, iris.analysis.Linear())
   

    lsm_cube3.var_name = 'land_mask'
    lsm_cube3.long_name = 'land_mask'
    
    return lsm_cube3

def check_lsm(lsm_lons, lsm_lats, lsm_data, latrq, lonrq):
    """
    if our model is a sea point then set index to nan
    """

    lat_ix = (np.abs(lsm_lats - latrq)).argmin()
    lon_ix = (np.abs(lsm_lons - lonrq)).argmin()
       
    if lsm_data[lat_ix, lon_ix] <  0.5:
        # check to south, north, east, west
#        print(lsm_data[lat_ix - 1, lon_ix],lsm_data[lat_ix + 1, lon_ix],lsm_data[lat_ix, lon_ix-1],lsm_data[lat_ix, lon_ix+1], lsm_data[lat_ix - 1, lon_ix - 1],lsm_data[lat_ix + 1, lon_ix+1 ],lsm_data[lat_ix + 1, lon_ix-1],lsm_data[lat_ix-1, lon_ix+1])
#        if lsm_data[lat_ix - 1, lon_ix] >  0.5:
#            lat_ix = lat_ix -1
#        elif lsm_data[lat_ix + 1, lon_ix] >  0.5:
#            lat_ix = lat_ix + 1
#        elif lsm_data[lat_ix, lon_ix - 1] > 0.5:
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix, lon_ix + 1] > 0.5:
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix + 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix + 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix - 1
       # elif lsm_data[lat_ix, lon_ix - 2] > 0.5:
       #     lon_ix = lon_ix - 2
       # elif lsm_data[lat_ix, lon_ix + 2] > 0.5:
       #     lon_ix = lon_ix + 2
       # elif lsm_data[lat_ix, lon_ix - 3] > 0.5:
       #     lon_ix = lon_ix - 3
       # elif lsm_data[lat_ix, lon_ix + 3] > 0.5:
       #     lon_ix = lon_ix + 3
       # elif lsm_data[lat_ix - 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix -2
       # elif lsm_data[lat_ix + 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix + 2
      
       
  #      else:
            lat_ix = np.nan
            lon_ix = np.nan
            
       # print('new',lsm_data[lat_ix, lon_ix], lat_ix, lon_ix, latrq, lonrq, lsm_lons[lon_ix], lsm_lats[lat_ix])
       # sys.exit(0)

            
        

    return lat_ix, lon_ix

def get_single_model(model, latreq, lonreq, period):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    # get lsm
    if period == 'E280':
        lsm_cube  = get_land_sea_mask(model, 0)
    if period == 'EOI400':
        lsm_cube  = get_land_sea_mask(model, 1)

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + period + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_minval_array = np.zeros(nsites)
    plio_maxval_array = np.zeros(nsites)

    plio_cube_lats = plio_cube.coord('latitude').points
    plio_cube_lons = plio_cube.coord('longitude').points

    lsm_cube_lats = lsm_cube.coord('latitude').points
    lsm_cube_lons = lsm_cube.coord('longitude').points
    lsm_cube_data = lsm_cube.data
   
  
    if plio_cube_lats.any() != lsm_cube_lats.any():
        print('data cube does not match lsm lat')
    if plio_cube_lats.any() != lsm_cube_lons.any():
        print('data cube does not match lsm lat')

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        (lat_ix, 
         lon_ix) = check_lsm(lsm_cube_lons, lsm_cube_lats, 
                                    lsm_cube_data, latreq[i], modlon)

        if np.isfinite(lat_ix):
            plio_array = plio_cube.data[:, lat_ix, lon_ix]
        else:
            plio_array = np.zeros(12)
            plio_array[:] = np.nan
        plio_minval_array[i] = np.min(plio_array)
        plio_maxval_array[i] = np.max(plio_array)
   
    return plio_minval_array, plio_maxval_array

 
def get_land_obs():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle, modern obs WMMT, modern obs CMMT
    #sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
    #                np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 
    #                 8.0, np.nan])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     np.nan, np.nan, -1.67, 1.07, np.nan, np.nan, 
                     15.3, -17.4])
    sitedata.append(['Lake El\'gygytgyn', 67, 172, -36.8, -30.4,
                    np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 
                     8.0, np.nan])
    sitedata.append(['Near Meighen Island', 77.5, 261, 19.6, 20.5,
                     11.5, 13.5, -11.6, -11.4,
                     -33.0, -18.5, 4.1, -42.5])
    sitedata.append(['Beaver Pond', 79, 278, 18.4, 20.9,
                     np.nan, np.nan, -12.2, -11.5,
                     np.nan, np.nan,  7.1, -39.7])
    sitedata.append(['Flyes Leaf Bed', 79, 278, 19.7, 21.1,
                     np.nan, np.nan, -12.8, -9.1,
                     np.nan, np.nan,  7.1, -39.7])
    sitedata.append(['Lost Chicken Mine', 64, 218, 12.0, 12.0, 
                     13.5, 16.0, -2.0, -2.0, -27.75, -19.25, 15.3, -25.1])
   

    sites_relaxed_coex = []
    # sitename WMMTmin WMMT max, CMMT min, CMMTmax, BMA WMMTmin, BMA_WMMTmax,
    # BMA_CMMTmin, BMA_CMMTmax
    sites_relaxed_coex.append(['Lake Baikal', np.nan, np.nan, np.nan, np.nan,
                               np.nan, np.nan, np.nan, np.nan])
    sites_relaxed_coex.append(['Lake El\'gygytgyn', np.nan, np.nan, np.nan, np.nan, 15.0, 16.0, -36.8, -30.4 ])
    sites_relaxed_coex.append(['Near Meighen Island', 18.1, 22.8, -21.7, -7.9,
                               np.nan, np.nan, np.nan, np.nan])
    sites_relaxed_coex.append(['Beaver Pond', 18.1, 22.4, -21.7, -8.1,
                               np.nan, np.nan, np.nan, np.nan])
    sites_relaxed_coex.append(['Flyes Leaf Bed', 18.1, 22.7, -16.9, -6.4,
                               np.nan, np.nan, np.nan, np.nan])
    sites_relaxed_coex.append(['Lost Chicken Mine', np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
             

    # lcm observations from elias and matthews 2002
    # lake baikal is from what ulrich sent me.  MI and BP and FLBfrom Fletcher
    # Lake E from Brigette-greeme mean july temp of +8 and average winter lows of 35degC
    # Meighen Island is 'upper'
    
    sites = []
    lats = []
    lons = []
    WMMT_veg_min = []
    WMMT_veg_max = []
    WMMT_coex_min = []
    WMMT_coex_max = []
    WMMT_BMA_min = []
    WMMT_BMA_max = []
    WMMT_beetle_min = []
    WMMT_beetle_max = []
    WMMT_modern_obs = []
    CMMT_veg_min = []
    CMMT_veg_max = []
    CMMT_coex_min = []
    CMMT_coex_max = []
    CMMT_BMA_min = []
    CMMT_BMA_max = []
    CMMT_beetle_min = []
    CMMT_beetle_max = []
    CMMT_modern_obs = []
    
    for i, info in enumerate(sitedata):
        info2 = sites_relaxed_coex[i]                          
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_veg_min.append(info[3])
        WMMT_veg_max.append(info[4])
        WMMT_beetle_min.append(info[5])
        WMMT_beetle_max.append(info[6])
        CMMT_veg_min.append(info[7])
        CMMT_veg_max.append(info[8])
        CMMT_beetle_min.append(info[9])
        CMMT_beetle_max.append(info[10])
        WMMT_modern_obs.append(info[11])
        CMMT_modern_obs.append(info[12])
        if info[0] == info2[0]:
           WMMT_coex_min.append(info2[1])
           WMMT_coex_max.append(info2[2])
           CMMT_coex_min.append(info2[3])
           CMMT_coex_max.append(info2[4])
           WMMT_BMA_min.append(info2[5])
           WMMT_BMA_max.append(info2[6])
           CMMT_BMA_min.append(info2[7])
           CMMT_BMA_max.append(info2[8])
        else:
           print,'check names match',info[0],info2[0]
           sys.exit(0)      
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + '\n (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return  (labels, lats, lons, np.asarray(WMMT_veg_min),
             np.asarray(WMMT_veg_max), np.asarray(WMMT_beetle_min),
             np.asarray(WMMT_beetle_max), np.asarray(CMMT_veg_min),
             np.asarray(CMMT_veg_max), np.asarray( CMMT_beetle_min),
             np.asarray(CMMT_beetle_max), np.asarray(WMMT_modern_obs),
             np.asarray(CMMT_modern_obs), np.asarray(WMMT_coex_min),
             np.asarray(WMMT_coex_max),np.asarray(CMMT_coex_min),
             np.asarray(CMMT_coex_max),np.asarray(WMMT_BMA_min),
             np.asarray(WMMT_BMA_max),np.asarray(CMMT_BMA_min),
             np.asarray(CMMT_BMA_max))


def plot_figure(WMMT_veg_min, WMMT_veg_max, WMMT_beetle_min, WMMT_beetle_max,
                CMMT_veg_min, CMMT_veg_max, CMMT_beetle_min, CMMT_beetle_max,
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                labels, pi_ind, cru_min, cru_max,WMMT_coex_min,
                WMMT_coex_max,CMMT_coex_min,CMMT_coex_max,WMMT_BMA_min,
                WMMT_BMA_max,CMMT_BMA_min,CMMT_BMA_max):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """


    fig1 = plt.figure(figsize=[10.0, 8.0])
    ax1 = plt.axes(frameon=False)
    ax1.get_xaxis().tick_top()
    ax1.axes.get_yaxis().set_visible(False)

    nsites=len(WMMT_veg_min)
    yarray = np.arange(1, nsites+1, 1)
 
    # plot warm and cold month temperature
    ax1.hlines(y=yarray, xmin=WMMT_veg_min, xmax= WMMT_veg_max,color='red')
    ax1.hlines(y=yarray, xmin=CMMT_veg_min, xmax= CMMT_veg_max,color='blue')
    if pi_ind == 'y': # plot cru temps
        plt.scatter((WMMT_veg_max + WMMT_veg_min) / 2.0, 
                yarray, color='red', marker='^',
                label='modern WMMT at site')
        plt.scatter((CMMT_veg_max + CMMT_veg_min) / 2.0, 
                 yarray, color='blue', marker='^',
                label =  'modern CMMT at site')
  
        plt.scatter(cru_max, yarray, color='red', marker = 'v', label = 'CRU WMMT')
        plt.scatter(cru_min, yarray, color='blue', marker = 'v', label = 'CRU CMMT')   
    else:
        plt.scatter((WMMT_veg_max + WMMT_veg_min) / 2.0, 
                yarray, color='red', marker='^',
                label='paleovegetation WMMT')
        plt.scatter((CMMT_veg_max + CMMT_veg_min) / 2.0, 
                 yarray, color='blue', marker='^',
                label =  'paleovegetation CMMT')
  
        # put an arrow on LCM
        plt.arrow(CMMT_veg_max[5], yarray[5], -10, 0, linestyle='dotted', 
                  color='blue', head_width=0.1, head_length=0.5)
   
    # try plotting axis
  
    for j in range(0, nsites):
        if pi_ind == 'y':
           plt.text(20.0,
                 yarray[j], labels[j], ha='left')
        else:
            plt.text(-35.0,
                 yarray[j], labels[j], ha='right')

     

    
    plt.scatter(mmm_WMT, yarray + 0.2, color='black', s=50)
    plt.scatter(mmm_WMT, yarray + 0.2, color='red', 
                s=25, label='MMM WMMT')
    plt.scatter(mmm_CMT, yarray + 0.2, color='black', s=50)
    plt.scatter(mmm_CMT, yarray + 0.2, color='blue', 
                s=25, label='MMM CMMT')
  
    plt.ylim(8, -0.5)

    # plot individual models for pliocene
    for i in range(0, len(MODELNAMES)):
        if i == 0:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='red', 
                        marker = 'x', s=10, label='models WMMT')
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='blue', 
                        marker = 'x', s=10, label='models CMMT')
        else:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='red', 
                        marker = 'x',s=10)
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='blue', 
                        marker = 'x', s=10)
        
    if pi_ind == 'y':
        plt.xlim(-50, 25)
        plt.hlines(y=-0.45, xmin=-50., xmax=25., linewidth=0.5)
        plt.text(-45.0, 0.0, 'Cold Month Temperature (degC)', ha='left', color='blue')
        plt.text(20.0, 0.0, 'Warm Month Temperature (deg C)', ha='right', color='red') 
    else:
        plt.xlim(-40, 30)
        plt.hlines(y=-0.45, xmin=-40., xmax=30., linewidth=0.5)
        plt.text(-35.0, 0.0, 'Cold Month Temperature (degC)', ha='left', color='blue')
        plt.text(25.0, 0.0, 'Warm Month Temperature (deg C)', ha='right', color='red')
        # plot all beetle assemblage results
        plt.hlines(yarray, xmin=WMMT_beetle_min, 
               xmax=WMMT_beetle_max, color='green',linestyle='dashed',
               label='beetle assemblage data')
        plt.hlines(yarray, xmin=CMMT_beetle_min, 
               xmax=CMMT_beetle_max, color='green', linestyle='dashed')
        # plot coexistence results
        plt.hlines(yarray+0.1, xmin=WMMT_coex_min, 
               xmax=WMMT_coex_max, color='red',linestyle='dotted',
               label='relaxed coexistence approach data')
        plt.hlines(yarray+0.1, xmin=CMMT_coex_min, 
               xmax=CMMT_coex_max, color='blue', linestyle='dotted')
        # plot BMA results
        plt.scatter((WMMT_BMA_max + WMMT_BMA_min) / 2.0, 
                yarray, color='red', marker='v',
                label='WMMT BMA')
        plt.scatter((CMMT_BMA_max + CMMT_BMA_min) / 2.0, 
                yarray, color='blue', marker='v',
                label='WMMT BMA')
        plt.hlines(yarray, xmin=WMMT_BMA_min, 
               xmax=WMMT_BMA_max, color='red')
        plt.hlines(yarray, xmin=CMMT_BMA_min, 
               xmax=CMMT_BMA_max, color='blue')
    
    
    plt.legend(loc='lower center', ncol=3)

    if pi_ind == 'y':
        fig1.suptitle('b) modern/preindustrial DMC', x=0.1, ha='left', fontsize=16)
        fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_pi.eps')
        plt.savefig(fileout)
        fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_pi.png')
        plt.savefig(fileout)
    else:
        fig1.suptitle('a) Pliocene DMC', x=0.1, ha='left', fontsize=16)
        fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/seasonal_dmc_plot.eps')
        plt.savefig(fileout)
        fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot.png')
        plt.savefig(fileout)
    plt.close()



####################################################
def plot_chg_seas_cyc(WMMT_veg,CMMT_veg, all_models_plio_WMT, 
                      all_models_plio_CMT, all_models_pi_WMT,
                      all_models_pi_CMT,
                      sites, mmm_WMT, mmm_CMT, mmm_WMT_pi, mmm_CMT_pi,cru):
    """
    plots the change in the seasonal cycle between pi and mpwp
    """
    colors = {'CESM2' : 'green',
              'HadGEM3': 'green',
              'IPSLCM6A' : 'green', 
              'COSMOS': 'red', 
              'MIROC4m': 'red', 
              'HadCM3': 'green',
              'GISS2.1G' : 'black', 
              'CCSM4' : 'black', 
              'CCSM4-Utr' : 'black', 
              'CCSM4-UoT' : 'purple', 
              'NorESM-L' : 'purple', 
              'NorESM1-F' : 'purple',
              'MRI2.3' : 'red'}

    marker = {'CESM2' : '^',
              'HadGEM3': 'x',
              'IPSLCM6A' : 'o', 
              'COSMOS': '^', 
              'MIROC4m': 'x', 
              'HadCM3': 'o',
              'GISS2.1G' : '^', 
              'CCSM4' : 'x', 
              'CCSM4-Utr' : 'o', 
              'CCSM4-UoT' : '^', 
              'NorESM-L' : 'x', 
              'NorESM1-F' : 'o',
              'MRI2.3' : 'v',
              'EC-Earth3.3' : '^',
              'CESM1.2' : 'o'}

    print('julia')
    fig1 = plt.figure(figsize=[10.0, 8.0])
    ax1 = plt.subplot(211)
   # ax1.get_xaxis().tick_top()
    ax1.axes.get_xaxis().set_visible(False)

    xarray = np.arange(0,len(sites))
    plt.scatter(xarray, np.asarray(WMMT_veg) - np.asarray(CMMT_veg), 
                color='red', marker = 'x', s=20, label='veg data')
    plt.scatter(xarray, np.asarray(cru),color='black',marker = 's',
                label='cru')
    

    for i, model in enumerate(MODELNAMES):
        plt.scatter(xarray+(0.02 * i), np.asarray(all_models_plio_WMT[:,i])
                    -np.asarray(all_models_plio_CMT[:,i]), 
                    color=colors.get(model,'blue'), 
                    marker = marker.get(model,'x'),label=model)
   
    plt.xlim(-1, 8)
    plt.hlines(y=0, xmin=-1., xmax=6., linewidth=0.5)
    plt.vlines(x=-1, ymin=-25, ymax=7, linewidth=0.5)

    plt.legend(loc='right')

    for i in range(0, len(sites)):
        plt.text(xarray[i], 0, sites[i], ha='right',rotation=90)


    # plot change in seasonal cycle
    seas_chg = (np.asarray(all_models_plio_WMT) 
                - np.asarray(all_models_plio_CMT) 
                - np.asarray(all_models_pi_WMT)
                + np.asarray(all_models_pi_CMT))
    ax2 = plt.subplot(212)
    for i, model in enumerate(MODELNAMES):
        plt.scatter(xarray+(0.02 * i),seas_chg[:,i],
                    color=colors.get(model,'blue'), 
                    marker = marker.get(model,'x'),label=model)
        print(model, all_models_plio_WMT[3,i],all_models_plio_CMT[3,i],
              all_models_pi_WMT[3,i],all_models_pi_CMT[3,i])
    sys.exit(0)
    plt.hlines(y=0, xmin=-1., xmax=6., linewidth=0.5)
   

    plt.show()
            
   

  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    d) plots change in seasonal cycle at MI, BP and LB
    """

   
    # get land observations and cru temperature at land points 
    (sites, land_lats, land_lons, WMMT_veg_min, WMMT_veg_max, WMMT_beetle_min,
     WMMT_beetle_max, CMMT_veg_min, CMMT_veg_max, CMMT_beetle_min,
     CMMT_beetle_max, WMMT_modern_obs, CMMT_modern_obs,WMMT_coex_min,
     WMMT_coex_max,CMMT_coex_min,CMMT_coex_max,WMMT_BMA_min,
     WMMT_BMA_max,CMMT_BMA_min,CMMT_BMA_max) =  get_land_obs()
    
    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)

    
    
    # get ind models data
    print(land_lons)
    all_models_plio_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_seas_cyc_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_CMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_WMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'EOI400')
        all_models_plio_WMT[:, i] = ind_WMT
        all_models_plio_CMT[:, i] = ind_CMT

        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'E280')
        all_models_pi_WMT[:, i] = ind_WMT
        all_models_pi_CMT[:, i] = ind_CMT

        all_models_seas_cyc_anom[:, i] = (
            (all_models_plio_WMT[:, i] - all_models_plio_CMT[:, i]) -
            (all_models_pi_WMT[:, i] - all_models_pi_CMT[:, i]))
        
        all_models_CMMT_anom[:, i] = (all_models_plio_CMT[:, i] -
                                      all_models_pi_CMT[:, i])

        all_models_WMMT_anom[:, i] = (all_models_plio_WMT[:, i] -
                                      all_models_pi_WMT[:, i])



    mmm_WMT = np.nanmean(all_models_plio_WMT, axis=1)
    mmm_CMT = np.nanmean(all_models_plio_CMT, axis=1)
    mmm_WMT_pi = np.nanmean(all_models_pi_WMT, axis=1)
    mmm_CMT_pi = np.nanmean(all_models_pi_CMT, axis=1)


    print('MMM WMMT avg all sites',np.nanmean(mmm_WMT), 'all',mmm_WMT)
    print('veg data WMMT avg all sites',
          np.nanmean(WMMT_veg_min + WMMT_veg_max) / 2.0, 'all',
          (WMMT_veg_min + WMMT_veg_max) / 2.0)
    print('MMM CMMT avg all sites',np.nanmean(mmm_CMT), 'all',mmm_CMT)
    print('veg data CMMT avg all sites',
          np.nanmean(CMMT_veg_min + CMMT_veg_max) / 2.0, 'all',
          (CMMT_veg_min + CMMT_veg_max) / 2.0)

    # plot data
    dummy = np.zeros(len(WMMT_modern_obs))
    dummy[:] = np.nan
 
    plot_figure(WMMT_veg_min, WMMT_veg_max, WMMT_beetle_min, WMMT_beetle_max,
                CMMT_veg_min, CMMT_veg_max, CMMT_beetle_min, CMMT_beetle_max,
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'n', dummy, dummy,WMMT_coex_min,
                WMMT_coex_max,CMMT_coex_min,CMMT_coex_max,WMMT_BMA_min,
                WMMT_BMA_max,CMMT_BMA_min,CMMT_BMA_max)
    # plot a dmc for the pi
    plot_figure (WMMT_modern_obs, WMMT_modern_obs, dummy, dummy,
                 CMMT_modern_obs, CMMT_modern_obs, dummy, dummy,
                 all_models_pi_WMT, all_models_pi_CMT, mmm_WMT_pi, mmm_CMT_pi,
                 sites,'y', cru_min_temp, cru_max_temp,dummy, dummy,
                 dummy, dummy, dummy, dummy, dummy, dummy)

    # plot change in seasonal cycle
    plot_chg_seas_cyc((WMMT_veg_min + WMMT_veg_max)/2.0, 
                (CMMT_veg_min +  CMMT_veg_max) / 2.0, 
                all_models_plio_WMT, all_models_plio_CMT,
                all_models_pi_WMT, all_models_pi_CMT,
                      sites, mmm_WMT, mmm_CMT, mmm_WMT_pi, mmm_CMT_pi,
                (cru_max_temp - cru_min_temp) / 2.0   )

##########################################################
# main program

LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'

MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
           #  ,  'MRI2.3'
              ]

#MODELNAMES = ['CESM2']

NSAT_MMM_FILE = (FILESTART + 
                 'regridded100/NearSurfaceTemperature_multimodelmean.nc')

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/seasonal_dmc_plot_v2_beetle.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created Sept 2021 by Julia

This program is based on seasonal_dmc_plot_v2  but will plot the DMC for
Beetles instead of paleoveg

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import netCDF4

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array

def get_lsm_names(model, period):
    """
    gets the names for each of the land sea masks
    period 0 =e280, period 1 = eoi400
    """
    if model == 'CESM2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'COSMOS':
        lsm = ["/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/E280_et_al/E280.slf.atm.nc", "/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/Eoi400_et_al/Eoi400.slf.atm.nc"]
        fieldlsm = "SLF"

    if model == 'EC-Earth3.3':
        lsm =  [DATABASE + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc',
                DATABASE + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc']
        fieldlsm = 'Land/sea mask'

    if model == 'CESM1.2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if model   ==  'MIROC4m':
        lsm = [DATABASE + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc', 
               DATABASE + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc']
        fieldlsm = "sftlf"

    if model  == 'HadCM3':
        lsm = [DATABASE+'LEEDS/HadCM3/e280/qrparm.mask.nc',
               DATABASE+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc']
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if model == 'CCSM4':
        lsm = [DATABASE + 'NCAR/b40.B1850.f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc', DATABASE + 'NCAR/b40.B1850.f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'CCSM4-Utr':
        lsm = [DATABASE + 'Utrecht/CESM1.0.5/E280/land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC_control_r1i1p1f1_gn.nc', DATABASE + 'Utrecht/CESM1.0.5/Eoi400/land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_f19g16_NESSC_control_r1i1p1f1_gn.nc']
        fieldlsm = 'LANDMASK[D=1]'
  
    if model == 'CCSM4-UoT':
        start = DATABASE + 'UofT/UofT-CCSM4/'
        lsm = [start + 'for_julia/E_mask.nc', start + 'for_julia/Eoi_mask.nc']
        fieldlsm = 'gridbox land fraction'
      
    if model == 'NorESM-L':
       lsm = [DATABASE + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc',
              DATABASE + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc']
       fieldlsm = 'Fraction of sfc area covered by land'


    if model  == 'MRI2.3':
        lsm = [DATABASE + 'MRI-CGCM2.3/sftlf.nc', 
               DATABASE + 'MRI-CGCM2.3/sftlf.nc']
        fieldlsm = 'landsea mask [0 - 1]'


    if model  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        lsm = [start + 'e280/NASA-GISS_PIctrl_all_fland.nc',
               start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc']
        fieldlsm = 'fland'

    if model == 'NorESM1-F':
        lsm = [DATABASE + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc',
               DATABASE + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc']
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if model == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        lsm = [start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc',
              start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc']
        fieldlsm = 'land_area_fraction'

    if model == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if model == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',
               start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if model == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        lsm = [start + 'hadgem3.mask.nc', start + 'hadgem3.mask.nc']
        fieldlsm = 'land_binary_mask'
            
            
    return lsm[period], fieldlsm


def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.mean_month.nc')
    cube = iris.load_cube(crufile)
    
    
    cru_min_temp = np.zeros(len(lats))
    cru_max_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        if lons[i] > 180.:
            lon = lons[i]-360.
        else:
            lon = lons[i]
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lon)).argmin()
        
   
        cru_temp = cube.data[:, lat_ix, lon_ix]
        if np.isfinite(cru_temp[0]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[:, lat_ix + 1, lon_ix],
                        cube.data[:, lat_ix - 1, lon_ix],
                        cube.data[:, lat_ix, lon_ix + 1],
                        cube.data[:, lat_ix, lon_ix -1],
                        ]
            cru_temp = np.ma.mean(surround, axis=0)
            cru_max_temp[i] = np.max(cru_temp)
        cru_min_temp[i] = np.min(cru_temp)
      
     
    return cru_min_temp, cru_max_temp

###########################
def get_land_sea_mask(model, period):
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d


    lsm, fieldlsm = get_lsm_names(model,period)

    ############################################
    if model == 'IPSLCM5A' or model == 'IPSLCM5A2':
        lsm_cube = get_ipsl_lsm(lsm, fieldlsm)
    elif model == 'HadGEM3':
        f = netCDF4.Dataset(lsm, "r")
        print(f.variables['longitude'])
        longitude = iris.coords.DimCoord(f.variables['longitude'], 
                             standard_name = 'longitude', units='degrees')
        latitude = iris.coords.DimCoord(f.variables['latitude'], 
                             standard_name = 'latitude', units='degrees')
        lsm_cube = iris.cube.Cube(np.squeeze(f.variables['lsm'][:]),
                             long_name='lsm', var_name='lsm', units=None, 
                             attributes=None, cell_methods=None, 
                             dim_coords_and_dims=[(latitude,0), (longitude,1)])
     
    else:
        lsm_cube = iris.util.squeeze(iris.load_cube(lsm, fieldlsm))
     
    lsm_cube2 = change_to_2d(lsm_cube)
   
   
    if model == 'IPSLCM6A':
        lsm_cube2.data = lsm_cube2.data / 100.0
       

    # regrid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsm_cube3 = lsm_cube2.regrid(cubegrid, iris.analysis.Linear())
   

    lsm_cube3.var_name = 'land_mask'
    lsm_cube3.long_name = 'land_mask'
    
    return lsm_cube3

def check_lsm(lsm_lons, lsm_lats, lsm_data, latrq, lonrq):
    """
    if our model is a sea point then set index to nan
    """

    lat_ix = (np.abs(lsm_lats - latrq)).argmin()
    lon_ix = (np.abs(lsm_lons - lonrq)).argmin()
       
    if lsm_data[lat_ix, lon_ix] <  0.5:
        # check to south, north, east, west
#        print(lsm_data[lat_ix - 1, lon_ix],lsm_data[lat_ix + 1, lon_ix],lsm_data[lat_ix, lon_ix-1],lsm_data[lat_ix, lon_ix+1], lsm_data[lat_ix - 1, lon_ix - 1],lsm_data[lat_ix + 1, lon_ix+1 ],lsm_data[lat_ix + 1, lon_ix-1],lsm_data[lat_ix-1, lon_ix+1])
#        if lsm_data[lat_ix - 1, lon_ix] >  0.5:
#            lat_ix = lat_ix -1
#        elif lsm_data[lat_ix + 1, lon_ix] >  0.5:
#            lat_ix = lat_ix + 1
#        elif lsm_data[lat_ix, lon_ix - 1] > 0.5:
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix, lon_ix + 1] > 0.5:
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix + 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix + 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix - 1
       # elif lsm_data[lat_ix, lon_ix - 2] > 0.5:
       #     lon_ix = lon_ix - 2
       # elif lsm_data[lat_ix, lon_ix + 2] > 0.5:
       #     lon_ix = lon_ix + 2
       # elif lsm_data[lat_ix, lon_ix - 3] > 0.5:
       #     lon_ix = lon_ix - 3
       # elif lsm_data[lat_ix, lon_ix + 3] > 0.5:
       #     lon_ix = lon_ix + 3
       # elif lsm_data[lat_ix - 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix -2
       # elif lsm_data[lat_ix + 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix + 2
      
       
  #      else:
            lat_ix = np.nan
            lon_ix = np.nan
            
       # print('new',lsm_data[lat_ix, lon_ix], lat_ix, lon_ix, latrq, lonrq, lsm_lons[lon_ix], lsm_lats[lat_ix])
       # sys.exit(0)

            
        

    return lat_ix, lon_ix

def get_single_model(model, latreq, lonreq, period):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    # get lsm
    if period == 'E280':
        lsm_cube  = get_land_sea_mask(model, 0)
    if period == 'EOI400':
        lsm_cube  = get_land_sea_mask(model, 1)

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + period + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_minval_array = np.zeros(nsites)
    plio_maxval_array = np.zeros(nsites)

    plio_cube_lats = plio_cube.coord('latitude').points
    plio_cube_lons = plio_cube.coord('longitude').points

    lsm_cube_lats = lsm_cube.coord('latitude').points
    lsm_cube_lons = lsm_cube.coord('longitude').points
    lsm_cube_data = lsm_cube.data
   
  
    if plio_cube_lats.any() != lsm_cube_lats.any():
        print('data cube does not match lsm lat')
    if plio_cube_lats.any() != lsm_cube_lons.any():
        print('data cube does not match lsm lat')

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        (lat_ix, 
         lon_ix) = check_lsm(lsm_cube_lons, lsm_cube_lats, 
                                    lsm_cube_data, latreq[i], modlon)

        if np.isfinite(lat_ix):
            plio_array = plio_cube.data[:, lat_ix, lon_ix]
        else:
            plio_array = np.zeros(12)
            plio_array[:] = np.nan
        plio_minval_array[i] = np.min(plio_array)
        plio_maxval_array[i] = np.max(plio_array)
   
    return plio_minval_array, plio_maxval_array

def land_reformat(sitedata):
    """
    reformats the land data into different arrays
    """
    
    sites = []
    lats = []
    lons = []
    WMMT_data_min = []
    WMMT_data_max = []
    CMMT_data_min = []
    CMMT_data_max = []
    refs = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_data_min.append(info[3])
        WMMT_data_max.append(info[4])
        CMMT_data_min.append(info[5])
        CMMT_data_max.append(info[6])
        refs.append(info[7])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + ' (' +  latstr + ',' +  lonstr + ')'
        labels.append(site)
   
    return  (labels, lats, lons, np.asarray(WMMT_data_min),
             np.asarray(WMMT_data_max),  np.asarray(CMMT_data_min),
             np.asarray(CMMT_data_max), refs)

 
def get_land_beetle_CLA():
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, 
    # min WMMT beetle, max WMMT beetle, 
    # min CMMT beetle, max CMMT beetle


    sitedata.append(['Near Meighen Island', 77.5, 261, [19.8, 12.5], 
                     [21.2, 13.6],
                    [-15.0, -9.4], [-11.6,-6.3], '~3.6Ma: Fletcher et al. 2019'])
    sitedata.append(['Beaver Pond', 79, 278, [13.6, 7.4], [13.9, 7.7],
                     [-23.5, -21.1], [-20.4, -18.3], '3.9 +1.5 / -0.5Ma: Fletcher et al. 2019'])

    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max, refs)

def get_land_beetle_CA():
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, 
    # min WMMT beetle, max WMMT beetle, 
    # min CMMT beetle, max CMMT beetle


    sitedata.append(['Near Meighen Island', 77.5, 261, [19.5, 11.2], 
                     [22.3, 14.9],
                    [-24.0, -15.7], [-13.0,-6.1], '~3.6Ma: Fletcher et al. 2019'])
    sitedata.append(['Beaver Pond', 79, 278, [9.4, 4.6], [16.5, 9.4],
                     [-25.0, -25.0], [-10.4, -4.1], '3.9 +1.5 / -0.5Ma: Fletcher et al. 2019'])

    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max, refs)

 
def get_land_beetle_mcr():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []   
    # site data is
    # sitename, sitelat, sitelon, 
    # min WMMT beetle, max WMMT beetle, 
    # min CMMT beetle, max CMMT beetle

    # all were reported in Elias and Matthews
    # note Pliocene Tmin - modern Tmin varies between 2.3 degC and 20degC

  
    sitedata.append(['Ballast Brook', 74, 237, 
                     14.0, 14.5, -21.0, -19.5,'~3-5Ma: Flyes et al 1994'])
    sitedata.append(['Strathcona Beaver Peat', 79, 278, 
                     11.7, 12.2, -28.7, -27.2, '>3.3Ma: Matthews and Flyes 2000'])
    sitedata.append(['Near Meighen Island', 77.5, 261, 
                     11.5, 13.5, -33.0, -18.5,'~3Ma: Elias and Matthews 2002'])
    sitedata.append(['Lost Chicken Mine', 64, 218,  
                     13.5, 16.0, -27.75, -19.25, '~3Ma: Matthews and Telka 1997'])
    sitedata.append(['Bluefish', 67, 221,  
                     12.7, 15.0, -30.0, -20.5, '~LP: Matthews and Telka 1997'])
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max, refs)


def plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                labels,titleinfo,ystart,refs):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """

    titlename = {'KM5c' : 'Near KM5c',
                 'LPCA' : 'Late Pliocene (Coexistence Approach)',
                 'EPCA' : 'Early Pliocene (Coexistence Approach)',
                 'CLA' : 'Coexistence Likelihood Estimation',
                 'CA' : 'Coexistence Approach',
                 'MCR' : 'Mutual Climatic Range'
        }
    labelname = {0 : 'palaeodata WMMT',
                 1 : 'palaeodata CMMT',
                 2 : 'MMM WMMT',
                 3 : 'MMM CMMT'}

    WMMT_data_mean = (WMMT_data_min + WMMT_data_max) / 2.0
 
    CMMT_data_mean = (CMMT_data_min + CMMT_data_max) / 2.0
    
    ax1 = plt.axes(frameon=False)
     
    if ystart == 0:
        ax1.get_xaxis().tick_top()
        ax1.axes.get_yaxis().set_visible(False)
        ax1.set_ylim([0, 17])
        plt.gca().invert_yaxis()
      

   
    nsites=len(WMMT_data_min)
    yarray = np.arange(ystart+1.5, nsites+ystart+1, 1)
    yend=ystart+nsites
 
    # plot warm and cold month temperature anomalies

    if titleinfo[0:2] == 'MC': # tamara fletchers data was not wmmt
        ax1.hlines(y=yarray, xmin=WMMT_data_min, xmax= WMMT_data_max,color='tab:red')
        ax1.hlines(y=yarray, xmin=CMMT_data_min, xmax= CMMT_data_max,color='tab:blue')
   
        plt.scatter(WMMT_data_mean, 
                    yarray, color='tab:red', marker='^',
                    label='palaeodata WMMT',s=60)
        plt.scatter(CMMT_data_mean, 
                    yarray, color='tab:blue', marker='^',
                    label='palaeodata CMMT' ,s=60)
    else:
        ax1.hlines(y=yarray, xmin=WMMT_data_min[:,0], xmax= WMMT_data_max[:,0],color='tab:red')
        ax1.hlines(y=yarray, xmin=CMMT_data_min[:,0], xmax= CMMT_data_max[:,0],color='tab:blue')
        ax1.hlines(y=yarray, xmin=WMMT_data_min[:,1], xmax= WMMT_data_max[:,1],color='tab:pink',linestyle='dashed')
        ax1.hlines(y=yarray, xmin=CMMT_data_min[:,1], xmax= CMMT_data_max[:,1],color='tab:cyan',linestyle='dashed')
        if titleinfo == 'CLA':
            plt.scatter(WMMT_data_mean[:,0], 
                        yarray, color='tab:red', marker='v',
                        label='max T warmest month',s=60)
            plt.scatter(CMMT_data_mean[:,0], 
                        yarray, color='tab:blue', marker='v',
                        label =  'min T coldest month ',s=60)
            plt.scatter(WMMT_data_mean[:,1], 
                        yarray, color='tab:pink', marker='^',
                        label='wamest quarter T',s=60)
            plt.scatter(CMMT_data_mean[:,1], 
                        yarray, color='tab:cyan', marker='^',
                        label =  'coldest quarter T ',s=60)
        else:
            plt.scatter(WMMT_data_mean[:,0], 
                        yarray, color='tab:red', marker='v',s=60)
            plt.scatter(CMMT_data_mean[:,0], 
                        yarray, color='tab:blue', marker='v',s=60)
            plt.scatter(WMMT_data_mean[:,1], 
                        yarray, color='tab:pink', marker='^',s=60)
            plt.scatter(CMMT_data_mean[:,1], 
                        yarray, color='tab:cyan', marker='^',s=60)

   
    # try plotting axis
  
    for j in range(0, nsites):
        plt.text(-40.0, yarray[j], labels[j], ha='right')
        #if ystart == 0:
        #    plt.text(27.0, yarray[j], refs[j], ha='left')
        #else:
        #    plt.text(35.0, yarray[j], refs[j], ha='left')

     

    
    plt.scatter(mmm_WMT, yarray + 0.2, color='black', s=60)
    plt.scatter(mmm_WMT, yarray + 0.2, color='tab:red', 
                s=25, label=labelname.get(ystart+2,None))
    plt.scatter(mmm_CMT, yarray + 0.2, color='black', s=60)
    plt.scatter(mmm_CMT, yarray + 0.2, color='tab:blue', 
                s=25, label=labelname.get(ystart+3,None))
  
  
    # plot individual models for pliocene
    for i in range(0, len(MODELNAMES)):
        if i == 0 and ystart==0:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='tab:red', 
                        marker = 'x', s=15, label='models WMMT')
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='tab:blue', 
                        marker = 'x', s=15, label='models CMMT')
        else:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='tab:red', 
                        marker = 'x',s=15)
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='tab:blue', 
                        marker = 'x', s=15)
        
    plt.hlines(y=ystart, xmin=-50., xmax=30., linewidth=0.5)
    plt.text(-30.0, -1.5, 'Cold Month Temperature (degC)', ha='left', color='tab:blue',fontsize=12)
    plt.text(10.0, -1.5, 'Warm Month Temperature (deg C)', ha='left', color='tab:red',fontsize=12)
    print(titleinfo, titlename.get(titleinfo))
    plt.text(-35.0,ystart+0.5,titlename.get(titleinfo),fontsize=12)
       
    
   # plt.legend(bbox_to_anchor=(0.5, -0.04), loc='lower center', ncol=5,
   #                prop = {'size':12})
    if ystart > 4:
       handles, labels = plt.gca().get_legend_handles_labels()
      # order = [2, 3, 4, 5, 0,6,8,1,7,9]
       order = [4,5,6,7,8,0,2,9,1,3]
       print(labels)
       print(ystart)
       plt.legend([handles[idx] for idx in order],
                  [labels[idx] for idx in order],
              bbox_to_anchor=(0.5, -0.04), loc='lower center', ncol=3,
              prop = {'size':12})
   
  
   
   
 
    return ax1,yend+2

  
def get_model_data(land_lats, land_lons):
    """
    get the model data for these latitude and longitudes
    """
    
    all_models_plio_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_seas_cyc_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_CMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_WMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'EOI400')
        all_models_plio_WMT[:, i] = ind_WMT
        all_models_plio_CMT[:, i] = ind_CMT

        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'E280')
        all_models_pi_WMT[:, i] = ind_WMT
        all_models_pi_CMT[:, i] = ind_CMT

        all_models_seas_cyc_anom[:, i] = (
            (all_models_plio_WMT[:, i] - all_models_plio_CMT[:, i]) -
            (all_models_pi_WMT[:, i] - all_models_pi_CMT[:, i]))
        
        all_models_CMMT_anom[:, i] = (all_models_plio_CMT[:, i] -
                                      all_models_pi_CMT[:, i])

        all_models_WMMT_anom[:, i] = (all_models_plio_WMT[:, i] -
                                      all_models_pi_WMT[:, i])



    mmm_WMT = np.nanmean(all_models_plio_WMT, axis=1)
    mmm_CMT = np.nanmean(all_models_plio_CMT, axis=1)
    mmm_WMT_pi = np.nanmean(all_models_pi_WMT, axis=1)
    mmm_CMT_pi = np.nanmean(all_models_pi_CMT, axis=1)


    return  (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom, 
     all_models_WMMT_anom, mmm_WMT, mmm_CMT, mmm_WMT_pi, mmm_CMT_pi)

def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    d) plots change in seasonal cycle at MI, BP and LB
    """


    # plot data
    fig1 = plt.figure(figsize=[11.7, 8.7])
  

     ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, refs) =  get_land_beetle_CLA()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

   
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'CLA',0,refs)
    

    ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max,refs) =  get_land_beetle_CA()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'CA',ystart,refs)
   
    
    ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max,refs) =  get_land_beetle_mcr()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_splio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data
    for i,site in enumerate(sites):
        print('warm',site, WMMT_data_min[i], WMMT_data_max[i],mmm_WMT[i])
        print('cold',site, CMMT_data_min[i], CMMT_data_max[i],mmm_CMT[i])
        print(' ')
 #       for j, model in enumerate(MODELNAMES):
 #           print(model, all_models_plio_CMT[i,j], all_models_plio_CMT[i,j] - all_models_plio_CMT[1,j])
    sys.exit(0)

    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'MCR',ystart,refs)
    ax1.set_ylim(None,0)    
    

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/seasonal_dmc_plot_v2_beetle.eps')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_v2_beetle.pdf')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_v2_beetle.png')
    plt.savefig(fileout)
    plt.close()

 
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'

MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 
               'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
         #    ,  'MRI2.3'
              ]

#MODELNAMES = ['IPSLCM6A']

NSAT_MMM_FILE = (FILESTART + 
                 'regridded100/NearSurfaceTemperature_multimodelmean.nc')

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/seasonal_dmc_plot_v2_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet
The difference between this and version 1 is that we will group the data
by dating / proxy

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import netCDF4

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array

def get_lsm_names(model, period):
    """
    gets the names for each of the land sea masks
    period 0 =e280, period 1 = eoi400
    """
    if model == 'CESM2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'COSMOS':
        lsm = ["/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/E280_et_al/E280.slf.atm.nc", "/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/Eoi400_et_al/Eoi400.slf.atm.nc"]
        fieldlsm = "SLF"

    if model == 'EC-Earth3.3':
        lsm =  [DATABASE + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc',
                DATABASE + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc']
        fieldlsm = 'Land/sea mask'

    if model == 'CESM1.2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if model   ==  'MIROC4m':
        lsm = [DATABASE + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc', 
               DATABASE + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc']
        fieldlsm = "sftlf"

    if model  == 'HadCM3':
        lsm = [DATABASE+'LEEDS/HadCM3/e280/qrparm.mask.nc',
               DATABASE+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc']
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if model == 'CCSM4':
        lsm = [DATABASE + 'NCAR/b40.B1850.f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc', DATABASE + 'NCAR/b40.B1850.f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'CCSM4-Utr':
        lsm = [DATABASE + 'Utrecht/CESM1.0.5/E280/land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC_control_r1i1p1f1_gn.nc', DATABASE + 'Utrecht/CESM1.0.5/Eoi400/land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_f19g16_NESSC_control_r1i1p1f1_gn.nc']
        fieldlsm = 'LANDMASK[D=1]'
  
    if model == 'CCSM4-UoT':
        start = DATABASE + 'UofT/UofT-CCSM4/'
        lsm = [start + 'for_julia/E_mask.nc', start + 'for_julia/Eoi_mask.nc']
        fieldlsm = 'gridbox land fraction'
      
    if model == 'NorESM-L':
       lsm = [DATABASE + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc',
              DATABASE + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc']
       fieldlsm = 'Fraction of sfc area covered by land'


    if model  == 'MRI2.3':
        lsm = [DATABASE + 'MRI-CGCM2.3/sftlf.nc', 
               DATABASE + 'MRI-CGCM2.3/sftlf.nc']
        fieldlsm = 'landsea mask [0 - 1]'


    if model  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        lsm = [start + 'e280/NASA-GISS_PIctrl_all_fland.nc',
               start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc']
        fieldlsm = 'fland'

    if model == 'NorESM1-F':
        lsm = [DATABASE + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc',
               DATABASE + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc']
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if model == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        lsm = [start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc',
              start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc']
        fieldlsm = 'land_area_fraction'

    if model == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if model == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',
               start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if model == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        lsm = [start + 'hadgem3.mask.nc', start + 'hadgem3.mask.nc']
        fieldlsm = 'land_binary_mask'
            
            
    return lsm[period], fieldlsm


def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.mean_month.nc')
    cube = iris.load_cube(crufile)
    
    
    cru_min_temp = np.zeros(len(lats))
    cru_max_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        if lons[i] > 180.:
            lon = lons[i]-360.
        else:
            lon = lons[i]
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lon)).argmin()
        
   
        cru_temp = cube.data[:, lat_ix, lon_ix]
        if np.isfinite(cru_temp[0]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[:, lat_ix + 1, lon_ix],
                        cube.data[:, lat_ix - 1, lon_ix],
                        cube.data[:, lat_ix, lon_ix + 1],
                        cube.data[:, lat_ix, lon_ix -1],
                        ]
            cru_temp = np.ma.mean(surround, axis=0)
            cru_max_temp[i] = np.max(cru_temp)
        cru_min_temp[i] = np.min(cru_temp)
      
     
    return cru_min_temp, cru_max_temp

###########################
def get_land_sea_mask(model, period):
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d


    lsm, fieldlsm = get_lsm_names(model,period)

    ############################################
    if model == 'IPSLCM5A' or model == 'IPSLCM5A2':
        lsm_cube = get_ipsl_lsm(lsm, fieldlsm)
    elif model == 'HadGEM3':
        f = netCDF4.Dataset(lsm, "r")
        print(f.variables['longitude'])
        longitude = iris.coords.DimCoord(f.variables['longitude'], 
                             standard_name = 'longitude', units='degrees')
        latitude = iris.coords.DimCoord(f.variables['latitude'], 
                             standard_name = 'latitude', units='degrees')
        lsm_cube = iris.cube.Cube(np.squeeze(f.variables['lsm'][:]),
                             long_name='lsm', var_name='lsm', units=None, 
                             attributes=None, cell_methods=None, 
                             dim_coords_and_dims=[(latitude,0), (longitude,1)])
     
    else:
        lsm_cube = iris.util.squeeze(iris.load_cube(lsm, fieldlsm))
     
    lsm_cube2 = change_to_2d(lsm_cube)
   
   
    if model == 'IPSLCM6A':
        lsm_cube2.data = lsm_cube2.data / 100.0
       

    # regrid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsm_cube3 = lsm_cube2.regrid(cubegrid, iris.analysis.Linear())
   

    lsm_cube3.var_name = 'land_mask'
    lsm_cube3.long_name = 'land_mask'
    
    return lsm_cube3

def check_lsm(lsm_lons, lsm_lats, lsm_data, latrq, lonrq):
    """
    if our model is a sea point then set index to nan
    """

    lat_ix = (np.abs(lsm_lats - latrq)).argmin()
    lon_ix = (np.abs(lsm_lons - lonrq)).argmin()
       
    if lsm_data[lat_ix, lon_ix] <  0.5:
        # check to south, north, east, west
#        print(lsm_data[lat_ix - 1, lon_ix],lsm_data[lat_ix + 1, lon_ix],lsm_data[lat_ix, lon_ix-1],lsm_data[lat_ix, lon_ix+1], lsm_data[lat_ix - 1, lon_ix - 1],lsm_data[lat_ix + 1, lon_ix+1 ],lsm_data[lat_ix + 1, lon_ix-1],lsm_data[lat_ix-1, lon_ix+1])
#        if lsm_data[lat_ix - 1, lon_ix] >  0.5:
#            lat_ix = lat_ix -1
#        elif lsm_data[lat_ix + 1, lon_ix] >  0.5:
#            lat_ix = lat_ix + 1
#        elif lsm_data[lat_ix, lon_ix - 1] > 0.5:
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix, lon_ix + 1] > 0.5:
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix + 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix + 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix - 1
       # elif lsm_data[lat_ix, lon_ix - 2] > 0.5:
       #     lon_ix = lon_ix - 2
       # elif lsm_data[lat_ix, lon_ix + 2] > 0.5:
       #     lon_ix = lon_ix + 2
       # elif lsm_data[lat_ix, lon_ix - 3] > 0.5:
       #     lon_ix = lon_ix - 3
       # elif lsm_data[lat_ix, lon_ix + 3] > 0.5:
       #     lon_ix = lon_ix + 3
       # elif lsm_data[lat_ix - 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix -2
       # elif lsm_data[lat_ix + 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix + 2
      
       
  #      else:
            lat_ix = np.nan
            lon_ix = np.nan
            
       # print('new',lsm_data[lat_ix, lon_ix], lat_ix, lon_ix, latrq, lonrq, lsm_lons[lon_ix], lsm_lats[lat_ix])
       # sys.exit(0)

            
        

    return lat_ix, lon_ix

def get_single_model(model, latreq, lonreq, period):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    # get lsm
    if period == 'E280':
        lsm_cube  = get_land_sea_mask(model, 0)
    if period == 'EOI400':
        lsm_cube  = get_land_sea_mask(model, 1)

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + period + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_minval_array = np.zeros(nsites)
    plio_maxval_array = np.zeros(nsites)

    plio_cube_lats = plio_cube.coord('latitude').points
    plio_cube_lons = plio_cube.coord('longitude').points

    lsm_cube_lats = lsm_cube.coord('latitude').points
    lsm_cube_lons = lsm_cube.coord('longitude').points
    lsm_cube_data = lsm_cube.data
   
  
    if plio_cube_lats.any() != lsm_cube_lats.any():
        print('data cube does not match lsm lat')
    if plio_cube_lats.any() != lsm_cube_lons.any():
        print('data cube does not match lsm lat')

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        (lat_ix, 
         lon_ix) = check_lsm(lsm_cube_lons, lsm_cube_lats, 
                                    lsm_cube_data, latreq[i], modlon)

        if np.isfinite(lat_ix):
            plio_array = plio_cube.data[:, lat_ix, lon_ix]
        else:
            plio_array = np.zeros(12)
            plio_array[:] = np.nan
        plio_minval_array[i] = np.min(plio_array)
        plio_maxval_array[i] = np.max(plio_array)
   
    return plio_minval_array, plio_maxval_array

def land_reformat(sitedata):
    """
    reformats the land data into different arrays
    """
    
    sites = []
    lats = []
    lons = []
    WMMT_data_min = []
    WMMT_data_max = []
    WMMT_modern_obs = []
    CMMT_data_min = []
    CMMT_data_max = []
    CMMT_modern_obs = []
    refs = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_data_min.append(info[3])
        WMMT_data_max.append(info[4])
        CMMT_data_min.append(info[5])
        CMMT_data_max.append(info[6])
        WMMT_modern_obs.append(info[7])
        CMMT_modern_obs.append(info[8])
        refs.append(info[9])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + ' (' +  latstr + ',' +  lonstr + ')'
        labels.append(label)
   
    return  (labels, lats, lons, np.asarray(WMMT_data_min),
             np.asarray(WMMT_data_max),  np.asarray(CMMT_data_min),
             np.asarray(CMMT_data_max),  np.asarray(WMMT_modern_obs),
             np.asarray(CMMT_modern_obs),refs)

 
def get_land_km5c():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle, modern obs WMMT, modern obs CMMT
    # reference and date

    # lake baikal is from what ulrich sent me.
    # Lake E from Brigette-greeme mean july temp of +8 and average winter lows of 35degC

    sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
                    -36.8, -30.4, 
                     8.0, np.nan,'CMMT 3.199Ma - 3.209Ma; Pavel Tarasov (pers. comm) \n WMMT Brigham-Grette et al. 2013'])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     -1.67, 1.07, 
                     15.3, -17.4,'Km5c - unpublished \n (Method of Klage et al 2020)'])
    
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)

    return (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs)

def get_land_LP():
    """
    these have been obtained from various sources so I am just typing them in
    LP means late pliocene
    """
    sitedata = []
    # lake baikal is from demske 2002.
    sitedata.append(['Lake Baikal', 56, 108, 13.0, 24.0, -15, 5,
                     np.nan, np.nan,'Prior to 3.5Ma (Demske et al 2002)'])
    # these are from popova et al 2012 using coexistence approach.
  
    sitedata.append(['Mirny', 55, 82, 18.8, 24.6, -0.3, 0.7,
                     np.nan, np.nan,'Popova et al 2012'])
    sitedata.append(['Merkutlinskiy', 56, 72, 17.3, 23.8, -3.8, 6.2,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Kabinet', 55, 80, 21.6, 24.4, -4.4, 4.6,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Delyankir', 63, 133, 18.9, 24.9, -6.9, 1.3,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Chernoluche', 55, 73, 19.6, 20.3, -5.9, 0.7,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Blizkiy', 64, 162, 15.6, 23.3, -12.8, 5.2,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['42km', 55, 80, 21.6, 23.3, -4.4, 0.7,
                     np.nan, np.nan,' --"--'])
  
    sitedata.append(['Lost Chicken Mine', 64, 218, 12.0, 12.0, 
                     -2.0, -2.0, 15.3, -25.1, '2.9 +/- 0.4Ma: Ager et al. 1994'])
  
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)

    return (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs)

def get_land_EP():
    """
    these have been obtained from various sources so I am just typing them in
    EP means early pliocene
    """
    sitedata = []
    # these are from popova et al 2012 using coexistence approach.
  
    sitedata.append(['Tnekveem', 66, 177, 18.9, 25.6, -11.8, 5.8, 
                     np.nan, np.nan,'Popova et al 2012'])
    sitedata.append(['Hydzhak', 63, 147, 18.8, 24.9, -8.7, 1.3,
                     np.nan, np.nan,' --"--'])

   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)

    return (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs)

def get_land_fletcher():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT data, max WMMT data,
    # min CMMT data, max CMMT data
    # modern obs WMMT, modern obs CMMT
    # we think FLB is 3.8Ma, BP originally was 3.4 but has been redated
   
    # for TF data we are reporting here temperature of warmest month
    # and temperature of warmest quater
    
    sitedata.append(['Meighen Island', 77.5, 261, [19.6, 12.8], 
                     [20.5, 13.3],
                    [-11.6, -6.8], [-11.4, -6.2],
                     4.1, -42.5, 'Fletcher et al. 2017'])
    sitedata.append(['Beaver Pond', 79, 278, [18.4, 12.4], [20.9, 13.1],
                     [-12.2, -7.3], [-11.5, -6.8],
                      7.1, -39.7, '3.9 +1.5 / -0.5Ma: Fletcher et al. 2017'])
    sitedata.append(['Fyles Leaf Beds', 79, 277, [19.7, 12.6], [21.1, 13.4],
                     [-12.8, -7.2], [-9.1, -5.5],
                      np.nan, np.nan, '3.8 +1/-0.7Ma: Fletcher et al. 2017'])
     
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max,  WMMT_modern_obs,
             CMMT_modern_obs,refs)

def get_land_fletcher_ca():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT data, max WMMT data,
    # min CMMT data, max CMMT data
    # modern obs WMMT, modern obs CMMT
    # we think FLB is 3.8Ma, BP originally was 3.4 but has been redated
   
    # for tf data we are reporting hwere temp of warmest month and
    # warmest quater
    
    sitedata.append(['Near Meighen Island', 77.5, 261, [18.1, 10.6], 
                     [22.8, 16.2],
                    [-21.7,-16.3,], [-7.9, -2.7],
                     4.1, -42.5,'Fletcher et al. 2017'])
    sitedata.append(['Beaver Pond', 79, 278, [18.1, 11.3],[22.4, 16.3],
                     [-21.7, -15.0], [-8.1, -3.5],
                      7.1, -39.7,'3.9 +1.5 / -0.5Ma: Fletcher et al. 2017'])
    sitedata.append(['Fyles Leaf Beds', 79, 277, [18.1, 10.9], [22.7, 15.0],
                     [-16.9, -12.4], [-6.4, -2.3],
                      np.nan, np.nan,'3.8 +1/-0.7Ma: Fletcher et al. 2017'])
     
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max,  WMMT_modern_obs,
             CMMT_modern_obs,refs)

def get_land_beetle():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle, modern obs WMMT, modern obs CMMT

    # all were reported in Elias and Matthews
    # note Pliocene Tmin - modern Tmin varies between 2.3 degC and 20degC

    sitedata.append(['Ballast Brook', 74, 237, 
                     14.0, 14.5, -21.0, -19.5, 2.4, -41.4,'~3-5Ma: Flyes et al 1994'])
    sitedata.append(['Strathcona Beaver Peat', 79, 278, 
                     11.7, 12.2, -28.7, -27.2, 2.4, -41.4,'>3.3Ma: Matthews and Flyes 2000'])
    sitedata.append(['Near Meighen Island', 77.5, 261, 
                     11.5, 13.5, -33.0, -18.5, 4.1, -42.5,'~3Ma: Elias and Matthews 2002'])
    sitedata.append(['Lost Chicken Mine', 64, 218,  
                     13.5, 16.0, -27.75, -19.25, 15.3, -25.1,'~3Ma: Matthews and Telka 1997'])
    sitedata.append(['Bluefish', 67, 221,  
                     12.7, 15.0, -30.0, -20.5, 16.0, -29.0,'~LP: Matthews and Telka 1997'])
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max,  WMMT_modern_obs,
             CMMT_modern_obs,refs)


def plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                labels,titleinfo,ystart,refs):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """

    titlename = {'KM5c' : 'Near KM5c',
                 'LPCA' : 'Late Pliocene (Coexistence Approach)',
                 'EPCA' : 'Early Pliocene (Coexistence Approach)',
                 'TF-CRACLE' : '< 3.9Ma (Coexistence Likelihood Estimation)',
                 'TF-CA' : '<3.9Ma (Coexistence Approach)',
                 'BA' : 'Beetle Assemblage data'
        }
    labelname = {0 : 'palaeodata WMMT',
                 1 : 'palaeodata CMMT',
                 2 : 'MMM WMMT',
                 3 : 'MMM CMMT'}

    WMMT_data_mean = (WMMT_data_min + WMMT_data_max) / 2.0
 
    CMMT_data_mean = (CMMT_data_min + CMMT_data_max) / 2.0
    
    ax1 = plt.axes(frameon=False)
     
    if ystart == 0:
        ax1.get_xaxis().tick_top()
        ax1.axes.get_yaxis().set_visible(False)
        ax1.set_ylim([0, 32])
        plt.gca().invert_yaxis()
        #ax.set_xlim([xmin, xmax])
      

   
    nsites=len(WMMT_data_min)
    yarray = np.arange(ystart+1.5, nsites+ystart+1, 1)
    yend=ystart+nsites
 
    # plot warm and cold month temperature anomalies

    if titleinfo[0:2] != 'TF': # tamara fletchers data was not wmmt
        ax1.hlines(y=yarray, xmin=WMMT_data_min, xmax= WMMT_data_max,color='tab:red')
        ax1.hlines(y=yarray, xmin=CMMT_data_min, xmax= CMMT_data_max,color='tab:blue')
   
        plt.scatter(WMMT_data_mean, 
                    yarray, color='tab:red', marker='^',
                    label=labelname.get(ystart,None),s=60)
        plt.scatter(CMMT_data_mean, 
                    yarray, color='tab:blue', marker='^',
                    label = labelname.get(ystart+1,None) ,s=60)
    else:
        print(WMMT_data_mean)
        ax1.hlines(y=yarray, xmin=WMMT_data_min[:,0], xmax= WMMT_data_max[:,0],color='tab:red')
        ax1.hlines(y=yarray, xmin=CMMT_data_min[:,0], xmax= CMMT_data_max[:,0],color='tab:blue')
        ax1.hlines(y=yarray, xmin=WMMT_data_min[:,1], xmax= WMMT_data_max[:,1],color='tab:pink',linestyle='dashed')
        ax1.hlines(y=yarray, xmin=CMMT_data_min[:,1], xmax= CMMT_data_max[:,1],color='tab:cyan',linestyle='dashed')
        if titleinfo == 'TF-CRACLE':
            plt.scatter(WMMT_data_mean[:,0], 
                        yarray, color='tab:red', marker='v',
                        label='max T warmest month',s=60)
            plt.scatter(CMMT_data_mean[:,0], 
                        yarray, color='tab:blue', marker='v',
                        label =  'min T coldest month ',s=60)
            plt.scatter(WMMT_data_mean[:,1], 
                        yarray, color='tab:pink', marker='^',
                        label='wamest quarter T',s=60)
            plt.scatter(CMMT_data_mean[:,1], 
                        yarray, color='tab:cyan', marker='^',
                        label =  'coldest quarter T ',s=60)
        else:
            plt.scatter(WMMT_data_mean[:,0], 
                        yarray, color='tab:red', marker='v',s=60)
            plt.scatter(CMMT_data_mean[:,0], 
                        yarray, color='tab:blue', marker='v',s=60)
            plt.scatter(WMMT_data_mean[:,1], 
                        yarray, color='tab:pink', marker='^',s=60)
            plt.scatter(CMMT_data_mean[:,1], 
                        yarray, color='tab:cyan', marker='^',s=60)

    # put an arrow on LCM
    for i, label in enumerate(labels):
        if label[0:4] == 'Lost' and i > 4:
            plt.arrow(CMMT_data_max[i], yarray[i], -10, 0, linestyle='dotted', 
                  color='tab:blue', head_width=0.2, head_length=1.0)
   
    # try plotting axis
  
    for j in range(0, nsites):
        plt.text(-35.0, yarray[j], labels[j], ha='right')
        if ystart == 0:
            plt.text(27.0, yarray[j], refs[j], ha='left')
        else:
            plt.text(35.0, yarray[j], refs[j], ha='left')

     

    
    plt.scatter(mmm_WMT, yarray + 0.2, color='black', s=60)
    plt.scatter(mmm_WMT, yarray + 0.2, color='tab:red', 
                s=25, label=labelname.get(ystart+2,None))
    plt.scatter(mmm_CMT, yarray + 0.2, color='black', s=60)
    plt.scatter(mmm_CMT, yarray + 0.2, color='tab:blue', 
                s=25, label=labelname.get(ystart+3,None))
  
  
    # plot individual models for pliocene
    for i in range(0, len(MODELNAMES)):
        if i == 0 and ystart==0:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='tab:red', 
                        marker = 'x', s=15, label='models WMMT')
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='tab:blue', 
                        marker = 'x', s=15, label='models CMMT')
        else:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='tab:red', 
                        marker = 'x',s=15)
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='tab:blue', 
                        marker = 'x', s=15)
        
    plt.hlines(y=ystart, xmin=-40., xmax=40., linewidth=0.5)
    plt.text(-30.0, -1.75, 'Cold Month Temperature (degC)', ha='left', color='tab:blue',fontsize=12)
    plt.text(10.0, -1.75, 'Warm Month Temperature (deg C)', ha='left', color='tab:red',fontsize=12)
    plt.text(-35.0,ystart+0.5,titlename.get(titleinfo),fontsize=12)
       
    
    #plt.legend(bbox_to_anchor=(0.5, -0.04), loc='lower center', ncol=3,
    #               prop = {'size':12})
    if ystart > 23:
        handles, labels = plt.gca().get_legend_handles_labels()
        order = [2, 3, 4, 5, 0,6,8,1,7,9]
        print(labels)
        print(ystart)
        plt.legend([handles[idx] for idx in order],
                   [labels[idx] for idx in order],
               bbox_to_anchor=(0.5, -0.04), loc='lower center', ncol=3,
               prop = {'size':12})
   
   
 
    return ax1,yend+2

  
def get_model_data(land_lats, land_lons):
    """
    get the model data for these latitude and longitudes
    """
    
    all_models_plio_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_seas_cyc_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_CMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_WMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'EOI400')
        all_models_plio_WMT[:, i] = ind_WMT
        all_models_plio_CMT[:, i] = ind_CMT

        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'E280')
        all_models_pi_WMT[:, i] = ind_WMT
        all_models_pi_CMT[:, i] = ind_CMT

        all_models_seas_cyc_anom[:, i] = (
            (all_models_plio_WMT[:, i] - all_models_plio_CMT[:, i]) -
            (all_models_pi_WMT[:, i] - all_models_pi_CMT[:, i]))
        
        all_models_CMMT_anom[:, i] = (all_models_plio_CMT[:, i] -
                                      all_models_pi_CMT[:, i])

        all_models_WMMT_anom[:, i] = (all_models_plio_WMT[:, i] -
                                      all_models_pi_WMT[:, i])



    mmm_WMT = np.nanmean(all_models_plio_WMT, axis=1)
    mmm_CMT = np.nanmean(all_models_plio_CMT, axis=1)
    mmm_WMT_pi = np.nanmean(all_models_pi_WMT, axis=1)
    mmm_CMT_pi = np.nanmean(all_models_pi_CMT, axis=1)


    return  (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom, 
     all_models_WMMT_anom, mmm_WMT, mmm_CMT, mmm_WMT_pi, mmm_CMT_pi)

def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    d) plots change in seasonal cycle at MI, BP and LB
    """

   
    # get land observations and cru temperature at km5c points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_km5c()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)


 #   for i,site in enumerate(sites):
 #       print(site, CMMT_data_min[i], CMMT_data_max[i],mmm_CMT[i])
 #       for j, model in enumerate(MODELNAMES):
 #           print(model, all_models_plio_CMT[i,j], all_models_plio_CMT[i,j] - all_models_plio_CMT[1,j])

    # plot data
    fig1 = plt.figure(figsize=[11.7, 11.7])
  
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'KM5c',0,refs)
   
    #####################################################################
    # get land observations and cru temperature at lp points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_LP()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'LPCA',ystart,refs)
   
   
    #####################################################################
    # get land observations and cru temperature at ep points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_EP()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'EPCA',ystart,refs)
   

    ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_fletcher()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'TF-CRACLE',ystart,refs)

     ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_fletcher_ca()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'TF-CA',ystart,refs)



     ####################################################################
    # get land observations and cru temperature at early pliocene points 
#    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
#     CMMT_data_min, CMMT_data_max, 
#     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_beetle()

#    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
#    (all_models_plio_WMT, all_models_plio_CMT,
#     all_models_pi_WMT, all_models_pi_CMT, 
#     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
#     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

#    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
#                CMMT_data_min, CMMT_data_max, 
#                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
#                sites,'BA',ystart,refs)
#    ax1.set_ylim(None,0)    
    

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/seasonal_dmc_plot_v2.eps')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_v2.pdf')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_v2.png')
    plt.savefig(fileout)
    plt.close()

 
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'

MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 
               'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
           #  ,  'MRI2.3'
              ]

#MODELNAMES = ['IPSLCM6A']

NSAT_MMM_FILE = (FILESTART + 
                 'regridded100/NearSurfaceTemperature_multimodelmean.nc')

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/seasonal_dmc_plot_v2.py
::::::::::::::

#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will produce a lat /lon dmc plot from Ulrichs spreadsheet
The difference between this and version 1 is that we will group the data
by dating / proxy

"""

import numpy as np
import pandas as pd
import iris
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.quickplot as qplt
import iris.plot as iplt
import cartopy.crs as ccrs
import netCDF4

import sys



def get_MMM_data(latreq, lonreq):
    """
    read in MMM data from the pliocene and the preindustrial 
    return the temperature at the list of sites
    """

    plio_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_mPWP')
    pi_cube = iris.load_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_pi')
   
    nsites = len(latreq)
    plio_mmm_array = np.zeros(nsites)
    pi_mmm_array = np.zeros(nsites)

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.


        lat_ix = ((np.abs(plio_cube.coord('latitude').points 
                         - latreq[i])).argmin())
        lon_ix = ((np.abs(plio_cube.coord('longitude').points 
                         - modlon)).argmin())
    
        plio_mmm_array[i] = plio_cube.data[lat_ix, lon_ix]
        pi_mmm_array[i] = pi_cube.data[lat_ix, lon_ix]
   
    return plio_mmm_array, pi_mmm_array

def get_lsm_names(model, period):
    """
    gets the names for each of the land sea masks
    period 0 =e280, period 1 = eoi400
    """
    if model == 'CESM2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'COSMOS':
        lsm = ["/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/E280_et_al/E280.slf.atm.nc", "/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/Eoi400_et_al/Eoi400.slf.atm.nc"]
        fieldlsm = "SLF"

    if model == 'EC-Earth3.3':
        lsm =  [DATABASE + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc',
                DATABASE + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc']
        fieldlsm = 'Land/sea mask'

    if model == 'CESM1.2':
        lsm = [DATABASE + 'NCAR/b.e12.B1850.f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc', DATABASE + 'NCAR/b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc']
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if model   ==  'MIROC4m':
        lsm = [DATABASE + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc', 
               DATABASE + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc']
        fieldlsm = "sftlf"

    if model  == 'HadCM3':
        lsm = [DATABASE+'LEEDS/HadCM3/e280/qrparm.mask.nc',
               DATABASE+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc']
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if model == 'CCSM4':
        lsm = [DATABASE + 'NCAR/b40.B1850.f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc', DATABASE + 'NCAR/b40.B1850.f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc']
        fieldlsm = 'Fraction of sfc area covered by land'

    if model == 'CCSM4-Utr':
        lsm = [DATABASE + 'Utrecht/CESM1.0.5/E280/land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC_control_r1i1p1f1_gn.nc', DATABASE + 'Utrecht/CESM1.0.5/Eoi400/land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_f19g16_NESSC_control_r1i1p1f1_gn.nc']
        fieldlsm = 'LANDMASK[D=1]'
  
    if model == 'CCSM4-UoT':
        start = DATABASE + 'UofT/UofT-CCSM4/'
        lsm = [start + 'for_julia/E_mask.nc', start + 'for_julia/Eoi_mask.nc']
        fieldlsm = 'gridbox land fraction'
      
    if model == 'NorESM-L':
       lsm = [DATABASE + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc',
              DATABASE + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc']
       fieldlsm = 'Fraction of sfc area covered by land'


    if model  == 'MRI2.3':
        lsm = [DATABASE + 'MRI-CGCM2.3/sftlf.nc', 
               DATABASE + 'MRI-CGCM2.3/sftlf.nc']
        fieldlsm = 'landsea mask [0 - 1]'


    if model  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        lsm = [start + 'e280/NASA-GISS_PIctrl_all_fland.nc',
               start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc']
        fieldlsm = 'fland'

    if model == 'NorESM1-F':
        lsm = [DATABASE + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc',
               DATABASE + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc']
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if model == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        lsm = [start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc',
              start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc']
        fieldlsm = 'land_area_fraction'

    if model == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if model == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        lsm = [start + 'E280_LSM_IPSLCM5A.nc',
               start + 'Eoi400_LSM_IPSLCM5A.nc']
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if model == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        lsm = [start + 'hadgem3.mask.nc', start + 'hadgem3.mask.nc']
        fieldlsm = 'land_binary_mask'
            
            
    return lsm[period], fieldlsm


def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.mean_month.nc')
    cube = iris.load_cube(crufile)
    
    
    cru_min_temp = np.zeros(len(lats))
    cru_max_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        if lons[i] > 180.:
            lon = lons[i]-360.
        else:
            lon = lons[i]
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lon)).argmin()
        
   
        cru_temp = cube.data[:, lat_ix, lon_ix]
        if np.isfinite(cru_temp[0]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[:, lat_ix + 1, lon_ix],
                        cube.data[:, lat_ix - 1, lon_ix],
                        cube.data[:, lat_ix, lon_ix + 1],
                        cube.data[:, lat_ix, lon_ix -1],
                        ]
            cru_temp = np.ma.mean(surround, axis=0)
            cru_max_temp[i] = np.max(cru_temp)
        cru_min_temp[i] = np.min(cru_temp)
      
     
    return cru_min_temp, cru_max_temp

###########################
def get_land_sea_mask(model, period):
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d


    lsm, fieldlsm = get_lsm_names(model,period)

    ############################################
    if model == 'IPSLCM5A' or model == 'IPSLCM5A2':
        lsm_cube = get_ipsl_lsm(lsm, fieldlsm)
    elif model == 'HadGEM3':
        f = netCDF4.Dataset(lsm, "r")
        print(f.variables['longitude'])
        longitude = iris.coords.DimCoord(f.variables['longitude'], 
                             standard_name = 'longitude', units='degrees')
        latitude = iris.coords.DimCoord(f.variables['latitude'], 
                             standard_name = 'latitude', units='degrees')
        lsm_cube = iris.cube.Cube(np.squeeze(f.variables['lsm'][:]),
                             long_name='lsm', var_name='lsm', units=None, 
                             attributes=None, cell_methods=None, 
                             dim_coords_and_dims=[(latitude,0), (longitude,1)])
     
    else:
        lsm_cube = iris.util.squeeze(iris.load_cube(lsm, fieldlsm))
     
    lsm_cube2 = change_to_2d(lsm_cube)
   
   
    if model == 'IPSLCM6A':
        lsm_cube2.data = lsm_cube2.data / 100.0
       

    # regrid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsm_cube3 = lsm_cube2.regrid(cubegrid, iris.analysis.Linear())
   

    lsm_cube3.var_name = 'land_mask'
    lsm_cube3.long_name = 'land_mask'
    
    return lsm_cube3

def check_lsm(lsm_lons, lsm_lats, lsm_data, latrq, lonrq):
    """
    if our model is a sea point then set index to nan
    """

    lat_ix = (np.abs(lsm_lats - latrq)).argmin()
    lon_ix = (np.abs(lsm_lons - lonrq)).argmin()
       
    if lsm_data[lat_ix, lon_ix] <  0.5:
        # check to south, north, east, west
#        print(lsm_data[lat_ix - 1, lon_ix],lsm_data[lat_ix + 1, lon_ix],lsm_data[lat_ix, lon_ix-1],lsm_data[lat_ix, lon_ix+1], lsm_data[lat_ix - 1, lon_ix - 1],lsm_data[lat_ix + 1, lon_ix+1 ],lsm_data[lat_ix + 1, lon_ix-1],lsm_data[lat_ix-1, lon_ix+1])
#        if lsm_data[lat_ix - 1, lon_ix] >  0.5:
#            lat_ix = lat_ix -1
#        elif lsm_data[lat_ix + 1, lon_ix] >  0.5:
#            lat_ix = lat_ix + 1
#        elif lsm_data[lat_ix, lon_ix - 1] > 0.5:
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix, lon_ix + 1] > 0.5:
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix - 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix -1
#            lon_ix = lon_ix - 1
#        elif lsm_data[lat_ix + 1, lon_ix + 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix + 1
#        elif lsm_data[lat_ix + 1, lon_ix - 1] > 0.5:
#            lat_ix = lat_ix +1
#            lon_ix = lon_ix - 1
       # elif lsm_data[lat_ix, lon_ix - 2] > 0.5:
       #     lon_ix = lon_ix - 2
       # elif lsm_data[lat_ix, lon_ix + 2] > 0.5:
       #     lon_ix = lon_ix + 2
       # elif lsm_data[lat_ix, lon_ix - 3] > 0.5:
       #     lon_ix = lon_ix - 3
       # elif lsm_data[lat_ix, lon_ix + 3] > 0.5:
       #     lon_ix = lon_ix + 3
       # elif lsm_data[lat_ix - 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix -2
       # elif lsm_data[lat_ix + 2, lon_ix] > 0.5:
       #     lat_ix = lat_ix + 2
      
       
  #      else:
            lat_ix = np.nan
            lon_ix = np.nan
            
       # print('new',lsm_data[lat_ix, lon_ix], lat_ix, lon_ix, latrq, lonrq, lsm_lons[lon_ix], lsm_lats[lat_ix])
       # sys.exit(0)

            
        

    return lat_ix, lon_ix

def get_single_model(model, latreq, lonreq, period):
    """
    read in the pliocene data from 'model'  return the temperatures
    at the list of sites
    """
    # get lsm
    if period == 'E280':
        lsm_cube  = get_land_sea_mask(model, 0)
    if period == 'EOI400':
        lsm_cube  = get_land_sea_mask(model, 1)

    filename = ('/nfs/hera1/earjcti/regridded100/' + model +
                '/' + period + '.NearSurfaceTemperature.mean_month.nc')
  
    print(filename)
    plio_cube = iris.load_cube(filename)
   
    nsites = len(latreq)
    plio_minval_array = np.zeros(nsites)
    plio_maxval_array = np.zeros(nsites)

    plio_cube_lats = plio_cube.coord('latitude').points
    plio_cube_lons = plio_cube.coord('longitude').points

    lsm_cube_lats = lsm_cube.coord('latitude').points
    lsm_cube_lons = lsm_cube.coord('longitude').points
    lsm_cube_data = lsm_cube.data
   
  
    if plio_cube_lats.any() != lsm_cube_lats.any():
        print('data cube does not match lsm lat')
    if plio_cube_lats.any() != lsm_cube_lons.any():
        print('data cube does not match lsm lat')

    for i in range(0,nsites):
        # modellon is whole numbers from 0-360
        # lat is half numbers from -89.5 to 89.5

        modlon = np.around(lonreq[i])
        if modlon < 0: modlon = modlon + 360.

        (lat_ix, 
         lon_ix) = check_lsm(lsm_cube_lons, lsm_cube_lats, 
                                    lsm_cube_data, latreq[i], modlon)

        if np.isfinite(lat_ix):
            plio_array = plio_cube.data[:, lat_ix, lon_ix]
        else:
            plio_array = np.zeros(12)
            plio_array[:] = np.nan
        plio_minval_array[i] = np.min(plio_array)
        plio_maxval_array[i] = np.max(plio_array)
   
    return plio_minval_array, plio_maxval_array

def land_reformat(sitedata):
    """
    reformats the land data into different arrays
    """
    
    sites = []
    lats = []
    lons = []
    WMMT_data_min = []
    WMMT_data_max = []
    WMMT_modern_obs = []
    CMMT_data_min = []
    CMMT_data_max = []
    CMMT_modern_obs = []
    refs = []
    
    for info in sitedata:
        sites.append(info[0])
        lats.append(info[1])
        lons.append(info[2])
        WMMT_data_min.append(info[3])
        WMMT_data_max.append(info[4])
        CMMT_data_min.append(info[5])
        CMMT_data_max.append(info[6])
        WMMT_modern_obs.append(info[7])
        CMMT_modern_obs.append(info[8])
        refs.append(info[9])
   
    labels = []
    deg= u'\N{DEGREE SIGN}'
    for i, site in enumerate(sites):
  #     label = ''.join([c for c in site if c.isupper()])
        latstr = np.str(lats[i]) + deg + 'N'
        if lons[i] >180:
           lonstr = np.str((lons[i] - 360.) * -1.0) + deg +  'W'
        else:
           lonstr = np.str(lons[i]) + deg + 'E'
         
        label = site + ' (' +  latstr + ',' +  lonstr + ')'
        labels.append(site)
   
    return  (labels, lats, lons, np.asarray(WMMT_data_min),
             np.asarray(WMMT_data_max),  np.asarray(CMMT_data_min),
             np.asarray(CMMT_data_max),  np.asarray(WMMT_modern_obs),
             np.asarray(CMMT_modern_obs),refs)

 
def get_land_km5c():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle, modern obs WMMT, modern obs CMMT
    # reference and date

    # lake baikal is from what ulrich sent me.
    # Lake E from Brigette-greeme mean july temp of +8 and average winter lows of 35degC

    sitedata.append(['Lake El\'gygytgyn', 67, 172, 15.0, 16.0,
                    -36.8, -30.4, 
                     8.0, np.nan,'CMMT 3.199Ma - 3.209Ma; Pavel Tarasov (pers. comm) \n WMMT Brigham-Grette et al. 2013'])
    sitedata.append(['Lake Baikal', 56, 108, 15.28, 17.52,
                     -1.67, 1.07, 
                     15.3, -17.4,'Km5c - unpublished \n (Method of Klage et al 2020)'])
    
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)

    return (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs)

def get_land_LP():
    """
    these have been obtained from various sources so I am just typing them in
    LP means late pliocene
    """
    sitedata = []
    # lake baikal is from demske 2002.
    sitedata.append(['Lake Baikal', 56, 108, 13.0, 24.0, -15, 5,
                     np.nan, np.nan,'Prior to 3.5Ma (Demske et al 2002)'])
    # these are from popova et al 2012 using coexistence approach.
  
    sitedata.append(['Mirny', 55, 82, 18.8, 24.6, -0.3, 0.7,
                     np.nan, np.nan,'Popova et al 2012'])
    sitedata.append(['Merkutlinskiy', 56, 72, 17.3, 23.8, -3.8, 6.2,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Kabinet', 55, 80, 21.6, 24.4, -4.4, 4.6,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Delyankir', 63, 133, 18.9, 24.9, -6.9, 1.3,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Chernoluche', 55, 73, 19.6, 20.3, -5.9, 0.7,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['Blizkiy', 64, 162, 15.6, 23.3, -12.8, 5.2,
                     np.nan, np.nan,' --"--'])
    sitedata.append(['42km', 55, 80, 21.6, 23.3, -4.4, 0.7,
                     np.nan, np.nan,' --"--'])
  
    sitedata.append(['Lost Chicken Mine', 64, 218, 12.0, 12.0, 
                     -2.0, -2.0, 15.3, -25.1, '2.9 +/- 0.4Ma: Ager et al. 1994'])
  
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)

    return (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs)

def get_land_EP():
    """
    these have been obtained from various sources so I am just typing them in
    EP means early pliocene
    """
    sitedata = []
    # these are from popova et al 2012 using coexistence approach.
  
    sitedata.append(['Tnekveem', 66, 177, 18.9, 25.6, -11.8, 5.8, 
                     np.nan, np.nan,'Popova et al 2012'])
    sitedata.append(['Hydzhak', 63, 147, 18.8, 24.9, -8.7, 1.3,
                     np.nan, np.nan,' --"--'])

   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)

    return (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs)

def get_land_fletcher():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT data, max WMMT data,
    # min CMMT data, max CMMT data
    # modern obs WMMT, modern obs CMMT
    # we think FLB is 3.8Ma, BP originally was 3.4 but has been redated
   
    # for TF data we are reporting here temperature of warmest month
    # and temperature of warmest quater
    
    sitedata.append(['Near Meighen Island', 77.5, 261, [19.6, 12.8], 
                     [20.5, 13.3],
                    [-11.6, -6.8], [-11.4, -6.2],
                     4.1, -42.5, 'Fletcher et al. 2017'])
    sitedata.append(['Beaver Pond', 79, 278, [18.4, 12.4], [20.9, 13.1],
                     [-12.2, -7.3], [-11.5, -6.8],
                      7.1, -39.7, '3.9 +1.5 / -0.5Ma: Fletcher et al. 2017'])
    sitedata.append(['Fyles Leaf Beds', 79, 277, [19.7, 12.6], [21.1, 13.4],
                     [-12.8, -7.2], [-9.1, -5.5],
                      np.nan, np.nan, '3.8 +1/-0.7Ma: Fletcher et al. 2017'])
     
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max,  WMMT_modern_obs,
             CMMT_modern_obs,refs)

def get_land_fletcher_ca():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT data, max WMMT data,
    # min CMMT data, max CMMT data
    # modern obs WMMT, modern obs CMMT
    # we think FLB is 3.8Ma, BP originally was 3.4 but has been redated
   
    # for tf data we are reporting hwere temp of warmest month and
    # warmest quater
    
    sitedata.append(['Near Meighen Island', 77.5, 261, [18.1, 10.6], 
                     [22.8, 16.2],
                    [-21.7,-16.3,], [-7.9, -2.7],
                     4.1, -42.5,'Fletcher et al. 2017'])
    sitedata.append(['Beaver Pond', 79, 278, [18.1, 11.3],[22.4, 16.3],
                     [-21.7, -15.0], [-8.1, -3.5],
                      7.1, -39.7,'3.9 +1.5 / -0.5Ma: Fletcher et al. 2017'])
    sitedata.append(['Fyles Leaf Beds', 79, 277, [18.1, 10.9], [22.7, 15.0],
                     [-16.9, -12.4], [-6.4, -2.3],
                      np.nan, np.nan,'3.8 +1/-0.7Ma: Fletcher et al. 2017'])
     
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max,  WMMT_modern_obs,
             CMMT_modern_obs,refs)

def get_land_beetle():
    """
    these have been obtained from various sources so I am just typing them in
    """
    sitedata = []
    # site data is
    # sitename, sitelat, sitelon, min WMMT veg, max WMMT veg,
    # min WMMT beetle, max WMMT beetle, min CMMT veg, max CMMT veg
    # min CMMT beetle, max CMMT beetle, modern obs WMMT, modern obs CMMT

    # all were reported in Elias and Matthews
    # note Pliocene Tmin - modern Tmin varies between 2.3 degC and 20degC

    sitedata.append(['Ballast Brook', 74, 237, 
                     14.0, 14.5, -21.0, -19.5, 2.4, -41.4,'~3-5Ma: Flyes et al 1994'])
    sitedata.append(['Strathcona Beaver Peat', 79, 278, 
                     11.7, 12.2, -28.7, -27.2, 2.4, -41.4,'>3.3Ma: Matthews and Flyes 2000'])
    sitedata.append(['Near Meighen Island', 77.5, 261, 
                     11.5, 13.5, -33.0, -18.5, 4.1, -42.5,'~3Ma: Elias and Matthews 2002'])
    sitedata.append(['Lost Chicken Mine', 64, 218,  
                     13.5, 16.0, -27.75, -19.25, 15.3, -25.1,'~3Ma: Matthews and Telka 1997'])
    sitedata.append(['Bluefish', 67, 221,  
                     12.7, 15.0, -30.0, -20.5, 16.0, -29.0,'~LP: Matthews and Telka 1997'])
   
    (labels, lats, lons, WMMT_data_min, WMMT_data_max,  
     CMMT_data_min, CMMT_data_max, WMMT_modern_obs,
     CMMT_modern_obs,refs) = land_reformat(sitedata)


    
   
    return  (labels, lats, lons, WMMT_data_min,
             WMMT_data_max,  CMMT_data_min,
             CMMT_data_max,  WMMT_modern_obs,
             CMMT_modern_obs,refs)


def plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                labels,titleinfo,ystart,refs):
    """
    this subroutine tries to plot the figure for the paper which shows a nice
    DMC 
    """

    titlename = {'KM5c' : 'Near KM5c',
                 'LPCA' : 'Late Pliocene',
                 'EPCA' : 'Early Pliocene',
                 'TF-CRACLE' : 'Pliocene (Coexistence Likelihood Estimation)',
                 'TF-CA' : 'Pliocene (Coexistence Approach)',
                 'BA' : 'Beetle Assemblage data'
        }
    labelname = {0 : 'palaeodata WMMT',
                 1 : 'palaeodata CMMT',
                 2 : 'MMM WMMT',
                 3 : 'MMM CMMT'}

    WMMT_data_mean = (WMMT_data_min + WMMT_data_max) / 2.0
 
    CMMT_data_mean = (CMMT_data_min + CMMT_data_max) / 2.0
    
    ax1 = plt.axes(frameon=False)
     
    if ystart == 0:
        ax1.get_xaxis().tick_top()
        ax1.axes.get_yaxis().set_visible(False)
        ax1.set_ylim([0, 32])
        plt.gca().invert_yaxis()
        #ax.set_xlim([xmin, xmax])
      

   
    nsites=len(WMMT_data_min)
    yarray = np.arange(ystart+1.5, nsites+ystart+1, 1)
    yend=ystart+nsites
 
    # plot warm and cold month temperature anomalies

    if titleinfo[0:2] != 'TF': # tamara fletchers data was not wmmt
        ax1.hlines(y=yarray, xmin=WMMT_data_min, xmax= WMMT_data_max,color='tab:red')
        ax1.hlines(y=yarray, xmin=CMMT_data_min, xmax= CMMT_data_max,color='tab:blue')
   
        plt.scatter(WMMT_data_mean, 
                    yarray, color='tab:red', marker='^',
                    label=labelname.get(ystart,None),s=60)
        plt.scatter(CMMT_data_mean, 
                    yarray, color='tab:blue', marker='^',
                    label = labelname.get(ystart+1,None) ,s=60)
    else:
        print(WMMT_data_mean)
        ax1.hlines(y=yarray, xmin=WMMT_data_min[:,0], xmax= WMMT_data_max[:,0],color='tab:red')
        ax1.hlines(y=yarray, xmin=CMMT_data_min[:,0], xmax= CMMT_data_max[:,0],color='tab:blue')
        ax1.hlines(y=yarray, xmin=WMMT_data_min[:,1], xmax= WMMT_data_max[:,1],color='tab:pink',linestyle='dashed')
        ax1.hlines(y=yarray, xmin=CMMT_data_min[:,1], xmax= CMMT_data_max[:,1],color='tab:cyan',linestyle='dashed')
        if titleinfo == 'TF-CRACLE':
            plt.scatter(WMMT_data_mean[:,0], 
                        yarray, color='tab:red', marker='v',
                        label='max T warmest month',s=60)
            plt.scatter(CMMT_data_mean[:,0], 
                        yarray, color='tab:blue', marker='v',
                        label =  'min T coldest month ',s=60)
            plt.scatter(WMMT_data_mean[:,1], 
                        yarray, color='tab:pink', marker='^',
                        label='wamest quarter T',s=60)
            plt.scatter(CMMT_data_mean[:,1], 
                        yarray, color='tab:cyan', marker='^',
                        label =  'coldest quarter T ',s=60)
        else:
            plt.scatter(WMMT_data_mean[:,0], 
                        yarray, color='tab:red', marker='v',s=60)
            plt.scatter(CMMT_data_mean[:,0], 
                        yarray, color='tab:blue', marker='v',s=60)
            plt.scatter(WMMT_data_mean[:,1], 
                        yarray, color='tab:pink', marker='^',s=60)
            plt.scatter(CMMT_data_mean[:,1], 
                        yarray, color='tab:cyan', marker='^',s=60)

    # put an arrow on LCM
    for i, label in enumerate(labels):
        if label[0:4] == 'Lost' and i > 4:
            plt.arrow(CMMT_data_max[i], yarray[i], -10, 0, linestyle='dotted', 
                  color='tab:blue', head_width=0.2, head_length=1.0)
   
    # try plotting axis
  
    for j in range(0, nsites):
        plt.text(-40.0, yarray[j], labels[j], ha='right')
#        if ystart == 0:
#            plt.text(27.0, yarray[j], refs[j], ha='left')
#        else:
#            plt.text(35.0, yarray[j], refs[j], ha='left')

     

    
    plt.scatter(mmm_WMT, yarray + 0.2, color='black', s=60)
    plt.scatter(mmm_WMT, yarray + 0.2, color='tab:red', 
                s=25, label=labelname.get(ystart+2,None))
    plt.scatter(mmm_CMT, yarray + 0.2, color='black', s=60)
    plt.scatter(mmm_CMT, yarray + 0.2, color='tab:blue', 
                s=25, label=labelname.get(ystart+3,None))
  
  
    # plot individual models for pliocene
    for i in range(0, len(MODELNAMES)):
        if i == 0 and ystart==0:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='tab:red', 
                        marker = 'x', s=15, label='models WMMT')
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='tab:blue', 
                        marker = 'x', s=15, label='models CMMT')
        else:
            plt.scatter(all_models_plio_WMT[:, i], yarray+0.2, color='tab:red', 
                        marker = 'x',s=15)
            plt.scatter(all_models_plio_CMT[:, i], yarray+0.2, color='tab:blue', 
                        marker = 'x', s=15)
        
    plt.hlines(y=ystart, xmin=-50., xmax=30., linewidth=0.5)
    plt.text(-30.0, -1.75, 'Cold Month Temperature (degC)', ha='left', color='tab:blue',fontsize=12)
    plt.text(5.0, -1.75, 'Warm Month Temperature (deg C)', ha='left', color='tab:red',fontsize=12)
    plt.text(-35.0,ystart+0.5,titlename.get(titleinfo),fontsize=12)
       
    
    #plt.legend(bbox_to_anchor=(0.5, -0.04), loc='lower center', ncol=3,
    #               prop = {'size':12})
    if ystart > 23:
        handles, labels = plt.gca().get_legend_handles_labels()
        order = [2, 3, 4, 5, 0,6,8,1,7,9]
        print(labels)
        print(ystart)
        plt.legend([handles[idx] for idx in order],
                   [labels[idx] for idx in order],
               bbox_to_anchor=(0.5, -0.04), loc='lower center', ncol=3,
               prop = {'size':12})
   
   
 
    return ax1,yend+2

  
def get_model_data(land_lats, land_lons):
    """
    get the model data for these latitude and longitudes
    """
    
    all_models_plio_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_plio_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_WMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_pi_CMT = np.zeros((len(land_lons), len(MODELNAMES)))
    all_models_seas_cyc_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_CMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    all_models_WMMT_anom = np.zeros((len(land_lons),len(MODELNAMES)))
    for i, model in enumerate(MODELNAMES):
        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'EOI400')
        all_models_plio_WMT[:, i] = ind_WMT
        all_models_plio_CMT[:, i] = ind_CMT

        (ind_CMT, ind_WMT) = get_single_model(model, land_lats, 
                                              land_lons, 'E280')
        all_models_pi_WMT[:, i] = ind_WMT
        all_models_pi_CMT[:, i] = ind_CMT

        all_models_seas_cyc_anom[:, i] = (
            (all_models_plio_WMT[:, i] - all_models_plio_CMT[:, i]) -
            (all_models_pi_WMT[:, i] - all_models_pi_CMT[:, i]))
        
        all_models_CMMT_anom[:, i] = (all_models_plio_CMT[:, i] -
                                      all_models_pi_CMT[:, i])

        all_models_WMMT_anom[:, i] = (all_models_plio_WMT[:, i] -
                                      all_models_pi_WMT[:, i])



    mmm_WMT = np.nanmean(all_models_plio_WMT, axis=1)
    mmm_CMT = np.nanmean(all_models_plio_CMT, axis=1)
    mmm_WMT_pi = np.nanmean(all_models_pi_WMT, axis=1)
    mmm_CMT_pi = np.nanmean(all_models_pi_CMT, axis=1)


    return  (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom, 
     all_models_WMMT_anom, mmm_WMT, mmm_CMT, mmm_WMT_pi, mmm_CMT_pi)

def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    d) plots change in seasonal cycle at MI, BP and LB
    """

   
    # get land observations and cru temperature at km5c points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_km5c()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)


   
    fig1 = plt.figure(figsize=[11.7, 11.7])
  
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'KM5c',0,refs)

   
    #####################################################################
    # get land observations and cru temperature at lp points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_LP()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'LPCA',ystart,refs)
   
   
    #####################################################################
    # get land observations and cru temperature at ep points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_EP()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)


   

    # plot data
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'EPCA',ystart,refs)
   

    ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_fletcher()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'TF-CRACLE',ystart,refs)

     ####################################################################
    # get land observations and cru temperature at early pliocene points 
    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
     CMMT_data_min, CMMT_data_max, 
     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_fletcher_ca()

    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
    (all_models_plio_WMT, all_models_plio_CMT,
     all_models_pi_WMT, all_models_pi_CMT, 
     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

    for i,site in enumerate(sites):
        print('warm',site, WMMT_data_min[i], WMMT_data_max[i],mmm_WMT[i])
        print('cold',site, CMMT_data_min[i], CMMT_data_max[i],mmm_CMT[i])
        print(' ')
 #       for j, model in enumerate(MODELNAMES):
 #           print(model, all_models_plio_CMT[i,j], all_models_plio_CMT[i,j] - all_models_plio_CMT[1,j])
  #  sys.exit(0)

  
    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
                CMMT_data_min, CMMT_data_max, 
                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
                sites,'TF-CA',ystart,refs)



     ####################################################################
    # get land observations and cru temperature at early pliocene points 
#    (sites, land_lats, land_lons, WMMT_data_min, WMMT_data_max,
#     CMMT_data_min, CMMT_data_max, 
#     WMMT_modern_obs, CMMT_modern_obs,refs) =  get_land_beetle()

#    cru_min_temp, cru_max_temp = get_cru_temp(land_lats, land_lons)
#    (all_models_plio_WMT, all_models_plio_CMT,
#     all_models_pi_WMT, all_models_pi_CMT, 
#     all_models_CMMT_anom,  all_models_WMMT_anom,  mmm_WMT, mmm_CMT, 
#     mmm_WMT_pi, mmm_CMT_pi) = get_model_data(land_lats, land_lons)

    # plot data

#    ax1,ystart = plot_figure(WMMT_data_min, WMMT_data_max, 
#                CMMT_data_min, CMMT_data_max, 
#                all_models_plio_WMT, all_models_plio_CMT, mmm_WMT, mmm_CMT,
#                sites,'BA',ystart,refs)
#    ax1.set_ylim(None,0)    
    

    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
               'vegetation/seasonal_dmc_plot_v2.eps')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_v2.pdf')
    plt.savefig(fileout)
    fileout = ('/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/PLIOMIP2/' + 
                   'vegetation/seasonal_dmc_plot_v2.png')
    plt.savefig(fileout)
    plt.close()

 
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
DATABASE = '/nfs/hera1/pliomip2/data/'

MODELNAMES = [
               'HadGEM3', 'CESM2',
              'IPSLCM6A', 
              'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 
               'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L',  'NorESM1-F'
           #  ,  'MRI2.3'
              ]

#MODELNAMES = ['IPSLCM6A']

NSAT_MMM_FILE = (FILESTART + 
                 'regridded100/NearSurfaceTemperature_multimodelmean.nc')

LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
PlioMIP_new/vegetation_data_analysis/setup_biome4_inputfiles_HadCM3.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will create the input files for biome4 for each 'regridded' model or the multimodel mean

This was altered in November 2021 to do BIOME4 for a HadCM3 sensitivity study

"""

import numpy as np
import iris
import iris.quickplot as qplt
from iris.experimental.equalise_cubes import equalise_attributes
import matplotlib.pyplot as plt
import sys
import warnings
warnings.filterwarnings("ignore")



def get_cube_avg(expt, long_name, units, long_name_req):
    """
    loads in renames and reformats that cube
    """
    monthnames = ['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
#    monthnames = ['ja','fb']

    allmonths_cubes = iris.cube.CubeList([])
    for i, month in enumerate(monthnames):
        cubes = iris.cube.CubeList([])

        for year in range(STARTYEAR,ENDYEAR):
            filename = (FILESTART + expt + '/netcdf/' + expt + 
                        'a@pdk' + np.str(year) + month + '.nc')
            indcube = iris.load_cube(filename, long_name)
            indcube.coord('t').points = year
           
            cubes.append(indcube)
        equalise_attributes(cubes)
        iris.util.unify_time_units(cubes)
        cube = cubes.concatenate_cube()
        monthcube = cube.collapsed('t', iris.analysis.MEAN)
        
        if monthcube.coord('t') != None:
            monthcube.remove_coord('t')
        nmoncube = iris.util.new_axis(monthcube)
        nmoncube.add_dim_coord(iris.coords.DimCoord(i+1, 
                standard_name='time', long_name='t', 
                var_name='t', 
                units=None,
                bounds=None,
                coord_system=None, circular=False),0)
       
        nmoncube.cell_methods=None
       
        allmonths_cubes.append(nmoncube)

    equalise_attributes(allmonths_cubes)
    iris.util.unify_time_units(allmonths_cubes)
  
    monthscube = allmonths_cubes.concatenate_cube()
    monthscube.coord('longitude').rename('lon')
    monthscube.coord('latitude').rename('lat')
    #'monthscube.coord('t').rename('time')
    monthscube.long_name = long_name_req
    monthscube.units = units
  
    return monthscube

def get_anom(field, plio_cube, pi_cube):
    """
    gets the standard data from biome 4 and regrids it onto our grid
    """
    filename = ('/nfs/see-fs-02_users/earjcti/BIOME4/'+
                'biome4_pliomip2/inputdata.nc')
    cube = iris.load_cube(filename, field)
    cubegrid = iris.load_cube('/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrparm.mask.nc', 'LAND MASK (LOGICAL: LAND=TRUE)')
    
    biomecube = cube.regrid(cubegrid, iris.analysis.Nearest())
    iris.util.squeeze(biomecube)
    sq_plio = iris.util.squeeze(plio_cube)
    sq_pi = iris.util.squeeze(pi_cube)
    biomecube_data = biomecube.data
    
    anomdata = sq_plio.data - sq_pi.data + biomecube_data  
    if field[0:4] != 'soil':
        anom_data = np.ma.masked_array(anomdata, biomecube.data.mask) 
    else:
        print('here')
        cubes = iris.load(filename)
        print(cubes)
        cubetemp = iris.load_cube(filename, 
                                  'annual absolute mimimum temperature')
        biomecube2 = cubetemp.regrid(cubegrid, iris.analysis.Nearest())
        biomearr3 = np.zeros((2,) +  np.shape(biomecube2.data))
        biomearr3[0, :, :] = biomecube2.data.mask
        biomearr3[1, :, :] = biomecube2.data.mask
        anom_datat = np.where(biomecube_data >= 0, anomdata, sq_plio.data)
        anom_data = np.ma.masked_array(anom_datat, biomearr3)          
    

    
    if (field == 'monthly total precipitation'
        or field ==  'mean monthly percent of possible sunshine'):
        anom_data = np.ma.where(anom_data > 0, anom_data, 0.0)
    if (field ==  'mean monthly percent of possible sunshine'):
        anom_data = np.ma.where(anom_data < 1000, anom_data, 1000.0)
   
    mpwp_anomcube = sq_plio.copy(data=anom_data)
    
    return mpwp_anomcube

    

def get_temp_precip():
    """
    get temperature and precipitation data in correct units
    plio_cube is from pliocene data
    anom_cube is model_plio - model_pi + observed_pi
    """
    allcubes_plio = iris.cube.CubeList([])
    allcubes_anom = iris.cube.CubeList([])

    # temperature
    cube = get_cube_avg(EXPTNAME, 'TEMPERATURE AT 1.5M', 'degC', 
                        'monthly mean temperature')
       
    cube.data = cube.data * 10.0
    allcubes_plio.append(cube)

    pi_cube = get_cube_avg(PI_EXPT, 'TEMPERATURE AT 1.5M', 'degC', 
                           'monthly mean temperature')
    pi_cube.data = pi_cube.data * 10.0

    anom_cube = get_anom('monthly mean temperature', cube, pi_cube)
    allcubes_anom.append(anom_cube)
  
    # get minimum temperature from the pliocene and the anomaly method
    print(cube)
    mintemp = cube.collapsed('time', iris.analysis.MIN)
    mintemp.long_name = 'annual absolute minimum temperature'
    mintemp.short_name = 'tmin'
    mintemp.units = 'degC'
    mintemp.remove_coord('time')
    #mintemp.remove_coord('surface')

    mintemp_anom = anom_cube.collapsed('time', iris.analysis.MIN)
    mintemp_anom.long_name = 'annual absolute minimum temperature'
    mintemp_anom.short_name = 'tmin'
    mintemp_anom.units = 'degC'
    mintemp_anom.remove_coord('time')
    #mintemp_anom.remove_coord('surface')
   


    # precipitation
    cube = get_cube_avg(EXPTNAME,'TOTAL PRECIPITATION RATE     KG/M2/S', 'mm',
                         'monthly total precipitation')
    cube.data = cube.data * 30.0
    allcubes_plio.append(cube)

    pi_cube = get_cube_avg(PI_EXPT,'TOTAL PRECIPITATION RATE     KG/M2/S', 'mm',
                         'monthly total precipitation')
    pi_cube.data = pi_cube.data * 30.0

    anom_cube = get_anom('monthly total precipitation', cube, pi_cube)
    allcubes_anom.append(anom_cube)

       
    return allcubes_plio, mintemp, allcubes_anom, mintemp_anom
    
def get_sunshine():
    """
    gets mean monthly percent of possible sunshine.  Steve P and James did
    this by:
    1. get pd total cloud (this is field30)
    2. multiplies this by -1 to get negative total cloud
    3. adds 1 to get total sun.
    4. multiplies by 1000 to get biome units.
    """

    # mPWP
    cube_mPWP = get_cube_avg(EXPTNAME, 'TOTAL CLOUD AMOUNT - RANDOM OVERLAP',
                             'percent', 
                             'mean monthly percent of possible sunshine')
    cube_mPWP.short_name = 'sun'
    cube_mPWP.data = cube_mPWP.data / 100.
    cube_mPWP.data = (cube_mPWP.data * (-1.0) + 1.0) * 1000.

    # PI
    cube_PI = get_cube_avg(PI_EXPT, 'TOTAL CLOUD AMOUNT - RANDOM OVERLAP',
                           'percent', 
                           'mean monthly percent of possible sunshine')
    cube_PI.data = cube_PI.data / 100.
  
    cube_PI.short_name = 'sun'
    cube_PI.data = (cube_PI.data * (-1.0) + 1.0) * 1000.

    # find anomaly from observations
    cube_anom = get_anom('mean monthly percent of possible sunshine',
                         cube_mPWP, cube_PI)
    
  
    return cube_mPWP, cube_anom
  
    

def get_soils():
    """
    gets the soils for input to biome4
    input cubegrid: the grid to put the soils on
    """

    def process_soils(filename):
        """
        processes each file for the soils
        """

        cube = iris.load_cube(filename)
        cube.attributes["missing_value"] = -9999

        # temporarily change sea points to average because we
        # will be adding a lsm later

        cube_data = cube.data
        avg_data = np.mean(cube_data)
        newcube_data = np.where(cube_data.mask, avg_data, cube_data)
        cube_full = cube.copy(data = newcube_data) 

        return cube_full

    
    # get grid
    cubegrid = iris.load_cube('/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrparm.mask.nc', 'LAND MASK (LOGICAL: LAND=TRUE)')
    

    # get mPWP

    file_soils_mPWP = ('/nfs/hera2/scripts/BIOME4/reference/' + 
                       'PRISM3_soil_alternative_whc.nc')
    soil_whc_cube_mPWP = process_soils(file_soils_mPWP)

    file_perc_mPWP = ('/nfs/hera2/scripts/BIOME4/reference/' + 
                      'PRISM3_soil_alternative_perc.nc')
    soil_perc_cube_mPWP = process_soils(file_perc_mPWP)
  


    # get PI

    file_soils_PI = ('/nfs/hera2/scripts/BIOME4/reference/' + 
                       'MODERN_soil_alternative_whc.nc')
    soil_whc_cube_PI = process_soils(file_soils_mPWP)

    file_perc_PI = ('/nfs/hera2/scripts/BIOME4/reference/' + 
                      'MODERN_soil_alternative_perc.nc')
    soil_perc_cube_PI = process_soils(file_perc_mPWP)
  
    # get anomaly

  
    whc_anom_cube = get_anom('soil water holding capacity',
                         soil_whc_cube_mPWP, soil_whc_cube_PI)
    
    perc_anom_cube = get_anom('soil water percolation index',
                         soil_perc_cube_mPWP, soil_perc_cube_PI)

 
    return (soil_whc_cube_mPWP, soil_perc_cube_mPWP, 
            whc_anom_cube, perc_anom_cube)

def apply_lsm(cubelist):
    """
    apply a lsm to the cubes
    """

# NOTE THE MASK IS NOT ON THE SAME GRID AS THE DATA

    masked_cubelist = iris.cube.CubeList([])
    lsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube_temp = iris.load_cube(lsm)
    cubegrid = iris.load_cube('/nfs/hera1/earjcti/ancil/P4_enh/P4_enh_qrparm.mask.nc', 'LAND MASK (LOGICAL: LAND=TRUE)')
   

    lsmcube = lsmcube_temp.regrid(cubegrid, iris.analysis.Linear())

    
    for cube in cubelist:
        print(np.shape(cube.data),np.shape(lsmcube.data))
        newcube_data = np.ma.where(lsmcube.data == 0, -9999, cube.data)
        newcube = cube.copy(data = newcube_data)
        masked_cubelist.append(newcube)
        print(newcube.data)
            
   
    return masked_cubelist


def main():
    """
    driver for program to get biome4 intput field
    """

    print('GETTING TEMP AND PRECIP')
    (allcubes_plio, abs_min_cube_plio,
     allcubes_anom, abs_min_cube_anom) = get_temp_precip()

    print('GETTING SUNSHINE')
    suncube_plio, suncube_anom = get_sunshine()
    allcubes_plio.append(suncube_plio)
    allcubes_anom.append(suncube_anom)

    print('GETTING SOIL')
    (soil_whc_plio, soil_perc_plio,
     soil_whc_anom, soil_perc_anom)= get_soils()

    allcubes_plio.append(soil_whc_plio)
    allcubes_plio.append(soil_perc_plio)
    allcubes_anom.append(soil_whc_anom)
    allcubes_anom.append(soil_perc_anom)

    allcubes_plio.append(abs_min_cube_plio)
    allcubes_anom.append(abs_min_cube_anom)
                                  # following SPickering this is monthly
                                  # mean minimum not absolute minimum
                                  # best to run in anomaly mode.
#    allcubes_land_plio = apply_lsm(allcubes_plio)
  
    print('saving',OUTFILE)
    iris.save(allcubes_plio,OUTFILE + 'absolute.nc', 
              netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  


    allcubes_land_anom = apply_lsm(allcubes_anom)
    iris.save(allcubes_anom,OUTFILE + 'anomaly.nc', 
              netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  
    #iris.save(allcubes_anom,OUTFILE + 'allanomaly.nc', 
    #          netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  
    
 

FILESTART = '/nfs/hera1/earjcti/um/'
EXPTNAME = 'xozzh'
PI_EXPT = 'xozza'
STARTYEAR = 40
ENDYEAR=60
OUTFILE = FILESTART + EXPTNAME + '/biome4/inputdata_'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/setup_biome4_inputfiles_MMM.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will create the input files for biome4 for each 'regridded' model or the multimodel mean

"""

import numpy as np
from iris.experimental.equalise_cubes import equalise_attributes
import iris
import sys


def cube_reformat(fileend, field, long_name, units):
    """
    loads in renames and reformats that cube
    """
    print(FILESTART, fileend)
    filename = (FILESTART + fileend)
    print(filename, field)
    cube = iris.load_cube(filename, field)
    #cube.remove_coord('time')
    #cube.remove_coord('year')
    cube.coord('longitude').rename('lon')
    cube.coord('latitude').rename('lat')
    #cube.coord('month').rename('time')
    cube.long_name = long_name
    cube.short_name = None
    cube.var_name = None
    cube.standard_name = None
    cube.units = units
    return cube


def cube_cloud(modelnames, fileend,long_name, units):
    """
    loads in renames and reformats that cube
    """

  
    for i, model in enumerate(modelnames):
        filename = (FILESTART + model + fileend)
        cube = iris.load_cube(filename)
        cube.remove_coord('time')
        cube.remove_coord('year')
        cube.coord('longitude').rename('lon')
        cube.coord('latitude').rename('lat')
        cube.coord('month').rename('time')

        for coord in cube.coords():
            coord.points = coord.points.astype('float32')
            coord.bounds = None
            coord.attributes = None
        cube.data = cube.data.astype('float32')

        cube.long_name = long_name
        cube.var_name = None
        cube.standard_name = None
        cube.cell_methods = None
   
        cube.units = units
        if model != 'HadCM3':
            cube.data = cube.data / 100.
        cube.data = (cube.data * (-1.0) + 1.0) * 1000.
        cube.attributes = None

        if i==0:
            cubedata = cube.data
            count = 1
        else:
            cubedata = cubedata + cube.data
            count = count + 1
        
  
    meandata = cubedata / count
   
    meancube = cube.copy(data = meandata)
    meancube.short_name = 'sun'
      

    return meancube

def get_anom(field, plio_cube, pi_cube):
    """
    gets the standard data from biome 4 and regrids it onto our grid
    """
    filename = ('/nfs/see-fs-02_users/earjcti/BIOME4/'+
                'biome4_pliomip2/inputdata.nc')
    cube = iris.load_cube(filename, field)
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    
    biomecube = cube.regrid(cubegrid, iris.analysis.Linear())
    biomecube_data = biomecube.data
    
    anom_data = np.where(biomecube_data.mask, 
                         plio_cube.data,
                         plio_cube.data - pi_cube.data + biomecube.data)

    if (field == 'monthly total precipitation'
        or field ==  'mean monthly percent of possible sunshine'):
        anom_data = np.where(anom_data > 0, anom_data, 0.0)
    if (field ==  'mean monthly percent of possible sunshine'):
        anom_data = np.where(anom_data < 1000, anom_data, 1000.0)
   
    mpwp_anomcube = plio_cube.copy(data=anom_data)
   
    return mpwp_anomcube

    

def get_temp_precip():
    """
    get temperature and precipitation data in correct units
    plio_cube is from pliocene data
    anom_cube is model_plio - model_pi + observed_pi
    everything is from the multimodel mean
    """
    allcubes_plio = iris.cube.CubeList([])
    allcubes_anom = iris.cube.CubeList([])

    # temperature
    cube = cube_reformat('/NearSurfaceTemperature_multimodelmean_month.nc',
                         'NearSurfaceTemperaturemean_plio', 
                         'monthly mean temperature','degC')
    cube.data = cube.data * 10.0
    cube.var_name = 'tas'
   
    allcubes_plio.append(cube)

    pi_cube = cube_reformat('/NearSurfaceTemperature_multimodelmean_month.nc',
                         'NearSurfaceTemperaturemean_pi', 
                          'monthly mean temperature','degC')
    pi_cube.data = pi_cube.data * 10.0
    pi_cube.var_name = 'tas'

    anom_cube = get_anom('monthly mean temperature', cube, pi_cube)
    allcubes_anom.append(anom_cube)


    # get minimum temperature from the pliocene and the anomaly method
    mintemp = cube.collapsed('time', iris.analysis.MIN)
    mintemp.long_name = 'annual absolute minimum temperature'
    mintemp.var_name = 'tas_0'
    mintemp.units = 'degC'
    mintemp.remove_coord('time')
    #mintemp.remove_coord('surface')

    mintemp_anom = anom_cube.collapsed('time', iris.analysis.MIN)
    mintemp_anom.long_name = 'annual absolute minimum temperature'
    mintemp_anom.var_name = 'tas_0'
    mintemp_anom.units = 'degC'
    mintemp_anom.remove_coord('time')
    #mintemp_anom.remove_coord('surface')
   


    # precipitation
    cube = cube_reformat('TotalPrecipitation_multimodelmean_month.nc',
                         'TotalPrecipitationmean_plio',
                         'monthly total precipitation', 'mm')
    cube.data = cube.data * 30.0
    cube.var_name = 'pr'
    allcubes_plio.append(cube)

    pi_cube = cube_reformat('TotalPrecipitation_multimodelmean_month.nc',
                         'TotalPrecipitationmean_pi',
                         'monthly total precipitation', 'mm')
    pi_cube.data = pi_cube.data * 30.0
    pi_cube.var_name = 'pr'

    anom_cube = get_anom('monthly total precipitation', cube, pi_cube)
    allcubes_anom.append(anom_cube)

       
    return allcubes_plio, mintemp, allcubes_anom, mintemp_anom
    
def get_sunshine():
    """
    gets mean monthly percent of possible sunshine.  Steve P and James did
    this by:
    1. get pd total cloud (this is field30)
    2. multiplies this by -1 to get negative total cloud
    3. adds 1 to get total sun.
    4. multiplies by 1000 to get biome units.
    """
    modelnames = ['CESM2', 'CCSM4-UoT', 'CCSM4-Utr', 'HadCM3',
                  'IPSLCM6A','MRI2.3']

    # mPWP
    cube_mPWP = cube_cloud(modelnames, '/EOI400.totcloud.mean_month.nc',
                           'mean monthly percent of possible sunshine',
                           'percent')
    cube_mPWP.var_name = 'clt'

   
    # PI
    cube_PI = cube_cloud(modelnames, '/E280.totcloud.mean_month.nc',
                            'mean monthly percent of possible sunshine',
                            'percent')
    cube_PI.var_name = 'clt'
 
    # find anomaly from observations
    cube_anom = get_anom('mean monthly percent of possible sunshine',
                         cube_mPWP, cube_PI)
   

    return cube_mPWP, cube_anom
  
    

def get_soils():
    """
    gets the soils for input to biome4
    input cubegrid: the grid to put the soils on
    """

    def process_soils(filename):
        """
        processes each file for the soils
        """

        cube1 = iris.load_cube(filename)
        cube = cube1.regrid(cubegrid, iris.analysis.Linear())
        cube.attributes["missing_value"] = -9999

        # temporarily change sea points to average because we
        # will be adding a lsm later

        cube_data = cube.data
        avg_data = np.mean(cube_data)
        newcube_data = np.where(cube_data.mask, avg_data, cube_data)
        cube_full = cube.copy(data = newcube_data) 

        return cube_full

    
    # get grid
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')

    # get mPWP

    file_soils_mPWP = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                       'PRISM3_soil_alternative_whc.nc')
    soil_whc_cube_mPWP = process_soils(file_soils_mPWP)

    file_perc_mPWP = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                      'PRISM3_soil_alternative_perc.nc')
    soil_perc_cube_mPWP = process_soils(file_perc_mPWP)
  

    # get PI

    file_soils_PI = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                       'MODERN_soil_alternative_whc.nc')
    soil_whc_cube_PI = process_soils(file_soils_mPWP)

    file_perc_PI = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                      'MODERN_soil_alternative_perc.nc')
    soil_perc_cube_PI = process_soils(file_perc_mPWP)
  
    # get anomaly

    whc_anom_cube = get_anom('soil water holding capacity',
                         soil_whc_cube_mPWP, soil_whc_cube_PI)
    
    perc_anom_cube = get_anom('soil water percolation index',
                         soil_perc_cube_mPWP, soil_perc_cube_PI)

    return (soil_whc_cube_mPWP, soil_perc_cube_mPWP, 
            whc_anom_cube, perc_anom_cube)

def apply_lsm(cubelist):
    """
    apply a lsm to the cubes
    """

# NOTE THE MASK IS NOT ON THE SAME GRID AS THE DATA

    masked_cubelist = iris.cube.CubeList([])
    lsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube_temp = iris.load_cube(lsm)
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    lsmcube = lsmcube_temp.regrid(cubegrid, iris.analysis.Linear())


   
    for cube in cubelist:
        newcube_data = np.ma.where(lsmcube.data == 0, -9999, cube.data)
        newcube = cube.copy(data = newcube_data)
        masked_cubelist.append(newcube)
        print(newcube.data)
            
   
    return masked_cubelist


def main():
    """
    driver for program to get biome4 intput field
    """

    
    (allcubes_plio, abs_min_cube_plio,
     allcubes_anom, abs_min_cube_anom) = get_temp_precip()
    
    suncube_plio, suncube_anom = get_sunshine()
    allcubes_plio.append(suncube_plio)
    allcubes_anom.append(suncube_anom)
    
    (soil_whc_plio, soil_perc_plio,
     soil_whc_anom, soil_perc_anom)= get_soils()

    allcubes_plio.append(soil_whc_plio)
    allcubes_plio.append(soil_perc_plio)
    allcubes_anom.append(soil_whc_anom)
    allcubes_anom.append(soil_perc_anom)

    allcubes_plio.append(abs_min_cube_plio)
    allcubes_anom.append(abs_min_cube_anom)
                                  # following SPickering this is monthly
                                  # mean minimum not absolute minimum
                                  # best to run in anomaly mode.
    allcubes_land_plio = apply_lsm(allcubes_plio)
  
    iris.save(allcubes_land_plio,OUTFILE + 'absolute.nc', 
              netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  


    allcubes_land_anom = apply_lsm(allcubes_anom)
    iris.save(allcubes_land_anom,OUTFILE + 'anomaly.nc', 
              netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  
    
 

FILESTART = '/nfs/hera1/earjcti/regridded/'
OUTFILE = FILESTART +  'BIOME4/inputdataMMM_'

main()
::::::::::::::
PlioMIP_new/vegetation_data_analysis/setup_biome4_inputfiles.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created January 2021 by Julia

This program will create the input files for biome4 for each 'regridded' model or the multimodel mean

"""

import numpy as np
import iris
from iris.cube import CubeList
import sys


def cube_reformat(fileend, long_name, units):
    """
    loads in renames and reformats that cube
    """
    filename = (FILESTART + MODELNAME + fileend)
    cube = iris.load_cube(filename)
    cube.remove_coord('time')
    cube.remove_coord('year')
    cube.coord('longitude').rename('lon')
    cube.coord('latitude').rename('lat')
    cube.coord('month').rename('time')
    cube.long_name = long_name
    cube.units = units
    return cube

def get_anom(field, plio_cube, pi_cube):
    """
    gets the standard data from biome 4 and regrids it onto our grid
    """
    filename = ('/nfs/see-fs-02_users/earjcti/BIOME4/'+
                'biome4_pliomip2/inputdata.nc')
    cube = iris.load_cube(filename, field)
    cubegrid = iris.load_cube('/nfs/hera1/earjcti/regridded/IPSLCM6A_origgrid/E280.NearSurfaceTemperature.allmean.nc')
    
    biomecube = cube.regrid(cubegrid, iris.analysis.Linear())
    biomecube_data = biomecube.data

    
    anom_data = np.where(biomecube_data.mask, 
                         plio_cube.data,
                         plio_cube.data - pi_cube.data + biomecube.data)

    if (field == 'monthly total precipitation'
        or field ==  'mean monthly percent of possible sunshine'):
        anom_data = np.where(anom_data > 0, anom_data, 0.0)
    if (field ==  'mean monthly percent of possible sunshine'):
        anom_data = np.where(anom_data < 1000, anom_data, 1000.0)
   
    mpwp_anomcube = plio_cube.copy(data=anom_data)
   
    return mpwp_anomcube

    

def get_temp_precip():
    """
    get temperature and precipitation data in correct units
    plio_cube is from pliocene data
    anom_cube is model_plio - model_pi + observed_pi
    """
    allcubes_plio = CubeList([])
    allcubes_anom = CubeList([])

    # temperature
    cube = cube_reformat('/EOI400.NearSurfaceTemperature.mean_month.nc',
                         'monthly mean temperature', 'degC')
    cube.data = cube.data * 10.0
    allcubes_plio.append(cube)

    pi_cube = cube_reformat('/E280.NearSurfaceTemperature.mean_month.nc',
                         'monthly mean temperature', 'degC')
    pi_cube.data = pi_cube.data * 10.0

    anom_cube = get_anom('monthly mean temperature', cube, pi_cube)
    allcubes_anom.append(anom_cube)


    # get minimum temperature from the pliocene and the anomaly method
    mintemp = cube.collapsed('time', iris.analysis.MIN)
    mintemp.long_name = 'annual absolute minimum temperature'
    mintemp.short_name = 'tmin'
    mintemp.units = 'degC'
    mintemp.remove_coord('time')
    #mintemp.remove_coord('surface')

    mintemp_anom = anom_cube.collapsed('time', iris.analysis.MIN)
    mintemp_anom.long_name = 'annual absolute minimum temperature'
    mintemp_anom.short_name = 'tmin'
    mintemp_anom.units = 'degC'
    mintemp_anom.remove_coord('time')
    #mintemp_anom.remove_coord('surface')
   


    # precipitation
    cube = cube_reformat('/EOI400.TotalPrecipitation.mean_month.nc',
                         'monthly total precipitation', 'mm')
    cube.data = cube.data * 30.0
    allcubes_plio.append(cube)

    pi_cube = cube_reformat('/E280.TotalPrecipitation.mean_month.nc',
                         'monthly total precipitation', 'degC')
    pi_cube.data = pi_cube.data * 30.0

    anom_cube = get_anom('monthly total precipitation', cube, pi_cube)
    allcubes_anom.append(anom_cube)

       
    return allcubes_plio, mintemp, allcubes_anom, mintemp_anom
    
def get_sunshine():
    """
    gets mean monthly percent of possible sunshine.  Steve P and James did
    this by:
    1. get pd total cloud (this is field30)
    2. multiplies this by -1 to get negative total cloud
    3. adds 1 to get total sun.
    4. multiplies by 1000 to get biome units.
    """

    # mPWP
    cube_mPWP = cube_reformat('/EOI400.totcloud.mean_month.nc',
                              'mean monthly percent of possible sunshine',
                              'percent')
    cube_mPWP.short_name = 'sun'
    if MODELNAME != 'HadCM3':
        cube_mPWP.data = cube_mPWP.data / 100.
    cube_mPWP.data = (cube_mPWP.data * (-1.0) + 1.0) * 1000.

    # PI
    cube_PI = cube_reformat('/E280.totcloud.mean_month.nc',
                              'mean monthly percent of possible sunshine',
                              'percent')
    if MODELNAME != 'HadCM3':
        cube_PI.data = cube_PI.data / 100.
  
    cube_PI.short_name = 'sun'
    cube_PI.data = (cube_PI.data * (-1.0) + 1.0) * 1000.

    # find anomaly from observations
    cube_anom = get_anom('mean monthly percent of possible sunshine',
                         cube_mPWP, cube_PI)
   

    return cube_mPWP, cube_anom
  
    

def get_soils():
    """
    gets the soils for input to biome4
    input cubegrid: the grid to put the soils on
    """

    def process_soils(filename):
        """
        processes each file for the soils
        """

        cube1 = iris.load_cube(filename)
        cube = cube1.regrid(cubegrid, iris.analysis.Linear())
        cube.attributes["missing_value"] = -9999

        # temporarily change sea points to average because we
        # will be adding a lsm later

        cube_data = cube.data
        avg_data = np.mean(cube_data)
        newcube_data = np.where(cube_data.mask, avg_data, cube_data)
        cube_full = cube.copy(data = newcube_data) 

        return cube_full

    
    # get grid
    cubegrid = iris.load_cube('/nfs/hera1/earjcti/regridded/IPSLCM6A_origgrid/E280.NearSurfaceTemperature.allmean.nc')
   
    # get mPWP

    file_soils_mPWP = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                       'PRISM3_soil_alternative_whc.nc')
    soil_whc_cube_mPWP = process_soils(file_soils_mPWP)

    file_perc_mPWP = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                      'PRISM3_soil_alternative_perc.nc')
    soil_perc_cube_mPWP = process_soils(file_perc_mPWP)
  

    # get PI

    file_soils_PI = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                       'MODERN_soil_alternative_whc.nc')
    soil_whc_cube_PI = process_soils(file_soils_mPWP)

    file_perc_PI = ('/nfs/hera2/scripts/BIOME4/reference_HadGEM/' + 
                      'MODERN_soil_alternative_perc.nc')
    soil_perc_cube_PI = process_soils(file_perc_mPWP)
  
    # get anomaly

    whc_anom_cube = get_anom('soil water holding capacity',
                         soil_whc_cube_mPWP, soil_whc_cube_PI)
    
    perc_anom_cube = get_anom('soil water percolation index',
                         soil_perc_cube_mPWP, soil_perc_cube_PI)

    return (soil_whc_cube_mPWP, soil_perc_cube_mPWP, 
            whc_anom_cube, perc_anom_cube)

def apply_lsm(cubelist):
    """
    apply a lsm to the cubes
    """

# NOTE THE MASK IS NOT ON THE SAME GRID AS THE DATA

    masked_cubelist = CubeList([])
    lsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube_temp = iris.load_cube(lsm)
    

    cubegrid = iris.load_cube('/nfs/hera1/earjcti/regridded/IPSLCM6A_origgrid/E280.NearSurfaceTemperature.allmean.nc')
 
    lsmcube = lsmcube_temp.regrid(cubegrid, iris.analysis.Linear())


   
    for cube in cubelist:
        newcube_data = np.ma.where(lsmcube.data == 0, -9999, cube.data)
        newcube = cube.copy(data = newcube_data)
        masked_cubelist.append(newcube)
        print(newcube.data)
            
   
    return masked_cubelist


def main():
    """
    driver for program to get biome4 intput field
    """

    
    (allcubes_plio, abs_min_cube_plio,
     allcubes_anom, abs_min_cube_anom) = get_temp_precip()
    
    suncube_plio, suncube_anom = get_sunshine()
    allcubes_plio.append(suncube_plio)
    allcubes_anom.append(suncube_anom)
    
    (soil_whc_plio, soil_perc_plio,
     soil_whc_anom, soil_perc_anom)= get_soils()

    allcubes_plio.append(soil_whc_plio)
    allcubes_plio.append(soil_perc_plio)
    allcubes_anom.append(soil_whc_anom)
    allcubes_anom.append(soil_perc_anom)

    allcubes_plio.append(abs_min_cube_plio)
    allcubes_anom.append(abs_min_cube_anom)
                                  # following SPickering this is monthly
                                  # mean minimum not absolute minimum
                                  # best to run in anomaly mode.
    allcubes_land_plio = apply_lsm(allcubes_plio)
  
    iris.save(allcubes_land_plio,OUTFILE + 'absolute.nc', 
              netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  


    allcubes_land_anom = apply_lsm(allcubes_anom)
    iris.save(allcubes_land_anom,OUTFILE + 'anomaly.nc', 
              netcdf_format="NETCDF3_CLASSIC", fill_value = -9999)  
    
  
 
FILESTART = '/nfs/hera1/earjcti/regridded/'
MODELNAME = 'IPSLCM6A_origgrid'
OUTFILE = FILESTART + MODELNAME + '/biome4/inputdata_'

main()
::::::::::::::
STUDENTS/ALICE/UCEI_index.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on 08/09/2020

@author: earjcti

This will plot the UCEI index 

"""


import sys
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import numpy as np
import matplotlib.pyplot as plt
import iris.analysis.cartography
import xarray as xr
import math as math


def low_pass_weights(window, cutoff):
    """
    # calculate weights for a running mean filter

    """
    # window: int -  The length of the filter window.

    # cutoff: float - The cutoff frequency in inverse time steps.

    order = ((window - 1) // 2) + 1
    nwts = 2 * order + 1
    w = np.zeros([nwts])
    n = nwts // 2
    w[n] = 2 * cutoff
    k = np.arange(1., n)
    sigma = np.sin(np.pi * k / n) * n / (np.pi * k)
    firstfactor = np.sin(2. * np.pi * cutoff * k) / (np.pi * k)
    w[n-1:0:-1] = firstfactor * sigma
    w[n+1:-1] = firstfactor * sigma
    return w[1:-1]


def get_nino_timeseries(cube, latmin, latmax, lonmin, lonmax):
   """
   gets the average timeseries from 'cube' in the region and apply a low pass filter
   """

   nino_slice = cube.extract(iris.Constraint(latitude = lambda cell: latmin <= cell <= latmax,
                                         longitude = lambda cell: lonmin <= cell <= lonmax,))
   nino_slice.coord('latitude').guess_bounds()
   nino_slice.coord('longitude').guess_bounds()
   grid_areas = iris.analysis.cartography.area_weights(nino_slice)

   nino_mean = nino_slice.collapsed(['longitude','latitude'], 
                                    iris.analysis.MEAN, weights = grid_areas)

   # apply running mean (Alice wasn't sure why you did this so have temporarily removed it))
   #window = 5
   # Construct 5 month low pass filters for the monthly mean sst data
   #wgts5 = low_pass_weights(window, 1. / 5.)
   # Apply  filter
   #Nino = nino_mean.rolling_window('time', iris.analysis.SUM, len(wgts5), weights = wgts5)
   Nino = nino_mean


   # sort out x axis
   nino_sst = Nino.data
   nmonths = len(nino_sst)
   nino_plio = np.zeros(nmonths)

   for i in range(1, nmonths-1):
        nino_plio[i] = (nino_sst[i-1] + nino_sst[i] + nino_sst[i+1]) / 3.0


 
   return nino_plio

def get_elnino_events(nino34_oni):
    """
    find out where we have el nino or la nina events
    returns event-array which is 1 for el nino and -1 for la nina
    and 0 for neutral
    """
  
    n = len(nino34_oni)
    event_array = np.zeros(n)

    for i in range(0, n):
        if nino34_oni[i] > 0.5:  # possible el nino event
            # if previous was an el nino than this one is
            if i > 0:
                if event_array[i-1] == 1.0: event_array[i] = 1.0
                pass # next loop
            # check next 4
            if i < n-5:
                if (nino34_oni[i+1] > 0.5 and nino34_oni[i+2] > 0.5
                    and nino34_oni[i+3] > 0.5
                    and nino34_oni[i+4] > 0.5):
                    event_array[i] = 1.0
        if nino34_oni[i] < -0.5:  # possible la nino event
            # if previous was an el nino than this one is
            if i > 0:
                if event_array[i-1] == -1.0: event_array[i] = -1.0
                pass # next loop
            # check next 4
            if i < n-5:
                if (nino34_oni[i+1] < -0.5 and nino34_oni[i+2] < -0.5
                    and nino34_oni[i+3] < -0.5
                    and nino34_oni[i+4] < -0.5):
                    event_array[i] = -1.0


    return event_array
                  
                
            



# define where to find data
#FILESTART = 'C:\\Users\\Team Knowhow\\OneDrive\\PlioMIP data\\CCSM4\\timeseries'
FILESTART = '/nfs/hera1/earjcti/regridded/HadCM3/timeseries/'
PERIOD = 'E280'
FIELDNAME = 'SST'
MODELNAME = 'HadCM3'
# define filename
FILENAME = (FILESTART + '/' + PERIOD + '.SST.timeseries_no_ann_cycle.nc')

# load cube
cube = iris.load_cube(FILENAME)

## need nino3 nino34 and nino4 timeseries

# limit domain to nino4 region
nino4_oni = get_nino_timeseries(cube, -5.5, 5.5, 160., 210.)
nino3_oni = get_nino_timeseries(cube, -5.5, 5.5, 210., 270.)
nino34_oni = get_nino_timeseries(cube, -5.5, 5.5, 190., 240.)

# get el nino array
elnino_array = get_elnino_events(nino34_oni) # e; nino array
                                             # will be 1 for 
                                             # el nino or -1 for la nina

# sort out x axis
nmonths = len(nino4_oni)
years = np.arange(0, nmonths, 1) / 12

# check that index looks correct
#plt.plot(years,nino34_oni)
#plt.plot(years,elnino_array, linestyle='dotted')
#plt.hlines(0.5,0,100)
#plt.hlines(-0.5,0,100)
#plt.show()
#sys.exit(0)

# normalise by standard deviation
mean_plio4 = np.mean(nino4_oni)
std_plio4 = np.std(nino4_oni)
standard_nino4_plio = ((nino4_oni - mean_plio4)/ std_plio4)

mean_plio3 = np.mean(nino3_oni)
std_plio3 = np.std(nino3_oni)
standard_nino3_plio = ((nino3_oni - mean_plio3)/ std_plio3)

# UCEI calculation


plio_N3 = standard_nino3_plio.data
plio_N4 = standard_nino4_plio.data

plio_N3_sq = np.square(plio_N3)
plio_N4_sq = np.square(plio_N4)

plio_r = np.sqrt( 2 * (plio_N3_sq + plio_N4_sq))
 
plio_real = np.add(plio_N3, plio_N4)
plio_imagine = np.subtract(plio_N3, plio_N4)
           
plio_var = np.divide(plio_imagine, plio_real)

plio_theta = np.zeros(np.shape(plio_var))

for i, x in enumerate(plio_real):
    if x >= 0:
         plio_theta[i] = np.arctan(plio_var[i])
    else:
         plio_theta[i] = np.arctan(plio_var[i]) - math.pi
       
plio_theta_deg = np.rad2deg(plio_theta)


# only plot those which are el ninos
plio_r = nino34_oni / elnino_array # may be a better indication of strength
plio_theta_deg = plio_theta_deg / np.abs(elnino_array)

## plot pliocene UCEI

plt.figure(figsize=(10,5))
plt.scatter(plio_theta_deg, plio_r, c ='k', s =1.5, marker = 'o')

plt.plot([-280, 100], [0.5, 0.5], c ='k', linestyle='--', linewidth=1.0)
plt.plot([-280, 100], [1.0, 1.0], c ='k', linestyle='--', linewidth=1.0)
plt.plot([-280, 100], [2.0, 2.0], c ='k', linestyle='--', linewidth=1.0)
plt.plot([-195, -195], [-0.0, 8.5], c ='k', linestyle='--', linewidth=1.0)
plt.plot([-165, -165], [-0.0, 8.5], c ='k', linestyle='--', linewidth=1.0)
plt.plot([-90, -90], [-0.0, 8.5], c ='k', linestyle='-', linewidth=1.0)
plt.plot([-15, -15], [-0.0, 8.5], c ='k', linestyle='--', linewidth=1.0)
plt.plot([15, 15], [-0.0, 8.5], c ='k', linestyle='--', linewidth=1.0)

plt.xlim(-270,90)
plt.ylim(0,8)

plt.title(PERIOD, fontsize = 14)
plt.xlabel('', fontsize = 12)
plt.ylabel('ENSO Strength', fontsize = 12)
plt.show()
::::::::::::::
STUDENTS/LAUREN/means_masked.py
::::::::::::::
#/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on 08/09/2020

@author: earjcti

This will plot global mean 

"""

import numpy as np
import numpy.ma as ma
import matplotlib.pyplot as plt
import iris
import iris.quickplot as qplt

def individual_model(model):
    period = 'EOI400'
    FILENAME = ('/nfs/hera1/earjcti/regridded/' 
                + model + 
            '/' + period + '.' + FIELD + '.allmean.nc')


    cube_pliocene = iris.load_cube(FILENAME, FIELD)

    period = 'E280'
    FILENAME = ('/nfs/hera1/earjcti/regridded/' + model + 
            '/' + period + '.' + FIELD + '.allmean.nc')

    cube_pi = iris.load_cube(FILENAME, FIELD)

    cube_anomaly = ((cube_pliocene - cube_pi)/cube_pi)*100

    V=np.arange(-30.0, 30.0, 5.0)
    
    cubedata = cube_anomaly.data
    new_array = np.ma.where(np.abs(cubedata) > 20, cubedata, -999)
    new_array2 = ma.masked_values(new_array, -999)
    print(new_array)
    newcube = cube_anomaly.copy(data=new_array2)

    #qplt.contourf(cube_anomaly, V, extend='both', cmap='coolwarm')
    qplt.contourf(newcube, V, extend='both', cmap='coolwarm')
             #should new_array replace cube_anomaly above??
    plt.title(model + ' % precip change EOI400-E280')
    plt.gca().coastlines()
    #plt.savefig(model + '_%_precip_change.jpg')
    plt.show()
    sys.exit(0)
    #plt.close()

    return cube_anomaly

def get_means(model, cube):
    """
    get global mean
    """
    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube)
    
    globalmean = cube.collapsed(['longitude','latitude'],
                                iris.analysis.MEAN,
                                weights = grid_areas)

    print(model, globalmean.data)

##########################################

#MODELNAMES = ['COSMOS', 'HadCM3', 'MIROC4m', 'CCSM4-UoT']
MODELNAMES = ['COSMOS']
FIELD = 'TotalPrecipitation'

for model in MODELNAMES:
    cube_anomaly = individual_model(model)
    global_means = get_means(model, cube_anomaly)

#################################################
# 1. COSMOS, HadCM3, MIROC  Eoi450 - Eoi400 anomaly
#    (optional changing limits, changing title, 
#     changing projection, changing colours (cmap - look in matplotlib documentation or iris))
#     (E400 - E280)  vs (Eoi400 - E280)
#
# 2. maybe tricky or maybe not.  Try plotting the global mean from all models on one figure.
#
# 3. latitudinal means and plot
#
# 4. [don't do this unless you can] as 1. but focus in on the north atlantic put.  Plotting SST. 
#
# 5. [very hard]  plotting some winds using quiverplot.  

# raw data /nfs/hera1/pliomip2/data/
# E400: COSMOS, LEEDS, MIROC4m, UoT
 
::::::::::::::
STUDENTS/LAUREN/test.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on 08/09/2020

@author: earjcti

This will plot global mean 

"""

import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.quickplot as qplt




def individual_model(model):
    period = 'EOI400'
    FILENAME = ('/nfs/hera1/earjcti/regridded/' 
                + model + 
            '/' + period + '.' + FIELD + '.allmean.nc')


    cube_pliocene = iris.load_cube(FILENAME, FIELD)

    period = 'E280'
    FILENAME = ('/nfs/hera1/earjcti/regridded/' + model + 
            '/' + period + '.' + FIELD + '.allmean.nc')

    cube_pi = iris.load_cube(FILENAME, FIELD)

    cube_anomaly = cube_pliocene - cube_pi

    V=np.arange(-10.0, 10.2, 0.2) 

    qplt.contourf(cube_anomaly, V, extend='both', cmap='RdBu_r')
    plt.title('anomaly' + model)
    plt.gca().coastlines()
    plt.savefig(model + '.eps')


    return cube_anomaly

def get_means(model, cube):
    """
    get global mean
    """
    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube)
    
    globalmean = cube.collapsed(['longitude','latitude'],
                                iris.analysis.MEAN,
                                weights = grid_areas)

    print(model, globalmean.data)

##########################################

MODELNAMES = ['CESM2', 'HadCM3', 'MIROC4m']
FIELD = 'NearSurfaceTemperature'

for model in MODELNAMES:
    cube_anomaly = individual_model(model)
    global_means = get_means(model, cube_anomaly)

#################################################
# 1. COSMOS, HadCM3, MIROC  Eoi450 - Eoi400 anomaly
#    (optional changing limits, changing title, 
#     changing projection, changing colours (cmap - look in matpl#     otlib documentation or iris)
#     (E400 - E280)  vs (Eoi400 - E280)
#
# 2. maybe tricky or maybe not.  Try plotting the global mean from all models on one figure.
#
# 3. latitudinal means and plot
#
# 4. [don't do this unless you can] as 1. but focus in on the north atlantic put.  Plotting SST. 
#
# 5. [very hard]  plotting some winds using quiverplot.  

# raw data /nfs/hera1/pliomip2/data/
# E400: COSMOS, LEEDS, MIROC4m, UoT
 
::::::::::::::
STUDENTS/LAUREN/winds2.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on 08/09/2020

@author: earjcti

This will plot winds as a quiverplot

"""

import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.quickplot as qplt
import cartopy
import cartopy.crs as ccrs


def individual_model(model):
    # eastward winds for pliocene and preindustrial
    period = 'EOI400'
    FILENAME_U = ('/nfs/hera1/earjcti/regridded/' 
                + model + '/EOI400.ua_850.0.allmean.nc')

    print(FILENAME_U)
    cubeU_plio = iris.load_cube(FILENAME_U, 'eastward_wind')


#  JULIA NOTE:  IF YOU XCONV THIS PI FILE YOU WILL SEE THAT THE LONG FIELD
# NAME IS UA_850.0
    period = 'E280'
    FILENAME_U = ('/nfs/hera1/earjcti/regridded/'
                  + model + '/E280.ua_850.0.allmean.nc')

    print(FILENAME_U)
    cubeU_pi = iris.load_cube(FILENAME_U, 'ua_850.0')


# JULIA NOTE YOU CANNOT DO A SUBTRACT IF THE UNITS ARE DIFFERENT SO WE
# WILL SET THE UNITS TO THE SAME
    cubeU_pi.units = 'm s-1'
    cubeU_anom = cubeU_plio - cubeU_pi

    # northward winds
    period = 'EOI400'
    FILENAME_V = ('/nfs/hera1/earjcti/regridded/' 
                + model + '/EOI400.va_850.0.allmean.nc')

    print(FILENAME_V)
    cubeV_plio = iris.load_cube(FILENAME_V, 'northward_wind')

    period = 'E280'
    FILENAME_V = ('/nfs/hera1/earjcti/regridded/'
                  + model + '/E280.va_850.0.allmean.nc')

    print(FILENAME_V)
    cubeV_pi = iris.load_cube(FILENAME_V, 'va_850.0')
    cubeV_pi.units = 'm s-1'
    

    cubeV_anom = cubeV_plio - cubeV_pi

    # plot
    ax1 = plt.axes(projection = ccrs.PlateCarree())
    ax1.coastlines()
    #ax1.set_extent([-35, 75, 20, 90], ccrs.PlateCarree())
    ulon = cubeU_anom.coord('longitude')
    x = ulon.points
    y = cubeU_anom.coord('latitude').points
    U_anom = cubeU_anom.data
    V_anom = cubeV_anom.data
    windspeed = (cubeU_anom ** 2 + cubeV_anom **2) ** 0.5
     
    # plot strength of winds
    V = np.arange(0, 5, 0.5)
    cs = qplt.contourf(windspeed,V, cmap='Blues')
    plt.title("Wind speed at 850mb in " + model)
        
  
    # Add arrows to show the wind vectors
    n=12 #plot every nth row
    scale=50
    Q = plt.quiver(x[::n], y[::n], U_anom[::n, ::n], V_anom[::n, ::n], pivot='middle', 
               scale=scale)
    qk = ax1.quiverkey(Q, 0.8, 1.05, 1, ' 1 m/s', labelpos='E',
                          fontproperties={'size':10})

    plt.show()
    #plt.savefig(model + '_850mb_winds'.jpg)
    #plt.close()

##########################################

MODELNAMES = ['CESM2']

for model in MODELNAMES:
    cube_anomaly = individual_model(model)

::::::::::::::
STUDENTS/LAUREN/winds.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on 08/09/2020

@author: earjcti

This will plot global mean 

"""

import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.quickplot as qplt
import cartopy
import cartopy.crs as ccrs




def individual_model(model):
    # eastward winds
    FILENAME_U = ('/nfs/hera1/earjcti/regridded/' 
                + model + '/EOI400.ua_850.0.allmean.nc')

    print(FILENAME_U)
    cubeu = iris.load_cube(FILENAME_U, 'eastward_wind')


    # northward winds
    FILENAME_V = ('/nfs/hera1/earjcti/regridded/' 
                + model + '/EOI400.va_850.0.allmean.nc')

    cubev = iris.load_cube(FILENAME_V, 'northward_wind')

    # plot
    ax1 = plt.axes(projection = ccrs.PlateCarree())
    ax1.coastlines()
    #ax1.set_extent([-35, 75, 20, 90], ccrs.PlateCarree())
    ulon = cubeu.coord('longitude')
    x = ulon.points
    y = cubeu.coord('latitude').points
    u = cubeu.data
    v = cubev.data
    windspeed = (cubeu ** 2 + cubev **2) ** 0.5
     
    # plot strength of winds
    cs = qplt.contourf(windspeed,20, cmap='Blues')
    plt.title("Wind speed at 850mb")
        
  
    # Add arrows to show the wind vectors
    n=12 #plot every nth row
    scale=400
    Q = plt.quiver(x[::n], y[::n], u[::n, ::n], v[::n, ::n], pivot='middle', 
               scale=scale)
    qk = ax1.quiverkey(Q, 0.8, 1.05, 10, ' 1 m/s', labelpos='E',
                          fontproperties={'size':10})

    plt.show()    

    


##########################################

MODELNAMES = ['CESM2']

for model in MODELNAMES:
    cube_anomaly = individual_model(model)

 
::::::::::::::
STUDENTS/SUZIE/avg_r_poc.py
::::::::::::::
#!/usr/bin/env python2.7
# julia 23/06/2021
# weighted average of r_dust_ancil.nc


import os
import iris
import numpy as np
import numpy.ma as ma
from iris.analysis.cartography import cosine_latitude_weights
import sys

cube = iris.load_cube('r_opal.ancil.nc')
new_cube_data = ma.masked_where(cube.data == 0, cube.data)

newcube = cube.copy(data=new_cube_data)

iris.save(newcube, 'r_opal_ancil_masked.nc', fill_value=-9999)
coslat = np.cos(np.deg2rad(newcube.coord('latitude').points))
depths = newcube.coord('unspecified_1').points


newcube.coord('latitude').guess_bounds()
newcube.coord('longitude').guess_bounds()
newcube.coord('unspecified_1').guess_bounds()


grid_areas = iris.analysis.cartography.area_weights(newcube)
grid_areas2 = np.zeros(np.shape(grid_areas))
for k, bound in enumerate(newcube.coord('unspecified_1').bounds):
    print(bound, bound[1] - bound[0],k, grid_areas.shape)
    grid_areas2[:, k, :, :] = grid_areas[:, k, :, :] * (bound[1] - bound[0])
  #  print('altered', grid_areas2[0, k, :, 0])

weighted_mean = newcube.collapsed(['longitude','latitude','unspecified_1'],
                                  iris.analysis.MEAN, weights=grid_areas2)
print('means',weighted_mean.data, np.mean(newcube.data), np.mean(cube.data))
weighted_mean_depth = newcube.collapsed(['longitude','latitude'],
                                  iris.analysis.MEAN, weights=grid_areas2)
print('weighted mean by depth',weighted_mean_depth.data, np.mean(weighted_mean_depth.data))

gridcube = newcube.copy(data = grid_areas2)
iris.save(gridcube,'grid_areas.nc')
sys.exit(0)

#
totval = 0
weighting = 0

for coord in newcube.coords():
    print(coord.name)
sys.exit(0)


print(newcube.coord('latitude').points)
print(coslat)


# get average of newcube
sys.exit(0)

cosweights = cosine_latitude_weights(newcube)

weights = cosweights


print(cosweights)
::::::::::::::
STUDENTS/SUZIE/regrid_miroc.py
::::::::::::::
#!/usr/bin/env python2.7
# julia 23/06/2021
# weighted average of r_dust_ancil.nc


import os
import iris
import numpy as np
import numpy.ma as ma
from iris.analysis.cartography import cosine_latitude_weights
from iris.cube import CubeList
import sys


origcube = iris.load_cube('/nfs/see-fs-01_users/ee14s2r/MIROC/woa18_decav_s00_01.nc')

gridcube = iris.load_cube('/nfs/see-fs-01_users/ee14s2r/MIROC/to.nc')
gridcube.attributes=None
gridcube.cell_methods=None

print(origcube)
print(gridcube)

# manually equalise attributes on latitude
latorig = origcube.coord('latitude')
latgrid = gridcube.coord('latitude')
latorig.points = latorig.points.astype('float64')
latgrid.attributes = latorig.attributes
latgrid.units='degrees'


# manually equalise attributes on latitude
lonorig = origcube.coord('longitude')
longrid = gridcube.coord('longitude')
lonorig.points = lonorig.points.astype('float64')
longrid.attributes = lonorig.attributes
longrid.units='degrees'

# check the depth coord
depthorig = origcube.coord('depth')
depthgrid = gridcube.coord('depth')
depthorig.points = depthorig.points.astype('float64')
depthgrid.attributes = depthorig.attributes
depthgrid.units='meters'
print(depthorig)
print(depthgrid)

newcube = origcube.regrid(gridcube,iris.analysis.Linear())
iris.save(newcube,'woa18_decav_s00_01_regrid.nc')
::::::::::::::
STUDENTS/TOM/plot_d18o.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_D18O
#PURPOSE
#    This program will plot some d18o data over the Middle east for modern
#     
#    The programs used to create the netcdf files are:
#    1.  IDLPRGS/AMAZON/ GNIP_SPATIAL_COMPARISON
#    2.  IDLPRGS/8.2ka/JONATHAN/average_field_from_timeseries.pro
#    3.                         average_d18o_from_timeseries.pro
# Julia 2/11/2016



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
import sys
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import MFDataset, Dataset


# functions start here
def plotdata(plotdata,fileno,lons,lats,titlename,minval,maxval,valinc,V,uselog,cbarname):
    lons, lats = np.meshgrid(lon,lat)
    plt.subplot(2,2,fileno+1)

    from mpl_toolkits.basemap import Basemap
    map=Basemap(llcrnrlon=10.0,urcrnrlon=70.0,llcrnrlat=10.0,urcrnrlat=55.0,projection='cyl',resolution='c')
    #map.drawmapboundary(fill_color='aqua')
    map.drawmapboundary
    x, y = map(lons, lats)
    map.drawcoastlines()
    if V == 0:
        V=np.arange(minval,maxval,valinc)
    if uselog =='y':
        cs = map.contourf(x,y,plotdata,V,norm=mp.colors.PowerNorm(gamma=1./3.))
        cbar = plt.colorbar(cs,orientation="horizontal",extend='max')
    else:
        if uselog =='la':
            cs = map.contourf(x,y,plotdata,V,norm=mp.colors.SymLogNorm(linthresh=2.0,linscale=2.0,vmin=-256,vmax=256),cmap='RdBu')
            cbar = plt.colorbar(cs,orientation="horizontal",extend='max')

        else:
            if uselog =='a':
                cs = map.contourf(x,y,plotdata,V,cmap='RdBu_r')
                cbar = plt.colorbar(cs,orientation="horizontal")
            else:
                cs = map.contourf(x,y,plotdata,V, extend='both')
                cbar = plt.colorbar(cs,orientation="horizontal")

    plt.title(titlename)
    cbar.set_label(cbarname,labelpad=-40)
    #cbar.set_label('test label',verticalalignment='bottom')
    
 #   if fileno ==0:
 #       cax=plt.axes([0.1,0.55,0.4,0.02])
 #   if fileno ==1:
 #       cax=plt.axes([0.55,0.55,0.4,0.02])
 #   if fileno ==2:
 #       cax=plt.axes([0.1,0.05,0.4,0.02])
 #   if fileno ==3:
 #       cax=plt.axes([0.55,0.05,0.4,0.02])
   # plt.colorbar(cax=cax,orientation="horizontal")


################################
# main program

# read in the data from Kanhu's modern simulation

os.chdir("/nfs/hera1/earjcti/um/netcdf/xiboi_netcdf/")
filename="xiboia@pdy11*.nc"
f=MFDataset(filename)
f.dimensions
f.variables

# retrieving a variable
stashcode338 = f.variables['QCL'][:]
lat = f.variables['latitude'][:]
lon = f.variables['longitude'][:]

f.close()
print(np.shape(stashcode338))

mean338 = np.mean(stashcode338,axis=0)
total16o = mean338[0,:,:]+mean338[3,:,:]+mean338[6,:,:]+mean338[9,:,:]
total18o = mean338[1,:,:]+mean338[4,:,:]+mean338[7,:,:]+mean338[10,:,:]
delta18o = (((total18o/total16o)-2005.2e-6))/2005.2e-9
print (delta18o)
#print (np.shape(mean338))
permille=u'\u2030'
plotdata(delta18o,0,lon,lat,'delta18o',-15,15,1.0,0.0,'n',permille)
plt.show()
sys.exit()

# plot modern data

degC=u'\N{DEGREE SIGN}'+'C'
permille=u'\u2030'


plt.figure(0)
plotdata(avg_d18o_mod,0,lon,lat,'avg d18O modern',-15,0,1.0,0.0,'n',permille)
plotdata(stdev_d18o_mod,1,lon,lat,'stdev d18O modern',0,7,1.0,0.0,'n',permille)

V=[0,2,4,8,16,32,64,128,256]
plotdata(avg_precip_mod,2,lon,lat,'avg precip modern',0,100,10.0,V,'y','mm/month')
V=[0,2,4,8,16,32,64,128,256,512]
plotdata(stdev_precip_mod,3,lon,lat,'stdev precip modern',0,100,10,V,'y','mm/month')

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/0ka_d18o_p.png' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/0ka_d18o_p.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()
plt.figure(1)


plotdata(avg_temp_mod,0,lon,lat,'avg temp modern',0,40,5.0,0,'n',degC)
plotdata(stdev_temp_mod,1,lon,lat,'stdev temp modern',0,1.5,0.1,0,'n',degC)

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/0ka_temperature.png' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/0ka_temperature.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()

#=========================================
# read in the data from the 9ka simulation

os.chdir("/nfs/hera1/earjcti/IDLPLOTS/8.2ka/")

filename="tcnkx_d18o_300_839.nc"
f=Dataset(filename, mode='r')
f.dimensions
f.variables
avg_d18o_9k = f.variables['d18o - annual'][:]
stdev_d18o_9k = f.variables['d18o - stdev'][:]
f.close()

filename="tcnkx_tempsurf_300_839.nc"
f=Dataset(filename, mode='r')
f.dimensions
f.variables
avg_temp_9k = f.variables['tempsurf mean'][:]
stdev_temp_9k = f.variables['tempsurf stdev'][:]
f.close()

filename="tcnkx_precip_300_839.nc"
f=Dataset(filename, mode='r')
f.dimensions
f.variables
avg_precip_9k = f.variables['precip mean mmmonth'][:]
stdev_precip_9k = f.variables['precip stdev mmyear)'][:]
f.close()

# plot 9ka data

plt.figure(0)
plotdata(avg_d18o_9k,0,lon,lat,'avg d18O 9ka',-15,0,1.0,0.0,'n',permille)
plotdata(stdev_d18o_9k,1,lon,lat,'stdev d18O 9ka',0,7,1.0,0.0,'n',permille)



V=[0,2,4,8,16,32,64,128,256]
avg_precip_9k=avg_precip_9k * 60. * 60. * 24. * 30.
plotdata(avg_precip_9k,2,lon,lat,'avg precip 9ka',0,100,10.0,V,'y','mm/month')
V=[0,2,4,8,16,32,64,128,256,512]
plotdata(stdev_precip_9k,3,lon,lat,'stdev precip 9ka',0,100,10,V,'y','mm/month')

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka_d18o_p.png' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka_d18o_p.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()
plt.figure(1)


plotdata(avg_temp_9k-273.15,0,lon,lat,'avg temp 9ka',0,40,5.0,0,'n',degC)
plotdata(stdev_temp_9k,1,lon,lat,'stdev temp 9ka',0,1.5,0.1,0,'n',degC)

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka_temperature.png' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka_temperature.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()



# plot anomaly data data


plt.figure(0)
plotdata(avg_d18o_9k-avg_d18O_mod,0,lon,lat,'avg d18O 9ka - modern',-5,5.5,0.5,0.0,'a',permille)
plotdata(stdev_d18o_9k-stdev_d18o_mod,1,lon,lat,'stdev d18O 9ka - modern',-3,3.5,0.5,0.0,'a',permille)

V=[-128,-64,-32,-16,-8,-4,-2,0,2,4,8,16,32,64,128]
plotdata(avg_precip_9k-avg_precip_mod,2,lon,lat,'avg precip 9ka-modern',-50,60,10.0,V,'la','mm/month')
V=[-128,-64,-32,-16,-8,-4,-2,0,2,4,8,16,32,64,128]
plotdata(stdev_precip_9k-stdev_precip_mod,3,lon,lat,'stdev precip 9ka - modern',-100,120,20,V,'la','mm/month')

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka-mod_d18o_p.png' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka-mod_d18o_p.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()
plt.figure(1)


plotdata(avg_temp_9k-273.15-avg_temp_mod,0,lon,lat,'avg temp 9ka - modern',-10,12,2.0,0,'a',degC)
plotdata(stdev_temp_9k-stdev_temp_mod,1,lon,lat,'stdev temp 9ka - modern',-1.0,1.2,0.2,0,'a',degC)

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka-mod_temperature.png' 
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/MIDDLE_EAST/plot_anomalies/9ka-mod_temperature.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()







import sys
sys.exit()

for plotno in range(0,6):
    if plotno % 2 == 0:
        monthname='jn'
    else:
        monthname='dc'
   
    #fileno=(plotno // 2) + 6 for xmrda
    if (plotno // 2) == 0:
        fileno='07'
    if (plotno // 2) == 1:
        fileno='13'
    if (plotno // 2) == 2:
        fileno='19'


    print(fileno,monthname)

    filename="xmrdaa@pd6%s%s.nc" % (fileno,monthname)
 
    print(filename)



    lons, lats = np.meshgrid(lon,lat)

    #X=np.zeros(lat.shape,lon.shape)
    X=np.zeros((lev.shape[0],lat.shape[0],lon.shape[0]))

    X[:,:,:] = np.squeeze(sm_data[0,:,:,:])

    # level is 0

    levuse=1
    sm=X[levuse,:,:] # single level only
    sm[sm>1e10]=0.0 # set missing values to zero

    plt.subplot(3,2,plotno+1)
    from mpl_toolkits.basemap import Basemap
    map=Basemap(projection='robin',resolution='l',lat_0=0,lon_0=180)
    map.drawmapboundary(fill_color='aqua')
    x, y = map(lons, lats)
    map.drawcoastlines()
    #if levuse == 0:
    #   V=np.arange(0,50,5)
    #if levuse == 3:
    V=np.arange(np.amin(sm),np.amax(sm),1)
       
    cs = map.contourf(x,y,sm,V)
    plt.title(filename)
    

    

plt.subplots_adjust(bottom=0.2)
cax=plt.axes([0.1,0.1,0.8,0.045])
plt.colorbar(cax=cax,orientation='horizontal')
plt.title('kg/m2')


fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/xmrda_smc_%s.eps' %levuse
plt.savefig(fileout, bbox_inches='tight')  

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/xmrda_smc_%s.png' %levuse
plt.savefig(fileout, bbox_inches='tight')  




::::::::::::::
STUDENTS/XIAOFANG/plot_differences.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_DIFFERENCES
#PURPOSE
#    This program will plot the differences between two files
# search for 'main program' to find end of functions
# Julia 11/1/2018



import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import numpy as np




################################
# main program

# annual mean

#cntrl_expt='xkvje'
#plio_expt='xkvjf'
#anom_expt='xkvjg'
#extra='n'
#HadCM3='n'

cntrl_expt='xoorb'
#plio_expt='xibot'
anom_expt='xoorf'
HadCM3='y'
extra=' '
field = 'TotalCloud'


fieldtitle=field
if field =='field207':
    fieldtitle = 'lw_cs_toa'

cntl_file = ('/nfs/hera1/earjcti/um/' + cntrl_expt + '/database_averages/' + 
             cntrl_expt + '_Annual_Average_a@pd_' + field + '.nc')
anom_file = ('/nfs/hera1/earjcti/um/' + anom_expt + '/database_averages/' + 
             anom_expt + '_Annual_Average_a@pd_' + field + '.nc')

cntl_cube = iris.util.squeeze(iris.load_cube(cntl_file))
anom_cube = iris.util.squeeze(iris.load_cube(anom_file))

print(cntl_cube)
print(anom_cube)
diff_cube = anom_cube - cntl_cube
V=np.arange(-10.0,11.0,1.0)
if field == 'TotalCloud':
   V = V / 100.0
qplt.contourf(diff_cube,levels=V,extend='both',cmap='bwr')
plt.title(fieldtitle)
plt.gca().coastlines()
plt.show()
::::::::::::::
STUDENTS/XIAOFANG/plot_energybal.py
::::::::::::::
#!/usr/bin/env python2.7
#NAME
#    PLOT_ENERGYBAL
#PURPOSE
#    This program will plot the energy balance for Xiaofangs simulation
# xoorb is the standard mid pliocene experiment, xoorf is the one with
# the reduced height of the antarctic ice sheet
#
# search for 'main program' to find end of functions
# Julia 11/1/2018



import os
import numpy as np
import scipy as sp
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from netCDF4 import Dataset, MFDataset
import sys
#from mpl_toolkits.basemap import Basemap, shiftgrid


#functions are:
#  def global_enbal
#  def seasmean


def global_enbal(exptname,HadCM3):

    #==============
    # read in data from  average temperature files produced for Dan

    if HadCM3 == 'y':
        filestart='/nfs/hera1/earjcti/um/'+exptname+'/database_averages/'+exptname
    else:
        filestart='/nfs/hera1/earjcti/um/HadGEM_data/'+exptname+'/database_averages/'+exptname

    print(filestart)

    f=Dataset(filestart+'_Annual_Average_a@pd_Temperature.nc')
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]
    atemp=f.variables['temp'][:]
    atemp=np.squeeze(atemp)
    ny,nx=np.shape(atemp)

    # get upward and downward sw radiation
    # incoming sw
    f=Dataset(filestart+'_Annual_Average_a@pd_field200.nc')
    sw_down=f.variables['field200'][:]
    sw_down=np.squeeze(sw_down)

    # outgoing sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field207.nc')
    sw_up=f.variables['field207'][:]
    sw_up=np.squeeze(sw_up)
 

   # outgoing lw  (toa)
    filename='olr'
    f=Dataset(filestart+'_Annual_Average_a@pd_olr.nc')
    lw_toa=f.variables['olr'][:]
    lw_toa=np.squeeze(lw_toa)
  
    # outgoing lw  (toa clear sky)
    f=Dataset(filestart+'_Annual_Average_a@pd_csolr.nc')
    lwcs_toa=f.variables['field207'][:]
    lwcs_toa=np.squeeze(lwcs_toa)
    

    # net downward longwave surface
    filename='longwave'
    f=Dataset(filestart+'_Annual_Average_a@pd_'+filename+'.nc')
    net_lwdown_surf=f.variables['longwave'][:]
    net_lwdown_surf=np.squeeze(net_lwdown_surf)

 
    # outgoing lw  (surface)
    filename='ilr'
    f=Dataset(filestart+'_Annual_Average_a@pd_'+filename+'.nc')
    lw_surf=f.variables['ilr'][:]
    lw_surf=np.squeeze(lw_surf)

    
    # JULIA NOTE  THIS IS MUCH WORSE WHEN SUBTRACTING NET LWDOWN SURF
    lw_surf=lw_surf-net_lwdown_surf # upward lw rad is downlw - net downlw
    
    # ====================================
    # get the global average fields

    grid_alpha=sw_up/sw_down
    grid_epsilon=lw_toa/lw_surf

    # create weighting array
    weightarr=np.zeros(np.shape(atemp))
    for i in range(0,len(lon)):
        weightarr[:,i]=np.cos(np.deg2rad(lat))

    meantemp=np.average(atemp,weights=weightarr)
    mean_sw_up=np.average(sw_up,weights=weightarr)
    mean_sw_down=np.average(sw_down,weights=weightarr)
    mean_lw_toa=np.average(lw_toa,weights=weightarr)
    mean_lwcs_toa=np.average(lwcs_toa,weights=weightarr)
    mean_lw_surf=np.average(lw_surf,weights=weightarr)
    mean_alpha=np.average(grid_alpha, weights=weightarr)
    mean_epsilon=np.average(grid_epsilon, weights=weightarr)

    #============================================
    # calculate terms in equation
    So=1367
    sigma=5.67 * (10.0 ** (-8.))

    alpha=mean_sw_up / mean_sw_down
    mean_lw_surf_up=sigma * (meantemp ** 4)

    epsilon=mean_lw_toa / mean_lw_surf # julia using mean lwcs_toa is worse


    print('alphas',alpha,mean_alpha)
    print('epsilon',epsilon,mean_epsilon)

    t4=So / 4.0 * (1.0-alpha) / (epsilon * sigma)
    t=t4 ** (1./4.)

    t4_mean=So / 4.0 * (1.0-mean_alpha) / (mean_epsilon * sigma)
    t_mean=t4_mean ** (1./4.)


    print('globvals',So/4.0,mean_alpha,mean_epsilon,sigma)



    print('    ')
    print(exptname)
    print('=========')
    print('t from formula=',t,' K   ',t-273.15,' degC')
    print('t obs=',meantemp,' K   ',meantemp-273.15,' degC')
    print('t mean=',t_mean,' K   ',t_mean-273.15,' degC')

    # julia note the mean energy balance is not quite right.  I am ignoring it because the latitudinal energy balance seems fine.

#end def global_enbal

################################################
def dh_zonal_enbal(exptname,HadCM3):

  
    if HadCM3 == 'y':
        filestart='/nfs/hera1/earjcti/um/'+exptname+'/database_averages/'+exptname
    else:
        filestart='/nfs/hera1/earjcti/um/HadGEM_data/'+exptname+'/database_averages/'+exptname


    #==============
    # read in data from  average temperature files produced for Dan

    f=Dataset(filestart+'_Annual_Average_a@pd_Temperature.nc')
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]
    atemp=f.variables['temp'][:]
    atemp=np.squeeze(atemp)
    ny,nx=np.shape(atemp)

    # get upward and downward sw radiation
    # incoming sw
    f=Dataset(filestart+'_Annual_Average_a@pd_field200.nc')
    sw_down=f.variables['field200'][:]
    sw_down=np.squeeze(sw_down)

    # outgoing sw 
    f=Dataset(filestart+'_Annual_Average_a@pd_field201.nc')
    sw_up=f.variables['field201'][:]
    sw_up=np.squeeze(sw_up)
 
    # outgoing sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field207.nc')
    swcs_up=f.variables['field207'][:]
    swcs_up=np.squeeze(swcs_up)
 

   # outgoing lw  (toa)
    f=Dataset(filestart+'_Annual_Average_a@pd_olr.nc')
    lw_toa_up=f.variables['olr'][:]
    lw_toa_up=np.squeeze(lw_toa_up)
  
    # outgoing lw  (toa clear sky)
    f=Dataset(filestart+'_Annual_Average_a@pd_csolr.nc')
    lwcs_toa=f.variables['field207'][:]
    lwcs_toa=np.squeeze(lwcs_toa)
    

    # net downward longwave surface
    f=Dataset(filestart+'_Annual_Average_a@pd_longwave.nc')
    net_lwdown_surf=f.variables['longwave'][:]
    net_lwdown_surf=np.squeeze(net_lwdown_surf)
 
 
    # incoming lw  (surface)
    f=Dataset(filestart+'_Annual_Average_a@pd_ilr.nc')
    lw_surf_down=f.variables['ilr'][:]
    lw_surf_down=np.squeeze(lw_surf_down)


    lw_surf_up=lw_surf_down-net_lwdown_surf # upward lw rad is downlw - net downlw


    print(net_lwdown_surf)    
    
    # ====================================
    # get the zonal average fields


    grid_alpha=sw_up/sw_down
    grid_alpha_cs=swcs_up/sw_down
    grid_epsilon=lw_toa_up/(lw_surf_up)
    grid_epsilon_cs=lwcs_toa / lw_surf_up

   
    meantemp=np.average(atemp,axis=1)
    mean_sw_up=np.average(sw_up,axis=1)
    mean_sw_down=np.average(sw_down,axis=1)
    mean_lw_toa_up=np.average(lw_toa_up,axis=1)
    mean_lwcs_toa_up=np.average(lwcs_toa,axis=1)
    mean_lw_surf_down=np.average(lw_surf_down,axis=1)
    mean_net_lwdown_surf=np.average(net_lwdown_surf,axis=1)
    mean_alpha=np.average(grid_alpha,axis=1)
    mean_alpha_cs=np.average(grid_alpha_cs,axis=1)
    mean_epsilon=np.average(grid_epsilon,axis=1)
    mean_epsilon_cs=np.average(grid_epsilon_cs,axis=1)
    mean_lw_surf_up=np.average(lw_surf_up,axis=1)

    plt.subplot(2,1,1)
    plt.plot(lat,mean_alpha,label='mean_alpha')
    plt.plot(lat,mean_alpha_cs,label='mean_alpha_cs')
    plt.legend()
    plt.title('different alphas')



    sigma=5.67 * (10.0 ** (-8.))

    #============================================
    # calculate terms in equation
    H=(-1.0) * ((mean_sw_down - mean_sw_up) -(mean_lw_toa_up))
   
    
    print('means',np.average(mean_sw_down),np.average(mean_sw_up), np.average(mean_net_lwdown_surf),np.average(mean_lw_toa_up))

    t4=((mean_sw_down * (1.0-mean_alpha)) + H) / (mean_epsilon * sigma)
    t=t4 ** (1./4.)

    plt.subplot(2,1,2)
    plt.plot(lat,meantemp-t,label='temperature')
    plt.legend()
    plt.title('temperature difference')
   
    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/dh_temp_anom'+anom_expt+'-'+cntrl_expt+'.eps' 
    plt.savefig(fileout, bbox_inches='tight')  

    plt.close()


    
    components=[mean_epsilon,mean_epsilon_cs,mean_alpha,mean_alpha_cs,H,t,mean_sw_down,lat]
    return(components)

#end def dh_zonal_enbal

def split_gge(deltaT_gge):
    """
    splits up the gge and topography for the DH energy balance
    """
    orog_fname_cntl ='/nfs/hera1/earjcti/Xiaofang/P4_enh_qrparm.orog.nc'
    orog_fname_expt = '/nfs/hera1/earjcti/Xiaofang/P4_enh_qrparm.orog_no_antarctica.nc'

    f=Dataset(orog_fname_cntl)
    cntl_orog=f.variables['ht'][:]
    cntl_orog=np.squeeze(cntl_orog)
    cntl_orog_lat=np.average(cntl_orog,axis=1)
   

    f=Dataset(orog_fname_expt)
    expt_orog=f.variables['ht'][:]
    expt_orog=np.squeeze(expt_orog)
    expt_orog_lat=np.average(expt_orog,axis=1)
 
    deltaT_topo=(expt_orog_lat - cntl_orog_lat) * (-5.5) / 1000.
    deltaT_gge_only = deltaT_gge - deltaT_topo

    return deltaT_topo, deltaT_gge_only

def rf_zonal_enbal(exptname,HadCM3):

    if HadCM3 == 'y':
        filestart='/nfs/hera1/earjcti/um/'+exptname+'/database_averages/'+exptname
    else:
        filestart='/nfs/hera1/earjcti/um/HadGEM_data/'+exptname+'/database_averages/'+exptname


    #==============
    # read in data from  average temperature files produced for Dan

    f=Dataset(filestart+'_Annual_Average_a@pd_Temperature.nc')
    lat = f.variables['latitude'][:]
    lon = f.variables['longitude'][:]
    atemp=f.variables['temp'][:]
    atemp=np.squeeze(atemp)
    ny,nx=np.shape(atemp)

    # get upward and downward sw radiation
    # incoming sw
    f=Dataset(filestart+'_Annual_Average_a@pd_field200.nc')
    sw_down=f.variables['field200'][:]
    sw_down=np.squeeze(sw_down)

    # outgoing sw 
    f=Dataset(filestart+'_Annual_Average_a@pd_field201.nc')
    sw_up=f.variables['field201'][:]
    sw_up=np.squeeze(sw_up)
 
    # surface upwards sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field207_1.nc')
    sw_surf_cs_up=f.variables['field207_1'][:]
    sw_surf_cs_up=np.squeeze(sw_surf_cs_up)
 
    # surface downwards sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field208.nc')
    sw_surf_cs_down=f.variables['field208'][:]
    sw_surf_cs_down=np.squeeze(sw_surf_cs_down)
 
  # outgoing sw (clear sky flux)
    f=Dataset(filestart+'_Annual_Average_a@pd_field207.nc')
    swcs_up=f.variables['field207'][:]
    swcs_up=np.squeeze(swcs_up)
 

   # outgoing lw  (toa)
    f=Dataset(filestart+'_Annual_Average_a@pd_olr.nc')
    lw_toa_up=f.variables['olr'][:]
    lw_toa_up=np.squeeze(lw_toa_up)
  
    # outgoing lw  (toa clear sky)
    f=Dataset(filestart+'_Annual_Average_a@pd_csolr.nc')
    lwcs_toa=f.variables['field207'][:]
    lwcs_toa=np.squeeze(lwcs_toa)
    

    # net downward longwave surface
    f=Dataset(filestart+'_Annual_Average_a@pd_longwave.nc')
    net_lwdown_surf=f.variables['longwave'][:]
    net_lwdown_surf=np.squeeze(net_lwdown_surf)

 
    # incoming lw  (surface)
    f=Dataset(filestart+'_Annual_Average_a@pd_ilr.nc')
    lw_surf_down=f.variables['ilr'][:]
    lw_surf_down=np.squeeze(lw_surf_down)


    # net downward shortwave flux surface
    f=Dataset(filestart+'_Annual_Average_a@pd_solar.nc')
    net_swdown_surf=f.variables['solar'][:]
    net_swdown_surf=np.squeeze(net_swdown_surf)

    # total downward shortwave flux surface
    f=Dataset(filestart+'_Annual_Average_a@pd_field203.nc')
    sw_surf_down=f.variables['field203'][:]
    sw_surf_down=np.squeeze(sw_surf_down)


    # total cloud amount random overlap
    f=Dataset(filestart+'_Annual_Average_a@pd_TotalCloud.nc')
    cloud_frac=f.variables['field30'][:]
    cloud_frac=np.squeeze(cloud_frac)


    lw_surf_up=lw_surf_down-net_lwdown_surf # upward lw rad is downlw - net downlw

    sw_surf_up=sw_surf_down - net_swdown_surf # upwards sw rad at surf

 
    # topography
    if exptname=='xkvje':
        orog_fname='/nfs/hera1/earjcti/um/HadGEM_ancils/qrparm.orog_new.nc'
    if exptname=='xkvjg':
        orog_fname='/nfs/hera2/apps/metadata/ancil/PRISM3_ALT/HadGEM2/HadGEM_pliocene_orog.nc'
    if exptname=='xibos':
        orog_fname='/nfs/hera2/apps/metadata/ancil/cntrl2/qrparm.orog.nc'
    if exptname=='xibot':
        orog_fname='/nfs/hera2/apps/metadata/ancil/PRISM3_ALT/qrparm.orog.nc'
    f=Dataset(orog_fname)
    orog=f.variables['ht'][:]
    orog=np.squeeze(orog)


    
    # ====================================
    # get gridded epsilon and alpha



    grid_epsilon=lw_toa_up/(lw_surf_up)
    grid_epsilon_cs=lwcs_toa / lw_surf_up


    # from Taylor 2007 
    # alpha is now surface albedo.  We denote planetary albedo:  A.
    # alpha=(1-c)*alpha_clr + (c*alpha_oc)
    # oc=overcast, clr=clear sky, c=cloud fraction

    grid_A=sw_up/sw_down
    grid_A_clr=swcs_up/sw_down
    grid_alpha_clr=sw_surf_cs_up / sw_surf_cs_down
    grid_alpha=sw_surf_up / sw_surf_down
    grid_alpha_oc=(grid_alpha - ((1-cloud_frac) * grid_alpha_clr)) / cloud_frac


    #===============================
    # get amount scattered (gamma) and 
    # amount not absorbed (mu)  (absorbtion =1-mu)
    # see Taylor 2007 equation 9 for the equations

    grid_mu=(sw_surf_down / sw_down) * (1.0 - grid_alpha) + grid_A
    grid_mu_clr=(sw_surf_cs_down / sw_down) * (1.0 - grid_alpha_clr)+grid_A_clr
    grid_mu_oc=(grid_mu - ((1-cloud_frac) * grid_mu_clr)) / cloud_frac
    grid_mu_cld=grid_mu_oc / grid_mu_clr

    grid_gamma=((grid_mu - (sw_surf_down / sw_down)) /
                (grid_mu - (grid_alpha * (sw_surf_down / sw_down))))
    grid_gamma_clr=((grid_mu_clr - (sw_surf_cs_down / sw_down)) /
                (grid_mu_clr - (grid_alpha_clr * (sw_surf_cs_down / sw_down))))
    grid_gamma_oc=(grid_gamma - ((1-cloud_frac) * grid_gamma_clr)) / cloud_frac
    grid_gamma_cld=1.0 - ((1.0 - grid_gamma_oc)/(1.0 - grid_gamma_clr))
                 

    # note these values of gamma, alpha and mu have been checked against
    # equations 7 and 8 of Taylor 2007.  Therefore if A is correct and
    # qs_hat_down is correct then gamma alpha and mu are also correct.  

  
    #===============================
    # get fields for use in energy balance calculation
 
    meantemp=np.average(atemp,axis=1)
    mean_sw_up=np.average(sw_up,axis=1)
    mean_sw_down=np.average(sw_down,axis=1)
    mean_sw_surf_cs_down=np.average(sw_surf_cs_down,axis=1)
    mean_sw_surf_down=np.average(sw_surf_down,axis=1)
    mean_lw_toa_up=np.average(lw_toa_up,axis=1)
    mean_lwcs_toa_up=np.average(lwcs_toa,axis=1)
    mean_lw_surf_down=np.average(lw_surf_down,axis=1)
    mean_net_lwdown_surf=np.average(net_lwdown_surf,axis=1)
    mean_alpha=np.average(grid_alpha,axis=1)
    mean_alpha_clr=np.average(grid_alpha_clr,axis=1)
    mean_alpha_oc=np.average(grid_alpha_oc,axis=1)
    mean_alpha=np.average(grid_alpha,axis=1)
    mean_epsilon=np.average(grid_epsilon,axis=1)
    mean_epsilon_cs=np.average(grid_epsilon_cs,axis=1)
    mean_lw_surf_up=np.average(lw_surf_up,axis=1)
    mean_cloud=np.average(cloud_frac,axis=1)
    mean_mu=np.average(grid_mu,axis=1)
    mean_mu_clr=np.average(grid_mu_clr,axis=1)
    mean_mu_oc=np.average(grid_mu_oc,axis=1)
    mean_mu_cld=np.average(grid_mu_cld,axis=1)
    mean_gamma=np.average(grid_gamma,axis=1)
    mean_gamma_clr=np.average(grid_gamma_clr,axis=1)
    mean_gamma_oc=np.average(grid_gamma_oc,axis=1)
    mean_gamma_cld=np.average(grid_gamma_cld,axis=1)
    mean_A=np.average(grid_A,axis=1)
    mean_orog=np.average(orog,axis=1)

    #plt.subplot(3,1,1)
    #plt.plot(lat,mean_alpha-0.1,label='mean_alpha-0.1')
    #plt.plot(lat,mean_alpha_clr,label='mean_alpha_clr')
    #plt.plot(lat,mean_alpha_oc,label='mean_alpha_oc')
    #plt.legend()
    #plt.title('different values')

    #plt.subplot(3,1,2)
    #plt.plot(lat,mean_mu,label='mean_mu')
    #plt.plot(lat,mean_mu_clr,label='mean_mu_clr')
    #plt.plot(lat,mean_mu_oc,label='mean_mu_oc')
    #plt.legend()


    #plt.subplot(3,1,3)
    #plt.plot(lat,mean_gamma,label='mean_gamma')
    #plt.plot(lat,mean_gamma_clr,label='mean_gamma_clr')
    #plt.plot(lat,mean_gamma_oc,label='mean_gamma_oc')  
    #plt.legend()

    #plt.show()
   


    sigma=5.67 * (10.0 ** (-8.))

    #============================================
    # calculate terms in equation
    H=(-1.0) * ((mean_sw_down - mean_sw_up) -(mean_lw_toa_up))
   
    

    t4=((mean_sw_down * (1.0-mean_alpha)) + H) / (mean_epsilon * sigma)
    t=t4 ** (1./4.)


    components=[mean_epsilon,mean_epsilon_cs,mean_alpha_clr,mean_alpha_oc,mean_mu_cld,mean_gamma_cld,mean_mu_clr,mean_gamma_clr,mean_cloud,H,meantemp,mean_sw_down,lat,mean_alpha,mean_mu,mean_gamma,mean_mu_oc,mean_gamma_oc,mean_A,mean_orog]
    return(components)

#end def rf_zonal_enbal


##########################
def main_dh_zonal(cntrl_expt,anom_expt,extra,HadCM3):

    icesheetred = {'xoorf' : '0%', 'xoorg' : '25%', 'xoorh': '50%', 'xoori': '75%'}

    components=dh_zonal_enbal(cntrl_expt,HadCM3)
    emis_pi=components[0]
    emis_pi_cs=components[1]
    alpha_pi=components[2]
    alpha_pi_cs=components[3]
    H_pi=components[4]
    temp_pi=components[5]
    sw_down_pi=components[6]
    lat=components[7]
    
    components=dh_zonal_enbal(anom_expt,HadCM3)
    emis_plio=components[0]
    emis_plio_cs=components[1]
    alpha_plio=components[2]
    alpha_plio_cs=components[3]
    H_plio=components[4]
    temp_plio=components[5]
    sw_down_plio=components[6]
    lat=components[7]

    sigma=5.67 * (10.0**-8)

  

    # we are now decomposing but will also find the average value from 55N-90N
    # create weighting array
    weightarr=np.cos(np.deg2rad(lat))
    for i in range(0,len(lat)):
        if lat[i] < 55:
            weightarr[i]=0.

    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio_cs * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_pi_cs * sigma)
    t_2=t4 ** (1./4.)

    deltaT_gge=t_1-t_2

    deltaT_height, deltaT_gge_only = split_gge(deltaT_gge)
    #plt.subplot(2,1,1)
    plt.plot(lat,deltaT_gge,label='Topography + GHG',color="cyan")
    #plt.plot(lat,deltaT_height,label='topography',color="blue")
    #print(deltaT_height)
    #plt.plot(lat,deltaT_gge_only,label='GHG',color="pink")
    print('Arctic T gge',np.average(deltaT_gge,weights=weightarr))


    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio_cs * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_pi * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_pi_cs * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ce=(t_1-t_2) - (t_3 - t_4)
    print('Arctic T cloud emisivity',np.average(deltaT_ce,weights=weightarr))
    plt.plot(lat,deltaT_ce, label='cloud emissivity',color="orange")

    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio_cs)) + H_plio) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_pi)) + H_plio) / (emis_plio * sigma)
    t_3=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_pi_cs)) + H_plio) / (emis_plio * sigma)
    t_4=t4 ** (1./4.)

    deltaT_ca=(t_1-t_2) - (t_3 - t_4)
    plt.plot(lat,deltaT_ca, label='cloud albedo',color="purple")
    print('Arctic T cloud albedo',np.average(deltaT_ca,weights=weightarr))



    t4=((sw_down_plio * (1.0-alpha_plio_cs)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_pi_cs)) + H_plio) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_csa=t_1-t_2
    plt.plot(lat,deltaT_csa,label='clear sky albedo',color="chocolate",linestyle="dashdot")
    print('Arctic T clear sky albedo',np.average(deltaT_csa,weights=weightarr))



    t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio * sigma)
    t_1=t4 ** (1./4.)
    t4=((sw_down_plio * (1.0-alpha_plio)) + H_pi) / (emis_plio * sigma)
    t_2=t4 ** (1./4.)

    deltaT_H=t_1-t_2
    plt.plot(lat,deltaT_H,label='heat transport',color="red")
    print('Arctic T Heat transport',np.average(deltaT_H,weights=weightarr))
    plt.legend()

    deltaT=temp_plio-temp_pi
    plt.plot(lat,deltaT,label='Total temperature change')
    print('Arctic T total change',np.average(deltaT,weights=weightarr))
    plt.legend()


    plt.ylim(-2.0,22.0)
    mp.pyplot.axhline(y=0,xmin=-90,xmax=90,color='black')
#    if HadCM3 == 'y':
#        plt.title('Energy balance HadCM3 - Dan Hill')
#    else:
#        plt.title('Energy balance HadGEM2- Dan Hill')
        
    plt.xlabel('latitude')
    #plt.ylabel('Warming ' + anom_expt + ' - ' + cntrl_expt)
    ylabel = 'Warming:' + icesheetred.get(anom_expt) + 'EAIS - MPControl, degC'
    plt.ylabel(ylabel)
    
    fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/DH_energybal'+anom_expt+'-'+cntrl_expt
    plt.savefig(fileout + '.eps', bbox_inches='tight')  
    plt.savefig(fileout + '.png', bbox_inches='tight')  

    plt.close()

    f = open(fileout + '.txt','w+')
    line = ('lat, Topo+GHG, cloud emissivity, cloud albedo, clear sky albedo,Total temperature change \n')
    f.write(line)

    for i, latuse in enumerate(lat):
        print(latuse)
        line = (str(latuse) + ',' + str(np.round(deltaT_gge[i],4)) + ',' +  str(np.round(deltaT_ce[i],4))+ ',' +  str(np.round(deltaT_ca[i],4))+ ',' +  str(np.round(deltaT_csa[i],4))+ ',' +  str(np.round(deltaT_H[i],4))+ ',' +  str(np.round(deltaT[i],4))+ '\n')
        f.write(line)
    f.close()

        

# end of main part of Dan Hills energy balance



################################
# main program

# annual mean

#cntrl_expt='xkvje'
#plio_expt='xkvjf'
#anom_expt='xkvjg'
#extra='n'
#HadCM3='n'

cntrl_expt='xoorb'
#plio_expt='xibot'
anom_expt='xoorf' # xoorf - Antarctica 0%
                  # xoorg - Antarctica 25%
                  # xoorh - Antarctica 50%
                  # xoori - Antarctica 75%
HadCM3='y'
extra=' '


global_enbal(cntrl_expt,HadCM3)
global_enbal(anom_expt,HadCM3)


# Dan Hills energy balance
main_dh_zonal(cntrl_expt,anom_expt,extra,HadCM3)


# Ran Fengss energy balance


components=rf_zonal_enbal(cntrl_expt,HadCM3)
emis_pi=components[0]
emis_pi_cs=components[1]
alpha_pi_clr=components[2]
alpha_pi_oc=components[3]
mu_pi_cld=components[4]
gamma_pi_cld=components[5]
mu_pi_clr=components[6]
gamma_pi_clr=components[7]
cloud_pi=components[8]
H_pi=components[9]
temp_pi=components[10]
sw_down_pi=components[11]
lat=components[12]
alpha_pi=components[13]
mu_pi=components[14]
gamma_pi=components[15]
mu_pi_oc=components[16]
gamma_pi_oc=components[17]
pi_A=components[18]
topo_pi=components[19]


components=rf_zonal_enbal(anom_expt,HadCM3)
emis_plio=components[0]
emis_plio_cs=components[1]
alpha_plio_clr=components[2]
alpha_plio_oc=components[3]
mu_plio_cld=components[4]
gamma_plio_cld=components[5]
mu_plio_clr=components[6]
gamma_plio_clr=components[7]
cloud_plio=components[8]
H_plio=components[9]
temp_plio=components[10]
sw_down_plio=components[11]
lat=components[12]
alpha_plio=components[13]
mu_plio=components[14]
gamma_plio=components[15]
mu_plio_oc=components[16]
gamma_plio_oc=components[17]
plio_A=components[18]
topo_plio=components[19]


delta_T_topo=(topo_plio-topo_pi) * (-5.5) / 1000.
sigma=5.67 * (10.0**-8)

# print j corresponding to 75deg
for j in range(0,len(lat)):
    if lat[j]==75:
        lat75=j
        print(j,lat[j])

#taylor 2007 equation 16 a-c and 7
# note the equation 16a is misleading.  I think it should be
#delta_A_alpha=(1-c)delta_A_alpha_clr + c*delta_A_alpha_oc 

# get the change in albedo due to alpha


# new use mean values
mu=(mu_plio+mu_pi)/2.0
gamma=(gamma_plio+gamma_pi)/2.0
alpha=(alpha_plio+alpha_pi)/2.0
cloud=(cloud_plio+cloud_pi)/2.0

A_alpha_pi_clr=((mu * gamma) + ((mu * alpha_pi_clr * (1.0-gamma)**2.0)/ (1.0 - alpha_pi_clr * gamma)))
A_alpha_plio_clr=((mu * gamma) + ((mu * alpha_plio_clr * (1.0-gamma)**2.0)/ (1.0 - alpha_plio_clr * gamma)))
A_alpha_pi_oc=((mu * gamma) + ((mu * alpha_pi_oc * (1.0-gamma)**2.0)/ (1.0 - alpha_pi_oc * gamma)))
A_alpha_plio_oc=((mu * gamma) + ((mu * alpha_plio_oc * (1.0-gamma)**2.0)/ (1.0 - alpha_plio_oc * gamma)))
A_alpha_diff=(1.0-cloud)*(A_alpha_plio_clr - A_alpha_pi_clr)+cloud*(A_alpha_plio_oc - A_alpha_pi_oc)
A_alpha_plio=(1.0-cloud)*A_alpha_plio_clr + cloud*A_alpha_plio_oc
A_alpha_pi=(1.0-cloud)*A_alpha_pi_clr + cloud*A_alpha_pi_oc
# julia print out at 75N
print('surface alpha at 75',A_alpha_diff[lat75],A_alpha_plio[lat75],A_alpha_pi[lat75])

# get the change in albedo due to clouds eqn 16b


A_cld_pi_gamma=((mu * gamma_pi_cld) + ((mu * alpha * (1.0-gamma_pi_cld)**2.0)/ (1.0 - alpha * gamma_pi_cld)))
A_cld_plio_gamma=((mu * gamma_plio_cld) + ((mu * alpha * (1.0-gamma_plio_cld)**2.0)/ (1.0 - alpha * gamma_plio_cld)))
A_cld_pi_mu=((mu_pi_cld * gamma) + ((mu_pi_cld * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))
A_cld_plio_mu=((mu_plio_cld * gamma) + ((mu_plio_cld * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))

# equation 15 A=(a-c)Aclr + c Aoc

mu_clr=(mu_plio_clr + mu_pi_clr)/2.0
gamma_clr=(gamma_plio_clr + gamma_pi_clr)/2.0
alpha_clr=(alpha_plio_clr + alpha_pi_clr)/2.0
mu_oc=(mu_plio_oc + mu_pi_oc)/2.0
gamma_oc=(gamma_plio_oc + gamma_pi_oc)/2.0
alpha_oc=(alpha_plio_oc + alpha_pi_oc)/2.0

clravg=((mu_clr * gamma_clr) + ((mu_clr * alpha_clr * (1.0-gamma_clr)**2.0)/ (1.0 - alpha_clr * gamma_clr)))
ocavg=((mu_oc * gamma_oc) + ((mu_oc * alpha_oc * (1.0-gamma_oc)**2.0)/ (1.0 - alpha_oc * gamma_oc)))
A_deltaC=((cloud_pi-cloud_plio)*clravg)+((cloud_plio-cloud_pi)*ocavg)


A_cld_diff=A_cld_plio_mu-A_cld_pi_mu + A_cld_plio_gamma-A_cld_pi_gamma + A_deltaC

# julia print out at 75N
print('cloud A at 75',A_cld_diff[lat75])



# get the change in albedo due to clear skies


A_clr_pi_mu=((mu_pi_clr * gamma) + ((mu_pi_clr * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))
A_clr_plio_mu=((mu_plio_clr * gamma) + ((mu_plio_clr * alpha * (1.0-gamma)**2.0)/ (1.0 - alpha * gamma)))
A_clr_pi_gamma=((mu * gamma_pi_clr) + ((mu * alpha * (1.0-gamma_pi_clr)**2.0)/ (1.0 - alpha * gamma_pi_clr)))
A_clr_plio_gamma=((mu * gamma_plio_clr) + ((mu * alpha * (1.0-gamma_plio_clr)**2.0)/ (1.0 - alpha * gamma_plio_clr)))
A_clr_diff=(A_clr_plio_mu - A_clr_pi_mu)+(A_clr_plio_gamma - A_clr_pi_gamma)
print('clr A at 75',A_clr_diff[lat75],A_clr_plio_mu[lat75],A_clr_pi_mu[lat75],A_clr_plio_gamma[lat75],A_clr_pi_gamma[lat75],cloud_pi[lat75],cloud_plio[lat75])


# get change in planetary albedo to check budgets

A_diff=plio_A-pi_A


#plt.subplot(2,1,1)
plt.plot(lat,A_alpha_diff,label='A_alpha')
plt.plot(lat,A_cld_diff,label='A_cld')
plt.plot(lat,A_clr_diff,label='A clr diff')
plt.plot(lat,A_diff,label='total planetary Albedo diff')
plt.plot(lat,A_alpha_diff+A_cld_diff+A_clr_diff,label='sum')
plt.legend()
plt.title('plio-pi A components')

fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_albedo'+anom_expt+'-'+cntrl_expt+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()


#print values at 75N in plot
print('A_alpha_diff',A_alpha_diff[lat75])
print('A_cld_diff',A_cld_diff[lat75])
print('A_clr_diff',A_clr_diff[lat75])
print('A_diff',A_diff[lat75])
print(' ')



# we are now decomposing but will also find the average value from 55N-90N
# create weighting array
weightarr_Arctic=np.cos(np.deg2rad(lat))
for i in range(0,len(lat)):
    if lat[i] < 55:
        weightarr_Arctic[i]=0.
weightarr=np.cos(np.deg2rad(lat))

t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_plio_cs * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-alpha_plio)) + H_plio) / (emis_pi_cs * sigma)
t_2=t4 ** (1./4.)

deltaT_gge=t_1-t_2 - delta_T_topo


#plt.subplot(2,1,2)
fig=plt.figure()
ax=plt.subplot(111)
ax.plot(lat,deltaT_gge,label='Greenhouse gas emissivity',color="blue")
#plt.plot(lat,delta_T_topo,label='topography')
print('Arctic T gge',np.average(deltaT_gge,weights=weightarr_Arctic))
print('Arctic T topo',np.average(delta_T_topo,weights=weightarr_Arctic))


t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio_cs * sigma)
t_2=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_pi * sigma)
t_3=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_pi_cs * sigma)
t_4=t4 ** (1./4.)

deltaT_ce=(t_1-t_2) - (t_3 - t_4)
print('Arctic T cloud emisivity',np.average(deltaT_ce,weights=weightarr_Arctic))
ax.plot(lat,deltaT_ce, label='cloud emissivity',color="orange")

# surface albedo

t4=((sw_down_plio * (1.0-A_alpha_plio)) + H_plio) / (emis_plio * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-A_alpha_pi)) + H_plio) / (emis_plio * sigma)
t_2=t4 ** (1./4.)

deltaT_surfalpha=(t_1-t_2) 
ax.plot(lat,deltaT_surfalpha, label='surface albedo',color="chocolate",linestyle='dotted')
print('Arctic T surface albedo',np.average(deltaT_surfalpha,weights=weightarr_Arctic))

# check alternate value for surface (this seems to work fine)
# we did this to check how we had done clear sky and cloud
t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-(plio_A-A_alpha_diff))) + H_plio) / (emis_plio * sigma)
t_2=t4 ** (1./4.)

deltaT_altsurf=t_1-t_2
#plt.plot(lat,deltaT_altsurf,label='alt surface albedo')
print('Arctic T altsurf',np.average(deltaT_altsurf,weights=weightarr_Arctic))


# clear sky albedo

t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-(plio_A-A_clr_diff))) + H_plio) / (emis_plio * sigma)
t_2=t4 ** (1./4.)

deltaT_csalbedo=t_1-t_2
ax.plot(lat,deltaT_csalbedo,label='clear sky albedo',color="chocolate",linestyle="dashed")
print('Arctic T csalbedo',np.average(deltaT_csalbedo,weights=weightarr_Arctic))

#cloud albedo

t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-(plio_A-A_cld_diff))) + H_plio) / (emis_plio * sigma)
t_2=t4 ** (1./4.)

deltaT_cldalbedo=t_1-t_2
ax.plot(lat,deltaT_cldalbedo,label='cloud albedo',color="purple")
print('Arctic T cloud albedo',np.average(deltaT_cldalbedo,weights=weightarr_Arctic))



t4=((sw_down_plio * (1.0-plio_A)) + H_plio) / (emis_plio * sigma)
t_1=t4 ** (1./4.)
t4=((sw_down_plio * (1.0-plio_A)) + H_pi) / (emis_plio * sigma)
t_2=t4 ** (1./4.)

deltaT_H=t_1-t_2
ax.plot(lat,deltaT_H,label='heat transport',color="red")
print('Arctic T Heat transport',np.average(deltaT_H,weights=weightarr_Arctic))

deltaT=temp_plio-temp_pi
#plt.plot(lat,deltaT,label='actual temperature change')
print('Arctic T total change',np.average(deltaT,weights=weightarr_Arctic))


total_components=deltaT_gge+deltaT_ce+deltaT_surfalpha+deltaT_csalbedo+deltaT_cldalbedo+deltaT_H
#plt.plot(lat,total_components,label='total accountable')


deltaT_cloud=deltaT_ce+deltaT_cldalbedo

print('Mean T gge',np.average(deltaT_gge,weights=weightarr))
print('Mean T topo',np.average(delta_T_topo,weights=weightarr))
print('Mean T cloud emisivity',np.average(deltaT_ce,weights=weightarr))
print('Mean T surface albedo',np.average(deltaT_surfalpha,weights=weightarr))
print('Mean T csalbedo',np.average(deltaT_csalbedo,weights=weightarr))
print('Mean T cloud albedo',np.average(deltaT_cldalbedo,weights=weightarr))
print('Mean T Heat transport',np.average(deltaT_H,weights=weightarr))
print('Mean T total change',np.average(deltaT,weights=weightarr))






print('Mean cloud changes',np.average(deltaT_cloud,weights=weightarr))


plt.ylim(-4.0,8.0)
mp.pyplot.axhline(y=0,xmin=-90,xmax=90,color='black')
if HadCM3 == 'y':
    plt.title('d) Energy Balance: HadCM3',loc='left',fontsize=18)
else:
    plt.title('c) Energy Balance: HadGEM2',loc='left',fontsize=18)
plt.xlabel('Latitude',fontsize=15)
degC=u'\N{DEGREE SIGN}'+'C'
plt.ylabel('Warming ('+degC+')',fontsize=15)


plt.legend(fontsize=15)
plt.tick_params(axis='both',labelsize=15)
box=ax.get_position()
ax.set_position([box.x0,box.y0+box.height*0.1,box.width,box.height*0.9])
ax.legend(loc='lower center',bbox_to_anchor=(0.5,-0.4),ncol=3)


fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_energybal'+anom_expt+'-'+cntrl_expt+'.eps' 
plt.savefig(fileout, bbox_inches='tight')  
fileout='/nfs/see-fs-02_users/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_energybal'+anom_expt+'-'+cntrl_expt+'.png' 
plt.savefig(fileout, bbox_inches='tight')  

plt.close()

filetext='/home/earjcti/PYTHON/PLOTS/HadGEM2/plot_energybal/RF_energybal'+anom_expt+'-'+cntrl_expt+'.tex'
f= open(filetext,"w+")
f.write('latitude,Greenhouse gas emissivity,Cloud emissivity,surfacealbedo,'+
        'clear sky albedo,cloud albedo,heat_transport \n')
for i in range(0,len(lat)):
    f.write(np.str(lat[i])+','+np.str(deltaT_gge[i])+','+
            np.str(deltaT_ce[i])+','+
            np.str(deltaT_surfalpha[i])+','+
            np.str(deltaT_csalbedo[i])+','+
            np.str(deltaT_cldalbedo[i])+','+np.str(deltaT_H[i])
            +'\n')
f.close()


####

