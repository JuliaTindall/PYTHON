# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities

updated from original emergent_constraints program in 2023 to account for
the new data that Lauren sent
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def get_mgca():
    """
    this will get the mg/ca from the new files that lauren sent.
    we want to return
    proxylat, proxylon, proxy_sst_anom, Npoints 
    all of these things are as a list
    
    """
    
    # reads into a dictionary
    filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/Updated_MgCa_summary.xlsx'
    dfs = pd.read_excel(filename, sheet_name='Sheet1')
    site = dfs.iloc[0:,0]
    lat = dfs.iloc[0:,1]
    lon = dfs.iloc[0:,2]
    sstanom = dfs.iloc[0:,10]
    nval = dfs.iloc[0:,7]

    proxysite=[]
    proxylat = []
    proxylon = []
    proxy_sst_anom = []
    Npoints = []
    for i in range(0,6):
        proxysite.append(site[i])
        proxylat.append(lat[i])
        proxylon.append(lon[i])
        proxy_sst_anom.append(sstanom[i])
        Npoints.append(nval[i])
    
    return proxysite,proxylat, proxylon, proxy_sst_anom, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from the excel files for alkenones
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'pre_hadcm3_new/modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)


        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)

        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        

        if BAYSPLINE_BAYMAG_IND == 'Y':
            data_tanom = self.sitetemp_bs - self.temppi
        if BAYSPLINE_BAYMAG_IND == 'N':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        sitename = []
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1
                # i think this is correct because pandas dataframe starts at 1
                sitename.append(self.sitenames[i+1])
        
        return sitename, latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return


    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            if np.float(vals[0]) > 180.:
                lons[count] = np.float(vals[0]) - 360.
            else:
                lons[count] = np.float(vals[0])
            lats[count] = np.float(vals[1])
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
            if lons[count] == 33.0 and lats[count] == 41.5:
                print('read',lons[count],lats[count],pvals[count],count)
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readmodel(latsreq,lonsreq):
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(len(latsreq))
    lons = np.zeros(len(latsreq))
    sstanom_min  = np.zeros(len(latsreq))
    sstanom_max = np.zeros(len(lonsreq))
    allsstanom = np.zeros((len(latsreq), 17)) # the 17th is the MMM

    # reads into a dictionary
    dfs = pd.read_excel(FILEMODEL, sheet_name='EOI400-E280near')

    # need to get lons, lats, sstanom_min, sstanom_max, allsstanom 
    sites_df = dfs.loc[:,'site']
    lons_df = np.asarray(dfs.loc[:,'lon'])
    lats_df = np.asarray(dfs.loc[:,'lat'])
    sstanom_mmm = dfs.loc[:,'MMM']
    allsstanom_df=dfs.loc[:,"CCSM4":"MMM"] 
    allsstanom_np =  allsstanom_df.to_numpy()
    modnames = list(allsstanom_df)
    sstanom_min_df = np.asarray(allsstanom_df.min(axis=1))
    sstanom_max_df = np.asarray(allsstanom_df.max(axis=1))
 
    for i, lon in enumerate(lonsreq):
        lat=latsreq[i]
        # find index of lons_df and lats_df which match
        index=np.abs(lons_df - lon).argmin()
        if ((np.abs(lats_df[index] - lat) > 0.1) or (np.abs(lons_df[index] - lon) > 0.1)):
            print('you have not got the right index')
            sys.exit(0)
        lats[i] = lats_df[index]
        lons[i] = lons_df[index]
        allsstanom[i,:] = allsstanom_np[index,:]
        sstanom_min[i] = sstanom_min_df[index]
        sstanom_max[i] = sstanom_max_df[index]
        #print(sites_df[index],allsstanom_np[index,:],sstanom_min[i],sstanom_max[i])
      

    #sys.exit(0)
     
  
    return lats, lons, sstanom_min, sstanom_max, allsstanom, modnames

def print_rmse(mod_allsst, proxy_allsst, modlats, modlons, 
               proxylats, proxylons, modnames):
    """
    just prints out the rmse for all the models

    """
    print('shape proxy', np.shape(proxy_allsst))
    print('shape data', np.shape(mod_allsst))
    npoints, nmods = np.shape(mod_allsst)
    
    f1 = open('ind_model_dmc.txt','w+')
    f1.write('model        rmse      bias   within 2deg/ 1deg/ 0.5deg \n')
    for i in range(0, nmods):
        sumsq = 0.0
        avger = 0.0
        count = 0.0
        within_1deg = 0
        within_2deg = 0
        within_05deg = 0
        for j in range(0, npoints):
            sumsq = sumsq + ((proxy_allsst[j] - mod_allsst[j, i])**2.0)
            avger = avger + (proxy_allsst[j] - mod_allsst[j, i])
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 1.0):
                within_1deg = within_1deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 2.0):
                within_2deg = within_2deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 0.5):
                within_05deg = within_05deg + 1
            count = count + 1.0
            
        rmse = np.sqrt(sumsq / count)
        avger = avger / count
      
        f1.write(modnames[i].ljust(12) + ',' +  np.str(np.around(rmse,2))
                 + ',   ' +  np.str(np.around(avger,2)) + ',   ' 
                 + np.str(within_2deg) +  ',' +  np.str(within_1deg)
                 +  ',' +  np.str(within_05deg) + '\n')
        print(modnames[i].ljust(12),',',np.around(rmse,2),
                 ',   ',np.around(avger,2),',   ',within_2deg,  
               '   ',within_1deg,',   ',within_05deg)
        
    f1.close()
   
  
def get_subscript(site,latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    if site =='U1313':
        print(site,latreq,lonreq)
        print(gridlon_arr,len(gridlon_arr),ngrid)
       
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
            if site == 'U1313':
                print(thisdiff,subscript,gridlat_arr[i], gridlon_arr[i])
    

    return subscript, diffvals

def plotdata(sitenames, lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('site, lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((sitenames[i] + ',' + np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(name, lats, lons, proxysst, clim_sens, 
               proxysst_full, proxy_fulllat, proxy_fulllon):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    (mod_lat, mod_lon, mod_minsst, mod_maxsst, 
     mod_allsst, modnames) =  readmodel(proxy_fulllat,proxy_fulllon)
    print_rmse(mod_allsst, proxysst_full, mod_lat, mod_lon, proxy_fulllat, 
               proxy_fulllon, modnames)
                                               
    nvals = 0
    new_nameredu = []
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    print(mod_lat)
    for i, lat_i in enumerate(lats):
        lon_i = lons[i]
        name_i = name[i]
        # FIND INDEX FOR MODELLED GRIDBOX
        j = -999.
        for ixj, modellat in enumerate(mod_lat):
            if modellat == lat_i and mod_lon[ixj] == lon_i:
                j=ixj
        if j < 0:
            print('j is', j)
            print('cant find model results')
            print('latlon',i, lat_i,lon_i)
            sys.exit(0)
            
        # check whether they are close
        # note we are manually removing U1387 and SicilyPuntaPiccola because
        # they are only close to EC_earth3.3 which is itself anomalous.
        # these site are contaminting the results.
        print(name[i],lat_i, lon_i,mod_minsst[j] - 1, proxysst[i], mod_maxsst[j]+1)
        if mod_minsst[j] - 2.0 < proxysst[i] < mod_maxsst[j] + 2.0 and name[i] != 'U1387' and name[i] !='SicilyPuntaPiccola':
                new_nameredu.append(name_i)
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
        else:
            print('site removed',name[i])
      
    print(' ')
    print(new_nameredu)
   # sys.exit(0)
    return (np.asarray(new_nameredu, dtype = object),
            np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     


def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data from mcclymont 2023
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data

    # mg/ca is in laurens new file
    # i have checked that the preindustrial sst is the same as what we used before
    sitenames, proxylat, proxylon, proxy_sst_anom, Npoints = get_mgca()
  
    print(sitenames)

    # alkenones is as in previous program
    obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
    (names_UK37, lats_UK37, 
     lons_UK37, proxy_tanom_UK37, Npoints_UK37) = obj.get_proxydata() 

    print(' ')
   
    for i in range(0, Npoints_UK37):
        proxylat.append(lats_UK37[i])
        proxylon.append(lons_UK37[i])
        proxy_sst_anom.append(proxy_tanom_UK37[i])
        sitenames.append(names_UK37[i])

    gridlat, gridlon, pval, intercept, slope = readfile()
    print('after read',gridlat[47194],gridlon[47194],pval[47194])
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(sitenames[i],proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        print(sitenames[i],proxylat[i], proxylon[i],grid_ss, gridlon[grid_ss],gridlat[grid_ss],pval[grid_ss])
        if pval[grid_ss] < 0.05: 
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
#            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs

    print('julia')
    sys.exit(0)
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    nameredu = np.empty(nvals,dtype=object)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            nameredu[count] = sitenames[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1

            
    plotdata(nameredu, latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
 
    (new_names, new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(nameredu, latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu,
                                                 proxy_sst_anom,
                                                 proxylat, proxylon)  

                                                 
    plotdata(new_names, new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)

    
##############################################################
FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
BAYSPLINE_BAYMAG_IND = 'Y'
DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
fileending = {"Y" : "Bays", "N": "orig"}
FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_Mcclymont2023_' + fileending.get(BAYSPLINE_BAYMAG_IND)+'.eps')
FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_Mcclymont2023_' + fileending.get(BAYSPLINE_BAYMAG_IND)+'.eps')
FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant_Mcclymont2023_' + fileending.get(BAYSPLINE_BAYMAG_IND)+'.eps')
FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/pre_hadcm3_new/modeloutput_pliovar.xls')
SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
SIGNIFICANCE_NAME = 'pvalue'
TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/data_like_figure9_mcclymont2023_' + fileending.get(BAYSPLINE_BAYMAG_IND)+'.txt'

main() 
