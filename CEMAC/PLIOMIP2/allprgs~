::::::::::::::
Arctic_amp_plots.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
#Created on 05/03/2020


#@author: earjcti

This program will plot the polar amplification diagrams
requested by IPCC in particular

Arctic amplification: I suggest reporting the
average mean annual land + sea temperature for six,
30 deg latitude bands,
 or at least the temperatures for 60-90N vs 0-90N.

So we will do:

For land/sea/total the temperature anomaly for the 6 bands
"""
import sys
import warnings
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import re

warnings.filterwarnings("ignore")

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match.
    """

    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube

def get_lsm(filein):
    """
    returns a  numpy array of land points and one of sea points
    """
    tempcube = iris.load_cube(filein)
    cubegrid = iris.load_cube('one_lev_one_deg.nc')
    lsmcube = tempcube.regrid(cubegrid, iris.analysis.Linear())
    landpoints = lsmcube.data
    seapoints = (lsmcube.data - 1.0) * (-1.0)

    return landpoints, seapoints

def get_mean_data(model, expt):
    """
    gets the cube of mean data for a single model

    Parameters
    ----------
    model : the name of the model we are interested in
    expt : whether it is the experiment or the control

    Returns
    -------
    a cube with the mean data from this file
    grid_areas = the size of the grid for averaging
    """

    filename = (FILESTART + 'regridded/' + model
                + '/' + expt + '.' + FIELD + '.allmean.nc')

    cube = iris.load_cube(filename)
    cube2 = resort_coords(cube)

    cube2.coord('latitude').guess_bounds()
    cube2.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube2)

    return cube2, grid_areas


def get_region(band_upper, band_lower, cube, mask, grid_areas):
    """
    Gets the average temeprature over land within the
    bounded range

    Parameters
    ----------
    band_upper : scalar value denoting the upper latitude of
                 the band
    band_lower : scalar value denoting the lower latitude of
                 the band
    cube : the cube containing average temperatures that
           we want to get the land temperature over
    mask : numpy array containing the mask we want.  Could be a land mask
           a sea mask or ones everywhere (ie all points)
    grid_areas : the size of each gridpoint for weighting

    Returns
    -------
    bound_land_avg : scalar containing the average land
                    temperature over the bounded region

    """

    grid_areas_mask = grid_areas * mask
    grid_areas_band = np.zeros(grid_areas.shape)
    lats = cube.coord('latitude').points

    for j, lat in enumerate(lats):
        if band_lower <= lat <= band_upper:
            grid_areas_band[j, :] = grid_areas_mask[j, :]

    bound_mask_avg = cube.collapsed(['longitude', 'latitude'],
                                    iris.analysis.MEAN,
                                    weights=grid_areas_band)

    return bound_mask_avg.data

def get_pliomip1_data():
    """
    Gets the pliomip1 data for each of the bands

    Returns
    -------
    pliomip1 data

    """
    if LINUX_WIN == 'l':
        PLIOMIP1_FILE = (FILESTART + '/PLIOMIP/means_for_' + FIELD + '.txt')
    else:
        PLIOMIP1_FILE = FILESTART + 'PLIOMIP1/means_for_' + FIELD + '.txt'
    f1 = open(PLIOMIP1_FILE)
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    # find line index which has the title 'modelname, latband mean '
    string = 'modelname, latband mean'
    for i, line in enumerate(lines):
        if line[0:23] == string:
            index = i
        
    # get bands by splitting the line
    bands_line = lines[index + 1]
    bands_str_array = bands_line[10:]
    print(bands_str_array)
    res = bands_str_array.split("], [")
    res = [x.strip('[') for x in res]
    res = [x.strip(']') for x in res]
    nbands = len(res)
    bands_array = np.zeros((nbands, 2))
    for i, x in enumerate(res):
        x1, x2 = x.split(',')
        bands_array[i, 0] = x1
        bands_array[i, 1] = x2
     
    # get the data from the next lines find the mean, min and max for each band
    # for the anomaly only
    minval = np.zeros(nbands)
    minval[:] = 100.
    maxval = np.zeros(nbands)
    meanval = np.zeros(nbands)
    
    for i in range(index + 2, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break      
        modname, eoi400, e280, anom = line.split(',')

        eoi400_val = np.array(eoi400.strip('[]').split(), dtype=float)
        e280_val = np.array(e280.strip('[]').split(), dtype=float)
        
        anom_val = np.array(anom.strip('[]').split(), dtype=float)
        
        for i, anom in enumerate(anom_val):
            if anom < minval[i]:
                minval[i] = anom
            if anom > maxval[i]:
                maxval[i] = anom
        if modname == 'MEAN':
            meanval = anom_val
        
    return meanval, minval, maxval
  
   
   
   
def plot_temp_by_lat(anomaly, uppervals, lowervals, plottype, fileoutstart,
                     mean_p1, min_p1, max_p1):
    """
    plot the temperature anomaly vs the region on one plot

    Parameters
    ----------
    anomaly : temperature anomaly to plot np.shape= nmodels, nbounds
    uppervals : the upper limit of the boundary range (np.shape = nbounds)
    lowervals : the lower limit of the boundary range (np.shape = nbounds)
    plottype : 'Land' 'Ocean' or '' if empty it is all surface types

    Returns
    -------
    None.

    """

    titlename = 'MPWP - PI ' + plottype + ' SAT anomaly'
    latns = {-90.0 : '90S', -60.0: '60S', -30.0 : '30S', 0.0 : '0N',
             30.0 : '30N', 60.0 : '60N', 90.0 : '90N'}


    labels = []
    for i, upval in enumerate(uppervals):
        labels.append(latns.get(upval) + '-' + latns.get(lowervals[i]))

    ax = plt.subplot(1, 1, 1)
    for i, model in enumerate(MODELNAMES):
        if i < len(MODELNAMES) / 2.0:
            ax.plot(anomaly[i, :], labels, label=model)
        else:
            ax.plot(anomaly[i, :], labels, label=model, linestyle='dashed')


    ax.plot(np.mean(anomaly, axis=0), labels, color='black',
            linestyle='dashed',
            linewidth=2, label='avg')
    plt.title(titlename)
    plt.xlabel(UNITS)

    if PLIOMIP1 == 'y' and plottype == '':
        ax.plot(mean_p1, labels, label = 'PlioMIP1', color='black',
                linestyle = 'dotted', linewidth=2)
        ax.fill_betweenx(labels, min_p1, max_p1, alpha=0.2, 
                        color="grey")
       
    FILETXT = (FILESTART + '/regridded/alldata/data_for_supp_fig2_' 
               + plottype + '.txt')

    txtout = open(FILETXT, "w+")
    
    writedata = 'modelname'
    for j in range(0, len(labels)):
        writedata = writedata + ',' + labels[j]
    writedata = writedata + '\n'
    txtout.write(writedata)

    for i, mod in enumerate(MODELNAMES):
        writedata = mod 
        for j in range(0, len(labels)):
            writedata = writedata + ',' + (np.str(np.around(anomaly[i,j],2)))
        writedata = writedata + '\n'
        txtout.write(writedata)
    txtout.close()
   

    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    fileout = (fileoutstart + '/polar_amplification'  + plottype + '_anomaly.eps')
    plt.savefig(fileout)
    fileout = (fileoutstart + '/polar_amplification'  + plottype + '_anomaly.pdf')
    plt.savefig(fileout)
    plt.show()
    plt.close()


#####################################
def main():
    """
    Tha main control of the program to plot the
    polar amplification by temperature by latitude band

    """

    if LINUX_WIN == 'w':
        fileoutstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD + '\\')
        exptlsm = (FILESTART + 'regridded/PlioMIP2_Boundary_conds' +
                   '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'regridded/PlioMIP2_Boundary_conds' +
                   '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
    else:
        fileoutstart = '/nfs/hera1/earjcti/regridded/allplots/' + FIELD + '/'
        dataoutstart = '/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm = (FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh' +
                   '/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'PlioMIP2_Boundary_conds/Modern_std/' +
                   'Modern_std/Modern_std_LSM_v1.0.nc')


    ########################################################
    # setup: get the lsm for the land sea contrast plot

    exptland, exptsea = get_lsm(exptlsm)
    cntlland, cntlsea = get_lsm(cntllsm)
    maskall = np.ones(np.shape(exptland))


    #########################################################
    # need to get data from annual mean plot

    bandmax = [-60., -30., 0., 30., 60., 90.]
    bandmin = [-90., -60., -30., 0., 30., 60.]


    land_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_anomaly = np.zeros((len(MODELNAMES), len(bandmax)))

    land_expt = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_expt = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_expt = np.zeros((len(MODELNAMES), len(bandmax)))

    land_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    sea_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    bands_cntl = np.zeros((len(MODELNAMES), len(bandmax)))
    
    land_expt_avg = np.zeros((len(MODELNAMES)))
    sea_expt_avg = np.zeros((len(MODELNAMES)))
    expt_avg = np.zeros((len(MODELNAMES)))

    land_cntl_avg = np.zeros((len(MODELNAMES)))
    sea_cntl_avg = np.zeros((len(MODELNAMES)))
    cntl_avg = np.zeros((len(MODELNAMES)))

    

    for modelno, modeluse in enumerate(MODELNAMES):

        # get mean data
        (exptcube, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME)
        (cntlcube, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME)

        # get data within bounds
        for i, band_upper in enumerate(bandmax):
            band_lower = bandmin[i]
            land_expt[modelno, i] = get_region(band_upper, band_lower,
                                               exptcube, exptland,
                                               grid_areas_expt)

            land_cntl[modelno, i] = get_region(band_upper, band_lower,
                                               cntlcube, cntlland,
                                               grid_areas_cntl)

            sea_expt[modelno, i] = get_region(band_upper, band_lower,
                                              exptcube, exptsea,
                                              grid_areas_expt)

            sea_cntl[modelno, i] = get_region(band_upper, band_lower,
                                              cntlcube, cntlsea,
                                              grid_areas_cntl)

            bands_expt[modelno, i] = get_region(band_upper, band_lower,
                                                exptcube, maskall,
                                                grid_areas_expt)

            bands_cntl[modelno, i] = get_region(band_upper, band_lower,
                                                cntlcube, maskall,
                                                grid_areas_cntl)
            
        #get average data for calculating polar amplification
        land_expt_avg[modelno] = get_region(90.0, -90.0, exptcube, exptland,
                                            grid_areas_expt)
        land_cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, cntlland,
                                            grid_areas_cntl)
        sea_expt_avg[modelno] = get_region(90.0, -90.0, exptcube, exptsea,
                                            grid_areas_expt)
        sea_cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, cntlsea,
                                            grid_areas_cntl)
        expt_avg[modelno] = get_region(90.0, -90.0, exptcube, maskall,
                                            grid_areas_expt)
        cntl_avg[modelno] = get_region(90.0, -90.0, cntlcube, maskall,
                                            grid_areas_cntl)

    land_anomaly = land_expt - land_cntl
    sea_anomaly = sea_expt - sea_cntl
    bands_anomaly = bands_expt - bands_cntl
    
    ##############################################################
    #  get pliomip1 data if required.  Note we will only get annual mean
    
    if PLIOMIP1 == 'y':
        pliomip1_mean, pliomip1_min, pliomip1_max = get_pliomip1_data()
    else:
        pliomip1_mean = 0
        pliomip1_min = 0
        pliomip1_max = 0

    #print polar amplification
    print('SH polar amplification')
    print('model, land amplification, sea amp, all amp')
    #for i, model in enumerate(MODELNAMES):
    #    print('bands',bandmax[0],bandmin[0])
    #    print(model, land_anomaly[i, 0] / (land_expt_avg[i] - land_cntl_avg[i]), 
    #          sea_anomaly[i, 0] / (sea_expt_avg[i] - sea_cntl_avg[i]),
    #          bands_anomaly[i,0] / (expt_avg[i] - cntl_avg[i]))
    all_sh_land_amp = [x1 / x2 for (x1, x2) in zip(land_anomaly[:,0], 
                                                   land_expt_avg - land_cntl_avg)]
    all_sh_sea_amp = [x1 / x2 for (x1, x2) in zip(sea_anomaly[:,0], 
                                                  sea_expt_avg - sea_cntl_avg)]
    all_sh_amp = [x1 / x2 for (x1, x2) in zip(bands_anomaly[:,0], 
                                              expt_avg - cntl_avg)]
    print('mean', 
          np.mean(land_anomaly[:, 0]) / np.mean(land_expt_avg[:] - land_cntl_avg[:]), 
          np.mean(sea_anomaly[:, 0]) / np.mean(sea_expt_avg[:] - sea_cntl_avg[:]),
          np.mean(bands_anomaly[:,0] / np.mean(expt_avg[:] - cntl_avg[:])))
    print('median', np.median(all_sh_land_amp), np.median(all_sh_sea_amp), 
          np.median(all_sh_amp))
    
    print('checking land',  np.mean(land_anomaly[:, 0]), np.mean(land_expt[:,0]),
          np.mean(land_cntl[:,0]))
          #,np.mean(land_expt_avg[:])-
          #np.mean(land_cntl_avg[:]))
    print('checking sea',  np.mean(sea_anomaly[:, 0]), np.mean(sea_expt[:,0]),
          np.mean(sea_cntl[:,0]))#,np.mean(sea_expt_avg[:])-
        #  np.mean(sea_cntl_avg[:]))
    print('checking avg',  np.mean(bands_anomaly[:, 0]),np.mean(bands_expt[:,0]),
          np.mean(bands_cntl[:,0]))#,np.mean(expt_avg[:])-
        #  np.mean(cntl_avg[:]))
      
    print(' ')    
    print('NH polar amplification')
    all_nh_land_amp = [x1 / x2 for (x1, x2) in zip(land_anomaly[:,5], 
                                                   land_expt_avg - land_cntl_avg)]
    all_nh_sea_amp = [x1 / x2 for (x1, x2) in zip(sea_anomaly[:,5], 
                                                  sea_expt_avg - sea_cntl_avg)]
    all_nh_amp = [x1 / x2 for (x1, x2) in zip(bands_anomaly[:,5], 
                                              expt_avg - cntl_avg)]
    print('model, land amp, sea amp, all amp')
    #for i, model in enumerate(MODELNAMES):
    #    print('bands',bandmax[5],bandmin[5])
    #    print(model, land_anomaly[i, 5] / (land_expt_avg[i] - land_cntl_avg[i]), 
    #          sea_anomaly[i, 5] / (sea_expt_avg[i] - sea_cntl_avg[i]),
    #          bands_anomaly[i,5] / (expt_avg[i] - cntl_avg[i]))
    print('mean', 
          np.mean(land_anomaly[:, 5]) / np.mean(land_expt_avg[:] - land_cntl_avg[:]), 
          np.mean(sea_anomaly[:, 5]) / np.mean(sea_expt_avg[:] - sea_cntl_avg[:]),
          np.mean(bands_anomaly[:,5] / np.mean(expt_avg[:] - cntl_avg[:])))
    print('median', np.median(all_nh_land_amp), np.median(all_nh_sea_amp), 
          np.median(all_nh_amp))

    # plot everything on one plot.

    plot_temp_by_lat(land_anomaly, bandmax, bandmin, 'Land', 
                     fileoutstart, 0, 0, 0)
    plot_temp_by_lat(sea_anomaly, bandmax, bandmin, 'Ocean', 
                     fileoutstart, 0, 0, 0)
    plot_temp_by_lat(bands_anomaly, bandmax, bandmin, '', 
                     fileoutstart, pliomip1_mean, pliomip1_min, pliomip1_max)



##########################################################
# DEFINITIONS

LINUX_WIN = 'l'
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'


MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
              'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
              'MIROC4m', 'IPSLCM5A2', 'HadCM3',
              'GISS2.1G', 'CCSM4', 
              'CCSM4-Utr', 'CCSM4-UoT', 
              'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]

PLIOMIP1 = 'y' # overplot PlioMIP1 models.

#MODELNAMES=['HadCM3','NorESM-L']
FIELD = 'NearSurfaceTemperature'
UNITS = 'degC'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
EXPTNAME = 'EOI400'
CNTLNAME = 'E280'

if FIELD == 'NearSurfaceTemperature':
    FILEOUT = FILESTART + 'regridded/alldata/data_for_arctic_amplification.txt'


main()
::::::::::::::
average_HadISST_NOAA.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Jul 25 16:18:28 2019

@author: earjcti
This program will calculate a 30 year (1870-1900) average
of HadISST or HadSST4 data and write it to a file.
This can then be compared with modelled SST data

"""

import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import sys


def get_var():
    """
    gets variable names depending on whether we are using
    HadISST or HadSST4.0

    returns, FILENAME, OUTSTART, FILECUBE
    """

    if DATASET == 'HadISST':
        filename = '/nfs/hera1/earjcti/PLIOMIP2/HadISST/HadISST_sst.nc'
        varname = 'sea_surface_temperature'
        fieldname = 'SST'

    if DATASET == 'HadSST4':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/HadSST4/' +
                    'HadSST.4.0.0.0_median.nc')
        varname = 'Sea water temperature anomaly at a depth of 20cm'
        fieldname = 'SST'


    if DATASET == 'NOAAERSST5':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/NOAAERSST5/' +
                    'sst.mnmean.nc')
        varname = 'Monthly Means of Sea Surface Temperature'
        fieldname = 'SST'


    if DATASET == 'CRUTEMP':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/CRU_DATA/' +
                    'cru_ts4.04.1901.2019.tmp.dat.nc')
        varname = 'near-surface temperature'
        fieldname = 'NearSurfaceTemperature'


    if DATASET == 'CRUPRECIP':
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/CRU_DATA/' +
                    'cru_ts4.04.1901.2019.pre.dat.nc')
        varname = 'precipitation'
        fieldname = 'TotalPrecipitation'


    outstart = ('/nfs/hera1/earjcti/regridded/'
                + DATASET + '/E280.' + fieldname + '.')
    filecube = iris.load_cube(filename, varname)


    return [filename, outstart, filecube]

def extract_nyrs(cube, startyear, endyear):
    """
    Extract data between startyear and endyear
    from a cube of monthly data

    Parameters:
    cube (iris cube): The cube containing a number of years
    startyear,endyear  (integer): years we wish to extract

    Returns:
    newcube (iris cube): The cube containing the subset of data
    """
    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        iris.coord_categorisation.add_year(t_slice, 'time', name='year')
        year = t_slice.coord('year').points

        if startyear <= year <= endyear:
            print('processing', year)
            t_slice.remove_coord('year')
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            # we have missing data at lsm and also some points are -1000.
            # change points that are -1000. to missing data
            newdata = np.ma.masked_where(t_slice2.data < -900., t_slice2.data)
            t_slice2.data = newdata
            cubelist.append(t_slice2)

    newcube = cubelist.concatenate_cube()

    return newcube


def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]

def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
        or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):
            mon_slice.coord('latitude').guess_bounds()
            mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):

        allmeancube.coord('latitude').guess_bounds()
        allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if (DATASET == 'HadISST' or DATASET == 'NOAAERSST5'
           or DATASET == 'CRUTEMP' or DATASET == 'CRUPRECIP'):

        yearmeancube.coord('latitude').guess_bounds()
        yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]


def textout(meanmon, stdevmon):
    """
    write out all the means to a text file
    this includes global means monthly means and latitudinal means
    """
    textfile = OUTSTART + 'data.txt'
    file1 = open(textfile, "w")
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2)) + ',' +
                np.str(np.round(stdevann, 3)) + '\n')

    # write monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    for i in range(0, 12):
        file1.write(np.str(i+1) + ','+np.str(np.round(meanmon[i], 2)) + ','
                    + np.str(np.round(stdevmon[i], 3))+'\n')

    # write latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')

    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i]) +
                    ',' + np.str(np.round(meanlat[i], 2)) +
                    ',' + np.str(np.round(stdevlat[i], 3)) + '\n')

    file1.close()

def plot_to_check(cube):
    """
    A test program to check that we have averaged properly.
    """


    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube=subcube_mean_mon.copy(data=)  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas = iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],
                             iris.analysis.MEAN, weights=grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt / 12)
    print(nyears)

    for i in range(0, nyears):
        tstart = i * 12
        tend = (i+1) * 12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color='r')

    # global mean from average

    grid_areas = iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean = mean_mon_data.collapsed(['latitude', 'longitude'],
                                            iris.analysis.MEAN, weights=grid_areas)

    plt.plot(temporal_mean.data, color='b', label='avg')
    plt.title('globavg HadISST')
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0] >= 32. >= bounds[i, 1] or bounds[i, 0] <= 32. < bounds[i, 1]):
            index = i

    subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color='r')

    #mean at 30N
    slice_30N = mean_mon_data.extract(iris.Constraint(latitude=32))
    mean_30N = slice_30N.collapsed(['longitude'], iris.analysis.MEAN)


    plt.plot(mean_30N.data, color='b', label='avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()

def writeout():
    """
    write the monthly mean and global mean cubes out to a netcdf file
    """

    outfile = OUTSTART + 'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)

    outfile = OUTSTART + 'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format='NETCDF3_CLASSIC',
              fill_value=2.0E20)



##########################################################
# MAIN PROGRAM

"""
This program will average over the first 30 years of the HadISST
dataset and write results out to a file
"""

DATASET = 'CRUPRECIP' # HadISST HadSST4 NOAAERSST5 CRUTEMP CRUPRECIP

# read in HadISST data
FILENAME, OUTSTART, FILECUBE = get_var()

# extract the years from filecube likely 30 years
#cube30 = extract_nyrs(FILECUBE, 1870, 1899) # where available
cube30 = extract_nyrs(FILECUBE, 1901, 1930)

# create averages
 # add year and month time axis
iris.coord_categorisation.add_month_number(cube30, 'time', name='month')
iris.coord_categorisation.add_year(cube30, 'time', name='year')
mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(cube30)

# create info about each month and get average monthly data
# stdevmon is a numpy array of standard deviation, monthly_mean is a numpy array
# of means
monthly_mean = mon_avg(mean_mon_data)
monthly_standard_deviation = get_monthly_sd(cube30)


# get global and latitinal means for writing to text file
# plot for

meanann, stdevann, meanlat, stdevlat = area_means(mean_data, mean_year_data)

# write means to a text file
textout(monthly_mean, monthly_standard_deviation)

# write iris cubes out to a file
writeout()

# plot to check we have averaged properly
plot_to_check(cube30)
::::::::::::::
calculate_land_sea_contrast_HadGEM3.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import netCDF4
import sys
#import os


###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """
    f = Dataset(LSM, "r")
    lsm_data = f.variables['lsm'][:]
    lats = f.variables['latitude'][:] 
    lons = f.variables['longitude'][:]
    f.close()
  
    lsm_data = np.squeeze(lsm_data)
    land_mask = lsm_data
    sea_mask = (lsm_data -1.0) * 1.0

    return land_mask, sea_mask, lats, lons

    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """

    f = Dataset(FILENAME_PLIO, "r")
    plio_data_all = f.variables[FIELDNAME][:]
    lats = f.variables['latitude'][:] 
    lons = f.variables['longitude'][:]
    f.close()

    plio_data = (np.mean(plio_data_all,axis=0)-273.15)

    f = Dataset(FILENAME_PI, "r")
    pi_data_all = f.variables[FIELDNAME][:]
    lats2 = f.variables['latitude'][:] 
    lons2 = f.variables['longitude'][:]
    f.close()

    pi_data = (np.mean(pi_data_all,axis=0)-273.15)


    for i, lat in enumerate(lats):
        if lat != lats2[i]:
            print('lats dont match', lat, i, lats2[i])
            sys.exit(0)

    for i, lon in enumerate(lons):
        if lon != lons2[i]:
           print('lons dont match', lon, i, lons2[i])
           sys.exit(0)

  
    return plio_data, pi_data, lats, lons


def get_global_avg(land_mask, sea_mask, dataarr, lats, lons):
    """
    gets global average temperature, and also global avg for
    the land and the ocean
    """
  
    grid_areas_lat = np.zeros(len(lats))
    for j, lat in enumerate(lats):
        grid_areas_lat[j] = np.cos(2. * np.pi * lat / 360.)

    print(np.shape(land_mask))
    global_mean = 0.
    global_mean_weights = 0.
    global_mean_land = 0.
    global_mean_land_weights = 0.
    global_mean_sea = 0.
    global_mean_sea_weights = 0.

    for j in range(0, len(lats)):
        for i in range(0, len(lons)):
            global_mean = global_mean + (dataarr[j, i] * grid_areas_lat[j])
            global_mean_weights = global_mean_weights + grid_areas_lat[j]
            if land_mask[j, i] == 1.0:
                global_mean_land = (global_mean_land + 
                                   (dataarr[j, i] * grid_areas_lat[j]))
                global_mean_land_weights = (global_mean_land_weights + 
                                            grid_areas_lat[j])
            else:
                global_mean_sea = (global_mean_sea + 
                                   (dataarr[j, i] * grid_areas_lat[j]))
                global_mean_sea_weights = (global_mean_sea_weights + 
                                            grid_areas_lat[j])

    global_mean = global_mean / global_mean_weights
    global_mean_land = global_mean_land / global_mean_land_weights
    global_mean_sea = global_mean_sea / global_mean_sea_weights

   
    return global_mean, global_mean_land, global_mean_sea, grid_areas_lat


def get_regional_landsea(rmax, rmin, land_mask, dataarr, lats, lons,
                         grid_areas):

    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    grid_areas_use = grid_areas * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j] = 0.0
    #grid_areas_land = grid_areas_use * land_cube.data
    #grid_areas_sea = grid_areas_use * sea_cube.data
    
    reg_mean = 0.
    reg_mean_weights = 0.
    reg_mean_land = 0.
    reg_mean_land_weights = 0.
    reg_mean_sea = 0.
    reg_mean_sea_weights = 0.

    for j in range(0, len(lats)):
        if grid_areas_use[j] != 0.0:
            for i in range(0, len(lons)):
                reg_mean = reg_mean + (dataarr[j, i] * grid_areas_use[j])
                reg_mean_weights = reg_mean_weights + grid_areas_use[j]
                if land_mask[j, i] == 1.0:
                    reg_mean_land = (reg_mean_land + 
                                     (dataarr[j, i] * grid_areas_use[j]))
                    reg_mean_land_weights = (reg_mean_land_weights + 
                                             grid_areas_use[j])
                else:
                    reg_mean_sea = (reg_mean_sea + 
                                    (dataarr[j, i] * grid_areas_use[j]))
                    reg_mean_sea_weights = (reg_mean_sea_weights + 
                                            grid_areas_use[j])
    print(rmax, rmin)
    print(rmax, rmin, reg_mean / reg_mean_weights)
    print(reg_mean, reg_mean_land, reg_mean_sea)
    print(reg_mean_weights, reg_mean_land_weights, reg_mean_sea_weights)
  
    reg_mean = reg_mean / reg_mean_weights
    reg_mean_land = reg_mean_land / reg_mean_land_weights
    reg_mean_sea = reg_mean_sea / reg_mean_sea_weights

    return [reg_mean, reg_mean_land, reg_mean_sea]
           
                                                       
def write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin):
    """
    write the information to a pandas dataframe
    """
    print(avg_T_anom, MODELNAME)

    data = [['Global', avg_T_anom], ['Global (over land)', avg_T_landanom],
            ['Global (over sea)', avg_T_seaanom]]

    for i, rmax in enumerate(regionmax):
        if rmax > 0:
            latrange = np.str(np.around(rmax)) + 'N'
        else:
            latrange = np.str(np.around(np.abs(rmax))) + 'S'
        if regionmin[i] > 0.:
            latrange = latrange + np.str(np.around(regionmin[i])) + 'N'
        else:
            latrange = latrange + np.str(np.around(np.abs(regionmin[i]))) + 'S'
        
        data.append(['glob_' + latrange, land_sea_anom[0,i]])
        data.append(['land_' + latrange, land_sea_anom[1,i]])
        data.append(['sea_' + latrange, land_sea_anom[2,i]])
        

    df = pd.DataFrame(data, columns = ['Simulated temperature', MODELNAME])
 

    # save dataframe as a excel file
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/IPCC/' + MODELNAME + 
          'mPWP_CMIP6_land_sea.csv')
    #df.to_excel(filename)
    df.to_csv(filename)


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM)

    # get land and sea mask
    land_mask, sea_mask, lsm_lats, lsm_lons = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    plio_data, pi_data, lats, lons = get_nsat_data()

    # check grid
    for i, lat in enumerate(lsm_lats):
        if lat != lats[i]:
           print('lsm lat doesnt match', i, lat, lats[i])
           sys.exit(0)
    for i, lon in enumerate(lsm_lons):
        if lon !=lons[i]:
           print('lsm lon doesnt match', i, lon, lons[i])
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, avg_land_T_plio, 
     avg_sea_T_plio, grid_areas) = get_global_avg(land_mask, sea_mask, 
                                                 plio_data, lats, lons)
                                    
    (avg_T_pi, avg_land_T_pi,
     avg_sea_T_pi, grid_areas) = get_global_avg(land_mask, sea_mask,
                                                pi_data, lats, lons)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask, pi_data,
                                                        lats, lons, 
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask, plio_data,
                                                        lats, lons,
                                                        grid_areas)

       
    
 # write to a spreadsheet
    land_sea_anom = land_sea_region_plio - land_sea_region_pi

    write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin)
    


    

   


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "HadGEM3"
FIELDNAMEIN = ['tas']


START = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
FILENAME_PI = START + 'climatologies/E280/atmos/clims_hadgem3_pi_airtemp_final.nc'
FILENAME_PLIO = START + 'climatologies/Eoi400/atmos/clims_hadgem3_pliocene_airtemp_final.nc'
FIELDNAME = 'temp'
LSM = START + 'hadgem3.mask.nc'
FIELDLSM = 'land_binary_mask'#

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

get_land_sea_contrast()

#sys.exit(0)
::::::::::::::
calculate_land_sea_contrast.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import netCDF4
import sys
#import os


###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d

    ############################################
    if MODELNAME == 'IPSLCM5A' or MODELNAME == 'IPSLCM5A2':
        plio_lsm_cube = get_ipsl_lsm(LSM_PLIO, FIELDLSM)
        pi_lsm_cube = get_ipsl_lsm(LSM_PI, FIELDLSM)
    elif MODELNAME == 'HadGEM3':
        test = iris.fileformats.netcdf.load_cubes(LSM_PLIO, callback=None)
        print(test)
        for data in test:
            print(data)
        sys.exit(0)
        f = netCDF4.Dataset(LSM_PLIO, "r")
        print(f)
        sys.exit(0)
    else:
        plio_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PI, FIELDLSM))
        pi_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PLIO, FIELDLSM))
   
    plio_lsm_cube2 = change_to_2d(plio_lsm_cube)
    pi_lsm_cube2 = change_to_2d(pi_lsm_cube)
   
    plio_lsm_data = plio_lsm_cube2.data
    pi_lsm_data = pi_lsm_cube2.data

    if MODELNAME == 'IPSLCM6A':
        plio_lsm_data = plio_lsm_data / 100.0
        pi_lsm_data = pi_lsm_data / 100.0

    if MODELNAME == 'EC-Earth3.3':
        plio_lsm_data = np.where(plio_lsm_data > 0.5, 1.0, 0.0)
        pi_lsm_data = np.where(pi_lsm_data > 0.5, 1.0, 0.0)
  
  
    land_mask = np.zeros(np.shape(plio_lsm_data))
    sea_mask = np.zeros(np.shape(plio_lsm_data))

    for ix, plio_mask in np.ndenumerate(plio_lsm_data):
        if plio_mask == 1.0 and pi_lsm_data[ix] == 1.0:
            land_mask[ix] = 1.0
        #if pi_lsm_data[ix] > 0:
        #    land_mask[ix] = 1.0
        if plio_mask == 0.0 and pi_lsm_data[ix] == 0.0:
            sea_mask[ix] = 1.0

    land_cube = plio_lsm_cube2.copy(data=land_mask)
    land_cube.var_name = 'land_mask'
    land_cube.long_name = 'land_mask'
    print('getting sea mask')
    sea_cube = pi_lsm_cube2.copy(data=sea_mask)
    sea_cube.var_name = 'sea_mask'
    sea_cube.long_name = 'sea_mask'
    print('got sea mask')

    return land_cube, sea_cube

def get_hadcm3_data(filestart):
    """
    gets the nsat data from HadCM3 and MRI
    called by get_nsat_data
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if MODELNAME  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filestart + yearuse + '.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if MODELNAME  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if MODELNAME  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')
  
    if MODELNAME  == 'HadCM3':
        cube_temp.coord('ht').rename('surface')

    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))

    return cube

def get_ipslcm6a_data(file):
    """
    for ipslcm6a we have 200 years in the file.  but we only need 100 years
    """
    cube = iris.load_cube(file, FIELDNAME)
    # reduce number of years
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube100yr = cubelist.concatenate_cube()
  
    return cube100yr

def get_ipsl5_data(filename, exptname):
    """
    gets nsat data from ipsl
    there is a bit of an error in the file calendar so we will
    """
# copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        elif ncattr !='_FillValue':
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = 'Temperature 2m'

        cube = iris.load_cube('temporary.nc', fieldreq)

        cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm5a2_data(filename):
    """
    gets the data for ipslcm5a2
    and removes all auxillary coordinates
    """
    cubelist = iris.cubeList
    cube = iris.load_cube(filename, FIELDNAME)
    for coord in cube.aux_coords:
        coord.rename('toremove')
        cube.remove_coord('toremove')
    return cube

def get_giss_data(filenames):
    """
    gets giss data: this is in two files
    """ 
    allcubes = iris.cube.CubeList([])
    for file in filenames:
        cubetemp = iris.load_cube(file, FIELDNAME)
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
  
    return(cube)
    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """
        

    if MODELNAME == 'HadCM3' or MODELNAME == 'MRI-CGCM2.3':
        cube_plio = get_hadcm3_data(FILENAME_PLIO)
        cube_pi = get_hadcm3_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM6A':
        cube_plio = get_ipslcm6a_data(FILENAME_PLIO)
        cube_pi = get_ipslcm6a_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM5A':
        cube_pi = iris.load(FILENAME_PI)[0]
        cube_plio = get_ipsl5_data(FILENAME_PLIO,'Eoi400')
    elif MODELNAME == 'IPSLCM5A2':
        cube_plio = get_ipslcm5a2_data(FILENAME_PLIO)
        cube_pi = get_ipslcm5a2_data(FILENAME_PI)
    elif MODELNAME == 'GISS2.1G':
        cube_plio = get_giss_data(FILENAME_PLIO)
        cube_pi = get_giss_data(FILENAME_PI)
    else:
        cube_plio = iris.load_cube(FILENAME_PLIO, FIELDNAME)
        cube_pi = iris.load_cube(FILENAME_PI, FIELDNAME)
    
   
    cube_plio_avg = cube_plio.collapsed('time', iris.analysis.MEAN)
    cube_pi_avg = cube_pi.collapsed('time', iris.analysis.MEAN)
  
    return cube_plio_avg, cube_pi_avg


def get_global_avg(land_cube, sea_cube, cube_nsat):
    """
    get's global average temperature, and also global avg for
    the land and the ocean
    """


    if cube_nsat.coord('latitude').has_bounds():
        cube_nsat.coord('latitude').bounds
    else:
        cube_nsat.coord('latitude').guess_bounds()

    if cube_nsat.coord('longitude').has_bounds():
        cube_nsat.coord('longitude').bounds
    else:
        cube_nsat.coord('longitude').guess_bounds()

  
    grid_areas = iris.analysis.cartography.area_weights(cube_nsat)
    grid_areas_land = grid_areas * land_cube.data
    grid_areas_sea = grid_areas * sea_cube.data

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)

    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15
   
    return avg_temp_data, avg_temp_land, avg_temp_sea, grid_areas


def get_regional_landsea(rmax, rmin, land_cube, sea_cube, cube_nsat,
                         grid_areas_region):
    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    lats = cube_nsat.coord('latitude').points
    grid_areas_use = grid_areas_region * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j, :] = 0.0
    grid_areas_land = grid_areas_use * land_cube.data
    grid_areas_sea = grid_areas_use * sea_cube.data
    

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_use)
   
    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)
   
    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15

    return [avg_temp_data, avg_temp_land, avg_temp_sea]
           
                                                       
def write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin):
    """
    write the information to a pandas dataframe
    """
    print(avg_T_anom, MODELNAME)

    data = [['Global', avg_T_anom], ['Global (over land)', avg_T_landanom],
            ['Global (over sea)', avg_T_seaanom]]

    for i, rmax in enumerate(regionmax):
        if rmax > 0:
            latrange = np.str(np.around(rmax)) + 'N'
        else:
            latrange = np.str(np.around(np.abs(rmax))) + 'S'
        if regionmin[i] > 0.:
            latrange = latrange + np.str(np.around(regionmin[i])) + 'N'
        else:
            latrange = latrange + np.str(np.around(np.abs(regionmin[i]))) + 'S'
        
        data.append(['glob_' + latrange, land_sea_anom[0,i]])
        data.append(['land_' + latrange, land_sea_anom[1,i]])
        data.append(['sea_' + latrange, land_sea_anom[2,i]])
        

    df = pd.DataFrame(data, columns = ['Simulated temperature', MODELNAME])
 

    # save dataframe as a excel file
    filename = ('/nfs/hera1/earjcti/PLIOMIP2/IPCC/' + MODELNAME + 
          'mPWP_CMIP6_land_sea.csv')
    #df.to_excel(filename)
    df.to_csv(filename)


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM_PLIO)

    # get land and sea mask
    land_mask_cube, sea_mask_cube = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    cube_nsat_plio, cube_nsat_pi = get_nsat_data()
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, 
     avg_land_T_plio, 
     avg_sea_T_plio,
     gridareas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                      cube_nsat_plio)

 

    (avg_T_pi, 
     avg_land_T_pi,
     avg_sea_T_pi,
     grid_areas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                     cube_nsat_pi)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))

    #plt.subplot(2,1,1)
    #V = [0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2]
    #qplt.contourf(cube_nsat_plio - cube_nsat_pi, levels=V)
    #plt.subplot(2,1,2)
    #qplt.contourf((sea_mask_cube + land_mask_cube),  levels=V)
    #plt.show()
    #sys.exit(0)
   


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_pi,
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_plio,
                                                        grid_areas)

       
    
 # write to a spreadsheet
    land_sea_anom = land_sea_region_plio - land_sea_region_pi

    write_to_spreadsheet(avg_T_anom, avg_T_landanom, avg_T_seaanom,
                         land_sea_anom, regionmax, regionmin)
    


    

   

#############################################################################
def getnames():

# this program will get the names of the files and the field for each
# of the model

  
    # get names for each model

    if MODELNAME == 'CESM2':
        file_e280 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                         'f09_g17.CMIP6-piControl.' + 
                         '001.cam.h0.TREFHT.110001-120012.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                           'f09_g17.PMIP4-midPliocene-eoi400.' + 
                           '001.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                      'f09_g17.PMIP4-midPliocene-eoi400.001.' + 
                      'cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'COSMOS':
        file_e280 = (FILESTART + 'AWI/COSMOS/E280/E280.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        file_eoi400 = (FILESTART + 'AWI/COSMOS/Eoi400/Eoi400.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        fielduse =  "2m temperature"
        lsm_e280 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                    "E280_et_al/E280.slf.atm.nc")
        lsm_eoi400 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                      "Eoi400_et_al/Eoi400.slf.atm.nc")
        fieldlsm = "SLF"

    if MODELNAME == 'EC-Earth3.3':
        file_e280 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_surface.nc'
        file_eoi400 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_surface.nc'
        fielduse = 'Air temperature at 2m'
        lsm_e280 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc'
        lsm_eoi400 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc'
        fieldlsm = 'Land/sea mask'

    if MODELNAME == 'CESM1.2':
        file_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0701.0800.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if MODELNAME   ==  'MIROC4m':
        file_e280 = (FILESTART + 'MIROC4m/tas/MIROC4m_E280_Amon_tas.nc')
        file_eoi400 = (FILESTART + 'MIROC4m/tas/MIROC4m_Eoi400_Amon_tas.nc')
        fielduse = "tas"
        lsm_e280 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc')
        lsm_eoi400 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc')
        fieldlsm = "sftlf"

    if MODELNAME  == 'HadCM3':
        file_e280 = (FILESTART+'LEEDS/HadCM3/e280/NearSurfaceTemperature/' + 
                     'e280.NearSurfaceTemperature.')
        file_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/NearSurfaceTemperature/' + 
                     'eoi400.NearSurfaceTemperature.')
        fielduse = "TEMPERATURE AT 1.5M"
        lsm_e280 = (FILESTART+'LEEDS/HadCM3/e280/qrparm.mask.nc')
        lsm_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc')
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if MODELNAME == 'CCSM4':
        file_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0081.0180.nc')
        file_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.TREFHT.1001.1100.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'CCSM4_Utr':
        file_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                     'tas_Amon_CESM1.0.5_E280_r1i1p1f1_gn_275001-285012.nc')  
        file_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                       'tas_Amon_CESM1.0.5_Eoi400_r1i1p1f1_gn_190001-200012.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                    'land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC' + 
                    '_control_r1i1p1f1_gn.nc')
        lsm_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                      'land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_' + 
                      'f19g16_NESSC_control_r1i1p1f1_gn.nc')
        fieldlsm = 'LANDMASK[D=1]'
  
    if MODELNAME == 'CCSM4_UoT':
        start = FILESTART + 'UofT/UofT-CCSM4/'
        file_e280 = (start + '/E280/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_piControl_r1i1p1f1_gn_150101-160012.nc')  
        file_eoi400 = (start + '/Eoi400/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_midPliocene-eoi400_r1i1p1f1_gn_' + 
                       '160101-170012.nc') 
        fielduse = 'air_temperature'
        lsm_e280 = start + 'for_julia/E_mask.nc'
        lsm_eoi400 = start + 'for_julia/Eoi_mask.nc'
        fieldlsm = 'gridbox land fraction'
      
    if MODELNAME == 'NorESM-L':
       file_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_TREFHT.nc')
       file_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_TREFHT.nc')
       fielduse = 'Reference height temperature'
       lsm_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc')
       lsm_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc')
       fieldlsm = 'Fraction of sfc area covered by land'


    if MODELNAME  == 'MRI-CGCM2.3':
        file_e280 = (FILESTART + 'MRI-CGCM2.3/tas/e280.tas.')
        file_eoi400 = (FILESTART + 'MRI-CGCM2.3/tas/eoi400.tas.')
        fielduse = 'near surface air temperature [degC]'
        lsm_e280 = (FILESTART + 'MRI-CGCM2.3/sftlf.nc')
        lsm_eoi400 = lsm_e280
        fieldlsm = 'landsea mask [0 - 1]'


    if MODELNAME  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        mid = 'e280/tas_Amon_GISS-E2-1-G_piControl_r1i1p1f1_gn_'
        file_e280 = ([start + mid + '490101-495012.nc', 
                      start + mid + '495101-500012.nc'])
        mid = 'eoi400/tas_Amon_GISS-E2-1-G_midPliocene-eoi400_r1i1p1f1_gn_'
        file_eoi400 = ([start + mid + '305101-310012.nc',
                        start + mid + '310101-315012.nc'])
        fielduse = 'air_temperature'
        lsm_e280 = start + 'e280/NASA-GISS_PIctrl_all_fland.nc'
        lsm_eoi400 = start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc'
        fieldlsm = 'fland'

    if MODELNAME == 'NorESM1-F':
        file_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_TREFHT.nc'
        file_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_TREFHT.nc'
        lsm_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc'
        lsm_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc'
        fielduse = 'Reference height temperature'
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if MODELNAME == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        file_e280 = start + 'tas_Amon_IPSL-CM6A-LR_piControl_r1i1p1f1_gr_285001-304912.nc'
        file_eoi400 = (start + 'tas_Amon_IPSL-CM6A-LR_midPliocene-eoi400_' + 
                       'r1i1p1f1_gr_185001-204912.nc')
        lsm_e280 = start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc'
        lsm_eoi400 = start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc'
        fielduse = 'air_temperature'
        fieldlsm = 'land_area_fraction'

    if MODELNAME == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A/PI.NearSurfaceTemp_tas_3600_3699_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A/Eoi400.NearSurfaceTemp_tas_3581_3680_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Tas'
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if MODELNAME == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A2/PI.NearSurfaceTemp_tas_6110_6209_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A2/Eoi400.NearSurfaceTemp_tas_3381_3480_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Temperature 2m'
        fieldlsm = ['Fraction ter', 'Fraction lic']

    if MODELNAME == 'HadGEM3':
        start = '/nfs/hera1/pliomip2/data/HadGEM3_new/'
        file_e280 = start + 'climatologies/E280/clims_hadgem3_pi_airtemp_final.nc'
        file_eoi400 = start + 'climatologies/Eoi400/clims_hadgem3_pliocene_airtemp_final.nc'
        fielduse = 'temp'
        lsm_e280 = start + 'hadgem3.mask.nc'
        lsm_eoi400 = lsm_e280
        fieldlsm = 'land_binary_mask'
            
            
      
    retdata = [fielduse, file_e280, file_eoi400,
               fieldlsm, lsm_e280, lsm_eoi400]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "HadGEM3" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                  
FIELDNAMEIN = ['tas']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


# call program to get model dependent names
# fielduse,  and  filename
retdata = getnames()

FIELDNAME = retdata[0]
FILENAME_PI = retdata[1]
FILENAME_PLIO = retdata[2]
FIELDLSM = retdata[3]
LSM_PI = retdata[4]
LSM_PLIO = retdata[5]

print('fieldname is',FIELDNAME)

get_land_sea_contrast()

#sys.exit(0)
::::::::::::::
CCSM4_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
"""
This program will average all of the CCSM4 models.
We will average the data file (text files) and also the mean average temperature file
"""

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys


#########################################################################
# stuff for doing text file is here  
def get_text_data(filename):
    """
    gets the text data for each filename
    returns globalmean, monthly mean, latitudinal mean
    """
    f=open(filename,"r")
    f1=f.readlines()
    f2 = [x.replace('\n', '') for x in f1]
            
    # get the means according to their position in the file
    all_mean_sd=f2[2]
    all_mon_mean_sd=f2[5:5+12]
    all_lat_mean_sd=f2[20:20+180]
           
    # extract global mean
    meanglob,sd=all_mean_sd.split(',')
    
   
    monmeans = np.zeros(12)
    latmeans = np.zeros(180)
    lats = np.zeros(180)
    # extract monthly means
    for x in all_mon_mean_sd:
        mon,mean,sd=x.split(',')
        monmeans[int(mon)-1]=float(mean)
            
            
    # extract latitude means
    for x in all_lat_mean_sd:
        lat,mean,sd=x.split(',')
        latss=int(float(lat)+89.5) # convert latitude to a subscript
               
        if mean != ' --' and mean != '--':
            latmeans[latss]=float(mean) # stores latitudinal means
        else:
            latmeans[latss]=np.nan
            
        lats[latss]=lat # stores latitudes
      
   
    return meanglob, monmeans, latmeans, lats
   
def writeout_textdata(mean_global, mean_mon, mean_lat, lats, expt):
    """
    writeout the text data to a file
    replace stdev with -999
    """
    
    textout = FILESTART + OUTMODEL + '/' + expt + '.' + FIELDNAME + '.data.txt'
    file1 =  open(textout, "w")

    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
   
    # write out global temperautre
    file1.write(np.str(np.round(mean_global, 2))+', -99.99\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(mean_mon[i], 2))+', -99.99\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(mean_lat)):
        file1.write(np.str(lats[i])+', '+np.str(np.round(mean_lat[i], 2))+', -99.99\n')

    file1.close()
    

def main_avg_text(expt):
    """
    loop over all the models and extract mean, monthlymeans, latitudinal means
    average all the means
    write out to a file in the format of the input.  (Put standard deviation to -999.999)
    """
    
    for i, model in enumerate(MODELNAMES):
        filename = FILESTART + model + '/' + expt + '.' + FIELDNAME + '.data.txt'
        
        globmean, monmeans, latmeans, lats = get_text_data(filename)
        
        if i == 0:
            allmeans = np.zeros(NMODELS)
            allmonmeans = np.zeros((NMODELS, len(monmeans)))
            alllatmeans = np.zeros((NMODELS, len(latmeans)))
        
        allmeans[i] = globmean
        allmonmeans[i, :] = monmeans
        alllatmeans[i, :] = latmeans
        
    mean_global = np.mean(allmeans)
    mean_mon = np.mean(allmonmeans, axis=0)
    mean_lat = np.mean(alllatmeans, axis=0)
    
    writeout_textdata(mean_global, mean_mon, mean_lat, lats, expt)
 
###############################################
## stuff for doing netcdf file is here
def main_avg_netcdf(expt):
    """
    loop over all the models and extract the global average netcdf file
    average all the means
    write out to a file in the format of the input.  
    """ 
    all_cubes=iris.cube.CubeList([])     
    for i, model in enumerate(MODELNAMES):
        print(i)
        filename = FILESTART + model + '/' + expt + '.' + FIELDNAME + '.allmean.nc'
        cube = iris.load_cube(filename)
        modelcube = resort_coords(cube, i)
        modelcube.data=modelcube.data.astype('float32') 
        all_cubes.append(modelcube)
    
    
    iris.experimental.equalise_cubes.equalise_attributes(all_cubes)
  
    cat_cubes = all_cubes.concatenate_cube()
    meancube = cat_cubes.collapsed(['model_level_number'], iris.analysis.MEAN)  
    
    fileout = FILESTART + OUTMODEL + '/' + expt + '.' + FIELDNAME + '.allmean.nc'
    iris.save(meancube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)
  
    
def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
       
       
     
    newcube=iris.util.new_axis(cube)
    newcube.add_dim_coord(iris.coords.DimCoord(levelno, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
   
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    newcube.rename('tas')
    
        
    return newcube      
    
    
def main():
    """ 
    main program
    1. average the text files for each of the models and writeout
    2. average the netcdf files from each of the models and writeout
    """
    
    for i, expt in enumerate(EXPTNAMES):
        avgtext = main_avg_text(expt)
        avgnetcdf = main_avg_netcdf(expt)
    
    
   

##########################################################
# fixed constants
        
LINUX_WIN='w'
if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'#
else:
    FILESTART = ' '

MODELNAMES=['CCSM4-1deg', 'CCSM4-2deg','CCSM4-UoT']
NMODELS = len(MODELNAMES)
OUTMODEL = 'CCSM4-avg'

FIELDNAME='NearSurfaceTemperature'
EXPTNAMES=['EOI400','E280']
#EXPTNAMES=['EOI400']


main()::::::::::::::
climate_sensitivity_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
import sys



#####################################
def  climate_sensitivity_analysis(modelnames,fieldname,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    else:
        filestart='/nfs/hera1/earjcti/regridded/'
     
        
    # set up a dictionary for the climate sensitivity
   
    # from my stuff
    #clim_sens ={'NorESM-L': 3.1,
    #             'NorESM1-F':2.29,
    #             'IPSLCM6A': 4.8,
    #             'IPSLCM5A':3.4,
    #             'HadCM3': 3.7,
    #             'MIROC4m':3.9,
    #             'COSMOS':4.1,
    #             'UofT':3.8,
    #             'EC-Earth3.1':3.2,
    #             'MRI-CGCM2.3':2.8,
    #             'CESM1.0.5': 3.1,
    #             'GISS': 3.31
    #             }
        
    # from Alan's table provided by authors
    clim_sens ={'NorESM-L': 3.1,
                 'NorESM1-F':2.3,
                 'IPSLCM6A': 4.8,
                 'IPSLCM5A2':3.6,
                 'IPSLCM5A':4.1,
                 'HadCM3': 3.5,
                 'MIROC4m':3.9,
                 'COSMOS':4.7,
                 'CCSM4-UoT':3.2,
                 'EC-Earth3.1':3.2,
                 'MRI2.3':2.8, # from my investigation
                 'CCSM4-2deg': 3.2,
                 'GISS2.1G': 3.3,
                 'CESM2': 5.3,
                 'CESM1.2' :4.1,
                 'CCSM4-1deg' :3.2
                 }
        
     
    # first get the data.  We need climate sensitivity, 
    # global temperature anomaly, latitude temperature anomaly
    # and gridbox by gridbox temperature anomaly
    climdiff=np.zeros(len(modelnames))
    climdiffmon=np.zeros((len(modelnames),12))
    climdifflat=np.zeros((len(modelnames),180))
    sensitivity_array=np.zeros(len(modelnames))
    alllats=np.arange(0,180,1)-89.5
    
    for mod in range(0,len(modelnames)):
        sensitivity_array[mod]=clim_sens.get(modelnames[mod])
        
        # get data from experiment file
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.data.txt'
        file1= open(fileexpt,"r")
        lines=list(file1)
       
        
        meanexpt,sdexpt=lines[2].split(",")
        monmeanexpt=np.zeros(12)
        latmeanexpt=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeanexpt[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeanexpt[index]=np.float(mean)
       
        
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.data.txt'
        file2= open(filecntl,"r")
        lines=list(file2)
        meancntl,sdexpt=lines[2].split(",")
        monmeancntl=np.zeros(12)
        latmeancntl=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeancntl[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeancntl[index]=np.float(mean)
        
        climdiff[mod]=np.float(meanexpt)-np.float(meancntl)
        
        climdiffmon[mod,:]=monmeanexpt-monmeancntl
        climdifflat[mod,:]=latmeanexpt-latmeancntl
        #print(modelnames[mod],climdifflat[mod])
    
     
    ########################################################
    # plot the climate sensitivity vs the global mean
    fig = plt.figure(figsize=(7.0, 4.0))
    plt.plot(climdiff,sensitivity_array,'x')
    plt.xlabel('Plio_Core - PI_CTL SAT anomaly',fontsize=15)
    plt.ylabel('ECS', fontsize=15)
    plt.xlim(np.floor(np.min(climdiff)),np.ceil(np.max(climdiff)))
    
    # do a linear regression
    print(modelnames)
    print(climdiff)
    print(sensitivity_array)
    print(np.mean(climdiff)*1.94, np.mean(sensitivity_array))
    sys.exit(0)
   
    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(climdiff, sensitivity_array)
    xarray=np.arange(0,5,1)
    yarray=intercept+(slope*xarray)
    plt.plot(xarray,yarray)
    plt.tick_params(axis='x',  labelsize=15)
    plt.tick_params(axis='y',  labelsize=15)
    #plt.title("R-squared: %2f, p-value: %2f" %'{:06.2f}'.format(r_value**2,), np.round(p_value, 2)), fontsize=15)
    plt.title("R-squared: " + np.str(np.around((r_value**2.), 2)) 
            + ",  p-value: " + np.str(np.around(p_value, 2)) , fontsize=15) 
    

    if redu == '':
        figtext = 'a)'
    else:
        figtext = 'a)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
  
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom' + 
             redu + '.eps')
    
    plt.tight_layout()
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    plt.close()

    rsq_std=r_value**2.
    
    ########################################################
    # plot the correlation between climate sensitivity vs the monthly mean
    rvals=np.zeros(12)
    pvals=np.zeros(12)
    for mon in range(0,12):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdiffmon[:,mon], sensitivity_array))
        rvals[mon]=r_value**2.
        pvals[mon]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    

    color = 'tab:red'
    ax1.set_xlabel('month', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=15)
    ax1.plot(np.arange(1,13,1), rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=15)
    ax1.tick_params(axis='x',  labelsize=15)
    #ax1.plot([0,13],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axi
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=15)  # we already handled the x-label with ax1
    ax2.plot(np.arange(1,13,1), pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=15)
    plt.title('ECS vs Plio_Core - PI_CTL by month', fontsize=15)
    
    
    if redu == '':
        figtext = 'c)'
    else:
        figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
   
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom' + 
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    plt.close()
    
    
     ########################################################
    # plot the correlation between climate sensitivity vs the latitudinal mean
    rvals=np.zeros(180)
    pvals=np.zeros(180)
    for lat in range(0,180):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdifflat[:,lat], sensitivity_array))
        rvals[lat]=r_value**2.
        pvals[lat]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))

    color = 'tab:red'
    ax1.set_xlabel('latitude', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=15)
    ax1.plot(alllats, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=15)
    ax1.tick_params(axis='x', labelsize=15)
    #ax1.plot([-90,90],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width * 0.8, box.height*0.8])
    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=15)  # we already handled the x-label with ax1
    ax2.plot(alllats, pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=15)
    ax2.set_ylim(0,0.1)
    plt.title('ECS vs Plio_Core - PI_CTL by latitude', fontsize=15)
    
    if redu == '':
        figtext = 'e)'
    else:
        figtext = 'c)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom' +
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom' + 
             redu + '.pdf')
    plt.savefig(fileout)
    
    
    # now get the global data and do a correlation
    for mod in range(0,len(modelnames)):
        # get average anomaly
        
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.allmean.nc'
        exptcube=iris.load_cube(fileexpt)
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.allmean.nc'
        cntlcube=iris.load_cube(filecntl)
        
        ny,nx=np.shape(exptcube.data)
        if mod==0:
            anommap=np.zeros((len(modelnames),ny,nx))
           
       
        anommap[mod,:,:]=exptcube.data-cntlcube.data
    
        
    
    rsqmap=np.zeros((ny,nx))
    pvalmap = np.zeros((ny, nx))
    slopemap = np.zeros((ny,nx))
    interceptmap = np.zeros((ny,nx))
    for j in range(0,ny):
         for i in range(0,nx):
             slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(anommap[:,j,i], sensitivity_array))
             rsqmap[j,i]=r_value**2.
             pvalmap[j,i] = p_value
             slopemap[j,i] = slope
             interceptmap[j,i] = intercept
           
             
    rsqmapcube=exptcube.copy(data=rsqmap) 
    rsqmapcube.units=None
    
    slopecube=exptcube.copy(data=slopemap) 
    slopecube.units=None
    
    interceptcube=exptcube.copy(data=interceptmap) 
    interceptcube.units=None
    
    temparr = np.where(pvalmap < 0.05, 1, 0) 
    significance_cube = rsqmapcube.copy(data=temparr)
    
    
    plt.subplot(1,1,1)
    V=np.arange(0.0,1,0.05)
    qplt.contourf(rsqmapcube,V,cmap='YlGnBu')
    iplt.contourf(significance_cube, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(significance_cube, 1, hatches=[None, '\\\''], colors='none')
    #titlename=modeluse+' '+exptname+': '+field
    #bar=plt.colorbar(cs,orientation="horizontal")
    plt.title('correlation between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    
    #plt.title(titlename,fontsize=8)
    plt.gca().coastlines()
 
    if redu == '':
        figtext = 'g)'
    else:
        figtext = 'd)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe'+
             redu + '.eps')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe' + 
             redu + '.pdf')
    plt.savefig(fileout)  
    plt.close()    
    
        
    # print out the slope and the intercept at each gridpoint
    plt.subplot(1,1,1)
    qplt.contourf(slopecube)
    plt.title('slope between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_slope_'+
                redu + '.eps')  
 
    plt.close()
    
    plt.subplot(1,1,1)
    qplt.contourf(interceptcube)
    plt.title('intercept between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_intercept_'+
                redu + '.eps')  
 
    plt.close()
    
    outfile = (filestart + 'allplots/' + 
              fieldname + '/climate_sensitivity_relationships'+
                redu + '.txt')
    
    txtfile1 = open(outfile,"w+") 
    txtfile1.write("longitude latitude rsq pvalue intercept slope \n")
    lons = interceptcube.coord('longitude').points
    lats = interceptcube.coord('latitude').points
    
    for j in range(0,ny):
         for i in range(0,nx):
             writestring = (np.str(np.around(lons[i],2)) + ',' + 
                            np.str(np.around(lats[j],2)) + ',' + 
                            np.str(np.around(rsqmap[j, i],2)) + ',' +
                            np.str(np.around(pvalmap[j, i],2)) + ',' +
                            np.str(np.around(interceptmap[j, i],2)) + ',' +
                            np.str(np.around(slopemap[j,i],2)) + '\n')
             txtfile1.write(writestring)
    txtfile1.close

##########################################################
# main program
        
filename=' '
linux_win='l'
#modelnames=['MIROC4m','COSMOS']   # MIROC4m  COSMOS UofT EC-Earth3.1

#modelnames=['CCSM4-2deg','COSMOS', 'CCSM4-1deg',
#            'EC-Earth3.1', 'CESM1.2',
#            'GISS2.1G','HadCM3',
#            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
#            'MIROC4m','MRI2.3',
#            'NorESM-L','NorESM1-F',
#            'CCSM4-UoT'
#            ]
#redu = ''

modelnames=['COSMOS', 'CESM1.2', 'CCSM4-1deg',
            'GISS2.1G','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI2.3',
            'NorESM-L','NorESM1-F',
            'CCSM4-UoT'
            ]
redu = '_redu'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
#fieldnames = ['SST']
units=['degC']
exptname='EOI400'
cntlname='E280'

for field in range(0,len(fieldnames)):
    climate_sensitivity_analysis(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)::::::::::::::
climate_sensitivity.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

#os.environ["PROJ_LIB"] = r'C:\Users\julia\Miniconda2\pkgs\proj4-5.2.0-hc56fc5f_1003\Library\share'
#from mpl_toolkits.basemap import Basemap, shiftgrid
import sys



#####################################
def  climate_sensitivity_analysis(modelnames,fieldname,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
        datatext = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\data_for_7a-b.txt'
        netcdfout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\data_for_7c.nc'
    else:
        filestart='/nfs/hera1/earjcti/regridded/'
        datatext = '/nfs/hera1/earjcti/regridded/alldata/data_for_7a-b.txt'
        netcdfout = '/nfs/hera1/earjcti/regridded/alldata/data_for_7c.nc'
     
        
    # set up a dictionary for the climate sensitivity
   
    # from my stuff
    #clim_sens ={'NorESM-L': 3.1,
    #             'NorESM1-F':2.29,
    #             'IPSLCM6A': 4.8,
    #             'IPSLCM5A':3.4,
    #             'HadCM3': 3.7,
    #             'MIROC4m':3.9,
    #             'COSMOS':4.1,
    #             'UofT':3.8,
    #             'EC-Earth3.1':3.2,
    #             'MRI-CGCM2.3':2.8,
    #             'CESM1.0.5': 3.1,
    #             'GISS': 3.31
    #             }
        
    # from Alan's table provided by authors
    clim_sens ={'NorESM-L': 3.1,
                 'NorESM1-F':2.3,
                 'IPSLCM6A': 4.8,
                 'IPSLCM5A2':3.6,
                 'IPSLCM5A':4.1,
                 'HadCM3': 3.5,
                 'MIROC4m':3.9,
                 'COSMOS':4.7,
                 'CCSM4-UoT':3.2,
                 'EC-Earth3.1':3.2,
                 'EC-Earth3.3':4.3,
                 'MRI2.3':2.8, # from my investigation
                 'CCSM4-Utr': 3.2,
                 'GISS2.1G': 3.3,
                 'CESM2': 5.3,
                 'CESM1.2' :4.1,
                 'CCSM4' :3.2,
                 'CCSM4-avg' : 3.2
                 }
        
     
    # first get the data.  We need climate sensitivity, 
    # global temperature anomaly, latitude temperature anomaly
    # and gridbox by gridbox temperature anomaly
    climdiff=np.zeros(len(modelnames))
    climdiffmon=np.zeros((len(modelnames),12))
    climdifflat=np.zeros((len(modelnames),180))
    sensitivity_array=np.zeros(len(modelnames))
    alllats=np.arange(0,180,1)-89.5
    
    for mod in range(0,len(modelnames)):
        sensitivity_array[mod]=clim_sens.get(modelnames[mod])
        
        # get data from experiment file
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.data.txt'
        file1= open(fileexpt,"r")
        lines=list(file1)
       
        
        meanexpt,sdexpt=lines[2].split(",")
        monmeanexpt=np.zeros(12)
        latmeanexpt=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeanexpt[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeanexpt[index]=np.float(mean)
       
        
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.data.txt'
        file2= open(filecntl,"r")
        lines=list(file2)
        meancntl,sdexpt=lines[2].split(",")
        monmeancntl=np.zeros(12)
        latmeancntl=np.zeros(180)
        for l in range(5,17):
            index,mean,sd=lines[l].split(",")
            monmeancntl[np.int(index)-1]=np.float(mean)
        for l in range(20,200):
            lat,mean,sd=lines[l].split(",")
            index=np.where(alllats==np.float(lat))
            latmeancntl[index]=np.float(mean)
        
        climdiff[mod]=np.float(meanexpt)-np.float(meancntl)
        
        climdiffmon[mod,:]=monmeanexpt-monmeancntl
        climdifflat[mod,:]=latmeanexpt-latmeancntl
        #print(modelnames[mod],climdifflat[mod])
    
     
    ########################################################
    # plot the climate sensitivity vs the global mean
   
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    ax1.plot(climdiff,sensitivity_array,'x')
    ax1.set_xlabel('Plio_Core - PI_CTL SAT anomaly',fontsize=15)
    ax1.set_ylabel('ECS', fontsize=15)
    ax1.set_xlim(np.floor(np.min(climdiff)),np.ceil(np.max(climdiff)))
    
    # do a linear regression
    print(modelnames)
    print(climdiff)
    print(sensitivity_array)
   
    slope, intercept, r_value, p_value, std_err = sp.stats.linregress(climdiff, sensitivity_array)
    xarray=np.arange(0,10,1)
    yarray=intercept+(slope*xarray)
    ax1.plot(xarray,yarray)
    ax1.tick_params(axis='x',  labelsize=15)
    ax1.tick_params(axis='y',  labelsize=15)
    #plt.title("R-squared: " + np.str(np.around((r_value**2.), 2)) 
    #        + ",  p-value: " + np.str(np.around(p_value, 2)) , fontsize=15) 
    
    print('julia',slope, intercept, r_value, p_value)
   # sys.exit(0)
    figtext = 'a)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])
  
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom.png')
    
    #plt.tight_layout()
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globalanom.pdf')
    plt.savefig(fileout)
    plt.close()

    rsq_std=r_value**2.
    
    f1 = openx=open(datatext,'w')
    f1.write('Data for Figure 7a\n')
    f1.write('model name, Plio_core - PI_Cntl, ECS\n')
    for i in range(0,len(modelnames)):
        f1.write(modelnames[i] + ',' + np.str(np.round(climdiff[i],2)) + ',' + 
                 np.str(np.round(sensitivity_array[i],2)) + '\n')
   

    
    ########################################################
    # plot the correlation between climate sensitivity vs the monthly mean
    rvals=np.zeros(12)
    pvals=np.zeros(12)
    for mon in range(0,12):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdiffmon[:,mon], sensitivity_array))
        rvals[mon]=r_value**2.
        pvals[mon]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

    color = 'tab:red'
    #ax1.set_xlabel('month', fontsize=15)
    ax1.set_ylabel('Rsq', color=color, fontsize=12)
    ax1.plot(labels, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=12)
    ax1.tick_params(axis='x',  labelsize=12)
    #ax1.plot([0,13],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axi
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=12)  # we already handled the x-label with ax1
    ax2.plot(labels, pvals, color=color)
    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)
    #plt.title('ECS vs Plio_Core - PI_CTL by month', fontsize=15)
    
    
    figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
   
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_monanom.pdf')
    plt.savefig(fileout)
    plt.close()
    
    
    
     ########################################################
    # plot the correlation between climate sensitivity vs the latitudinal mean
    rvals=np.zeros(180)
    pvals=np.zeros(180)
    for lat in range(0,180):
        slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(climdifflat[:,lat], sensitivity_array))
        rvals[lat]=r_value**2.
        pvals[lat]=p_value
    
    fig, ax1 = plt.subplots(figsize=(7.0, 4.0))

    color = 'tab:red'
    ax1.set_xlabel('latitude', fontsize=12)
    ax1.set_ylabel('Rsq', color=color, fontsize=12)
    ax1.plot(alllats, rvals, color=color)
    ax1.tick_params(axis='y', labelcolor= color, labelsize=12)
    ax1.tick_params(axis='x', labelsize=12)
    #ax1.plot([-90,90],[rsq_std,rsq_std],color='black',linestyle='dashed',linewidth=2)
    box = ax1.get_position()
    ax1.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_position([box.x0, box.y0+(0.1*box.height), box.width, box.height*0.9])
    color = 'tab:blue'
    ax2.set_ylabel('p-value', color=color, fontsize=12)  # we already handled the x-label with ax1
    ax2.plot(alllats, pvals, color=color)
    #for i, lat in enumerate(alllats):
    #    print(lat, pvals[i], rvals[i])
   
    ax2.tick_params(axis='y', labelcolor=color, labelsize=12)
    #ax2.set_ylim(0,0.1)
    #plt.title('ECS vs Plio_Core - PI_CTL by latitude', fontsize=15)
    
    figtext = 'b)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_latanom.pdf')
    plt.savefig(fileout)
    
    f1.write('\n')
    f1.write('Data for Figure 7b\n')
    f1.write('latitude, Pvalue, Rsqvalue\n')
    for i in range(0,len(alllats)):
        f1.write(np.str(np.round(alllats[i],1)) + ',' + np.str(np.round(pvals[i],3)) + ',' + 
                 np.str(np.round(rvals[i],2)) + '\n')
    f1.close()
    
    #############################################################################
    # now get the global data and do a correlation
    
    cubelist = iris.cube.CubeList([])
    for mod in range(0,len(modelnames)):
        # get average anomaly
        
        fileexpt=filestart+modelnames[mod]+'/'+exptname+'.'+fieldname+'.allmean.nc'
        exptcube=iris.load_cube(fileexpt)
        filecntl=filestart+modelnames[mod]+'/'+cntlname+'.'+fieldname+'.allmean.nc'
        cntlcube=iris.load_cube(filecntl)
        
        ny,nx=np.shape(exptcube.data)
        if mod==0:
            anommap=np.zeros((len(modelnames),ny,nx))
           
       
        anommap[mod,:,:]=exptcube.data-cntlcube.data
    
        
    
    rsqmap=np.zeros((ny,nx))
    pvalmap = np.zeros((ny, nx))
    slopemap = np.zeros((ny,nx))
    interceptmap = np.zeros((ny,nx))
    for j in range(0,ny):
         for i in range(0,nx):
             slope,intercept, r_value, p_value, std_err = (
                sp.stats.linregress(anommap[:,j,i], sensitivity_array))
             rsqmap[j,i]=r_value**2.
             pvalmap[j,i] = p_value
             slopemap[j,i] = slope
             interceptmap[j,i] = intercept
           
             
    rsqmapcube=exptcube.copy(data=rsqmap) 
    rsqmapcube.units=None
    rsqmapcube.long_name = 'Rsq'
    rsqmapcube.standard_name = None
    rsqmapcube.var_name = 'Rsq'

    
    slopecube=exptcube.copy(data=slopemap) 
    slopecube.units=None
    
    interceptcube=exptcube.copy(data=interceptmap) 
    interceptcube.units=None
    
    temparr = np.where(pvalmap < 0.05, 1, 0) 
    significance_cube = rsqmapcube.copy(data=temparr)
    pval_cube = rsqmapcube.copy(data=pvalmap)
    pval_cube.units = None
    pval_cube.long_name = 'pvalue'
    pval_cube.standard_name = None
    pval_cube.var_name = 'pvalue'
    
    cubelist.append(rsqmapcube)
    cubelist.append(pval_cube)
    print(cubelist)
    iris.save(cubelist, netcdfout, netcdf_format='NETCDF3_CLASSIC')

    
    # plot the map with Rsq and the significance
    
    fig = plt.subplots(figsize=(7.0, 5.0))
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(0.0,1,0.05)
    
    qplt.contourf(rsqmapcube,V,cmap='YlGnBu')
    iplt.contourf(significance_cube, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(significance_cube, 1, hatches=[None, '\\\''], colors='none')
    #titlename=modeluse+' '+exptname+': '+field
    #bar=plt.colorbar(cs,orientation="horizontal")
    #plt.title('correlation between climate sensitivity and \n mPWP-PI anomaly '
    #          'at this point')
    plt.title('')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    
    #plt.title(titlename,fontsize=8)
    plt.gca().coastlines()
 
    figtext = 'c)'
    plt.figtext(0.02, 0.97,figtext,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe.png')
    plt.savefig(fileout)
    fileout=(filestart + 'allplots/' + 
             fieldname + '/climate_sensitivity_vs_globe.pdf')
    plt.savefig(fileout)  
    plt.close()    
    
        
    # print out the slope and the intercept at each gridpoint
    plt.subplot(1,1,1)
    qplt.contourf(slopecube)
    plt.title('slope between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_slope.eps')  
 
    plt.close()
    
    plt.subplot(1,1,1)
    
    qplt.contourf(interceptcube)
    plt.title('intercept between climate sensitivity and \n mPWP-PI anomaly '
              'at this point')
    plt.gca().coastlines()
    plt.savefig(filestart + 'allplots/' + 
                fieldname + '/climate_sensitivity_map_intercept.eps')  
 
    plt.close()
    
    outfile = (filestart + 'allplots/' + 
              fieldname + '/climate_sensitivity_relationships.txt')
    
    txtfile1 = open(outfile,"w+") 
    txtfile1.write("longitude latitude rsq pvalue intercept slope \n")
    lons = interceptcube.coord('longitude').points
    lats = interceptcube.coord('latitude').points
    
    for j in range(0,ny):
         for i in range(0,nx):
             writestring = (np.str(np.around(lons[i],2)) + ',' + 
                            np.str(np.around(lats[j],2)) + ',' + 
                            np.str(np.around(rsqmap[j, i],2)) + ',' +
                            np.str(np.around(pvalmap[j, i],2)) + ',' +
                            np.str(np.around(interceptmap[j, i],2)) + ',' +
                            np.str(np.around(slopemap[j,i],2)) + '\n')
             txtfile1.write(writestring)
    txtfile1.close

##########################################################
# main program
        
filename=' '
linux_win='l'
#modelnames=['MIROC4m','COSMOS']   # MIROC4m  COSMOS UofT EC-Earth3.1

modelnames=['CCSM4-Utr','COSMOS', 'CCSM4',
            'EC-Earth3.3', 'CESM1.2','CESM2',
            'GISS2.1G','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI2.3',
            'NorESM-L','NorESM1-F',
            'CCSM4-UoT'
            ]

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
#fieldnames = ['SST']
units=['degC']
exptname='EOI400'
cntlname='E280'

for field in range(0,len(fieldnames)):
    climate_sensitivity_analysis(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)
::::::::::::::
create_grid.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 14 15:53:19 2019

#@author: earjcti

# create a blank netcdf file on the correct grid for putting the pliomip data on


import netCDF4
import numpy as np
from netCDF4 import Dataset

dataset=Dataset('one_lev_one_deg.nc', 'w',format='NETCDF3_CLASSIC') 
#create dimensions
level = dataset.createDimension('level', 1) 
latitude = dataset.createDimension('latitude', 180)
longitude = dataset.createDimension('longitude', 360) 
time = dataset.createDimension('time', None)

# create variables
times = dataset.createVariable('time', np.float64, ('time',)) 
levels = dataset.createVariable('level', np.int32, ('level',)) 
latitudes = dataset.createVariable('latitude', np.float32,('latitude',))
longitudes = dataset.createVariable('longitude', np.float32,('longitude',)) 
# Create the actual 4-d variable
dummy = dataset.createVariable('dummy', np.float32, ('time','level','latitude','longitude')) 

# Variable Attributes  
latitudes.units = 'degree_north'  
longitudes.units = 'degree_east'  
levels.units = 'Surface' 
dummy.units = 'None' 
times.units = 'hours since 0001-01-01 00:00:00'  
times.calendar = 'gregorian' 

# add variables
lats = np.linspace(-89.5,89.5,num=180) 
print(lats)
lons = np.arange(0,360,1.0)
print(len(lats)) 
print(len(lons))
latitudes[:] = lats  
longitudes[:] = lons 
dummy[:,:,:,:]=0.0
levels[0]=0

dataset.close()
::::::::::::::
dmc_by_latitude.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
This program will do a DMC plot.  But it will be latitude vs deltSST

"""
import pandas as pd
import matplotlib as mp
import matplotlib.pyplot as plt
import numpy as np
import iris
import sys

def is_number(s):
    try:
        float(s)
        return True
    except ValueError:
        return False

def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints
    
def plot_data(lats, Tanom, stdev):
    """
    plots the data and the errorbars
    """
    
    stdevplot = np.zeros(len(stdev))
    for i in range(0, len(stdev)):
       numeric = is_number(stdev[i])
       if numeric:
            stdevplot[i] = stdev[i]
    
    print(stdevplot)

    plt.errorbar(lats, Tanom, yerr=stdevplot, fmt='o')
    

def get_multimodel_mean(fieldname):
    """
    gets the multimodel mean and calculates a zonal average
    """
    mmm_cube = iris.load_cube(MULTIMODELMEAN, fieldname)
    zm_cube = mmm_cube.collapsed(['longitude'], iris.analysis.MEAN)
    zm_cube_max = mmm_cube.collapsed(['longitude'], iris.analysis.MAX)
    zm_cube_min = mmm_cube.collapsed(['longitude'], iris.analysis.MIN)
    
    return zm_cube, zm_cube_max, zm_cube_min
    
def plot_zm(cubemean, cubemax, cubemin, max_cubemax, min_cubemin):
    """
    latitudinal plot + zonal range of multimodel mean
    cubemean, cubemax, cubemin are the mean and the range from the multimodel mean
    max_cubemax is the maximum longitude from the model with the maximum difference
    min_cubemin is the minimum longitude from the model with the minimum difference
    """
    
    lats = cubemean.coord('latitude').points
    datamean = cubemean.data
    datamax = cubemax.data
    datamin = cubemin.data
    
    fig, ax = plt.subplots() 
    ax.plot(lats, datamean)
    ax.fill_between(lats, min_cubemin.data, max_cubemax.data, alpha=0.4)
    ax.fill_between(lats, datamin, datamax, alpha=0.4)
    ax.set_ylim(-5.0,20.0)
   
    ax.set_xlabel('latitude')
    ax.set_ylabel('SST anomaly')
    


def main():
    """
    1. get the data
    2. get multimodel mean SST 
    3. plot the data on the same file as the MMM
    """

    lats, lons, data_tanom, data_stdev, Npoints = get_data()
    #
    zonal_mean_cube, zonal_max_cube, zonal_min_cube = get_multimodel_mean('SSTmean_anomaly')
    max_zonal_mean_cube, max_zonal_max_cube, max_zonal_min_cube = get_multimodel_mean('SSTmax_anomaly')
    min_zonal_mean_cube, min_zonal_max_cube, min_zonal_min_cube = get_multimodel_mean('SSTmin_anomaly')
   
   
    plot_zm(zonal_mean_cube, zonal_max_cube, 
            zonal_min_cube, max_zonal_max_cube, min_zonal_min_cube)
    plot_data(lats, data_tanom, data_stdev)
    
    
    plt.savefig(OUTNAME + '.eps')
    plt.savefig(OUTNAME + '.pdf')
    plt.close()

DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
MULTIMODELMEAN = '/nfs/hera1/earjcti/regridded/SST_multimodelmean.nc'
OUTNAME = '/nfs/hera1/earjcti/regridded/allplots/SST/dmc_by_latitude'

main()
::::::::::::::
DMC_for_IPCC.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on August 2020


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
#import matplotlib as mp
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    import matplotlib as mpl
    import numpy as np
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=60, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

  
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    model_anom_cube, lsmplio_cube = get_model_data()

    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata() 
        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])

    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'H' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
main()

#sys.exit(0)
::::::::::::::
DMC_for_IPCC_with_land.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    #plt.show()
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         land_lats, land_lons, land_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
DMC_for_IPCC_with_land_split.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 2021
# note this differs from DMC_for_IPCC_land in that it will produce 
# seperate plots for the land and the ocean
# it will also produce a plot where the ocean mmm and data disagree.


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        print(temp,'julia')
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons


def outside_x(x, lats, lons_shift, data_tanom, model_anom_cube):
    """
    reduces data points to those that are more than x degs away from the model
    """

    cubelats = model_anom_cube.coord('latitude').points
    cubelons = model_anom_cube.coord('longitude').points

    new_lats = []
    new_lons = []
    new_data = []

    for i, lat in enumerate(lats):
        lon = lons_shift[i]
      # find nearest latitude and lontiude to the value
        latix = np.abs(cubelats-lat).argmin()
        lonix = np.abs(cubelons-lon).argmin()

        model_slice  =  model_anom_cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
           
        modelanom = model_slice.data

        if np.abs(data_tanom[i] - modelanom) > x:
            new_lats.append(lat)
            new_lons.append(lon)
            new_data.append(data_tanom[i])
        
    return new_lats, new_lons, new_data


def plot(model_cube, mask_cube, lats, lons, data, fileout):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data 
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


  
    #plt.show()
    plt.savefig(fileout + '.png')
    plt.savefig(fileout + '.eps')
    plt.close()


def plot_2figs(model_cube, mask_cube, land_lats, land_lons, 
         land_data, ocean_lats, ocean_lons, ocean_data):
    """
    prepares a 2 part figure with land and ocean 
    """

    fig = plt.figure(figsize=[10.0,4.0])


    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    # OCEAN DATA
    # model
    ax = fig.add_subplot(1,2,1, projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('a) Ocean DMC')
    

    # overplot data   
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    plt.scatter(ocean_lons, ocean_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())
    plt.scatter(ocean_lons, ocean_lats, c=ocean_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())



    # LAND DATA
    # model
    ax = fig.add_subplot(1,2,2, projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('b) Land DMC')
    

    # overplot data 
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())
    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())

    # colorbar
  #  fig.tight_layout()
    fig.subplots_adjust(bottom=0.15)
    cbar_ax = fig.add_axes([0.15, 0.15, 0.75, 0.05])
    cbar = fig.colorbar(cs, cax=cbar_ax,orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C',fontsize=15)
    cbar.ax.tick_params(labelsize=10)

    plt.savefig('/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_ocn.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_ocn.eps')
    plt.close()

    
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)

    # put land and ocean on same figure
    plot_2figs(model_anom_cube, lsmplio_cube, land_lats, land_lons, 
         land_tanom, lats, lons_shift, data_tanom)
    
    # put land and ocean on seperate figures
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land'
    plot(model_anom_cube, lsmplio_cube, land_lats, land_lons, 
         land_tanom,fileout)
    
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/ocean'
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         fileout)

    # reduce points to those not within x deg of data
    x=2
    (llat_new, llon_new, ldata_new) = outside_x(x, land_lats, land_lons, 
                                               land_tanom, model_anom_cube)
    (olat_new, olon_new, odata_new) = outside_x(x, lats, lons_shift, 
                                               data_tanom, model_anom_cube)

    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/land_unmatched'
    plot(model_anom_cube, lsmplio_cube, llat_new, llon_new, 
         ldata_new,fileout)
    
    fileout = '/nfs/hera1/earjcti/regridded/allplots/IPCCtype/ocean_unmatched'
    plot(model_anom_cube, lsmplio_cube, olat_new, olon_new, odata_new,
         fileout)

   
##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
DMC_for_paper_with_land_alt.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys


def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap

def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """

    colors = [(5,48,97),(22,75,124),(39,102,151),(56,130,178),
              (80,154,199),(114,173,209),(147,192,219),(181,211,228),
              (215,230,238),(255,255,255),(255,255,255),(255,255,255),
              (242,220,217),(236,192,185),
              (229,163,153),(223,135,121),(216,107,89),(195,80,69),
              (164,53,56),(133,26,43),(103,0,31)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube

def get_data_cube(filename, fieldname, grid_cube):
    """
    get the data from the given file and regrid change missing data to 1000.
    """

    cube = iris.load_cube(filename, fieldname)
    regrid_cube = cube.regrid(grid_cube, iris.analysis.Linear())

    regrid_data = regrid_cube.data
    regrid_data = np.where(regrid_data < 1000., regrid_data, 1000.)
    new_cube = regrid_cube.copy(data=regrid_data)
  
    return new_cube


def get_model_data():
    """
    first read in the lsm
    read in data from the pliocene and the preindustrial and regrid
    if a point is land get the data from the NSAT file
    if a point is ocean get the data from the SST file
    """
    
    (lsm_cube, lsm_plio_cube) = get_lsm()

    nsat_cube = get_data_cube(NSAT_MMM_FILE,
                              'NearSurfaceTemperaturemean_anomaly',
                              lsm_cube)
    sst_cube = get_data_cube(SST_MMM_FILE,'SSTmean_anomaly', lsm_cube)
   
   
    # anom_cube = nsat anomaly over land and sst anomaly over ocean)
    anom_cube = ((nsat_cube * lsm_cube) - 
                 (sst_cube * (lsm_cube - 1.0)))

    # there are still a few points which are using the ocean value and 
    # should be using the land value  Change these.
    # check there are not too many

    anom_data = anom_cube.data
    nsat_data = nsat_cube.data
    anom_new_data = np.where(anom_data > 900, nsat_data, anom_data)
    anom_new_cube = anom_cube.copy(anom_new_data)

    return anom_new_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns temperatures
    """

    dfs = pd.read_excel(LAND_DATAFILE)
    sites = []
    lats = []
    lons = []
    temps = []
    temp_modern = []

    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        # if temp ne nan then move to array
        temp = dfs.iloc[rl, 9]
        if np.isfinite(temp):
            sites.append(dfs.iloc[rl, 0])
            lats.append(dfs.iloc[rl, 2])
            lons.append(dfs.iloc[rl, 3])
            temp_modern.append(dfs.iloc[rl, 4])
            print(dfs.iloc[rl,0])
  
            if (dfs.iloc[rl,0] == 'Lake Baikal'):
                temps.append(temp - 5.8)
                print('LB found')
            elif (dfs.iloc[rl,0] == 'Lost Chicken Mine'):
                temps.append(temp - 4.0)
                print('LCM found')
            elif (dfs.iloc[rl,0] == 'James Bay Lowland'):
                temps.append(temp - 4.0)
                print('JBL found')
            else:
                temps.append(temp)

    return lats, lons, temps, temp_modern

def get_cru_temp(lats, lons):
    """
    get's the cru temperature at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUTEMP/' + 
               'E280.NearSurfaceTemperature.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_temp = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_temp[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_temp[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_temp[i] = np.nanmean(surround)
           
    return cru_temp


def get_data():
    """
    this function willl open the file containing the data and will return 
    arrays containing:
        1. site_longitude
        2. site_latitudes
        3. T anomaly from (NOAA-ERSSTv5)
        4. standard deviation of the data
        5. number of sites
    """
    dfs = pd.read_excel(DATAFILE)
    dfs_subset = dfs[["Latitude (N)", "Longitude (E)", "NOAA_anom", "Standard dev.", "N"]]
    
    lats = dfs_subset.iloc[:,0]
    lons = dfs_subset.iloc[:,1]
    data_tanom = dfs_subset.iloc[:,2]
    data_stdev = dfs_subset.iloc[:,3]
    Npoints = dfs_subset.iloc[:,4]
    
    
    return lats, lons, data_tanom, data_stdev, Npoints

class GetPliovar:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = DATASTART + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = DATASTART +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = DATASTART + 'pliovar_metadata_global_02102019.csv'
        self.pifile = DATASTART + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        if HARRY_ERIN == 'Eb':
            data_tanom = self.sitetemp_bs - self.temppi
        if HARRY_ERIN == 'En':
            data_tanom = self.sitetemp - self.temppi

        latuse = []
        lonuse = []
        tanom_use = []
        nsites_use = 0
        for i, tanom in enumerate(data_tanom):
            if np.isfinite(tanom):
                latuse.append(self.lat[i])
                lonuse.append(self.lon[i])
                tanom_use.append(tanom)
                nsites_use = nsites_use + 1

        
        return latuse, lonuse, tanom_use, nsites_use
       
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
 
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data, land_lats, land_lons,
         land_data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -10.0
    vmax = 10.0
    incr = 1.0
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    cbar = plt.colorbar(cs,  orientation= 'horizontal',
                        ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar.set_label('deg C')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI temperature anomaly')
    

    # overplot data ocean
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
    print(norm)

    plt.scatter(lons, lats, c='black',  marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())


    # overplot data lane
 
    plt.scatter(land_lons, land_lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(land_lons, land_lats, c=land_data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    plt.savefig('/nfs/hera1/earjcti/regridded/alternative_IPCC_Tanom_' + OUTSS + '.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/alternative_IPCC_Tanom_' + OUTSS + '.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsmplio_cube = get_model_data()

    # get land observations and cru temperature at land points
    
    land_lats, land_lons, land_temp, modern_temp = get_land_obs()
    cru_land_temp = get_cru_temp(land_lats, land_lons)

    land_tanom = land_temp - cru_land_temp
    
   # for i, lat in enumerate(land_lats):
   #     print(lat, land_lons[i], land_temp[i], modern_temp[i], 
   #           cru_land_temp[i], land_tanom[i])

   
    # get ocean observations
    if HARRY_ERIN == 'H':
        lats, lons, data_tanom, data_stdev, Npoints = get_data()
    if HARRY_ERIN == 'En' or HARRY_ERIN == 'Eb':
        obj = GetPliovar('t1', 'MGCA') # get data for t1 timeslice
        lats, lons, data_tanom, Npoints = obj.get_proxydata() 
        obj = GetPliovar('t1', 'UK37') # get data for t1 timeslice
        lats_UK37, lons_UK37, data_tanom_UK37, Npoints_UK37 = obj.get_proxydata()
        for i, lat in enumerate(lats_UK37):
            print (lat, lons_UK37[i], data_tanom_UK37[i])
        #sys.exit(0)
       

        for i in range(0, Npoints_UK37):
            lats.append(lats_UK37[i])
            lons.append(lons_UK37[i])
            data_tanom.append(data_tanom_UK37[i])


  
    # if two points are same shift them so they are both visible
    lons_shift = shift_lons(lons, lats, data_tanom)
        
    plot(model_anom_cube, lsmplio_cube, lats, lons_shift, data_tanom,
         land_lats, land_lons, land_tanom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'
HARRY_ERIN = 'Eb' # H=Harry, En Erin Normal, Eb Erin Bayspline



LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')
NSAT_MMM_FILE = (FILESTART + 
                 'regridded/NearSurfaceTemperature_multimodelmean.nc')
SST_MMM_FILE = (FILESTART + 'regridded/SST_multimodelmean.nc')


if HARRY_ERIN == 'H':
    DATAFILE = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.xlsx'
    OUTSS = 'FD30'

if HARRY_ERIN == 'Eb':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Bayspline'

if HARRY_ERIN == 'En':
    DATASTART = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
    OUTSS = 'McClymont_Standard'
    
LAND_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
emergent_constraints_old.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu)
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    print('mean',np.mean(new_clim_sens_redu))
    print('stdev',np.std(new_clim_sens_redu))
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
    

   
    return lats, lons, sstanom_min, sstanom_max


def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    mod_lat, mod_lon, mod_minsst, mod_maxsst =  readmodel()

                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
  
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
emergent_constraints.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 13:37:24 2019

@author: julia
This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) plot a map of all the climate sensitivities
        d) print out the range of all the climate sensitivities
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import iris
import iris.quickplot as qplt
import iris.plot as iplt
import sys
import os

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid




def main():
    """ 
    This program will estimate the climate sensitivity from the proxy data as follows:
    1.  read in proxy data
    2.  read in the file from the model which see's whether there is a significant relationship
        between Plio(Tanom) and ECS at each gridbox
    3.  For each proxy point
        a) check if there should be a significant relationship
        b) if so estimate the climate sensitivity using the slope and the intercept
        c) print out the range of all the climate sensitivities
    4.  Read in the cube showing data from figure 7d.  Which shows the 
        p value at each gridcell
    
    5. plot a map of regions where there is a significant relationship.
    6. as 5. but with overplot the climate sensitivties derived from each point
    """
   
    
    #1. read in proxy data
    proxylat, proxylon, proxy_sst_anom = readproxy()
    gridlat, gridlon,  pval, intercept, slope = readfile()
    
    nproxies = len(proxylat)
    ngrids = len(gridlat)
    clim_sens = np.zeros(nproxies)
    
    # 2. 3. check significance and estimate climate sensitivity
    for i in range(0,nproxies):
        # get the subscript from the model relationship file
        grid_ss, griddiff = get_subscript(proxylat[i], proxylon[i], gridlat, gridlon, ngrids)
        # see if it is significant (p < 0.05
        if pval[grid_ss] < 0.05:
           # if significant CS = intercept + (proxy_sst_anom) * slope
            clim_sens[i] = intercept[grid_ss] + (slope[grid_ss] * proxy_sst_anom[i])
            print('ind sens',proxylat[i],proxylon[i],clim_sens[i])
        else:
            clim_sens[i] = np.nan
        # print cs
      
    # 4. find regions that there is a significant relationship
    pval_cube = iris.load_cube(SIGNIFICANCE_FILE, SIGNIFICANCE_NAME)
    sign_data = np.where(pval_cube.data < 0.05, 1.0, 0.0)
    sign_cube=pval_cube.copy(data=sign_data)
    
    
    # put into a reduced array and plot
    nvals = np.count_nonzero(~np.isnan(clim_sens))
    print(nvals)
    count=0
    latredu = np.zeros(nvals)
    lonredu = np.zeros(nvals)
    sstanomredu = np.zeros(nvals)
    clim_sens_redu = np.zeros(nvals)
    
    for i in range(0,nproxies):
        if np.isfinite(clim_sens[i]):
            latredu[count] = proxylat[i]
            if proxylon[i] > 180:
                lonredu[count] = proxylon[i]-360.
            else:
                lonredu[count] = proxylon[i]
            clim_sens_redu[count] = clim_sens[i]
            sstanomredu[count] = proxy_sst_anom[i]
            count=count+1
            
    plotdata(latredu,lonredu,clim_sens_redu,nvals, FILEOUT)
    
    # remove sites which we are not sure about.
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all
    
    (new_latredu, new_lonredu, 
     new_nvals, new_clim_sens_redu) = redu_sites(latredu, 
                                                 lonredu, 
                                                 sstanomredu,
                                                 clim_sens_redu,
                                                 proxy_sst_anom,
                                                 proxylat, proxylon)  
                                                 
    plotdata(new_latredu,new_lonredu,new_clim_sens_redu,new_nvals, FILEOUT_R)
    print('new clim_sens_redu',new_clim_sens_redu)
    sys.exit(0)
    
    
    # now plpot data but overplot where there is a significant relationship
    plotdata_overplot(new_latredu,new_lonredu,
                      new_clim_sens_redu,new_nvals, 
                      pval_cube, sign_cube,
                      FILEOUT_S)
    
    
def readfile():
    """
    reads data from the file
    returns numpy arrays of, lat, lon, pval, intercept, slope
    """
    f1 = open(FILECS,'r') # to count lines
    count=0
    for line in f1.readlines():
        count = count + 1
    f1.close()
     
    nvals = count 
    lats = np.zeros(nvals)
    lons = np.zeros(nvals)
    intercepts = np.zeros(nvals)
    pvals = np.zeros(nvals)
    slopes = np.zeros(nvals)
    
    f1 = open(FILECS,'r') # to read
    count=0
    for line in f1.readlines():
        if line[0:4] == 'long': # titleline ignore
            print('titleline is',line)
            pass
        else:
            vals = line.split(',')
            lons[count] = vals[0]
            lats[count] = vals[1]
            pvals[count] = vals[3]
            intercepts[count] = vals[4]
            slopes[count] = vals[5]
            
        count = count + 1
    f1.close()
    
    return lats, lons, pvals, intercepts, slopes
   
def readproxy():
    """
    reads in the excel spreadsheet of the proxy dataset
    returns arrays of the latitude and longitude and sstanom of the proxy dataset
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom  = np.zeros(37)
    f1 = open(FILEPROXY,'r') # to read
    count=0
    for line in f1.readlines():
        if count >= 37:
            pass
        else:
            if line[0:4] == 'Loca': # titleline ignore
                print('titleline is',line)
                pass
            else:
                vals = line.split(',')
             
                if np.float(vals[2]) > 0:
                    lons[count] = np.float(vals[2])
                else:
                    lons[count] = np.float(vals[2]) + 360.
                lats[count] = np.float(vals[1])
                sstanom[count] = np.float(vals[15]) # this is plio - noaa
                count = count + 1
          
            
       
    f1.close()
   
    return lats, lons, sstanom

def readmodel():
    """
    reads in the excel spreadsheet of the model dataset
    returns arrays of the latitude and longitude, the minimum modelled ssta and
    the maximum modelled ssta
    """
    
    lats = np.zeros(37)
    lons = np.zeros(37)
    sstanom_min  = np.zeros(37)
    sstanom_max = np.zeros(37)
    allsstanom = np.zeros((37, 17)) # the 17th is the MMM
    f1 = open(FILEMODEL,'r') # to read
    count=0
    lines = f1.readlines()
    line = lines[0]
    titles = line.split(',')
    modnames = titles[3:20]
    
    # assume data starts at row index 1
    for i in range(1,len(lines)):
      line = lines[i]
      vals = line.split(',')
      lats[i-1] = vals[1]
      lons[i-1] = vals[2]
      print(i, vals[3:19])
      sstanom_min[i-1] = np.min(np.asarray(vals[3:19], dtype = float))
      sstanom_max[i-1] = np.max(np.asarray(vals[3:19], dtype = float))
      allsstanom[i-1, :] = np.asarray(vals[3:20], dtype = float)
    

   
    return lats, lons, sstanom_min, sstanom_max, allsstanom, modnames

def print_rmse(mod_allsst, proxy_allsst, modlats, modlons, 
               proxylats, proxylons, modnames):
    """
    just prints out the rmse for all the models

    """
    print('shape proxy', np.shape(proxy_allsst))
    print('shape data', np.shape(mod_allsst))
    npoints, nmods = np.shape(mod_allsst)
    
    f1 = open('ind_model_dmc.txt','w+')
    f1.write('model        rmse      bias   within 2deg/ 1deg/ 0.5deg \n')
    for i in range(0, nmods):
        sumsq = 0.0
        avger = 0.0
        count = 0.0
        within_1deg = 0
        within_2deg = 0
        within_05deg = 0
        for j in range(0, npoints):
            sumsq = sumsq + ((proxy_allsst[j] - mod_allsst[j, i])**2.0)
            avger = avger + (proxy_allsst[j] - mod_allsst[j, i])
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 1.0):
                within_1deg = within_1deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 2.0):
                within_2deg = within_2deg + 1
            if (np.abs(proxy_allsst[j] - mod_allsst[j, i]) < 0.5):
                within_05deg = within_05deg + 1
            count = count + 1.0
            
        rmse = np.sqrt(sumsq / count)
        avger = avger / count
      
        f1.write(modnames[i].ljust(12) + ',' +  np.str(np.around(rmse,2))
                 + ',   ' +  np.str(np.around(avger,2)) + ',   ' 
                 + np.str(within_2deg) +  ',' +  np.str(within_1deg)
                 +  ',' +  np.str(within_05deg) + '\n')
        print(modnames[i].ljust(12),',',np.around(rmse,2),
                 ',   ',np.around(avger,2),',   ',within_2deg,  
               '   ',within_1deg,',   ',within_05deg)
        
    f1.close()
   

def get_subscript(latreq, lonreq, gridlat_arr, gridlon_arr, ngrid):
    """
    this program is passed a latitude and longitude (latreq, lonreq)
    and also two array containing (ngrid) values.  The arrays each contain
    latitudes and longitudes
    we want to find the subscript of the array that most closely matches the
    required values and return it
    """
    
    diffvals = 100.
    subscript = 0
    
    for i in range(0, ngrid):
        thisdiff = np.abs(gridlat_arr[i] - latreq) + np.abs(gridlon_arr[i] - lonreq)
        if thisdiff < diffvals:
            diffvals = thisdiff
            subscript = [i]
    

    return subscript, diffvals

def plotdata(lat,lon,clim_sens,nproxies,fileout):
    """
    plots the cliate sensitivity on a map
    """
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    cs = m.scatter(x1,y1,s=60,c=clim_sens,marker="o",cmap='rainbow')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    #plt.show()
    print('saving figure as',fileout)
    plt.savefig(fileout)
    plt.close()
    
    txtfile1 = open(TEXTFILE,"w+")
   
    txtfile1.write('lon, lat, est_ECS \n')
    for i, lon in enumerate(x1):
        txtfile1.write((np.str(np.around(lon,2)) + 
                       ',' + np.str(np.around(y1[i],2)) + 
                       ',' + np.str(np.around(clim_sens[i],2)) +  '\n'))
        
    txtfile1.write('\n')
  
def plotdata_overplot(lat,lon,clim_sens,nproxies,pcube, signcube, fileout):
    """
    this will plot on a single map all the gridpoints where there is 
    a significant relationship between pliocene temp anomaly and climate sensitivity
    on top of this it will plot the climate sensitivities obtained from the data
    """
    
  
    m=Basemap(llcrnrlon=-180.0,urcrnrlon=180.0,llcrnrlat=-90.0,
              urcrnrlat=90.0,projection='cyl',resolution='c')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    
    iplt.contourf(signcube, 1, colors=[[0.8, 0.8, 0.8], [1, 1, 1]])
    #iplt.contourf(signcube, 1, hatches=[None, '///'], colors='none')
    #iplt.contourf(signcube, 1, hatches=[None, '\\\''], colors='none')
   
    x1,y1=m(lon,lat)
    
    print(clim_sens)
    #m.scatter(x1,y1,s=sizes,c=cols,marker="o",cmap=cm.cool,alpha=0.7)
    #cs = m.scatter(x1,y1,s=65,marker="o","black")
    cs = m.scatter(x1, y1, s=60, c=clim_sens, marker="o",
                   cmap='rainbow', edgecolors='black')
    cbar = plt.colorbar(cs,orientation="horizontal",extend='both')
    #cbar.set_label('climate sensitivity (degC)',labelpad=-40,size=15)
    cbar.set_label('climate sensitivity (deg C)')
    m.drawmapboundary
    m.drawcoastlines()
    parallels=np.arange(-90.,90.,50.)
    m.drawparallels(parallels,labels=[True,True,True,True],fontsize=10) # labels right
    meridians=np.arange(-180.,180.,60.)
    m.drawmeridians(meridians,labels=[True,True,True,True],fontsize=10)
    plt.savefig(fileout)
    plt.close()
    
def redu_sites(lats, lons, proxysst, clim_sens, 
               proxysst_full, proxy_fulllat, proxy_fulllon):
    """
    # remove sites which we are not sure abou
    # this is where the datapoint is not within 1 deg of the modelled range
    # ie the data does not even nearly agree with any of the models
    # ie where data and model do not agree at all   
    
    input latitude longitude and original climate sensitivity
    output new latitude longitude climate sensitivitiy and nsites
    """
    print('in redu sites')
    (mod_lat, mod_lon, mod_minsst, mod_maxsst, 
    mod_allsst, modnames) =  readmodel()
    print_rmse(mod_allsst, proxysst_full, mod_lat, mod_lon, proxy_fulllat, 
               proxy_fulllon, modnames)
                                               
    nvals = 0
    new_latredu = []
    new_lonredu = []
    new_clim_sens_redu = []

    for i, lat_i in enumerate(lats):
        lon_i = np.around(lons[i],2)
        j = np.where(mod_lat == lat_i)
        
        if lon_i == mod_lon[j]:
            print('here', lat_i, lon_i, proxysst[i], mod_minsst[j], mod_maxsst[j])
            if mod_minsst[j] - 1.0 < proxysst[i] < np.min([mod_maxsst[j] + 1.0, 9.0]) :
                new_latredu.append(lat_i)
                new_lonredu.append(lon_i)
                new_clim_sens_redu.append(clim_sens[i])
                nvals = nvals + 1
         
        else:
            print('j is', j)
            print('latlon mismatch',i, lat_i, mod_lat[j],lon_i,mod_lon[j])
            sys.exit(0)
   
            
    return (np.asarray(new_latredu, dtype=float), 
            np.asarray(new_lonredu,dtype=float), 
            nvals, np.asarray(new_clim_sens_redu, dtype = float)) 
                     
    
##############################################################
LINUX_WIN = 'l'
if LINUX_WIN == 'l':
    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
#    FILECS = ('/nfs/hera1/earjcti/regridded/allplots/NearSurfaceTemperature/climate_sensitivity_relationships.txt')
    FILEPROXY = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/cs_mp_sst_data_30k_plusNOAA.csv')
    FILEOUT = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity.pdf')
    FILEOUT_R = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu.pdf')
    FILEOUT_S = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/climate_sensitivity_redu_significant.pdf')
    FILEMODEL = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/modeloutput_CSCD_nearsites.csv')
    SIGNIFICANCE_FILE = ('/nfs/hera1/earjcti/regridded/alldata/data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = '/nfs/hera1/earjcti/regridded/alldata/figure9.txt'
else:
    FILECS = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_relationships.txt'
    FILEPROXY = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\cs_mp_sst_data_30k_plusNOAA.csv')
    FILEMODEL = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\modeloutput_CSCD_nearsites.csv')
    FILEOUT = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity.pdf')
    FILEOUT_R = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu.pdf')
    FILEOUT_S = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\NearSurfaceTemperature\\climate_sensitivity_redu_significant.pdf')
    SIGNIFICANCE_FILE = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_7d.nc')
    SIGNIFICANCE_NAME = 'pvalue'
    TEXTFILE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata/figure9.txt'

main()
::::::::::::::
extract_data_locations_monthly.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_02102019.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.outend = 'modeloutput_pliovar'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'modeloutput_CSCD_localities'
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'test_localities'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []

        with open(self.filenamein) as csvfile:
            print(self.filenamein)
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:#
                print(row)
                print(' ')
                print(' ')
                if count != 0: # not titleline
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period
        self.nmonths = 12

        if test == 'y':
            self.modelnames = ['NorESM-L']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]
        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros((len(self.lonlist), self.nmonths))
        model_data_near = np.zeros((len(self.lonlist), self.nmonths)) # values near the point
        near_distance = np.zeros((len(self.lonlist), self.nmonths)) # how far away we have to look to get data
        ngbox_avg = np.zeros((len(self.lonlist), self.nmonths)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            data_slice_months = data_slice.data
            for j in range(0,12):
                if -100. < data_slice_months[j] < 100.: 
                    model_data[i, j] = data_slice_months[j]
                    model_data_near[i, j] = model_data[i, j]
                else:
                    model_data[i, j] = float('NaN')
                    model_data_near[i, j] = float('NaN')

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            # check at month zero as we assume that lsm is same for all months
            while np.isnan(model_data_near[i, 0]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i, :] = neardata

            near_distance[i, :] = count_near_gb # how far away are we looking for data
            ngbox_avg[i, :] = ngboxes


        returndata = [model_data, model_data_near, near_distance, ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)

        sitenear = np.zeros((nmodels, npoints, self.nmonths)) # data near point
        sitevals = np.zeros((nmodels, npoints, self.nmonths)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints, self.nmonths)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints, self.nmonths)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.' + fieldnames + '.mean_month.nc')

            # get model points and how far away they are from data
            (sitevals[model, :, :], 
             sitenear[model, :, :], 
             sitenear_dist[model, :, :], 
             sitenear_ngbox_avg[model, :,: ]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'lat')
    sheet.write(0,1,'lon')
    for model in range(0,nmodels):
        sheet.write(0,2+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,latlist[i],style)
        sheet.write(i+1,1,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,2+model,datawrite[model,i],style)

    sheet.write(0,2+nmodels,'MMM')
    sheet.write(0,3+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,2+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,3+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

   
    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,4+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,4+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, monthno):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    print('writing out monthno',monthno, np.shape(eoi400))
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400[:, :, monthno])
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280[:, :, monthno])
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels, :, monthno]-e280[0:nmodels, :, monthno])

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near[:, :, monthno])
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near[:, :, monthno])
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels, :, monthno]-e280_near[0:nmodels, :, monthno])

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance[:, :, monthno])
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance[:, :, monthno])

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg[:, :, monthno])
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg[:, :, monthno])


    monthname = {0:'jan', 1:'feb', 2:'mar',3:'apr', 4:'may', 
                 5:'jun',6:'jul', 7:'aug', 8:'sep',9:'oct', 
                 10:'nov', 11:'dec' }

    fileoutwrite = fileout + monthname.get(monthno) + '.xls'

    # remove output file if it exists
    exists  =  os.path.isfile(fileoutwrite)
    if exists:
        os.remove(fileoutwrite)
    wb.save(fileoutwrite)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Erin' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
modelstart,outputfile,longitudes,latitudes,longitudes_alt = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points




##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
nmonths = len(eoi400[0, 0, :])

#######################################
# write data out to a workbook

for mon in range(0, 12):
    write_to_book(outputfile,longitudes,latitudes,longitudes_alt, mon)

###################################
# plot model points
for model in range(0,1):
        #plot points from january
    plotpoints(longitudes_alt,latitudes,eoi400[model,:,0])
          #plot points from july
    plotpoints(longitudes_alt,latitudes,eoi400[model,:,6])

::::::::::::::
extract_data_locations_pliomip1.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations 
#   from Aislings PlioMIP1 data
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_alan.csv'
            self.latcolumn = 2
            self.loncolumn = 3
            self.outend = 'PlioMIP1output_pliovar.xls'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.outend = 'PlioMIP1output_CSCD_localities.xls'

        if self.linuxwin == 'l':
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []

        with open(self.filenamein) as csvfile:
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:
                if count != 0: # not titleline
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.fileout,lonlist, latlist, lonlist_alt
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, field, latlist, lonlist, lonlist_alt, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.lonlist_alt=lonlist_alt
        self.period = period

        if test == 'y':
            self.modelnames = ['COSMOS']
        else:
            self.modelnames = ['COSMOS', 'GISS', 'HAD', 
                               'IPSL', 'MIROC', 'MRI',
                               'NOR'
                               ]

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, allcube_,fieldreq_):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        
        ncubes = len(allcube_)
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]

      
        print(cube,'found')
        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist_alt[i])).argmin()
            
            print(self.lonlist_alt[i],self.latlist[i],latix,lonix)

            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            model_data[i] = data_slice.data
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = np.nan # if no data near set to nan

        return data_near,count_finite

    def get_fieldreq(self,model_):
        
        PeriodE280Use={
                       "COSMOS" : "Ctrl",
                       "GISS" : "Ctrl",
                       "HAD" : "ctrl",
                       "IPSL" : "ctrl",
                       "MIROC" : "ctrl",
                       "MRI" : "ctrl",
                       "NOR" : "ctrl"
                       }
        
        PeriodEoi400Use={
                         "COSMOS" : "Plio",
                         "GISS" : "Plio",
                         "HAD" : "plio",
                         "IPSL" : "plio",
                         "MIROC" : "plio",
                         "MRI" : "plio",
                         "NOR" : "plio"
                         }
        
        fieldname={
                   "COSMOS" : "SST",
                   "GISS" : "SST",
                   "HAD" : "sst",
                   "IPSL" : "sst",
                   "MIROC" : "sst",
                   "MRI" : "sst",
                   "NOR" : "sst"
                   }
         
        if self.period == 'E280':
            self.fieldreq = (model_ + '_' + 
                        PeriodE280Use.get(model_)+
                        '_' + fieldname.get(model_))
        if self.period == 'EOI400':
            self.fieldreq = (model_ + '_' + 
                        PeriodEoi400Use.get(model_)+
                        '_' + fieldname.get(model_)) 
            
            

    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
          
            filename = ('/nfs/hera1/earjcti/PLIOMIP/PlioMIP1_regridded.nc')
            self.get_fieldreq(self.modelnames[model])
            print('fieldreq is',self.fieldreq)
            
            allcube = iris.load(filename)
            
            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(allcube,self.fieldreq)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'lat')
    sheet.write(0,1,'lon')
    for model in range(0,nmodels):
        sheet.write(0,2+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,latlist[i],style)
        sheet.write(i+1,1,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,2+model,datawrite[model,i],style)

    # add multimodel mean and multimodel standard deviation
    sheet.write(0,2+nmodels,'MMM')
    sheet.write(0,3+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,2+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,3+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,4+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,4+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels])

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near)
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near)
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels]-e280_near[0:nmodels])

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance)
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance)

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg)
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Harry' # could have Harry or Erin
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
outputfile,longitudes,latitudes,longitudes_alt = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points


######################################
# setup a map and plot the points

#if linuxwin = ='l':
#    plotpoints(lonlist_alt,latlist,np.zeros(count))


##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,fieldnames,
                       latitudes,longitudes,longitudes_alt,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,fieldnames,
                       latitudes,longitudes,longitudes_alt,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt)

###################################
# plot model points
for model in range(0,len(modelnames_eoi400)):
    #plotpoints(lonlist,latlist,eoi400[model,:]-e280[model,:])

    plotpoints(longitudes_alt,latitudes,eoi400[model,:])

::::::::::::::
extract_data_locations.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#"""
#Created on Fri Jul  5 15:11:26 2019
#
#@author: earjcti
#"""
#
#   This program will obtain the SST data from the pliovar site locations and process
#
#
import csv
import sys
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import iris
import xlwt
from xlwt import Workbook
import os
import matplotlib.cm as cm
from matplotlib.colors import Normalize



############################################################################
class Getinitialdata:
# get all of the initial data, including filenames and the lons and lats where
# we require model output
    def __init__(self, linuxwin_, datafile_):
        self.linuxwin = linuxwin_
        if self.linuxwin == 'l':
            filename = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'
        else:
            filename = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'

        if datafile_ == 'Erin':
            self.filenamein = filename + 'pliovar_metadata_global_02102019.csv'
            self.latcolumn = 3
            self.loncolumn = 2
            self.sitecolumn = 1
            self.outend = 'modeloutput_pliovar.xls'
        if datafile_ == 'Harry':
            self.filenamein = filename + 'Copy_of_CSCD_localities.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'modeloutput_CSCD_localities.xls'
        if datafile_ == 'Other':
            self.filenamein = filename + 'one_locality.csv'
            self.latcolumn = 1
            self.loncolumn = 2
            self.sitecolumn = 0
            self.outend = 'test_localities.xls'

        if self.linuxwin == 'l':
            self.filestart = '/nfs/hera1/earjcti/regridded/'
            self.fileout = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/'+self.outend
        else:
            self.filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
            self.fileout = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\proxydata\\'+self.outend


    def read_file(self):
        count = 0
        latlist = []
        lonlist = []
        lonlist_alt = []
        sitelist = []

        with open(self.filenamein) as csvfile:
            readCSV = csv.reader(csvfile, delimiter=',')
            for row in readCSV:
                if count != 0: # not titleline
                    sitelist.append(row[self.sitecolumn])
                    latlist.append(np.float(row[self.latcolumn]))
                    lon = (np.float(row[self.loncolumn]))
                    lonlist_alt.append(lon)
                    if lon < 0.: # longitude goes from 0-360 in models
                        lonlist.append(lon+360.)
                    else:
                        lonlist.append(lon)
                count = count+1
        returndata = self.filestart,self.fileout,lonlist, latlist, lonlist_alt, sitelist
        return returndata

# end of class getinitdata
###############################################################################

class Getmodeldata:
    # get all of the data from the model at the required gridpoints
    def __init__(self, test, modelstart_, field, latlist, lonlist, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day"
                        }


        self.fieldnames = field
        self.latlist = latlist
        self.lonlist = lonlist
        self.modelstart = modelstart_ # the start of the filename for the model
        self.period = period

        if test == 'y':
            self.modelnames = ['NorESM1-F', 'HadCM3']
        else:
            self.modelnames = ['CCSM4', 'CCSM4-UoT',
                               'CCSM4-Utr',  
                               'CESM1.2','CESM2',
                               'COSMOS', 'EC-Earth3.3', 
                               'GISS2.1G', 'HadCM3',
                               'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                               'MIROC4m', 'MRI2.3',
                               'NorESM-L', 'NorESM1-F'
                               ]

        if period == 'E280':
            self.modelnames.append('HadISST')
            self.modelnames.append('NOAAERSST5')

        self.units = fieldunits.get(fieldnames)


        #
    def extract_model_points(self, filenameuse):
        """
        will extract the data at each point from 'filenameuse'

        calls: get_near_data
        """

        cube = iris.load_cube(filenameuse)

        cubelats = cube.coord('latitude').points
        cubelons = cube.coord('longitude').points

        model_data = np.zeros(len(self.lonlist))
        model_data_near = np.zeros(len(self.lonlist)) # values near the point
        near_distance = np.zeros(len(self.lonlist)) # how far away we have to look to get data
        ngbox_avg = np.zeros(len(self.lonlist)) # how many gridboxes we are averaging over to get data

        for i in range(0,len(self.lonlist)):
            # find nearest latitude and lontiude to the value
            latix = (np.abs(cubelats-self.latlist[i])).argmin()
            lonix = (np.abs(cubelons-self.lonlist[i])).argmin()

            print(self.latlist[i], self.lonlist[i])
            # get data from this location
            data_slice  =  cube.extract(iris.Constraint(
                        latitude = cubelats[latix],longitude = cubelons[lonix]))
            
            data_slice_data = data_slice.data
            if -100. < data_slice_data < 100.: 
                model_data[i] = data_slice_data
            else:
                model_data[i] = float('NaN')
        
            model_data_near[i] = model_data[i]

            count_near_gb = 0 # how many gridboxes away are we looking for data
            ngboxes = 1 # number of gridboxes we are averaging over when looking at 'near points'

            # while value is unknown gradually expand the region to look for near gridboxes
            while np.isnan(model_data_near[i]):
                # get nearest neighbours within 'count_near_gb' gridboxes
                count_near_gb = count_near_gb+1
                print(count_near_gb)
                neardata,ngboxes = self.get_near_data(cube,lonix,latix,cubelons,cubelats,count_near_gb)
                model_data_near[i] = neardata

            near_distance[i] = count_near_gb # how far away are we looking for data
            ngbox_avg[i] = ngboxes


        returndata = [model_data,model_data_near,near_distance,ngbox_avg]
        return returndata



    def get_near_data(self, cube, lonix, latix, cubelons, cubelats, npt):
    # if there is no data at the given gridpoint get the data near the gridpoint

        count_finite = 0
        count_nan = 0
        totdata = 0.
        nlons = len(cubelons)
        for i2 in range(lonix-npt,lonix+npt+1):
            i3 = i2
            if i2 >=  nlons:
                i3 = i2-nlons
            for j2 in range(latix-npt,latix+npt+1):
                data_slice_new = cube.extract(iris.Constraint(
                     latitude = cubelats[j2],longitude = cubelons[i3]))
                data2 = data_slice_new.data
                if np.ma.is_masked(data2):
                    count_nan = count_nan+1
                else:
                    count_finite = count_finite+1
                    totdata = totdata+data2
        if count_finite > 0:
            data_near = totdata/count_finite
        else:
            data_near = float('NaN') # if no data near set to nan

        return data_near,count_finite


    def extract_all(self):
        """
        extract points from all models for timeperiod.
        timeperiod is likely to be 'E280' or 'EOI400'

        returns
        modelnames (strarr) modelnames used for this period
        sitevals (np.arr): the values at the sites
        sitenear (np.arr): an average of the values nearest to the sites
        sitenear_dist (np.arr): how far away the values presented are
        sitenear_ngbox_avg) (np.arr): the number of gridboxes averaged where the
                      values near to the sites are used

        """

        npoints = len(self.lonlist)
        nmodels = len(self.modelnames)
        sitenear = np.zeros((nmodels, npoints)) # data near point
        sitevals = np.zeros((nmodels, npoints)) # data at point
        sitenear_dist = np.zeros((nmodels, npoints)) # how far away we have to look
        sitenear_ngbox_avg = np.zeros((nmodels, npoints)) # how many gridboxes we are averaging over


        for model in range(0, len(self.modelnames)):
            print(self.modelnames[model])
            filename = (self.modelstart + self.modelnames[model] + '/' +
                               self.period + '.SST.allmean.nc')

            # get model points and how far away they are from data
            (sitevals[model,:],sitenear[model,:],sitenear_dist[model,:],
                sitenear_ngbox_avg[model,:]) = self.extract_model_points(filename)

        return [self.modelnames,sitevals,
                sitenear,sitenear_dist,sitenear_ngbox_avg]

# end of class Getmodeldata

###############################################################################
def plotpoints(lonlist,latlist,datalist):
# plot the points we have got from the file



    fig,ax = plt.subplots()
    alllons = np.arange(-180,180,1)
    alllats = np.arange(-90,90,1)
    lons,lats = np.meshgrid(alllons,alllats)
    map = Basemap(llcrnrlon = -180.0,urcrnrlon = 180.0,llcrnrlat = -90.0,
                urcrnrlat = 90.0,projection = 'cyl',resolution = 'c')
    map.drawmapboundary
    x,y = map(lons,lats)
    map.drawcoastlines()

    valmin = np.nanmin(datalist)
    valmax = np.nanmax(datalist)

    norm  =  mpl.colors.Normalize(vmin = valmin, vmax = valmax)
    cmap  =  cm.brg


    xpts,ypts = map(lonlist,latlist)
    incr = (valmax-valmin+1.0)/10.
    V = np.arange(valmin,valmax,incr)
    cvals = (datalist-valmin)/(valmax-valmin) # scale cval onto same scale as colorbar
    coluse = cmap(cvals)
    cs  =  map.scatter(xpts,ypts,color = coluse,marker = 'o')

    sm  =  plt.cm.ScalarMappable(cmap = cmap, norm = norm)
    sm.set_array([])
    plt.colorbar(sm, ticks = V,#ticks = np.linspace(valmin,valmax,incr),
             orientation = "horizontal",extend = "both")

    #plt.show()



def write_sheet(wb, style, sheetname, modelnames, lonlist_alt, latlist, datawrite, sitename):
    sheet  =  wb.add_sheet(sheetname)
    sheet.write(0,0,'site')
    sheet.write(0,1,'lat')
    sheet.write(0,2,'lon')
    for model in range(0,nmodels):
        sheet.write(0,3+model,modelnames[model])

    for i in range(0,npoints):
        sheet.write(i+1,0,sitename[i],style)
        sheet.write(i+1,1,latlist[i],style)
        sheet.write(i+1,2,lonlist_alt[i],style)
        for model in range(0,nmodels):
            sheet.write(i+1,3+model,datawrite[model,i],style)

    sheet.write(0,3+nmodels,'MMM')
    sheet.write(0,4+nmodels,'MM-SD')
    for i in range(0,npoints):
        sheet.write(i+1,3+nmodels,np.nanmean(datawrite[0:nmodels,i]),style)
        sheet.write(i+1,4+nmodels,np.nanstd(datawrite[0:nmodels,i]),style)

    # add extra columns if we have them  this is likely to be hadisst
    if len(modelnames) > nmodels:
        print(modelnames,nmodels)
        for model in range(nmodels,len(modelnames)):
            sheet.write(0,5+model,modelnames[model])
            for i in range(0,npoints):
                sheet.write(i+1,5+model,datawrite[model,i],style)



######################################################################################
def write_to_book(fileout,lonlist,latlist,lonlist_alt, sitename):
    # write to workbook
    # calls write_sheet

    # Workbook is created
    wb  =  Workbook()

    style  =  xlwt.XFStyle()
    style.num_format_str  =  '0.00'


    # add_sheet for Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400', modelnames_eoi400, lonlist_alt, latlist, eoi400, sitename)
    write_sheet(wb,style, 'E280', modelnames_e280, lonlist_alt, latlist,e280, sitename)
    write_sheet(wb,style, 'EOI400-E280',modelnames_eoi400,lonlist_alt,latlist,
                eoi400[0:nmodels]-e280[0:nmodels], sitename)

    # add_sheet near Eoi400 E280 and difference
    write_sheet(wb,style, 'EOI400near',modelnames_eoi400,lonlist_alt,latlist,eoi400_near, sitename)
    write_sheet(wb,style, 'E280near',modelnames_e280,lonlist_alt,latlist,e280_near, sitename)
    write_sheet(wb,style, 'EOI400-E280near',modelnames_eoi400,lonlist_alt,latlist,
                eoi400_near[0:nmodels]-e280_near[0:nmodels], sitename)

    # add sheet for how far away we need to look for data
    write_sheet(wb,style, 'EOI400distance',modelnames_eoi400,lonlist_alt,latlist,eoi400_near_distance, sitename)
    write_sheet(wb,style, 'E280distance',modelnames_e280,lonlist_alt,latlist,e280_near_distance, sitename)

    # add sheet for how many gridboxes we are averaging over
    write_sheet(wb,style, 'EOI400nboxes',modelnames_eoi400,lonlist_alt,latlist,eoi400_ngbox_avg, sitename)
    write_sheet(wb,style, 'E280nboxes',modelnames_e280,lonlist_alt,latlist,e280_ngbox_avg, sitename)




    # remove output file if it exists
    exists  =  os.path.isfile(fileout)
    if exists:
        os.remove(fileout)
    wb.save(fileout)

#################################################################################
#def plot_points():
    # plot all the points from eoi400_near[model,i]-3280_near[model,i] to a map
#    for model in range(0,len(modelnames)):


#################
# MAIN PROGRAM
################

###################################
# get initial data including the lats and longs we require

linuxwin = 'l'
datafile = 'Harry' # could have Harry or Erin or Other for test file
testdata = 'n'   # yes use one model no use full range of models
fieldnames = 'SST'

indata = Getinitialdata(linuxwin,datafile)
(modelstart, outputfile, longitudes, 
 latitudes, longitudes_alt, sitenames) = indata.read_file() # get the lats lons required and the number of sites
npoints = len(longitudes) # get the number of points


######################################
# setup a map and plot the points

#if linuxwin = ='l':
#    plotpoints(lonlist_alt,latlist,np.zeros(count))


##############################
# get the SST data from IRIS cubes

#eoi400
modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'EOI400')
(modelnames_eoi400,eoi400,eoi400_near,
 eoi400_near_distance,eoi400_ngbox_avg)=modeldata.extract_all()


modeldata=Getmodeldata(testdata,modelstart,fieldnames,latitudes,longitudes,'E280')
(modelnames_e280,e280,e280_near,
 e280_near_distance,e280_ngbox_avg)=modeldata.extract_all() # extract the data from all the models

nmodels=len(modelnames_eoi400) # we also have HadISST in e280
#######################################
# write data out to a workbook
write_to_book(outputfile,longitudes,latitudes,longitudes_alt, sitenames)

###################################
# plot model points
#for model in range(0,len(modelnames_eoi400)):
    #plotpoints(lonlist,latlist,eoi400[model,:]-e280[model,:])

#    plotpoints(longitudes_alt,latitudes,eoi400[model,:])

::::::::::::::
extract_HadCM3_MOC.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 18 14:13:50 2019

#@author: earjcti
#
# This program will extract the fields needed by Zhongshi Zhang for the
# PlioMIP MOC paper.  It will extract the monthly averages from
# steve hunters processed data 
#
#
#

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.plot as iplt
import sys

#####################################
def extract_fields(filename,fieldnames,fileoutstart,filefieldnames):

    # load required cubes
    cubes=iris.load_cubes(filename,fieldnames)
    
    for i in range(0,len(fieldnames)):
        fileout=fileoutstart+filefieldnames[i]+'.nc'
        if filefieldnames[i]=='SSS':
            subcube=cubes[i]
            cube_extract= subcube.extract(iris.Constraint(z2=5))
            iris.save(cube_extract,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
        else:
            iris.save(cubes[i],fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)


def avg_MOC(fileMOC,fileout,fieldname):
    cubes=iris.load(fileMOC,fieldname)
  
    print(np.shape(cubes[0]))
    print(cubes)
    print(cubes[0].data)
    avgcube=cubes[0].data
    for i in range(1,len(cubes)):
        subcube=cubes[i]
        subcube_1d=subcube[0,:,50]
        iplt.plot(subcube_1d,color='r')
        avgcube=avgcube+cubes[i].data
        cubedata=cubes[i].data
       
    avgcube=avgcube/len(cubes)
    # put average cube in subcube area
    subcube.data=avgcube
    subcube_1d=subcube[0,:,50]
    iplt.plot(subcube_1d,color='blue')
    iris.save(subcube,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
    
    

    
   
   

##########################################################
# main program

#####################################################
# this is extracting fields from a mean file

fieldnames=['OCN TOP-LEVEL TEMPERATURE K',
            'SALINITY (OCEAN) (PSU)',
            'AICE : ICE CONCENTRATION',
            'SALINITY (OCEAN) (PSU)',]
filefieldnames=['SST','SSS','iceconc','salinity']
#filename='C:\\Users\\julia\\OneDrive\\DATA\\HadCM3_DATA\\Eoi400_2400-2499_Monthly.nc'
#fileoutstart='C:\\Users\\julia\\OneDrive\\DATA\\HadCM3_DATA\\EOI400_2400_2499_Monthly_'

filename='/nfs/a103/palaeo_share/PlioMIP2/processed/Eoi400_2400-2499_Monthly.nc'
fileoutstart='/nfs/hera1/earjcti/um/tenvj/pk2/EOI400_2400_2499_Monthly_'
extract_fields(filename,fieldnames,fileoutstart,filefieldnames)

print('here')
filename='/nfs/a103/palaeo_share/PlioMIP2/processed/Preind_E280_2900-2999_Monthly.nc'
fileoutstart='/nfs/hera1/earjcti/um/tenvo/pk2/E280_2900_2999_Monthly_'
extract_fields(filename,fieldnames,fileoutstart,filefieldnames)


####################################################
# this is averaging MOC from the MOC scripts
#Basin=['Atlantic','Indian','Pacific','Global']

#for i in range(0,len(Basin)):
#    fieldname='Meridional Overturning Stream Function ('+Basin[i]+')'

#    fileMOC='/nfs/hera1/earjcti/um/tenvo/pk2/tenvoo@pgt*c1.nc'
#    fileout='/nfs/hera1/earjcti/um/tenvo/pk2/E280_avg_'+Basin[i]+'_MOC.nc'
#    avg_MOC(fileMOC,fileout,fieldname)

#    fileMOC='/nfs/hera1/earjcti/um/tenvj/pk2/tenvjo@pgo*c1.nc'
#    fileout='/nfs/hera1/earjcti/um/tenvj/pk2/EOI400_avg_'+Basin[i]+'_MOC.nc'
#    avg_MOC(fileMOC,fileout,fieldname)

#sys.exit(0)
::::::::::::::
extract_HadCM3_PlioMIP.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on Thu Mar 18 14:13:50 2019

#@author: earjcti1
#
# This program will extract the fields needed for the PlioMIP2 database
#  It will extract the monthly averages from
# steve hunters processed data 
#
#
#

import os
import numpy as np
import scipy as sp
#import cf
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
from iris.experimental.equalise_cubes import equalise_attributes
import sys
import warnings

warnings.filterwarnings("ignore")

def simplify_cube(cube):
    """
    gets cube and makes sure dimensions are longitude, latitude surface and
    t. 
    """    
    for coord in cube.coords():
        if coord.var_name == 'level275':
            coord.var_name = 'surface'
    
    cube.coord('surface').points = 0.0
    cube.coord('surface').units = 'm'
    cube.coord('surface').attributes = None
    
    cube.data = np.where(cube.data > 1.0E10, 0., cube.data)
    return cube

def get_evap(filename_):
    """
    will add up all the fluxes that make evaporation and returns
    total evaporation within a cube
    The fluxes are:
      evaporation from canopy
      evaporation from sea
      transpiration (this is exactly the same as evaporation from soil
                     I have checked some examples)
      sublim from surface
    """

    varnames_sec = ["EVAPORATION FROM SEA (GBM)   KG/M2/S",
                "TRANSPIRATION RATE           KG/M2/S"]
    
    varnames_ts = ["EVAP FROM CANOPY - AMOUNT   KG/M2/TS",
                "SUBLIM. FROM SURFACE (GBM)  KG/M2/TS"]
    
    
    for i, var in enumerate(varnames_sec):
        cube = iris.load_cube(filename_,var)
        cube = simplify_cube(cube)
        if i == 0:
            cubetot = cube
        else:
            cubetot = cubetot + cube
        
        
    for i, var in enumerate(varnames_ts):
        cube = iris.load_cube(filename_,var)
        cube = simplify_cube(cube)
        cube.data = cube.data / (30. * 60.)
        cube.units = 'kg m-2 s-1'
        cubetot = cubetot + cube
     
    return cubetot

#####################################
def extract_fields(filestart,expt,filetype,extra,startyear,endyear,timeperiod,
                   fileoutstart,varnamein,varnameout):

    # load required cubes
    #cubes=iris.load(filename)
    #print(cubes)
    #sys.exit(0)
    monthnames=['ja','fb','mr','ar','my','jn','jl','ag','sp','ot','nv','dc']
    
    
    # loop over years
    
   
    for year in range(startyear,endyear):
        allcubes=iris.cube.CubeList([])
        stringyear=np.str(year).zfill(2)
        for mon in range(0,len(monthnames)):
            print(mon)
            filename=(filestart + expt + filetype + 
                      extra + stringyear + monthnames[mon] + '.nc')
            if varnamein == 'TOTAL EVAPORATION':
                cube = get_evap(filename)
            else:
                print(filename,varnamein)
                cube=iris.load_cube(filename,varnamein)
                cube.coord('t').points=mon+1
            allcubes.append(cube)
            if mon==11:
                print(allcubes)
            
        #make sure the metadata on all cubes are the same
        equalise_attributes(allcubes)
        catcube=allcubes.concatenate()[0]
        
        # if precipitation convert to mm/day
        if varnameout=='TotalPrecipitation':
            catcube.data=catcube.data * 60.*60.*24.
            catcube.long_name='TOTAL PRECIPITATION RATE    MM/DAY'
            catcube.units='mm/day'
       
        if varnameout=='evap':
            catcube.data=catcube.data * 60.*60.*24.
            catcube.long_name='TOTAL EVAPORATION    MM/DAY'
            catcube.units='mm/day'
      
        if varnameout=='so':
            catcube.data=(catcube.data * 1000.) + 35.
            catcube.long_name='SALMINTY (OCEAN)     (PSU)'
            catcube.units='ppt'
      
            
        # if temperature convert to Celcius
        print(varnameout)
        if varnameout=='SurfaceTemperature':
            print('convert to celcius')
            catcube.convert_units('celsius')
       
    
        stringyear=np.str(year).zfill(3)
        print(stringyear)
        fileout=fileoutstart+varnameout+'/'+timeperiod+'.'+varnameout+'.'+stringyear+'.nc'        
        iris.save(catcube,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=2.0E20)
     
    
   

##########################################################
# main program

# this is regridding where all results are in a single file
# create a dictionary with the long field names in and the field names we want
# we are also using dictionaries so that we only have to change timeperiod name
# when rerunning
            
fileextra = {
		"SURFACE TEMPERATURE AFTER TIMESTEP" : "a@pd",
        "TOTAL PRECIPITATION RATE     KG/M2/S" : "a@pd",
        "TEMPERATURE AT 1.5M": "a@pd",
        "OCN TOP-LEVEL TEMPERATURE          K" : "o@pf",
        "AICE : ICE CONCENTRATION" : "o@pf", 
                "TOTAL CLOUD AMOUNT - RANDOM OVERLAP" : "a@pd",
       
		"U COMPNT OF WIND ON PRESSURE LEVELS" : "a@pc",
		"V COMPNT OF WIND ON PRESSURE LEVELS" : "a@pc",
        	"NET DOWN SURFACE SW FLUX: SW TS ONLY" : "a@pd",
		"NET DOWN SURFACE LW RAD FLUX" : "a@pd",
        	"SURFACE & B.LAYER HEAT FLUXES   W/M2" : "a@pd",
		"SURFACE LATENT HEAT FLUX        W/M2" : "a@pd",
        "OMEGA ON PRESSURE LEVELS" : "a@pc",
		"SPECIF HUM;P LEVS;U GRID.  USE MACRO" : "a@pc",
		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS" : "a@pc",
		"TEMPERATURE ON PRESSURE LEVELS" : "a@pc",
		"SURFACE LATENT HEAT FLUX        W/M2" : "a@pd",
        "DOWNWARD LW RAD FLUX: SURFACE" : "a@pd",
        "TOTAL DOWNWARD SURFACE SW FLUX" : "a@pd",
        "INCOMING SW RAD FLUX (TOA): ALL TSS" : "a@pd",
        "OUTGOING SW RAD FLUX (TOA)" : "a@pd",
        "OUTGOING LW RAD FLUX (TOA)" : "a@pd",
        "SURFACE & B.LAYER HEAT FLUXES   W/M2" : "a@pd",
		"PRESSURE AT MEAN SEA LEVEL" : "a@pd",
		"PSTAR AFTER TIMESTEP" : "a@pd",
		"TOTAL EVAPORATION" : "a@pd",
		"X-COMP OF SURF & BL WIND STRESS N/M2" : "a@pd",
                "Y-COMP OF SURF & BL WIND STRESS N/M2" : "a@pd",
		"TOTAL OCEAN U-VELOCITY      CM S**-1" : "o@pf",
		"TOTAL OCEAN V-VELOCITY      CM S**-1" : "o@pf",
		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S" : "o@pf",
                "POTENTIAL TEMPERATURE (OCEAN)  DEG.C" : "o@pf",
                "SALINITY (OCEAN)       (PSU-35)/1000" : "o@pf"
	}

shortname = {
		"SURFACE TEMPERATURE AFTER TIMESTEP" : "SurfaceTemperature",
        "TOTAL PRECIPITATION RATE     KG/M2/S" : "TotalPrecipitation",
        "TEMPERATURE AT 1.5M": "NearSurfaceTemperature",
        "TOTAL CLOUD AMOUNT - RANDOM OVERLAP" : "totcloud",
        "OCN TOP-LEVEL TEMPERATURE          K" : "SST",
        "AICE : ICE CONCENTRATION" : "SeaIceConc", 
		"U COMPNT OF WIND ON PRESSURE LEVELS" : "ua",
		"V COMPNT OF WIND ON PRESSURE LEVELS" : "va",
        	"NET DOWN SURFACE SW FLUX: SW TS ONLY" : "fsns",
		"NET DOWN SURFACE LW RAD FLUX" : "flns",
        "OMEGA ON PRESSURE LEVELS" : "wap",
		"SPECIF HUM;P LEVS;U GRID.  USE MACRO" : "spechumid",
		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS" : "zg",
		"TEMPERATURE ON PRESSURE LEVELS" : "ta",
		"SURFACE LATENT HEAT FLUX        W/M2" : "hfls",
        "DOWNWARD LW RAD FLUX: SURFACE" : "rlds",
        "TOTAL DOWNWARD SURFACE SW FLUX" : "rsds",
        "INCOMING SW RAD FLUX (TOA): ALL TSS" : "rsdt",
        "OUTGOING SW RAD FLUX (TOA)" : "rsut",
        "OUTGOING LW RAD FLUX (TOA)" : "rlut",
        "SURFACE & B.LAYER HEAT FLUXES   W/M2" : "surfheatflux",
		"PRESSURE AT MEAN SEA LEVEL" : "MSLP",
		"PSTAR AFTER TIMESTEP" : "ps",
		"TOTAL EVAPORATION" : "evap",
		"X-COMP OF SURF & BL WIND STRESS N/M2" : "tauu",
                "Y-COMP OF SURF & BL WIND STRESS N/M2" : "tauv",
		"TOTAL OCEAN U-VELOCITY      CM S**-1" : "uo",
		"TOTAL OCEAN V-VELOCITY      CM S**-1" : "vo",
		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S" : "wo",
                "POTENTIAL TEMPERATURE (OCEAN)  DEG.C" : "thetao",
                "SALINITY (OCEAN)       (PSU-35)/1000" : "so"
	}


exptname = {
        "e280" : "tenvo",
        "e400" : "tenvq",
        "e560":"tenvs",
        "eoi400" : "tenvj",
        "eoi350" : "tenvk",
        "eoi450" : "tenvl",
        "eoi280" : "tenvm"
      
}

extraname = {
        "e280" : "t",
        "e400" : "t",
        "eoi400" : "o",
        "eoi350" : "o",
        "eoi450" : "o",
        "eoi280" : "o",
        "e560" : "v"}

#fieldname=["Y-COMP OF SURF & BL WIND STRESS N/M2","X-COMP OF SURF & BL WIND STRESS N/M2" 
#           ]
fieldname = [#"SURFACE TEMPERATURE AFTER TIMESTEP",
         #"TOTAL PRECIPITATION RATE     KG/M2/S",
        #"TEMPERATURE AT 1.5M",
        #"OCN TOP-LEVEL TEMPERATURE          K",
        # "AICE : ICE CONCENTRATION", 
        #  "TOTAL CLOUD AMOUNT - RANDOM OVERLAP"
       
#		"U COMPNT OF WIND ON PRESSURE LEVELS",
#		"V COMPNT OF WIND ON PRESSURE LEVELS",
#        	"NET DOWN SURFACE SW FLUX: SW TS ONLY",
#		"NET DOWN SURFACE LW RAD FLUX",
#        	"SURFACE & B.LAYER HEAT FLUXES   W/M2",
#		"SURFACE LATENT HEAT FLUX        W/M2",
#        "OMEGA ON PRESSURE LEVELS",
#		"SPECIF HUM;P LEVS;U GRID.  USE MACRO",
#		"GEOPOTENTIAL HEIGHT: PRESSURE LEVELS",
#		"TEMPERATURE ON PRESSURE LEVELS",
	#	"SURFACE LATENT HEAT FLUX        W/M2",
#        "DOWNWARD LW RAD FLUX: SURFACE",
#        "TOTAL DOWNWARD SURFACE SW FLUX",
#        "INCOMING SW RAD FLUX (TOA): ALL TSS",
#        "OUTGOING SW RAD FLUX (TOA)",
#        "OUTGOING LW RAD FLUX (TOA)",
#        "SURFACE & B.LAYER HEAT FLUXES   W/M2",
#		"PRESSURE AT MEAN SEA LEVEL",
#		"PSTAR AFTER TIMESTEP",
#		"TOTAL EVAPORATION",
#		"X-COMP OF SURF & BL WIND STRESS N/M2",
 #               "Y-COMP OF SURF & BL WIND STRESS N/M2",
#		"TOTAL OCEAN U-VELOCITY      CM S**-1",
#		"TOTAL OCEAN V-VELOCITY      CM S**-1",
#		"VERT.VEL. ON OCEAN HALF LEVELS  CM/S"
#           "POTENTIAL TEMPERATURE (OCEAN)  DEG.C",
             "SALINITY (OCEAN)       (PSU-35)/1000" 
	]
	       
#fieldname=["V COMPNT OF WIND ON PRESSURE LEVELS"]

linux_win='l'
startyear=0
endyear=100
timeperiod='e280'
expt=exptname.get(timeperiod)
extra=extraname.get(timeperiod)


if linux_win=='w':
    filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\HadCM3\\'+exptname.get(timeperiod)+'/'
    fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\HadCM3_UPLOAD\\'+timeperiod+'/'
else:
    filestart='/nfs/hera1/earjcti/um/'+expt+'/pf/'
    fileoutstart='/nfs/hera1/earjcti/PLIOMIP2/LEEDS/HadCM3/'+timeperiod+'/'

for i in range(0,len(fieldname)):
    varnamein=fieldname[i]
    varnameout=shortname.get(varnamein)
    filetype=fileextra.get(varnamein)

    extract_fields(filestart,expt,filetype,extra,startyear,endyear,timeperiod,fileoutstart,varnamein,varnameout)

#sys.exit(0)
::::::::::::::
extract_ipcc_data.py
::::::::::::::
# -*- coding: utf-8 -*-
"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with all the data that we had in the supplementary and
have asked for it extracted as spreadsheet.  This program will do this. 

This is superceeded by calculate_land_sea_contrast.py

@author: julia
"""


import pandas as pd
import numpy as np
import sys

def get_global_temperature_anomaly():
    """
    
    extract model name and global_tanom from data_for_fig1a
    
    """
    
    data = [['CESM2',19.31,0.21,14.14,0.2],
            ['IPSLCM6A',16.02,0.23,12.54,0.25],
            ['COSMOS',16.83,0.48,13.5,0.46],
            ['EC-Earth3.3',18.17,0.15,13.33,0.19],
            ['CESM1.2',17.33,0.17,13.3,0.21],
            ['IPSLCM5A',14.38,0.22,12.09,0.27],
            ['MIROC4m',15.91,0.18,12.77,0.22],
            ['IPSLCM5A2',15.33,0.27,13.16,0.34],
            ['HadCM3',16.95,0.2,14.06,0.21],
            ['GISS2.1G',15.91,0.31,13.78,0.27],
            ['CCSM4',15.98,0.15,13.37,0.19],
            ['CCSM4-Utr',18.52,0.13,13.78,0.27],
            ['CCSM4-UoT',16.8,0.13,13.01,0.21],
            ['NorESM-L',14.57,0.13,12.48,0.14],
            ['MRI2.3',15.12,0.26,12.67,0.2],
            ['NorESM1-F',16.22,0.15,14.49,0.14]]
    
    modelnames = ['Simulated Temperature Changes', 'Fraction of Global Area']
    global_tanom = ['GLOBAL', 1.0]
    
    for row in data:
        modelnames.append(row[0])
        plio_temp = row[1]
        pi_temp = row[3]
        temp_anom = plio_temp - pi_temp
        global_tanom.append(temp_anom)
        
    
    return [modelnames, global_tanom]

def get_global_land_sea():
    """
    
    extract model name and global_tanom from data_for_fig3b
    
    """
    
    data = [['CESM2',6.58,4.69,4.23,3.52],
            ['IPSLCM6A',4.61,3.14,2.72,2.21],
            ['COSMOS',4.97,2.74,3.33,2.35],
            ['EC-Earth3.3',6.64,4.21,3.9,2.96],
            ['CESM1.2',5.09,3.72,2.89,2.46],
            ['IPSLCM5A',3.37,1.96,2.17,1.62],
            ['MIROC4m',4.63,2.6,2.95,2.14],
            ['IPSLCM5A2',3.15,1.87,1.99,1.51],
            ['HadCM3',4.47,2.34,2.97,1.69],
            ['GISS2.1G',2.57,2.08,1.46,1.09],
            ['CCSM4',3.51,2.36,1.64,1.42],
            ['CCSM4-Utr',5.6,4.5,2.9,2.67],
            ['CCSM4-UoT',4.77,3.51,2.32,2.13],
            ['NorESM-L',2.64,2.0,0.95,1.08],
            ['MRI2.3',3.71,2.04,2.04,1.42],
            ['NorESM1-F',2.52,1.52,1.13,1.07]]
    
    modelnames = ['Simulated Temperature Changes', 'Fraction of Global Area']
    global_landanom = ['GLOBAL (over land)', 1.0]
    global_seaanom = ['GLOBAL (over ocean)', 1.0]
    
    for row in data:
        modelnames.append(row[0])
        global_landanom.append(row[1])
        global_seaanom.append(row[2])
        
        
    land_dict = {} # set up dictionaries for new dataframe row
    sea_dict = {}
    for i, model in enumerate(modelnames):
        land_dict[model] = global_landanom[i]
        sea_dict[model] = global_seaanom[i]
    
        
    return [land_dict, sea_dict]


def get_latitude_bands_global(glob_l_s_ind):
    """
    data from data-for_supp_2
    """
    
    if glob_l_s_ind == 'g':
        data = [['CESM2',10.78,4.57,3.74,3.8,5.62,10.52],
                ['IPSLCM6A',8.68,2.87,2.14,2.5,3.79,7.73],
                ['COSMOS',6.87,2.01,2.45,2.76,3.88,7.26],
                ['EC-Earth3.3',8.45,3.37,3.07,3.67,6.64,11.36],
                ['CESM1.2',9.32,3.67,2.58,2.68,4.19,9.76],
                ['IPSLCM5A',4.11,1.29,1.5,2.03,3.04,5.18],
                ['MIROC4m',6.41,2.05,2.3,2.48,3.59,7.07],
                ['IPSLCM5A2',4.34,1.41,1.49,1.79,2.59,4.89],
                ['HadCM3',6.55,1.85,1.83,2.36,3.92,5.22],
                ['GISS2.1G',7.32,2.38,1.26,1.2,1.79,3.93],
                ['CCSM4',6.28,2.57,1.46,1.56,2.85,6.6],
                ['CCSM4-Utr',12.89,4.56,2.82,2.71,5.25,10.45],
                ['CCSM4-UoT',8.97,3.6,2.06,2.31,4.2,9.96],
                ['NorESM-L',7.59,2.26,1.11,1.06,1.66,4.82],
                ['MRI2.3',5.39,1.05,1.47,1.84,3.17,7.37],
                ['NorESM1-F',3.18,1.58,1.12,1.1,1.87,5.02]]
        titleend = ''
        frac = [0.067, 0.183, 0.25, 0.25, 0.183, 0.067]
        
    if glob_l_s_ind == 'l':
        data = [['CESM2',6.7,4.61,4.4,4.47,6.1,10.17],
                ['IPSLCM6A',4.55,2.61,2.6,3.01,4.03,7.34],
                ['COSMOS',5.82,2.11,3.9,3.31,3.92,7.32],
                ['EC-Earth3.3',5.24,3.92,4.13,4.68,7.14,10.7],
                ['CESM1.2',4.77,3.24,3.03,3.16,4.39,8.95],
                ['IPSLCM5A',1.63,1.61,1.98,2.5,2.97,4.97],
                ['MIROC4m',4.1,2.37,3.43,3.02,3.95,7.18],
                ['IPSLCM5A2',1.51,1.74,1.95,2.2,2.55,4.72],
                ['HadCM3',3.43,2.51,3.25,3.07,4.4,5.22],
                ['GISS2.1G',4.19,2.19,1.64,1.6,1.43,3.17],
                ['CCSM4',2.69,2.04,1.71,1.96,2.94,6.36],
                ['CCSM4-Utr',7.68,3.72,3.2,2.86,5.3,9.61],
                ['CCSM4-UoT',3.48,2.74,2.3,2.95,4.59,9.02],
                ['NorESM-L',5.31,1.71,1.14,0.94,1.71,4.67],
                ['MRI2.3',2.94,1.92,2.21,2.23,2.82,6.63],
                ['NorESM1-F',0.36,1.56,1.31,1.2,1.83,5.03]]
        titleend = ' (over land)'
        frac = [0.118, 0.312, 0.247, 0.196, 0.037, 0.09]
    
    if glob_l_s_ind == 's':
        data = [['CESM2',9.23,4.55,3.53,3.53,5.35,11.17],
                ['IPSLCM6A',6.91,2.87,2.02,2.33,3.87,8.69],
                ['COSMOS',3.85,1.98,2.01,2.55,4.04,7.36],
                ['EC-Earth3.3',6.53,3.31,2.77,3.27,6.41,12.45],
                ['CESM1.2',7.89,3.69,2.46,2.51,4.27,11.27],
                ['IPSLCM5A',1.78,1.25,1.36,1.86,3.37,5.09],
                ['MIROC4m',3.92,2.01,1.96,2.27,3.44,6.97],
                ['IPSLCM5A2',2.24,1.36,1.35,1.64,2.89,4.82],
                ['HadCM3',4.45,1.79,1.4,2.07,3.63,5.36],
                ['GISS2.1G',5.1,2.37,1.15,1.04,2.41,4.98],
                ['CCSM4',5.03,2.59,1.41,1.43,3.03,6.86],
                ['CCSM4-Utr',12.35,4.6,2.72,2.68,5.44,11.26],
                ['CCSM4-UoT',8.51,3.64,2.02,2.09,4.08,11.16],
                ['NorESM-L',5.25,2.28,1.11,1.15,1.91,4.96],
                ['MRI2.3',2.77,0.97,1.26,1.69,3.73,7.45],
                ['NorESM1-F',1.31,1.57,1.08,1.08,2.18,5.19]]
        titleend = ' (over sea)'
        frac = [0.045, 0.129, 0.251, 0.273, 0.244, 0.057]
    
    modelnames = ['Simulated Temperature Changes','Fraction of Global Area']
    l60S_90S = ['90-60oS ' + titleend, frac[5]]
    l30S_60S = ['60-30oS ' + titleend, frac[4]]
    l0_30S = ['30oS-0 ' + titleend, frac[3]]
    l30N_0N = ['0-30oN ' + titleend, frac[2]]
    l60N_30N = ['30-60oN ' + titleend, frac[1]]
    l90N_60N = ['60-90oN' + titleend, frac[0]]
    
    for row in data:
        modelnames.append(row[0])
        l60S_90S.append(row[1])
        l30S_60S.append(row[2])
        l0_30S.append(row[3])
        l30N_0N.append(row[4])
        l60N_30N.append(row[5])
        l90N_60N.append(row[6])
    
    dict_60S90S = {} # set up dictionaries for new dataframe row
    dict_30S60S = {}
    dict_0S30S = {}
    dict_30N0 = {}
    dict_60N30N = {}
    dict_90N60N = {}
    
    for i, model in enumerate(modelnames):
        print(l60S_90S[i], model, i)
        dict_60S90S[model] = l60S_90S[i]
        dict_30S60S[model] = l30S_60S[i]
        dict_0S30S[model] = l0_30S[i]
        dict_30N0[model] = l30N_0N[i]
        dict_60N30N[model] = l60N_30N[i]
        dict_90N60N[model] = l90N_60N[i]
    
        
    return [dict_60S90S, dict_30S60S, dict_0S30S, dict_30N0,
            dict_60N30N, dict_90N60N]

    
################################
modelnames, global_tanom = get_global_temperature_anomaly()
land_dfrow, sea_dfrow =  get_global_land_sea()
[glob_60S90S, glob_30S60S, glob_0S30S, glob_30N0,
            glob_60N30N, glob_90N60N] = get_latitude_bands_global('g')
[land_60S90S, land_30S60S, land_0S30S, land_30N0,
            land_60N30N, land_90N60N] = get_latitude_bands_global('l')
[sea_60S90S, sea_30S60S, sea_0S30S, sea_30N0,
            sea_60N30N, sea_90N60N] = get_latitude_bands_global('s')

#print(len(modelnames))
#print(len(global_tanom))
#print(modelnames)
#print(global_tanom)
print(land_dfrow)

# create the dataframe
df = pd.DataFrame([global_tanom], columns = modelnames, dtype=float)
df = df.append([land_dfrow], ignore_index=True)
df = df.append(sea_dfrow, ignore_index=True)
df = df.append(glob_90N60N,ignore_index= True)
df = df.append(land_90N60N,ignore_index= True)
df = df.append(sea_90N60N,ignore_index= True)
df = df.append(glob_60N30N, ignore_index= True)
df = df.append(land_60N30N, ignore_index= True)
df = df.append(sea_60N30N, ignore_index= True)
df = df.append(glob_30N0,ignore_index= True)
df = df.append(land_30N0,ignore_index= True)
df = df.append(sea_30N0,ignore_index= True)
df = df.append(glob_0S30S, ignore_index= True)
df = df.append(land_0S30S, ignore_index= True)
df = df.append(sea_0S30S, ignore_index= True)
df = df.append(glob_30S60S, ignore_index= True)
df = df.append(land_30S60S, ignore_index= True)
df = df.append(sea_30S60S, ignore_index= True)
df = df.append(glob_60S90S, ignore_index= True)
df = df.append(land_60S90S, ignore_index= True)
df = df.append(sea_60S90S, ignore_index= True)



# save dataframe as a csv file
df.to_csv('C:/Users/julia/OneDrive/WORK/MY_PAPERS/PlioMIP2/IPCC_box/' + 
          'mPWP_CMIP6_land_sea_by_latitude_jct.csv')
    
    


::::::::::::::
get_means_from_pliomip1.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Fri Jul  5 15:11:26 2019
@author: earjcti

 This program will get the means from the PlioMIP1 models that
 can be added to the pliomip2 figures

#
 """

#import os
import warnings
import sys
#from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
#import matplotlib as mpl
#import matplotlib.cm as cm
#from matplotlib.colors import Normalize
import numpy as np
import iris
#import xlwt
#from xlwt import Workbook

warnings.filterwarnings("ignore")


###############################################################################

class Getmodeldata:
    """
    get all of the data from the model with the required averaing
    ie e280 or eoi400
    """
    def __init__(self, field, period):

        fieldunits = {
            "SST" : "degC",
            "TotalPrecipitation" : "mm/day",
            "NearSurfaceTemperature" : "degC"
                        }


        self.fieldname = field
        self.period = period
        if period == 'E280':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Ctrl_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Ctrl_landmasks_p3grid.nc'
        if period == 'EOI400':
            if LINUX_WIN == 'l':
                self.lsm_file = ('/nfs/a103/eardjh/Datasets/PlioMIP/' +
                                 'data/Exp2_files/lsm/PlioMIP_Plio_landmasks_p3grid.nc')
            else:
                self.lsm_file = FILESTART + '/landmasks/PlioMIP_Plio_landmasks_p3grid.nc'


        self.units = fieldunits.get(field)



    def get_fieldreq(self, model_):
        """
        All fields are in a single file.  This will get the fieldname
        required via a number of dictionaries
        """


        PeriodE280Use = {
            "CCSM" : "ctrl", "COSMOS" : "Ctrl", "GISS" : "Ctrl",
            "HAD" : "ctrl", "IPSL" : "ctrl", "MIROC" : "ctrl",
            "MRI" : "ctrl", "NOR" : "ctrl"
                        }

        PeriodEoi400Use = {
            "CCSM" : "plio", "COSMOS" : "plio", "GISS" : "Plio",
            "HAD" : "plio", "IPSL" : "plio", "MIROC" : "plio",
            "MRI" : "plio", "NOR" : "plio"
                          }

        fieldname_sst = {
            "CCSM" : "sst", "COSMOS" : "SST", "GISS" : "SST",
            "HAD" : "sst", "IPSL" : "sst", "MIROC" : "sst",
            "MRI" : "sst", "NOR" : "sst"
                        }

        fieldname_sat = {
            "CCSM" : "sat", "COSMOS" : "sat", "GISS" : "SAT",
            "HAD" : "SAT", "IPSL" : "sat", "MIROC" : "sat",
            "MRI" : "sat", "NOR" : "sat"
                        }


        if FIELDNAME == 'NearSurfaceTemperature':
            fielduse = fieldname_sat.get(model_)
        if FIELDNAME == 'TotalPrecipitation':
            fielduse = 'precip'

        if self.period == 'E280':
            fieldreq = (model_ + '_' +
                             PeriodE280Use.get(model_) + '_' + fielduse)
        if self.period == 'EOI400':
            fieldreq = (model_ + '_' +
                             PeriodEoi400Use.get(model_) + '_' + fielduse)


        return fieldreq

    def extract_cube(self, allcube_, fieldreq_):
        """
        will extract the cube from the list of cubes
        this is needed because the cube comes from varname not long name
        """
        ncubes = len(allcube_)

        # look for the cube
        cube_found = 'n'
        for i in range(0, ncubes):
            if allcube_[i].var_name == fieldreq_:
                cube = allcube_[i]
                cube_found = 'y'

        # if cube not found then change the capitalisation of the first
        # letter of the time period (ie plio ==> Plio,  Plio ==> plio)
        if cube_found == 'n':
            newstring = ' '
            if fieldreq_.find("Plio") > 0:
                newstring = fieldreq_.replace("Plio", "plio")
            if fieldreq_.find("plio") > 0:
                newstring = fieldreq_.replace("plio", "Plio")
            if fieldreq_.find("ctrl") > 0:
                newstring = fieldreq_.replace("ctrl", "Ctrl")
            if fieldreq_.find("Ctrl") > 0:
                newstring = fieldreq_.replace("Ctrl", "ctrl")
            for i in range(0, ncubes):
                if allcube_[i].var_name == newstring:
                    cube = allcube_[i]
                    cube_found = 'y'

        cube = iris.util.squeeze(cube)

        return cube

    def get_lsm(self, modname, datacube):
        """
        gets the lsm from the lsm file
        """

        alllsm = iris.load(self.lsm_file)
        ncubes = len(alllsm)
        fieldreq = modname + '_landmask'
        cube_found = 'n'

        time = {'E280': 'Ctrl',
                'EOI400' : 'Plio'}
        for i in range(0, ncubes):
            if alllsm[i].var_name == fieldreq:
                cube = alllsm[i]
                cube = iris.util.squeeze(cube)
                cube_found = 'y'

        # if cube not found then add the time period before the landseamask
        if cube_found == 'n':
            newstring = fieldreq.replace("landmask",
                                         time.get(self.period) + "_landmask")
            for i in range(0, ncubes):
                if alllsm[i].var_name == newstring:
                    cube = alllsm[i]
                    cube_found = 'y'

        if cube_found == 'n': #cube still not found then error
            print('cannot find lsm ' + fieldreq + ' or ' + newstring)
            sys.exit(0)

        # check coords of lsm cube are the same as the
        # coordinates of the data cube
        if np.array_equal(datacube.coord('longitude').points,
                          cube.coord('longitude').points):
            pass

        else:
            print('longitudes dont match')
            sys.exit(0)

        if np.array_equal(datacube.coord('latitude').points,
                          cube.coord('latitude').points):
            pass
        else:
            print('latitudes dont match')
            sys.exit(0)


        return np.squeeze(cube.data)

    def get_globalmean(self, cube):
        """
        calculates the area weighted global mean from the cube given
        """
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)

        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas)
        if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 270.:
            meancube.data = meancube.data - 273.15

        return meancube.data, grid_areas

    def get_latmean(self, cube, grid_areas, nbands):
        """
        calculates the area weighted mean in latitude bounds
        from the cube given
        """

        model_latbands = np.zeros((nbands))
        for boundno, thisbound in enumerate(LATBANDS):
            grid_areas_thisbound = np.zeros((np.shape(grid_areas)))
            for j, lat in enumerate(cube.coord('latitude').points):
                if thisbound[0] <= lat < thisbound[1]:
                    if cube.ndim == 3:
                        grid_areas_thisbound[:, j, :] = grid_areas[:, j, :]
                    if cube.ndim == 4:
                        grid_areas_thisbound[:, :, j, :] = grid_areas[:, :, j, :]
                    if cube.ndim == 2:
                        grid_areas_thisbound[j, :] = grid_areas[j, :]

            meancube = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_thisbound)

            if FIELDNAME == 'NearSurfaceTemperature' and meancube.data > 250.:
                meancube.data = meancube.data - 273.15

            model_latbands[boundno] = meancube.data


        return model_latbands

    def get_land_sea_means(self, cube, grid_areas, modname):
        """
        extract the land and the sea anomaly
        """

        # get lsm
        lsm = self.get_lsm(modname, cube)


        # get grid_areas (_20 means from 20N-20S)
        grid_areas_land = grid_areas * lsm
        grid_areas_sea = grid_areas - grid_areas_land
        grid_areas_land_20 = np.zeros(np.shape(grid_areas))
        grid_areas_sea_20 = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if -20 <= lat <= 20:
                grid_areas_land_20[j, :] = grid_areas_land[j, :]
                grid_areas_sea_20[j, :] = grid_areas_sea[j, :]

        mean_land = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_land)
        mean_sea = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_sea)
        mean_land_20 = cube.collapsed(['longitude', 'latitude'],
                                      iris.analysis.MEAN,
                                      weights=grid_areas_land_20)
        mean_sea_20 = cube.collapsed(['longitude', 'latitude'],
                                     iris.analysis.MEAN,
                                     weights=grid_areas_sea_20)

        if FIELDNAME == 'NearSurfaceTemperature' and mean_land.data > 250.:
            mean_land.data = mean_land.data - 273.15
            mean_sea.data = mean_sea.data - 273.15
            mean_land_20.data = mean_land_20.data - 273.15
            mean_sea_20.data = mean_sea_20.data - 273.15

        return (mean_land.data, mean_sea.data, mean_land_20.data,
                mean_sea_20.data)

    def get_highlatmeans(self, cube, grid_areas, latval):
        """
        gets the means polewards of latval(north) and polewards of latval(S)
        """
        grid_areas_NH = np.zeros(np.shape(grid_areas))
        grid_areas_SH = np.zeros(np.shape(grid_areas))
        for j, lat in enumerate(cube.coord('latitude').points):
            if lat >= latval:
               grid_areas_NH[j, :] = grid_areas[j, :]
            if lat <= (-1.0) * latval:
                grid_areas_SH[j, :] = grid_areas[j, :]
                
        mean_NH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_NH)
        mean_SH = cube.collapsed(['longitude', 'latitude'],
                                   iris.analysis.MEAN,
                                   weights=grid_areas_SH)

        return (mean_NH.data, mean_SH.data)

    def extract_means(self):
        """
        the top level function for this class.  This will return all the
        means to the main program
        """
        # arrays to store data
        meanvals = np.zeros(len(MODELNAMES))
        meanvals_land = np.zeros(len(MODELNAMES))
        meanvals_sea = np.zeros(len(MODELNAMES))
        meanvals_land_20 = np.zeros(len(MODELNAMES))
        meanvals_sea_20 = np.zeros(len(MODELNAMES))
        meanvals_NH45 = np.zeros(len(MODELNAMES))
        meanvals_SH45 = np.zeros(len(MODELNAMES))
        meanvals_NH60 = np.zeros(len(MODELNAMES))
        meanvals_SH60 = np.zeros(len(MODELNAMES))

        nbands, nlims = np.shape(LATBANDS)
        latmeans = np.zeros((len(MODELNAMES), nbands))

        filename = (FILESTART + 'PlioMIP1_regridded.nc')
        allcubes = iris.load(filename)

        for i, model in enumerate(MODELNAMES):


            fieldreq = self.get_fieldreq(model)

            # extract the cube we want
            cube = self.extract_cube(allcubes, fieldreq)
            meanvals[i], grid_areas = self.get_globalmean(cube)
            latmeans[i, :] = self.get_latmean(cube, grid_areas, nbands)

            (meanvals_land[i],
             meanvals_sea[i],
             meanvals_land_20[i],
             meanvals_sea_20[i]) = self.get_land_sea_means(cube, grid_areas, model)
            
            (meanvals_NH45[i], 
             meanvals_SH45[i]) = self.get_highlatmeans(cube,grid_areas, 45.0)
            
            (meanvals_NH60[i], 
             meanvals_SH60[i]) = self.get_highlatmeans(cube,grid_areas, 60.0)


        return (meanvals, latmeans, meanvals_land,
                meanvals_sea, meanvals_land_20, meanvals_sea_20,
                meanvals_NH45, meanvals_SH45,
                meanvals_NH60, meanvals_SH60)




# end of class Getmodeldata
##################################################################################
def get_nh_seascyc():
    """
    get the NH seasonal cycle from each model
    """


    shortfield = {"NearSurfaceTemperature" : "SAT",
                  "TotalPrecipitation" : "precip"}
    longfield_sat = {"CCSM" : "Reference height temperature",
                     "COSMOS" : "2m temperature",
                     "IPSL" : "t2m",
                     "MIROC": "tas",
                     "MRI" : "near surface air temperature [degC]"
                     }

    longfield_precip = {
        "CCSM" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "COSMOS" : "total precipitation",
        "IPSL" : "IPSL_precip",
        "MIROC": "MIROC_precip",
        "MRI" : "total precipitation [mm/day]"
                       }
    nh_means = np.zeros((len(MODELNAMES), 12))

    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + '/' + FIELDNAME + '/' +
                    model + '_Exp2_anom_' +
                    shortfield.get(FIELDNAME) + '_p3grid.nc')

        if FIELDNAME == "NearSurfaceTemperature":
            fieldreq = longfield_sat.get(model, "air_temperature")
        if FIELDNAME == "TotalPrecipitation":
            fieldreq = longfield_precip.get(model, "precipitation_flux")

        cube = iris.load_cube(filename, fieldreq)
        cube = iris.util.squeeze(cube)

        # get grid areas for seasonal average
        cube.coord('latitude').guess_bounds()
        cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)
        grid_areas_nh = np.zeros(np.shape(grid_areas))

        for j, lat in enumerate(cube.coord('latitude').points):
            if lat > 0:
                grid_areas_nh[:, j, :] = grid_areas[:, j, :]

        # get seasonal average
        meancube = cube.collapsed(['longitude', 'latitude'],
                                  iris.analysis.MEAN,
                                  weights=grid_areas_nh)

        nh_means[i, :] = meancube.data
        plt.plot(meancube.data, label=model)
    plt.plot(np.mean(nh_means, axis=0), label='mean')
    plt.legend()
    #plt.show()

    return nh_means


def write_global_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write global means from each model to a text file

    """

    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)

    filetext.write("modelname, global mean EOI400," +
                   "global mean E280, anomaly \n")

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(np.mean(modelmean_eoi400), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_e280), 3)) +
                   ',' +
                   np.str(np.around(np.mean(modelmean_anomaly), 3)) + '\n')



def write_lat_means(filetext, modelmean_eoi400, modelmean_e280):
    """
    write the latitudinal mean from each model to a textfile
    """


    modelmean_anomaly = (modelmean_eoi400 -modelmean_e280)
    mean_eoi400 = np.mean(modelmean_eoi400, axis=0)
    mean_e280 = np.mean(modelmean_e280, axis=0)
    mean_anomaly = mean_eoi400 - mean_e280

    filetext.write("modelname, latband mean EOI400," +
                   "latband mean E280, latband anomaly \n")
    filetext.write("bands are " + np.str(LATBANDS) + '\n')

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400 = np.str(np.around(modelmean_eoi400[i], 3))
        e280 = np.str(np.around(modelmean_e280[i], 3))
        anom = np.str(np.around(modelmean_anomaly[i], 3))

        filetext.write((model + ',' + eoi400 + ',' + e280 +
                        ',' + anom + '\n'))
    filetext.write('MEAN,' +
                   np.str(np.around(mean_eoi400, 3)) + ',' +
                   np.str(np.around(mean_e280, 3)) +',' +
                   np.str(np.around(mean_anomaly, 3)) + '\n')

def write_nh_seascycle(filetext, seasanom):
    """
    write the NH seasonal anomaly from each model to a textfile
    """


    filetext.write("modelname, [jan feb mar apr may jun jul aug sep oct nov dec] \n")
    mean_anomaly = np.mean(seasanom, axis=0)

    for i, model in enumerate(MODELNAMES_FULL):
        anom = np.str(np.around(seasanom[i], 3))

        filetext.write((model + ',' + anom + '\n'))

    filetext.write('MEAN,' +  np.str(np.around(mean_anomaly, 3)) + '\n')

def write_landsea_means(filetext, landmean_eoi400_allmodels,
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, region):
    """
    write the land sea averages to a text file
    """
    filetext.write("modelname" + region + "[mean_ocean_eoi400, meanocean_e280, meanocean_anom, " +
                   "mean_land_eoi400, mean_land_e280, mean_land_anom ] \n")

    eoi400_sea_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels), 3))
    e280_sea_mean = np.str(np.around(np.mean(seamean_e280_allmodels), 3))
    eoi400_land_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels), 3))
    e280_land_mean = np.str(np.around(np.mean(landmean_e280_allmodels), 3))
    sea_anom_mean = np.str(np.around(np.mean(seamean_eoi400_allmodels)
                                     - np.mean(seamean_e280_allmodels), 3))
    land_anom_mean = np.str(np.around(np.mean(landmean_eoi400_allmodels)
                                      - np.mean(landmean_e280_allmodels), 3))

    for i, model in enumerate(MODELNAMES_FULL):
        eoi400_sea = np.str(np.around(seamean_eoi400_allmodels[i], 3))
        e280_sea = np.str(np.around(seamean_e280_allmodels[i], 3))
        eoi400_land = np.str(np.around(landmean_eoi400_allmodels[i], 3))
        e280_land = np.str(np.around(landmean_e280_allmodels[i], 3))
        sea_anom = np.str(np.around((seamean_eoi400_allmodels[i]
                                     - seamean_e280_allmodels[i]), 3))
        land_anom = np.str(np.around((landmean_eoi400_allmodels[i]
                                      - landmean_e280_allmodels[i]), 3))


        filetext.write(model + ',' + eoi400_sea + ',' + e280_sea + ',' + sea_anom +
                       ',' + eoi400_land + ',' + e280_land + ',' + land_anom + '\n')

    filetext.write('MEAN,' + eoi400_sea_mean + ',' + e280_sea_mean + ',' + sea_anom_mean +
                   ',' + eoi400_land_mean + ',' + e280_land_mean + ',' + land_anom_mean + '\n')

def write_hemisphere_anom(filetext, NH_anomaly45,SH_anomaly45,
                          NH_anomaly60, SH_anomaly60):
    """
    writes the anomalies polewards of 45N and 45S and 60S and 60N
    """
    filetext.write("modelname, 45N-90N_anom, 45S-90S_anom, 60N-90N_anom, " +
                   "60S-90S_anom \n")
    for i, model in enumerate(MODELNAMES_FULL):

        filetext.write(model + ',' + np.str(np.around(NH_anomaly45[i], 3)) + ',' +
                       np.str(np.around(SH_anomaly45[i],3)) + ',' +
                       np.str(np.around(NH_anomaly60[i],3)) + ',' +
                       np.str(np.around(SH_anomaly60[i],3)) + '\n')
    filetext.write('MEAN,' + np.str(np.around(np.mean(NH_anomaly45), 3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly45),3)) + ',' +
                   np.str(np.around(np.mean(NH_anomaly60),3)) + ',' +
                   np.str(np.around(np.mean(SH_anomaly60),3)) +  '\n')

def main():

    # get data means and latitudinal anomalies
    modeldata = Getmodeldata(FIELDNAME, 'EOI400')
    (globalmean_eoi400_allmodels,
     latmean_eoi400_allmodels,
     landmean_eoi400_allmodels,
     seamean_eoi400_allmodels,
     landmean20_eoi400_allmodels,
     seamean20_eoi400_allmodels,
     NH45_eoi400_anomaly, SH45_eoi400_anomaly,
     NH60_eoi400_anomaly, SH60_eoi400_anomaly) = modeldata.extract_means()

    modeldata = Getmodeldata(FIELDNAME, 'E280')
    (globalmean_e280_allmodels,
     latmean_e280_allmodels,
     landmean_e280_allmodels,
     seamean_e280_allmodels,
     landmean20_e280_allmodels,
     seamean20_e280_allmodels,
     NH45_e280_anomaly, SH45_e280_anomaly,
     NH60_e280_anomaly, SH60_e280_anomaly) = modeldata.extract_means()

    # get seasonal cycle
    NH_seas_anom = get_nh_seascyc()


    # write to text

    filetext = open((FILESTART + '/means_for_' + FIELDNAME + '.txt'), "w+")
    write_global_means(filetext, globalmean_eoi400_allmodels,
                       globalmean_e280_allmodels)
    write_lat_means(filetext, latmean_eoi400_allmodels,
                    latmean_e280_allmodels)
    write_nh_seascycle(filetext, NH_seas_anom)
    write_landsea_means(filetext, landmean_eoi400_allmodels, # global land sea
                        seamean_eoi400_allmodels, landmean_e280_allmodels,
                        seamean_e280_allmodels, "global")
    write_landsea_means(filetext, landmean20_eoi400_allmodels, # global land sea
                        seamean20_eoi400_allmodels, landmean20_e280_allmodels,
                        seamean20_e280_allmodels, "20N-20S")
    write_hemisphere_anom(filetext, NH45_eoi400_anomaly - NH45_e280_anomaly,
                          SH45_eoi400_anomaly - SH45_e280_anomaly,
                          NH60_eoi400_anomaly - NH60_e280_anomaly,
                          SH60_eoi400_anomaly - SH60_e280_anomaly)

    filetext.close()


####################################
# definitions

LINUX_WIN = 'l'
#FIELDNAME = 'TotalPrecipitation'
FIELDNAME = 'NearSurfaceTemperature'
if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/PLIOMIP/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\PLIOMIP1\\'

MODELNAMES = ['CCSM', 'COSMOS', 'GISS', 'HAD', 'IPSL',
              'MIROC', 'MRI', 'NOR']

CONVERT_MODELS = {'CCSM' : 'CCSM4',
                  'GISS' : 'GISS-E2-R',
                  'HAD'  : 'HadCM3',
                  'IPSL' : 'IPSLCM5A',
                  'MIROC': 'MIROC4m',
                  'MRI' : 'MRI2.3',
                  'NOR' : 'NORESM-L'}

MODELNAMES_FULL = []
for MOD in MODELNAMES:
    MODELNAMES_FULL.append(CONVERT_MODELS.get(MOD, MOD))

LATBANDS = [[-90., -60.], [-60., -30.], [-30., 0.], [0., 30],
            [30., 60.], [60., 90.]]


main()

::::::::::::::
globalmean_timeseries.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 21 2020

#
# This program will produce a regridded 1X1degree timeseries of a given field.  
# We will remove the annual cycle in order to look for interannual variability 
# etc.  


import numpy as np
from netCDF4 import Dataset
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
from scipy import stats
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname, lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse == "sst":
        if exptname == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'

        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data = np.ma.array(cube.data, mask=mymask)
    return cube

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u = unit.Unit('days since 0800-01-01 00:00:00',
                      calendar=unit.CALENDAR_360_DAY)
        if model == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model == 'HadCM3' and fielduse == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model == 'HadCM3' and fielduse == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube = cube_temp.extract(iris.Constraint(surface=0.))

    return cube

def get_noresm_ocn(exptnamein, fieldnamein):
    """
    get noresm ocean things.  

    """
    
    print('need to do this')
    print('it is in monthly data and on a tripolar grid')
    sys.exit(0)

    return cube

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src, Dataset("temporary.nc", "w", format='NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name, src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name, dimension in src.dimensions.items():

            if name != 'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name, (len(dimension)))

        # copy all file data
        for name, variable in src.variables.items():
            print('name is', name, variable)
            print('datatype',variable.datatype)
            print('dimensions',variable.dimensions)
            if name not in ('time_counter_bnds', 'time_centered'):
                x = dst.createVariable(name, variable.datatype,
                                       variable.dimensions)
                if name == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:] = ((src.variables[name][:] / (60.*60.*24))
                                              -(src.variables[name][0] / (60.*60.*24)) + 15.)
                else:
                    dst.variables[name][:] = src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    #print(ncattr, attribute, exptname)
                    if ncattr == 'calendar' and exptname == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr == 'units' and name == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            if ncattr != "_FillValue":
                                dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein in ('ts', 'tas'):
            cube.convert_units('Celsius')

        if exptname == 'Eoi400':
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_360_DAY)
        else:
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return cube

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return cube

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return cube

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return cube


def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """

    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]

    # put units as celcius if required
    if fielduse == 'tas':
        cube.units = 'Celsius'

    cube2 = iris.util.new_axis(cube, 'time')

    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """

    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

    cube.add_dim_coord(iris.coords.DimCoord(points,
                                            standard_name='time', long_name='time',
                                            var_name='time',
                                            units=u,
                                            bounds=None,
                                            coord_system=None, circular=False), 0)

    if fieldnamein == 'pr':
        cube.data = cube.data * 60. *60. *24. *1000.
        cube.name = 'Total precipitation'
        cube.long_name = 'Total precipitation'
        cube.units = 'mm/day'


    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]

    return singlecube

def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """

    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
        cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
        cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
        if fieldnameout == 'TotalPrecipitation':
            cube = cube1 + cube2
        if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
            cube1.convert_units('Celsius')
            cubedata = np.where((cube2.data > 0.01),
                                -1.8, cube1.data)
            cube = cube1.copy(data=cubedata)
    else:
        cube = get_cesm12_singlecube(filename, fielduse)

    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b.e21.B1850.f09_g17.' +
                           'PMIP4-midPliocene-eoi400.001.'+
                           'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' +
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280':
            #if modelname == 'CESM2':
            #    filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
           #                'b.e21.B1850.f09_g17.' +
           #                'CMIP6-piControl.001.cam.h0.'+
           #                'LANDFRAC.1300.1399.nc')
           # else:
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                       'b.e12.B1850.f09_g16.preind.' +
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data = np.ma.array(cube.data, mask=mymask)


    return cube



######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube

    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years


    """

    if modelname in ('CCSM4', 'CESM1.2', 'CESM2','NorESM1-F','NorESM-L'):
        print('CCSM',cube.coord('month').points)
        months = cube.coord('month').points
        months = months -1
        for i, month in enumerate(months):
            if month == 0: months[i] = 12
        print(months)
        cube.coord('month').points = months
 
   


    return cube

######################################################
def cube_avg(cube):
    """
    Extract global annual averaged data from an array

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanyear (numpy array): the global mean of the field

    """

    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    
    meanyearcube.coord('latitude').guess_bounds()
    meanyearcube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(meanyearcube)
    
    yearglobavg_cube = (meanyearcube.collapsed(['longitude', 'latitude'],
                                       iris.analysis.MEAN, weights = grid_areas))
    global_avg = yearglobavg_cube.data
  

    return yearglobavg_cube.data




##############################################
def get_timeseries_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

    # outfile
    if linux_win == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/' + modelname +
                    '/timeseries/' + exptnameout + '.' + fieldnameout + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
                    + modelname + '\\timeseries\\' + exptnameout 
                    + '.' + fieldnameout + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


    #####################################
    # get all data in a single cube
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3'): # all fields in one file
        cube = get_ecearth_cube(exptnamein, lsmstart)
    elif modelname in ('HadCM3', 'MRI2.3'):
        cube = get_hadcm3_cube(modelname)
    elif modelname in ('IPSLCM5A', 'IPSLCM5A2') and fieldnamein != 'tos':
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif modelname in ('NorESM1-F', 'NorESM-L') and fieldnamein == 'tos':
        cube = get_noresm_ocn(exptnamein, fieldnamein)
    elif modelname == 'IPSLCM6A':
        cube = get_ipslcm6()
    elif modelname in ('MIROC4m', 'tos'):
        cube = get_miroc_tos()
    elif modelname == 'GISS2.1G':
        cube = get_giss()
    elif modelname == 'CCSM4-Utr':
        cube = get_ccsm4_2deg()
    elif modelname in ('CESM1.2', 'CCSM4', 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube = get_ccsm4_uot(fieldnamein)
    else:
        cube = iris.load_cube(filename)

    

    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid

    if ((modelname == 'CCSM4-UoT')
            or (modelname == 'IPSLCM5A' and fieldnamein == 'tos')
            or (modelname == 'IPSLCM5A2' and fieldnamein == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A',
                     'IPSLCM5A2', 'IPSLCM6A', 'CCSM4-Utr', 'GISS2.1G'):
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if modelname in ('NorESM1-F', 'NorESM-L', 'CESM1.2', 'CESM2', 'CCSM4'):

       # if precipitation is in m/s convert to mm/day
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. * 60. * 24. * 1000.
            cube.data = cube.data * 60. * 60. * 24. * 1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if modelname in ('CCSM4-UoT', 'NorESM1-F', 'NorESM-L', 'IPSLCM6A',
                     'EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A', 'IPSLCM5A2',
                     'HadCM3', 'GISS2.1G'):
         # convert to celcius
        if fieldnamein in ('tas', 'tos'):
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname in ('COSMOS', 'MIROC4m', 'IPSLCM6A', 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate


    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube, 'time', name='month')
    iris.coord_categorisation.add_year(regridded_cube, 'time', name='year')


     # correct the start month if required
    regridded_cube = correct_start_month(regridded_cube)
    
    # calculate averages
    mean_year_array = cube_avg(regridded_cube)
    
    return mean_year_array




#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                     }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }

    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }

    NorESM_FIELDS = {"pr" : "PRECT",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "sst"
                    }

    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                      }

    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA = {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                    "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }

    CESM2_EXTRA = {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                   "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                   }

    CCSM4_EXTRA = {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                   "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                   }

    ECearth_EXPT = {"Eoi400": "mPlio",
                    "E280":"PI"
                   }

    CESM12_EXPT = {"Eoi400": "PlioMIP2",
                   "E280":"PI"
                   }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
                     "E280":"PI"
                    }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }

    CESM2_TIME = {"E280" : ".110001-120012",
                  "Eoi400" : ".1101.1200"
                 }

    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
                     "E280":"3600_3699"
                    }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
                       "E280":"6110_6209",
                      }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
                     "E280":"piControl_r1i1p1f1_gr_285001-304912",
                     }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                         "E280":"piControl_r1i1p1f1_gn_285001-304912",
                        }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
                  "E280":"piControl_r1i1p1f1_gn_490101-495012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
                 }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
                       "E280":"piControl_r1i1p1f1_gr1_150101-160012",
                       "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                      }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
                  "E280":"piControl_r1i1p1f1_gn_495101-500012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                  }
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"}
    cosmos_version = {"tas": "",
                      "pr": "",
                      "tos":"_remapbil"}

    # get names for each model
    if modelname == 'MIROC4m':
        filename = filestart + modelname + '/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename + fielduse + '/MIROC4m_'+exptnamein
                    + '_' + atm_ocn_ind.get(fieldnamein) + '_' + fielduse + '.nc')
    if modelname == 'COSMOS':
        if linux_win == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename + exptnamein + '.' + fielduse +
                    '_CMIP6_name_' + fieldnamein +
                    '_2650-2749_monthly_mean_time_series' +
                    cosmos_version.get(fieldnamein) + '.nc')
    if modelname == 'CCSM4-UoT':
        if linux_win == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' +
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart + 'UofT-CCSM4\\' + exptnamein + '\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)

        filename = (filename + fielduse +
                    '_' + atm_ocn_ind.get(fieldnamein) +
                    '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart + 'LEEDS/HadCM3/' + exptuse + '/' + fielduse + '/'
                    + exptuse + '.' + fielduse + '.')
    if modelname == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                    +exptuse+'.'+fielduse+'.')
    if modelname == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname
                    + '_'
                    + ECearth_EXPT.get(exptnamein)
                    + fileend)
    if modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse 
                        + '_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse + '_monthly_TS.nc')

    if modelname == 'NorESM1-F' or modelname == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' +fielduse + '/'
                    + modelname + '_' + exptnamein + '.' + fielduse + '.')
        else:
            filename = (filestart + modelname + '/' + modelname + '_' +
                    exptnamein + '_' + fielduse + '.nc')
    if modelname == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' + fielduse +
                        '_Omon_IPSL-CM6A-LR_' + IPSLCM6A_TIME_ALT.get(exptnamein) 
                        + '_rectilinear.nc')
        else:
            filename = (filestart + modelname + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) + '_IPSL-CM6A-LR_' 
                        + IPSLCM6A_TIME.get(exptnamein) + '.nc')
    if modelname == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart + modelname + '/' + exptuse + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME1.get(exptnamein)
                        + '.nc')
        filename.append(filestart + modelname + '/' + exptuse + '/' 
                        + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME2.get(exptnamein) + '.nc')

    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein

    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                      CESM2_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECL' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']

        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'TS' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'ICEFRAC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']

        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' +
                      CCSM4_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)


    retdata = [fielduse, filename]
    return retdata

def checkdrift(plio_ts, pi_ts):
    """
    this program needs a pliocene timeseries and a preindustrial timeseries
    it will calculate the linear regression and use the slope to find
    the expected drift over 100 years (in the format ??degC / century)
    
    it will do this for the pliocene the preindustrial and the anomaly
    """
    
    nyears = len(plio_ts)
    allyears = np.arange(0.0, nyears, 1.0)
    nyears_pi = len(pi_ts)
    allyears_pi = np.arange(0.0, nyears_pi, 1.0)
    nyears_min = np.min([nyears, nyears_pi])
    allyears_min = np.arange(0.0, nyears_min, 1.0)
     
    print(nyears, nyears_pi, nyears_min)
  
    print(modelname)
    print('=======')
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears_min, plio_ts[0:nyears_min])
    print('pliocene drift = ' + np.str(np.around((slope * 100.),2)) 
          + 'dec C / centuary')
    
    
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears_min, pi_ts[0:nyears_min])
    print('pi drift = ' + np.str(np.around((slope * 100.), 2))
          + 'dec C / centuary')
    
    anomaly = plio_ts[0:nyears_min] - pi_ts[0:nyears_min]
    allyears = np.arange(0.0, nyears_min, 1.0)
    
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(allyears, anomaly)
    print('anomaly drift = ' +  np.str(np.around((slope * 100.),2) )
              + 'dec C / centuary')


##########################################################
# main program

filename =  ' '
linux_win =  'l'
modelname = "CESM2" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"
            }

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"
            }

fieldname = {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']

fieldnamein = 'tas'
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


if (modelname in ('IPSLCM5A', 'IPSLCM5A2', 'CCSM4-Utr') 
    and fieldnamein == 'tos'):
        filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
if modelname in ('IPSLCM6A', 'GISS2.1G'):
    filestart = '/nfs/hera1/earjcti/PLIOMIP2/'

# call program to get model dependent names
# fielduse,  and  filename and process for eoi400
fielduse, filename = getnames(modelname, filestart, fieldnamein, 'Eoi400')
fieldnameout = fieldname.get(fieldnamein)
exptnameout = exptname.get('Eoi400')
plio_timeseries = get_timeseries_data(fieldnamein, 'Eoi400')

# call program to get model dependent names
# fielduse,  and  filename and process for e280
fielduse, filename = getnames(modelname, filestart, fieldnamein, 'E280')
fieldnameout = fieldname.get(fieldnamein)
exptnameout = exptname.get('E280')
pi_timeseries = get_timeseries_data(fieldnamein, 'E280')

checkdrift(plio_timeseries, pi_timeseries)




::::::::::::::
means_all_models_pre_hadgem.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import iris.util
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

import sys


def plotmean_newaxis(cube, modelno_):
     tempcube=iris.util.new_axis(cube)
     tempcube.add_dim_coord(iris.coords.DimCoord(modelno_, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
     return tempcube
 

def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
         
    newcube = plotmean_newaxis(cube, [levelno])
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    
        
    return newcube
###########################################
def get_NH_mean(modelname, expt, field):
     """
     gets the mean of the NH for model - modelname
                             and expt - experiment name (ie PI)
     returns a numpy array of length 12 with the average for each month
     """
     # read data into iris cube
     filename = (FILESTART + modelname + 
                  '/' + expt + '.' + field + 
                  '.mean_month.nc')
     
     if (modelname == 'GISS2.1G' or modelname == 'IPSLCM6A'
         or (modelname == 'IPSLCM5A' and field == 'SST')
         or (modelname == 'IPSLCM5A2' and field == 'SST')
         or (modelname == 'NorESM-L' and field == 'SST')
         or (modelname == 'NorESM1-F' and field == 'SST')):
          cubes = iris.load(filename)
          cube = cubes[0]
     else:
          cube = iris.load_cube(filename, field)
        
     # get weights and average over NH
     cube.coord('latitude').guess_bounds()
     cube.coord('longitude').guess_bounds()
     grid_areas = iris.analysis.cartography.area_weights(cube)
     grid_areas_nh = np.zeros(grid_areas.shape)
     for j, lat in enumerate(cube.coord('latitude').points):
          if lat > 0:
               grid_areas_nh[:, j, :] = grid_areas[:, j, :]

     cube_nh = (cube.collapsed(['longitude', 'latitude'],
                iris.analysis.MEAN, weights = grid_areas_nh))
     
     return cube_nh.data
 
def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART[:-10] + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    
    # means are at the start of the file
    allanoms_list = []
    for i in range(1, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break    # we have now got all the means
        modname, eoi400, e280, anom = line.split(',')
        if modname == 'MEAN':
            meananom=anom
        else:
            allanoms_list.append(anom)
            
    # find line which contains 'jan feb mar ' which is the start of the seasonal cycle
    string = 'jan feb mar'
    min_seas_cyc = np.zeros(12) + 1000.
    max_seas_cyc = np.zeros(12) - 1000.
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        modname, anomstr = line.split(',')
        anom_arr = np.array(anomstr.strip('[]').split(), dtype=float)
        if modname == 'MEAN':
            mean_seas_cyc = anom_arr
        else:
            for j in range(0,12):
                min_seas_cyc[j] = np.min([anom_arr[j], min_seas_cyc[j]]) 
                max_seas_cyc[j] = np.max([anom_arr[j], max_seas_cyc[j]]) 
    

    allanoms = np.asarray(allanoms_list, dtype=float)
    
        
    return allanoms, min_seas_cyc, max_seas_cyc, mean_seas_cyc
  
   

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units,individual_plot):
   
    
    
    names = {"EOI400" : "Plio_Core",
             "E280" : "PI_Ctrl"
            }
    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }
    
    # set up lists to store all values for both experiment and control    
    model_global_mean=[]
    model_global_sd=[]
    
    # store months in a numpy array (model, experiment,month)
    monmeans=np.zeros((len(modelnames),2,12))   
    monsd=np.zeros((len(modelnames),2,12))
    
    # store latitudes in a numpy array (model, experiment,latitude)
    latmeans=np.zeros((len(modelnames),2,180))   
    latsd=np.zeros((len(modelnames),2,180))
    lats=np.zeros(180)
     
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        filenames=[]
        filenames.append(FILESTART+modeluse+'/'+exptname+'.'+field+'.data.txt')
        filenames.append(FILESTART+modeluse+'/'+cntlname+'.'+field+'.data.txt')
       
        # set up temporary lists to store data from each experiment
        means=[]
        sds=[]
        nmon=12
       
        for fileno in range(0,len(filenames)):
            
            f=open(filenames[fileno],"r")
            f1=f.readlines()
            f2 = [x.replace('\n', '') for x in f1]
            
            # get the means according to their position in the file
            all_mean_sd=f2[2]
            all_mon_mean_sd=f2[5:5+12]
            all_lat_mean_sd=f2[20:20+180]
           
            # extract global mean
            mean,sd=all_mean_sd.split(',')
            means.append(mean)
            sds.append(sd)
            
            # extract monthly means 
            for x in all_mon_mean_sd:
                mon,mean,sd=x.split(',')
                monmeans[modelno,fileno,int(mon)-1]=float(mean)
                monsd[modelno,fileno,int(mon)-1]=float(sd)
            
            
            # extract latitude means
            for x in all_lat_mean_sd:
                lat,mean,sd=x.split(',')
                latss=int(float(lat)+89.5) # convert latitude to a subscript
               
                if mean != ' --' and mean != '--':
                    latmeans[modelno,fileno,latss]=float(mean) # stores latitudinal means
                else:
                    latmeans[modelno,fileno,latss]=np.nan
                if sd != ' --' and sd != '--' :
                    latsd[modelno,fileno,latss]=float(sd)
                else:
                    latsd[modelno,fileno,latss]=np.nan
                lats[latss]=lat # stores latitudes
            
            
        model_global_mean.append(means)
        model_global_sd.append(sds)
           
    ############################################################
    # get the monthly means for the NH
    # 
    # store months in a numpy array (model, experiment,month)
    # ss 1 is experiment ss 2 is control

    monmeans_NH = np.zeros((len(modelnames),2,12))   
    monsd = np.zeros((len(modelnames),2,12))

    for i, model in enumerate(modelnames):
         expt_mon_mean = get_NH_mean(model, exptname, field)
         monmeans_NH[i, 0, :] = expt_mon_mean

         cntl_mon_mean = get_NH_mean(model, cntlname, field)
         monmeans_NH[i, 1, :] = cntl_mon_mean

    
    #===============================================================
    # if pliomip1 is set get pliomip1 data
    if PLIOMIP1 == 'y':
        (mean_pliomip1, min_seas_pliomip1,
         max_seas_pliomip1, mean_seas_pliomip1) = get_pliomip1_data(field)
    
    #############################################################
    # plot the global mean and error bars from each model.
    
   
    expt_global_mean=[float(item[0]) for item in model_global_mean]
    expt_global_2sigma=([float(item[0])*2.0 for item in model_global_sd])

    cntl_global_mean=[float(item[1]) for item in model_global_mean]
    cntl_global_2sigma=([float(item[1])*2.0 for item in model_global_sd])
  
    
   
   
    #fig,ax=plt.subplots(2,1,1)
    ax=plt.subplot(2,1,1)
   
    ax.errorbar(modelnames,expt_global_mean,
                yerr=expt_global_2sigma,fmt='x',label=names.get(exptname))
    ax.errorbar(modelnames,cntl_global_mean,
                yerr=cntl_global_2sigma,fmt='x',label=names.get(cntlname))
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.15*box.height), box.width * 0.8, box.height])
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    titlename='Global mean ' + namefield.get(field)
    plt.title(titlename)
    ax.tick_params(axis='x',labelbottom='False')
    plt.ylabel(units)
    
    
    
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)],'x')
    print('means',[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    anomalies = [x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)]
    print('multimodelmean = ',
          np.mean(expt_global_mean) - np.mean(cntl_global_mean), np.mean(anomalies))
    print('multimodelmedian = ', np.median(anomalies))
    print('percentiles 10/50/90',np.percentile(anomalies, 10),
          np.percentile(anomalies,50), np.percentile(anomalies,90))
    sys.exit(0)
    
    sorted_anomalies = np.sort(anomalies)
    print(sorted_anomalies)
    #print('means % change',[((x1 - x2) *100 / x2) for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    #print('multimodelmean = ',(np.mean(expt_global_mean) - np.mean(cntl_global_mean)) *100. / np.mean(cntl_global_mean))
     # if pliomip1 is set overplot pliomip1 means as grey horizontal bars
    if (PLIOMIP1 == 'y' and field !='SST'):
        for mean_mod in mean_pliomip1:
            ax.axhline(y=mean_mod, color='grey', alpha=0.4)
    # Shrink axis as appropriate
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, box.height*0.8])
    plt.title('Global mean ' + namefield.get(field) + ' anomaly')
    plt.ylabel(units)
    plt.xticks(rotation='90', fontsize=8)
    #plt.xticks(x, labels, rotation='vertical')
   

    fileout=FILESTART+'allplots/'+field+'/global_means.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/global_means.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out data
    
    txtfile1 = open(FILEOUT,"w+")
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_5a \n")
        
    txtfile1.write('modelname, Pliocore_global_mean, pliocore_global_mean_2sigma, ' + 
                   'picntl_global_mean, picntl_global_mean_2sigma \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + np.str(np.around(expt_global_mean[i],2)) + 
                       ',' + np.str(np.around(expt_global_2sigma[i],2)) + 
                       ',' + np.str(np.around(cntl_global_mean[i],2)) + 
                       ',' + np.str(np.around(cntl_global_2sigma[i],2)) + '\n'))
        
    txtfile1.write('\n')
    
      
    
    ################################################################
    #  plot the NH seasonal cycle from each model
   

    plt.subplot(2,1,1)
    for i in range(0,len(monmeans_NH[:,0,0])):
        # plot experiment data
       
        plt.plot(monmeans_NH[i,0,:],color='r')
        # plot control data
        plt.plot(monmeans_NH[i,1,:],color='b')
        plt.title('NH annual cycle of '+field)
        plt.ylabel(units)
        
    plt.subplot(2,1,2)
    plt.plot(np.mean(monmeans_NH[:,0,:],axis=0),label=exptname,color='r')
    plt.plot(np.mean(monmeans_NH[:,1,:],axis=0),label=cntlname,color='b')
    plt.ylabel(units)
    plt.xlabel('month')    
    plt.legend()
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.pdf'
    plt.savefig(fileout)
    plt.close
    
    
    ax=plt.subplot(1,1,1)
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    for i in range(0,len(monmeans_NH[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
           ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i])
        else:
            ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i],
                    linestyle='dashed')
        
        
    ax.plot(labels,np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0),
        color='black',linestyle='dashed',linewidth=2,label='avg')
    
    print('monthmeans',np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0))
   
    # plot pliomip1 data if appropriate
    if PLIOMIP1 == 'y':
        ax.plot(labels, mean_seas_pliomip1, color='black', linestyle='dotted',
                linewidth=2, label='PlioMIP1')
        ax.fill_between(labels, min_seas_pliomip1, max_seas_pliomip1, alpha=0.2, 
                        color="grey")
        
    
    plt.title(exptname+'-'+cntlname+': NH '+ field + ' anomaly')
    plt.ylabel(units)
    #plt.xlabel('month') 
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' NH anomaly')
    
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_3a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_6a \n")
       
        
    txtfile1.write('modelname, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + 
                        np.str(np.around((monmeans_NH[i,0,0] - monmeans_NH[i,1,0]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,1] - monmeans_NH[i,1,1]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,2] - monmeans_NH[i,1,2]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,3] - monmeans_NH[i,1,3]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,4] - monmeans_NH[i,1,4]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,5] - monmeans_NH[i,1,5]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,6] - monmeans_NH[i,1,6]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,7] - monmeans_NH[i,1,7]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,8] - monmeans_NH[i,1,8]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,9] - monmeans_NH[i,1,9]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,10] - monmeans_NH[i,1,10]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,11] - monmeans_NH[i,1,11]),2)) + '\n'))
        
    txtfile1.write('\n')
    txtfile1.close

    ###################################################################
    # plot the latitudinal range from each model
    
    # won't print this out as it doesn't look very useful.
    # absolute value of temperature by latitude
    #plt.subplot(2,1,1)
    #for i in range(0,len(latmeans[:,0,0])):
    #    # plot experiment data
    #   
    #    plt.plot(latmeans[i,0,:],lats,color='r')
    #    # plot control data
    #    plt.plot(latmeans[i,1,:],lats,color='b')
    #    plt.title('latitudinal average of '+field)
    #    plt.xlabel(units)
    #    
    #plt.subplot(2,1,2)
    #plt.plot(np.mean(latmeans[:,0,:],axis=0),lats,label=exptname,color='r')
    #plt.plot(np.mean(latmeans[:,1,:],axis=0),lats,label=cntlname,color='b')
    #plt.xlabel(units)
    #plt.ylabel('latitude')    
    #plt.legend()
    
    
    ax=plt.subplot(1,1,1)
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    for i in range(0,len(latmeans[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i])
        else:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i],
                    linestyle='dashed')
    ax.plot(np.mean(latmeans[:,0,:],axis=0)-np.mean(latmeans[:,1,:],axis=0),
             lats,label='avg',color='black',linestyle='dashed',linewidth=2)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' anomaly')
    plt.xlabel(units)
    plt.ylabel('latitude') 
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
  
     # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1c \n")
    elif field == 'SST':
        txtfile1.write("zonal mean data for SST \n")
   
   
        for i, model in enumerate(modelnames):
            txtfile1.write(model + '\n')
            txtfile1.write('latitude, Tanom \n')
            for j, lat in enumerate(lats):
                txtfile1.write(np.str(np.around(lat,2)) + ',' + 
                               np.str(np.around(latmeans[i,0,j] - latmeans[i,1,j],2)) + '\n')
        
        txtfile1.write('\n')
        
    txtfile1.close
   
    #########################################################
    # plot a picture of the monthly anomaly from each model
    
    
    if field=='SurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='NearSurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='SST':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -5.
            datamax = 32.
            dataincr = 2.
    if field=='TotalPrecipitation':
            valmin=-1.4
            valmax=1.6
            incr=0.2
            cmapname='RdBu'
            
            datamin = 0.
            datamax = 5.
            dataincr = 0.1
    Vanom=np.arange(valmin,valmax,incr)
    Vdata= np.arange(datamin, datamax, dataincr)
    
   
    
    #########################################################
    # plot a picture of the change from each model and the 
    # multimodelmean
    
    
    anom_cubes=iris.cube.CubeList([])
    expt_cubes=iris.cube.CubeList([])
    cntl_cubes=iris.cube.CubeList([])
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        
        exptcube=iris.load_cube(exptfile)
        cntlcube=iris.load_cube(cntlfile)
       
       
        if modeluse == 'EC-Earth3.1' and field == 'SST':
           cntlcube.coord('latitude').bounds = None
           cntlcube.coord('longitude').bounds = None
           
        if modeluse == 'CCSM4-UoT' and field == 'TotalPrecipitation':
           cntlcube.coord('latitude').var_name = 'latitude'
           cntlcube.coord('longitude').var_name = 'longitude'
           exptcube.coord('latitude').var_name = 'latitude'
           exptcube.coord('longitude').var_name = 'longitude'
           cntlcube.coord('latitude').long_name = None
           cntlcube.coord('longitude').long_name = None
           exptcube.coord('latitude').long_name = None
           exptcube.coord('longitude').long_name = None
           cntlcube.coord('latitude').points = cntlcube.coord('latitude').points.astype('float32')
           cntlcube.coord('longitude').points = cntlcube.coord('longitude').points.astype('float32')
           exptcube.coord('latitude').points =  exptcube.coord('latitude').points .astype('float32')
           exptcube.coord('longitude').points = exptcube.coord('longitude').points.astype('float32')
        
        diffcube=exptcube-cntlcube
        
        # check float 32 for concatenation
        diffcube.data=diffcube.data.astype('float32') 
        exptcube.data=exptcube.data.astype('float32') 
        cntlcube.data=cntlcube.data.astype('float32') 
        if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                diffcube.units='Celsius'
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                diffcube.convert_units('Celsius')
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')
        
        # remove scalar coordinates so that we can concatenate
        # also add a new axis with the model number
        
        newcube = resort_coords(diffcube,modelno)
        newdata = newcube.data
        anom_cubes.append(newcube)    
    
        newcube = resort_coords(exptcube,modelno)
        newcube.rename(field)
        print(modeluse)
        print('MINIMUM',np.min(newcube.data))
        expt_cubes.append(newcube)
        
        newcube = resort_coords(cntlcube,modelno)
        newcube.rename(field)
        cntl_cubes.append(newcube)          
         
        
        # plot individual values if required
        if individual_plot=='y':
            plt.subplot(1,2,1)
            cs=iplt.contourf(exptcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+exptname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
        
            plt.subplot(1,2,2)
            cs=iplt.contourf(cntlcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
       
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.pdf'
            plt.savefig(fileout)
            plt.close()
        
            # plot a picture of the anomaly from each of the models
            plt.subplot(1,1,1)
            
            
            cs=iplt.contourf(exptcube-cntlcube,Vanom,extend='both',cmap=cmapname)
            titlename=modeluse+' '+exptname+'-'+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.pdf'
            plt.savefig(fileout)
            plt.close()
        
   
       
    #############################    
    # get the multi-modelmean and standard deviation
    
   
    iris.experimental.equalise_cubes.equalise_attributes(expt_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_cubes)
    allpliocube = expt_cubes.concatenate_cube()
   
    meanplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanplio.rename(field + 'mean_mPWP')
    maxplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxplio.rename(field + 'max_mPWP')
    minplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MIN)
    minplio.rename(field + 'min_mPWP')
    stdplio = allpliocube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdplio.rename(field + 'std_mPWP')
    
    allpicube = cntl_cubes.concatenate_cube()
    meanpi = allpicube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpi.rename(field + 'mean_pi')
    maxpi = allpicube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxpi.rename(field + 'max_pi')
    minpi = allpicube.collapsed(['model_level_number'], iris.analysis.MIN)
    minpi.rename(field + 'min_pi')
    stdpi = allpicube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdpi.rename(field + 'std_pi')
    
    allmeancube=anom_cubes.concatenate_cube() # all model mean anomalies
    meancube=allmeancube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meancube.rename(field + 'mean_anomaly')
    maxcube=allmeancube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxcube.rename(field + 'max_anomaly')
    mincube=allmeancube.collapsed(['model_level_number'], iris.analysis.MIN)
    mincube.rename(field + 'min_anomaly')
    stdcube=allmeancube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdcube.rename(field + 'anomaly_multimodel_stddev')
    
    ###############################################
    # write out the mean and standard deviation to a netcdf file
    
    cubelist = iris.cube.CubeList([meanplio, stdplio, maxplio, minplio,
                                   meanpi, stdpi, maxpi, minpi,
                                   meancube, stdcube, maxcube, mincube])
    fileout = (FILESTART + field + '_multimodelmean.nc')
    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    
    
    ###########################
    # plot the mean value and standard deviation
    
   
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(valmin,valmax,incr)
    mycmap = plt.cm.get_cmap(cmapname,len(V+2))
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    if linux_win == 'l':
        exptlsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        exptlsm = FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.figtext(0.02, 0.97,'b)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(namefield.get(field) +' anomaly: multimodel mean')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.pdf')
    plt.savefig(fileout)
    plt.close()
    
    if field=='NearSurfaceTemperature':
        V=np.arange(0,5.0,incr)
        textout = 'd)'
    if field=='TotalPrecipitation':
        textout = 'c)'
        V=np.arange(0,1.3, 0.1)
    ax = plt.axes(projection = ccrs.PlateCarree())
    qplt.contourf(stdcube, V,extend='both',cmap='plasma')
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.title(namefield.get(field) +' anomaly:Standard Deviation')
    plt.figtext(0.02, 0.97,textout,
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.pdf')
    plt.savefig(fileout)
    plt.close()
     

    cubelist = iris.cube.CubeList([meancube, stdcube])
    iris.save(cubelist,FILEOUTNC,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
        
        
    ##########################################################
    # plot the monthly anomaly from each model
    if linux_win=='l' and individual_plot=='y':
        for modelno in range(0,len(modelnames)):
            modeluse=modelnames[modelno]
            exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.mean_month.nc'
            cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.mean_month.nc'
            exptcube=iris.load_cube(exptfile)
            cntlcube=iris.load_cube(cntlfile)
     
            for mon in range(0,12):
                anom=exptcube.data[mon,:,:]-cntlcube.data[mon,:,:]
                lat=exptcube.coord('latitude').points
                lon=exptcube.coord('longitude').points
                lons,lats=np.meshgrid(lon,lat)
                map=Basemap(llcrnrlon=0.0,urcrnrlon=360.0,
                         llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',
                         resolution='c')
                x, y = map(lons, lats)
                map.drawcoastlines()
                V=np.arange(valmin,valmax,incr)
                cs = map.contourf(x,y,anom,V,cmap=cmapname,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
                plt.title(modeluse+':'+'month is '+str(mon+1))
                cbar.set_label(units)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.eps')
                plt.savefig(fileout)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.pdf')
                plt.savefig(fileout)
                plt.close()
          
        
  


##########################################################
# main program
        
filename=' '
linux_win='l'

modelnames=['CESM2', 'HadGEM3','IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]

#modelnames = ['CCSM4-Utr']
            
PLIOMIP1 = 'n'
#modelnames=['COSMOS',
#            'CCSM4-UoT']   

fieldnames=['TotalPrecipitation']
units=['mm/day']
#fieldnames=['NearSurfaceTemperature']
#units=['degC']
#fieldnames=['SST']
#units=['degC']
exptname='EOI400'
cntlname='E280'
individual_plot='n' # do you want to plot the anomalies for all of the individual models

if linux_win=='w':
   FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded/'
else:
   FILESTART='/nfs/hera1/earjcti/regridded/'

FILEOUT = FILESTART + 'dummy.txt'
FILEOUTNC = FILESTART + 'dummy.nc'

for field in range(0,len(fieldnames)):

    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'alldata/data_for_5a_6a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_5b_5c.nc'
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'alldata/data_for_1a_1c_3a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_1b_1d.nc'
    if fieldnames[field] == 'SST':
        FILEOUT = FILESTART + 'alldata/zonal_mean_SST_data.txt'
  
   
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field],individual_plot)

#sys.exit(0)
\
::::::::::::::
means_all_models.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
#import cf
import iris
import iris.util
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER

import sys


def plotmean_newaxis(cube, modelno_):
     tempcube=iris.util.new_axis(cube)
     tempcube.add_dim_coord(iris.coords.DimCoord(modelno_, 
            standard_name='model_level_number', long_name='model', 
            var_name='model', 
            units=None,
            bounds=None,
            coord_system=None, circular=False),0) 
     return tempcube
 

def resort_coords(cube,levelno):
    """
    this will make all the dimensions of the cube match.  They will all be
    longitude, latitude, level-no (ie 1 for first model, 2 for second model...)
    
    input is the cube and the level number
    output is the cube with the new dimensions
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
         
    newcube = plotmean_newaxis(cube, [levelno])
    # this will make sure cell_methods match and that cubes can
    # be concatenated
    newcube.cell_methods = None
    
        
    return newcube
###########################################
def get_NH_mean(modelname, expt, field):
     """
     gets the mean of the NH for model - modelname
                             and expt - experiment name (ie PI)
     returns a numpy array of length 12 with the average for each month
     """
     # read data into iris cube
     filename = (FILESTART + modelname + 
                  '/' + expt + '.' + field + 
                  '.mean_month.nc')
     
     if (modelname == 'GISS2.1G' or modelname == 'IPSLCM6A'
         or (modelname == 'IPSLCM5A' and field == 'SST')
         or (modelname == 'IPSLCM5A2' and field == 'SST')
         or (modelname == 'NorESM-L' and field == 'SST')
         or (modelname == 'NorESM1-F' and field == 'SST')):
          cubes = iris.load(filename)
          cube = cubes[0]
     else:
          print(filename,field)
          cube = iris.load_cube(filename, field)
        
     # get weights and average over NH
     cube.coord('latitude').guess_bounds()
     cube.coord('longitude').guess_bounds()
     grid_areas = iris.analysis.cartography.area_weights(cube)
     grid_areas_nh = np.zeros(grid_areas.shape)
     for j, lat in enumerate(cube.coord('latitude').points):
          if lat > 0:
               grid_areas_nh[:, j, :] = grid_areas[:, j, :]

     cube_nh = (cube.collapsed(['longitude', 'latitude'],
                iris.analysis.MEAN, weights = grid_areas_nh))
     
     return cube_nh.data
 
def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART[:-10] + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
    
    # means are at the start of the file
    allanoms_list = []
    for i in range(1, len(lines)):
        line = lines[i]
        if line[0:9] == 'modelname':
            break    # we have now got all the means
        modname, eoi400, e280, anom = line.split(',')
        if modname == 'MEAN':
            meananom=anom
        else:
            allanoms_list.append(anom)
            
    # find line which contains 'jan feb mar ' which is the start of the seasonal cycle
    string = 'jan feb mar'
    min_seas_cyc = np.zeros(12) + 1000.
    max_seas_cyc = np.zeros(12) - 1000.
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        modname, anomstr = line.split(',')
        anom_arr = np.array(anomstr.strip('[]').split(), dtype=float)
        if modname == 'MEAN':
            mean_seas_cyc = anom_arr
        else:
            for j in range(0,12):
                min_seas_cyc[j] = np.min([anom_arr[j], min_seas_cyc[j]]) 
                max_seas_cyc[j] = np.max([anom_arr[j], max_seas_cyc[j]]) 
    

    allanoms = np.asarray(allanoms_list, dtype=float)
    
        
    return allanoms, min_seas_cyc, max_seas_cyc, mean_seas_cyc
  
   

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units,individual_plot):
   
    
    
    names = {"EOI400" : "Plio_Core",
             "E280" : "PI_Ctrl"
            }
    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }
    
    # set up lists to store all values for both experiment and control    
    model_global_mean=[]
    model_global_sd=[]
    
    # store months in a numpy array (model, experiment,month)
    monmeans=np.zeros((len(modelnames),2,12))   
    monsd=np.zeros((len(modelnames),2,12))
    
    # store latitudes in a numpy array (model, experiment,latitude)
    latmeans=np.zeros((len(modelnames),2,180))   
    latsd=np.zeros((len(modelnames),2,180))
    lats=np.zeros(180)
     
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        print(modeluse)
        filenames=[]
        filenames.append(FILESTART+modeluse+'/'+exptname+'.'+field+'.data.txt')
        filenames.append(FILESTART+modeluse+'/'+cntlname+'.'+field+'.data.txt')
       
        # set up temporary lists to store data from each experiment
        means=[]
        sds=[]
        nmon=12
       
        for fileno in range(0,len(filenames)):
            
            f=open(filenames[fileno],"r")
            f1=f.readlines()
            f2 = [x.replace('\n', '') for x in f1]
            
            # get the means according to their position in the file
            all_mean_sd=f2[2]
            all_mon_mean_sd=f2[5:5+12]
            all_lat_mean_sd=f2[20:20+180]
           
            # extract global mean
            mean,sd=all_mean_sd.split(',')
            means.append(mean)
            sds.append(sd)
            
            # extract monthly means 
            for x in all_mon_mean_sd:
                mon,mean,sd=x.split(',')
                monmeans[modelno,fileno,int(mon)-1]=float(mean)
                monsd[modelno,fileno,int(mon)-1]=float(sd)
            
            
            # extract latitude means
            for x in all_lat_mean_sd:
                lat,mean,sd=x.split(',')
                latss=int(float(lat)+89.5) # convert latitude to a subscript
               
                if mean != ' --' and mean != '--':
                    latmeans[modelno,fileno,latss]=float(mean) # stores latitudinal means
                else:
                    latmeans[modelno,fileno,latss]=np.nan
                if sd != ' --' and sd != '--' :
                    latsd[modelno,fileno,latss]=float(sd)
                else:
                    latsd[modelno,fileno,latss]=np.nan
                lats[latss]=lat # stores latitudes
            
            
        model_global_mean.append(means)
        model_global_sd.append(sds)
           
    ############################################################
    # get the monthly means for the NH
    # 
    # store months in a numpy array (model, experiment,month)
    # ss 1 is experiment ss 2 is control

    monmeans_NH = np.zeros((len(modelnames),2,12))   
    monsd = np.zeros((len(modelnames),2,12))

    for i, model in enumerate(modelnames):
         expt_mon_mean = get_NH_mean(model, exptname, field)
         monmeans_NH[i, 0, :] = expt_mon_mean

         cntl_mon_mean = get_NH_mean(model, cntlname, field)
         monmeans_NH[i, 1, :] = cntl_mon_mean

    
    #===============================================================
    # if pliomip1 is set get pliomip1 data
    if PLIOMIP1 == 'y':
        (mean_pliomip1, min_seas_pliomip1,
         max_seas_pliomip1, mean_seas_pliomip1) = get_pliomip1_data(field)
    
    #############################################################
    # plot the global mean and error bars from each model.
    
   
    expt_global_mean=[float(item[0]) for item in model_global_mean]
    expt_global_2sigma=([float(item[0])*2.0 for item in model_global_sd])

    cntl_global_mean=[float(item[1]) for item in model_global_mean]
    cntl_global_2sigma=([float(item[1])*2.0 for item in model_global_sd])
  
    
   
   
    #fig,ax=plt.subplots(2,1,1)
    ax=plt.subplot(2,1,1)
   
    ax.errorbar(modelnames,expt_global_mean,
                yerr=expt_global_2sigma,fmt='x',label=names.get(exptname))
    ax.errorbar(modelnames,cntl_global_mean,
                yerr=cntl_global_2sigma,fmt='x',label=names.get(cntlname))
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.15*box.height), box.width * 0.8, box.height])
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    titlename='Global mean ' + namefield.get(field)
    plt.title(titlename)
    ax.tick_params(axis='x',labelbottom='False')
    plt.ylabel(units)
    
    
    
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)],'x')
    print('means',[x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    anomalies = [x1 - x2 for (x1, x2) in zip(expt_global_mean, cntl_global_mean)]
    print('multimodelmean = ',
          np.mean(expt_global_mean) - np.mean(cntl_global_mean), np.mean(anomalies))
    print('multimodelmedian = ', np.median(anomalies))
    print('percentiles 10/50/90',np.percentile(anomalies, 10),
          np.percentile(anomalies,50), np.percentile(anomalies,90))
    print('percentiles 5/95/90',np.percentile(anomalies, 5),
          np.percentile(anomalies,95))
      
    
    sorted_anomalies = np.sort(anomalies)
    print(sorted_anomalies)
    print('means % change',[((x1 - x2) *100 / x2) for (x1, x2) in zip(expt_global_mean, cntl_global_mean)])
    print('multimodelmean = ',(np.mean(expt_global_mean) - np.mean(cntl_global_mean)) *100. / np.mean(cntl_global_mean))
     # if pliomip1 is set overplot pliomip1 means as grey horizontal bars
    if (PLIOMIP1 == 'y' and field !='SST'):
        for mean_mod in mean_pliomip1:
            ax.axhline(y=mean_mod, color='grey', alpha=0.4)
    # Shrink axis as appropriate
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, box.height*0.8])
    plt.title('Global mean ' + namefield.get(field) + ' anomaly')
    plt.ylabel(units)
    plt.xticks(rotation='90', fontsize=8)
    #plt.xticks(x, labels, rotation='vertical')
   

    fileout=FILESTART+'allplots/'+field+'/global_means.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/global_means.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out data
    
    txtfile1 = open(FILEOUT,"w+")
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_5a \n")
        
    txtfile1.write('modelname, Pliocore_global_mean, pliocore_global_mean_2sigma, ' + 
                   'picntl_global_mean, picntl_global_mean_2sigma \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + np.str(np.around(expt_global_mean[i],2)) + 
                       ',' + np.str(np.around(expt_global_2sigma[i],2)) + 
                       ',' + np.str(np.around(cntl_global_mean[i],2)) + 
                       ',' + np.str(np.around(cntl_global_2sigma[i],2)) + '\n'))
        
    txtfile1.write('\n')
    
      
    
    ################################################################
    #  plot the NH seasonal cycle from each model
   

    plt.subplot(2,1,1)
    for i in range(0,len(monmeans_NH[:,0,0])):
        # plot experiment data
       
        plt.plot(monmeans_NH[i,0,:],color='r')
        # plot control data
        plt.plot(monmeans_NH[i,1,:],color='b')
        plt.title('NH annual cycle of '+field)
        plt.ylabel(units)
        
    plt.subplot(2,1,2)
    plt.plot(np.mean(monmeans_NH[:,0,:],axis=0),label=exptname,color='r')
    plt.plot(np.mean(monmeans_NH[:,1,:],axis=0),label=cntlname,color='b')
    plt.ylabel(units)
    plt.xlabel('month')    
    plt.legend()
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models.pdf'
    plt.savefig(fileout)
    plt.close
    
    
    ax=plt.subplot(1,1,1)
    labels=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    for i in range(0,len(monmeans_NH[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
           ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i])
        else:
            ax.plot(labels,monmeans_NH[i,0,:]-monmeans_NH[i,1,:],label=modelnames[i],
                    linestyle='dashed')
        
        
    ax.plot(labels,np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0),
        color='black',linestyle='dashed',linewidth=2,label='avg')
    
    print('monthmeans',np.mean(monmeans_NH[:,0,:],axis=0)-np.mean(monmeans_NH[:,1,:],axis=0))
   
    # plot pliomip1 data if appropriate
    if PLIOMIP1 == 'y':
        ax.plot(labels, mean_seas_pliomip1, color='black', linestyle='dotted',
                linewidth=2, label='PlioMIP1')
        ax.fill_between(labels, min_seas_pliomip1, max_seas_pliomip1, alpha=0.2, 
                        color="grey")
        
    
    plt.title(exptname+'-'+cntlname+': NH '+ field + ' anomaly')
    plt.ylabel(units)
    #plt.xlabel('month') 
    plt.figtext(0.02, 0.97,'a)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' NH anomaly')
    
    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
   
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/seasonal_cycle_all_models_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_3a \n")
    elif field == 'TotalPrecipitation':
        txtfile1.write("data_for_6a \n")
       
        
    txtfile1.write('modelname, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec \n')
    for i, model in enumerate(modelnames):
        txtfile1.write((model + ',' + 
                        np.str(np.around((monmeans_NH[i,0,0] - monmeans_NH[i,1,0]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,1] - monmeans_NH[i,1,1]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,2] - monmeans_NH[i,1,2]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,3] - monmeans_NH[i,1,3]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,4] - monmeans_NH[i,1,4]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,5] - monmeans_NH[i,1,5]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,6] - monmeans_NH[i,1,6]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,7] - monmeans_NH[i,1,7]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,8] - monmeans_NH[i,1,8]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,9] - monmeans_NH[i,1,9]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,10] - monmeans_NH[i,1,10]),2)) + ',' + 
                        np.str(np.around((monmeans_NH[i,0,11] - monmeans_NH[i,1,11]),2)) + '\n'))
        
    txtfile1.write('\n')
    txtfile1.close

    ###################################################################
    # plot the latitudinal range from each model
    
    # won't print this out as it doesn't look very useful.
    # absolute value of temperature by latitude
    #plt.subplot(2,1,1)
    #for i in range(0,len(latmeans[:,0,0])):
    #    # plot experiment data
    #   
    #    plt.plot(latmeans[i,0,:],lats,color='r')
    #    # plot control data
    #    plt.plot(latmeans[i,1,:],lats,color='b')
    #    plt.title('latitudinal average of '+field)
    #    plt.xlabel(units)
    #    
    #plt.subplot(2,1,2)
    #plt.plot(np.mean(latmeans[:,0,:],axis=0),lats,label=exptname,color='r')
    #plt.plot(np.mean(latmeans[:,1,:],axis=0),lats,label=cntlname,color='b')
    #plt.xlabel(units)
    #plt.ylabel('latitude')    
    #plt.legend()
    
    
    ax=plt.subplot(1,1,1)
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    for i in range(0,len(latmeans[:,0,0])):
        if i < len(latmeans[:,0,0]) / 2.0:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i])
        else:
            ax.plot(latmeans[i,0,:]-latmeans[i,1,:],lats,label=modelnames[i],
                    linestyle='dashed')
    ax.plot(np.mean(latmeans[:,0,:],axis=0)-np.mean(latmeans[:,1,:],axis=0),
             lats,label='avg',color='black',linestyle='dashed',linewidth=2)
    plt.title(names.get(exptname) + '-'
              + names.get(cntlname) + ': ' 
              + namefield.get(field) + ' anomaly')
    plt.xlabel(units)
    plt.ylabel('latitude') 
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.eps'
    plt.savefig(fileout)
    fileout=FILESTART+'allplots/'+field+'/latitude_anomaly.pdf'
    plt.savefig(fileout)
    plt.close()
  
     # write out to a file
    if field == 'NearSurfaceTemperature':
        txtfile1.write("data_for_1c \n")
    elif field == 'SST':
        txtfile1.write("zonal mean data for SST \n")
   
   
        for i, model in enumerate(modelnames):
            txtfile1.write(model + '\n')
            txtfile1.write('latitude, Tanom \n')
            for j, lat in enumerate(lats):
                txtfile1.write(np.str(np.around(lat,2)) + ',' + 
                               np.str(np.around(latmeans[i,0,j] - latmeans[i,1,j],2)) + '\n')
        
        txtfile1.write('\n')
        
    txtfile1.close
   
    #########################################################
    # plot a picture of the monthly anomaly from each model
    
    
    if field=='SurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='NearSurfaceTemperature':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -30.
            datamax = 35.
            dataincr = 5.
    if field=='SST':
            valmin=0.0
            valmax=10.5
            incr=0.5
            cmapname='Reds'
            
            datamin = -5.
            datamax = 32.
            dataincr = 2.
    if field=='TotalPrecipitation':
            valmin=-1.4
            valmax=1.6
            incr=0.2
            cmapname='RdBu'
            
            datamin = 0.
            datamax = 5.
            dataincr = 0.1
    Vanom=np.arange(valmin,valmax,incr)
    Vdata= np.arange(datamin, datamax, dataincr)
    
   
    
    #########################################################
    # plot a picture of the change from each model and the 
    # multimodelmean
    
    
    anom_cubes=iris.cube.CubeList([])
    expt_cubes=iris.cube.CubeList([])
    cntl_cubes=iris.cube.CubeList([])
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        
        exptcube=iris.load_cube(exptfile)
        cntlcube=iris.load_cube(cntlfile)
       
       
        if modeluse == 'EC-Earth3.1' and field == 'SST':
           cntlcube.coord('latitude').bounds = None
           cntlcube.coord('longitude').bounds = None
           
        if modeluse == 'CCSM4-UoT' and field == 'TotalPrecipitation':
           cntlcube.coord('latitude').var_name = 'latitude'
           cntlcube.coord('longitude').var_name = 'longitude'
           exptcube.coord('latitude').var_name = 'latitude'
           exptcube.coord('longitude').var_name = 'longitude'
           cntlcube.coord('latitude').long_name = None
           cntlcube.coord('longitude').long_name = None
           exptcube.coord('latitude').long_name = None
           exptcube.coord('longitude').long_name = None
           cntlcube.coord('latitude').points = cntlcube.coord('latitude').points.astype('float32')
           cntlcube.coord('longitude').points = cntlcube.coord('longitude').points.astype('float32')
           exptcube.coord('latitude').points =  exptcube.coord('latitude').points .astype('float32')
           exptcube.coord('longitude').points = exptcube.coord('longitude').points.astype('float32')
        
        diffcube=exptcube-cntlcube
        
        # check float 32 for concatenation
        diffcube.data=diffcube.data.astype('float32') 
        exptcube.data=exptcube.data.astype('float32') 
        cntlcube.data=cntlcube.data.astype('float32') 
        if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'HadGEM3')
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                diffcube.units='Celsius'
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                diffcube.convert_units('Celsius')
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')
        
        # remove scalar coordinates so that we can concatenate
        # also add a new axis with the model number
        
        newcube = resort_coords(diffcube,modelno)
        newdata = newcube.data
        anom_cubes.append(newcube)    
    
        newcube = resort_coords(exptcube,modelno)
        newcube.rename(field)
        print(modeluse)
        print('MINIMUM',np.min(newcube.data))
        expt_cubes.append(newcube)
        
        newcube = resort_coords(cntlcube,modelno)
        newcube.rename(field)
        cntl_cubes.append(newcube)          
         
        
        # plot individual values if required
        if individual_plot=='y':
            plt.subplot(1,2,1)
            cs=iplt.contourf(exptcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+exptname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
        
            plt.subplot(1,2,2)
            cs=iplt.contourf(cntlcube,Vdata,extend='both',
                             cmap='terrain')
            titlename=modeluse+' '+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.ax.tick_params(labelsize=8, labelrotation=60) 
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
       
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'.pdf'
            plt.savefig(fileout)
            plt.close()
        
            # plot a picture of the anomaly from each of the models
            plt.subplot(1,1,1)
            
            
            cs=iplt.contourf(exptcube-cntlcube,Vanom,extend='both',cmap=cmapname)
            titlename=modeluse+' '+exptname+'-'+cntlname+': '+field
            cbar=plt.colorbar(cs,orientation="horizontal")
            cbar.set_label(units)
            plt.title(titlename,fontsize=8)
            plt.gca().coastlines()
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.eps'
            plt.savefig(fileout)
            fileout=FILESTART+'allplots/'+field+'/individual_models/'+modeluse+'_'+field+'_anomaly.pdf'
            plt.savefig(fileout)
            plt.close()
        
   
       
    #############################    
    # get the multi-modelmean and standard deviation
    
   
    iris.experimental.equalise_cubes.equalise_attributes(expt_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_cubes)
    allpliocube = expt_cubes.concatenate_cube()
   
    meanplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanplio.rename(field + 'mean_mPWP')
    maxplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxplio.rename(field + 'max_mPWP')
    minplio = allpliocube.collapsed(['model_level_number'], iris.analysis.MIN)
    minplio.rename(field + 'min_mPWP')
    stdplio = allpliocube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdplio.rename(field + 'std_mPWP')
    
    allpicube = cntl_cubes.concatenate_cube()
    meanpi = allpicube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpi.rename(field + 'mean_pi')
    maxpi = allpicube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxpi.rename(field + 'max_pi')
    minpi = allpicube.collapsed(['model_level_number'], iris.analysis.MIN)
    minpi.rename(field + 'min_pi')
    stdpi = allpicube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdpi.rename(field + 'std_pi')
    
    allmeancube=anom_cubes.concatenate_cube() # all model mean anomalies
    meancube=allmeancube.collapsed(['model_level_number'], iris.analysis.MEAN)
    meancube.rename(field + 'mean_anomaly')
    maxcube=allmeancube.collapsed(['model_level_number'], iris.analysis.MAX)
    maxcube.rename(field + 'max_anomaly')
    mincube=allmeancube.collapsed(['model_level_number'], iris.analysis.MIN)
    mincube.rename(field + 'min_anomaly')
    stdcube=allmeancube.collapsed(['model_level_number'], iris.analysis.STD_DEV)
    stdcube.rename(field + 'anomaly_multimodel_stddev')
    
    ###############################################
    # write out the mean and standard deviation to a netcdf file
    
    cubelist = iris.cube.CubeList([meanplio, stdplio, maxplio, minplio,
                                   meanpi, stdpi, maxpi, minpi,
                                   meancube, stdcube, maxcube, mincube])
    fileout = (FILESTART + field + '_multimodelmean.nc')
    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)

    
    
    ###########################
    # plot the mean value and standard deviation
    
   
    ax = plt.axes(projection = ccrs.PlateCarree())
    V=np.arange(valmin,valmax,incr)
    mycmap = plt.cm.get_cmap(cmapname,len(V+2))
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    if linux_win == 'l':
        exptlsm = '/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        exptlsm = FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.figtext(0.02, 0.97,'b)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    plt.title(namefield.get(field) +' anomaly: multimodel mean')
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelmean.pdf')
    plt.savefig(fileout)
    plt.close()
    
    if field=='NearSurfaceTemperature':
        V=np.arange(0,5.0,incr)
        textout = 'd)'
    if field=='TotalPrecipitation':
        textout = 'c)'
        V=np.arange(0,1.3, 0.1)
    ax = plt.axes(projection = ccrs.PlateCarree())
    qplt.contourf(stdcube, V,extend='both',cmap='plasma')
    lsmcube=iris.load_cube(exptlsm)
    qplt.contour(lsmcube,1,colors='black') 
    plt.title(namefield.get(field) +' anomaly:Standard Deviation')
    #plt.figtext(0.02, 0.97,textout,
    # horizontalalignment='left',
    # verticalalignment='top',
    # fontsize=20)
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.eps')
    plt.savefig(fileout)
    fileout=(FILESTART+'allplots/'+field+'/multimodelstdev.pdf')
    plt.savefig(fileout)
    plt.close()
     

    cubelist = iris.cube.CubeList([meancube, stdcube])
    iris.save(cubelist,FILEOUTNC,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
        
        
    ##########################################################
    # plot the monthly anomaly from each model
   
    expt_month_cubes=iris.cube.CubeList([])
    cntl_month_cubes=iris.cube.CubeList([])

    for modelno in range(0,len(modelnames)):
         modeluse=modelnames[modelno]
         exptfile=FILESTART+modeluse+'/'+exptname+'.'+field+'.mean_month.nc'
         cntlfile=FILESTART+modeluse+'/'+cntlname+'.'+field+'.mean_month.nc'

         cube=iris.load_cube(exptfile)
         cube.data=cube.data.astype('float32') 
         exptcube = resort_coords(cube,modelno)
         exptcube.rename(field)
        
         cube=iris.load_cube(cntlfile)
         cube.data=cube.data.astype('float32') 
         cntlcube = resort_coords(cube,modelno)
         cntlcube.rename(field)

         if field=='NearSurfaceTemperature' or field == 'SST':
            if (modeluse=='MIROC4m' or modeluse=='COSMOS'
                or (modeluse == 'HadGEM3')
                or (modeluse == 'CCSM4-Utr' and field =='SST')):
                exptcube.units='Celsius'
                cntlcube.units='Celsius'
            else:
                exptcube.convert_units('Celsius')
                cntlcube.convert_units('Celsius')

         expt_month_cubes.append(exptcube)
         cntl_month_cubes.append(cntlcube)

         if linux_win=='l' and individual_plot=='y':
   
            for mon in range(0,12):
                anom=exptcube.data[mon,:,:]-cntlcube.data[mon,:,:]
                lat=exptcube.coord('latitude').points
                lon=exptcube.coord('longitude').points
                lons,lats=np.meshgrid(lon,lat)
                map=Basemap(llcrnrlon=0.0,urcrnrlon=360.0,
                         llcrnrlat=-90.0,urcrnrlat=90.0,projection='cyl',
                         resolution='c')
                x, y = map(lons, lats)
                map.drawcoastlines()
                V=np.arange(valmin,valmax,incr)
                cs = map.contourf(x,y,anom,V,cmap=cmapname,extend="both")
                cbar = plt.colorbar(cs,orientation="horizontal")
                plt.title(modeluse+':'+'month is '+str(mon+1))
                cbar.set_label(units)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.eps')
                plt.savefig(fileout)
                fileout=(FILESTART+'allplots/'+field+'/global_months/'+modeluse+'_'+field+
                     '_anomaly'+str(mon+1)+'.pdf')
                plt.savefig(fileout)
                plt.close()
        
    iris.experimental.equalise_cubes.equalise_attributes(expt_month_cubes)
    iris.experimental.equalise_cubes.equalise_attributes(cntl_month_cubes)
     
    allpimonthcubes = cntl_month_cubes.concatenate_cube()
    meanpimonth = allpimonthcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpimonth.rename(field + 'mean_pi')
    meanpimonth.add_dim_coord(iris.coords.DimCoord(np.arange(1,13),
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                bounds = None,
                coord_system = None,  circular = False), 0)
   
  
    allpliomonthcubes = expt_month_cubes.concatenate_cube()
    meanpliomonth = allpliomonthcubes.collapsed(['model_level_number'], iris.analysis.MEAN)
    meanpliomonth.rename(field + 'mean_plio')
    meanpliomonth.add_dim_coord(iris.coords.DimCoord(np.arange(1,13),
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                bounds = None,
                coord_system = None,  circular = False), 0)


    meananommonth = meanpliomonth - meanpimonth
    meananommonth.rename(field + 'plio - pi')
   
    cubelist = iris.cube.CubeList([meanpimonth, meanpliomonth, meananommonth])
    fileout = (FILESTART + field + '_multimodelmean_month.nc')

    iris.save(cubelist,fileout,netcdf_format='NETCDF3_CLASSIC',fill_value=1.0E20)           
  
    print('end of program')


##########################################################
# main program
        
filename=' '
linux_win='l'

modelnames=['CESM2', 'HadGEM3','IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]

#modelnames = ['CCSM4-Utr']
            
PLIOMIP1 = 'n'
#modelnames=['COSMOS',
#            'CCSM4-UoT']   

#fieldnames=['TotalPrecipitation']
#units=['mm/day']
fieldnames=['NearSurfaceTemperature']
units=['degC']
#fieldnames=['SST']
#units=['degC']
exptname='EOI400'
cntlname='E280'
individual_plot='n' # do you want to plot the anomalies for all of the individual models

if linux_win=='w':
   FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded/'
else:
   FILESTART='/nfs/hera1/earjcti/regridded/'

FILEOUT = FILESTART + 'dummy.txt'
FILEOUTNC = FILESTART + 'dummy.nc'

for field in range(0,len(fieldnames)):

    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'alldata/data_for_5a_6a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_5b_5c.nc'
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'alldata/data_for_1a_1c_3a.txt'
        FILEOUTNC = FILESTART + 'alldata/data_for_1b_1d.nc'
    if fieldnames[field] == 'SST':
        FILEOUT = FILESTART + 'alldata/zonal_mean_SST_data.txt'
  
   
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field],individual_plot)

#sys.exit(0)
\
::::::::::::::
model_rankings.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on October 2019


#@aauthor: earjcti
"""
This program will plot model diagnostics against model information
the information it will use is
1. year of first use
2. IPCC report where it was first used
3. resolution
"""


import numpy as np
import scipy as sp
import iris
import matplotlib.pyplot as plt



#####################################
def plot_regress(field1, field2, cluster, xtitle, ytitle, fileout, xmin, xmax):
    """
    plots the regression of the field vs the ranking
    """
    # plot the climate sensitivity vs the global mean

   
    ax = plt.subplot(111)
    for i, fielduse in enumerate(field2):
        model = MODELNAMES[i]
        print('j1',i,'j2', field1[i], 'j3',fielduse,'j4', MODELNAMES[i])
        if i % 4 == 0: # i divides 4 with no remainder
            ax.scatter(field1[i], fielduse, label = model) 
        elif i % 4 == 1 :
            ax.scatter(field1[i], fielduse, label = model, marker='^') 
        elif i % 4 == 2 :
            ax.scatter(field1[i], fielduse, label = model, marker='<') 
        else:
            ax.scatter(field1[i], fielduse, label = model, marker='v') 
   
    plt.xlabel(xtitle)
    plt.ylabel(ytitle)
    #plt.legend()

    plt.xlim(xmin, xmax)


    slope, intercept, r_value, p_value, std_err = (
        sp.stats.linregress(field1, field2))

    xarray = np.arange(xmin, xmax, 1)
    yarray = intercept+(slope*xarray)
    #ax.plot(xarray,yarray)
    rsq_val_small = np.str(np.around(r_value **2, 2))
    p_value_small = np.str(np.around(p_value, 2))

    if FIELDNAME == 'NearSurfaceTemperature':
       if xtitle == 'year':
           subplotno = 'a'
       else:
           subplotno = 'a'
    if FIELDNAME == 'TotalPrecipitation':
       if xtitle == 'year':
           subplotno = 'b'
       else:
           subplotno = 'b'
    plt.title((subplotno + ") R-squared: "  + rsq_val_small + 
              "  p-value: " + p_value_small))
              

    box = ax.get_position()
    ax.set_position([box.x0, box.y0,
                     box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))

    #plt.show()
    plt.savefig(fileout+'.eps')
    plt.savefig(fileout+'.pdf')
    plt.close()


def get_model_years():
    """
    get the year when the model was first used from a dictionary
    """

    year = {'NorESM-L': 2011,
            'NorESM1-F':2017,
            'IPSLCM6A': 2018,
            'IPSLCM5A2':2017,
            'IPSLCM5A':2010,
            'HadCM3': 1997,
            'MIROC4m':2004,
            'COSMOS':2009,
            'CCSM4-UoT':2011,
            'EC-Earth3.3':2019,
            'MRI2.3': 2006, # from my investigation
            'CCSM4-Utr': 2011,
            'GISS2.1G': 2019,
            'CESM1.2' :2013,
            'CESM2': 2019,
            'CCSM4' :2011
            }

    allyears = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        allyears[i] = year.get(model)

    return allyears

def get_resolution():
    """
    get the resolution of the model
    """

    xres_a = {'NorESM-L': 3.75,
              'NorESM1-F':2.5,
              'IPSLCM6A': 2.5,
              'IPSLCM5A2':3.75,
              'IPSLCM5A':3.75,
              'HadCM3': 3.75,
              'MIROC4m':2.8,
              'COSMOS':3.75,
              'CCSM4-UoT':1.25,
              'EC-Earth3.3':1.125,
              'MRI-CGCM2.3':2.8, # from my investigation
              'CESM4-Utr': 2.5,
              'CESM2' : 1.25,
              'GISS': 2.5,
              'CESM1.2' :1.25,
              'CCSM4' :1.25
              }

    yres_a = {'NorESM-L': 3.75,
              'NorESM1-F':1.9,
              'IPSLCM6A': 1.26,
              'IPSLCM5A2':1.9,
              'IPSLCM5A':1.9,
              'HadCM3': 2.5,
              'MIROC4m':2.8,
              'COSMOS':3.75,
              'CCSM4-UoT':0.9,
              'EC-Earth3.3':1.125,
              'MRI-CGCM2.3':2.8, # from my investigation
              'CESM-Utr': 1.9,
              'CESM2' : 0.95,
              'GISS': 2.0,
              'CESM1.2' :0.9,
              'CCSM4' :0.9
              }

    horiz_boxes = {'NorESM-L': 4608,
                   'NorESM1-F':13824,
                   'IPSLCM6A': 20592,
                   'IPSLCM5A2':9216,
                   'IPSLCM5A':9216,
                   'HadCM3': 7008,
                   'MIROC4m':8192,
                   'COSMOS':4608,
                   'CCSM4-UoT':55296,
                   'EC-Earth3.3':51200,
                   'MRI2.3':8192, # from my investigation
                   'CCSM4-Utr': 13824,
                   'GISS2.1G': 12960,
                   'CESM1.2' :55296,
                   'CESM2' : 55296,
                   'CCSM4' :55296
                   }

    horiz_gb_atm = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        horiz_gb_atm[i] = horiz_boxes.get(model)

    return horiz_gb_atm

def get_cluster():
    """
    this will put models of the same family into a cluster
    """

    year ={'NorESM-L': 1,
           'NorESM1-F': 1,
           'IPSLCM6A': 2,
           'IPSLCM5A2':2,
           'IPSLCM5A':2,
           'HadCM3': 0,
           'MIROC4m': 0,
           'COSMOS': 0,
           'CCSM4-UoT':3,
           'EC-Earth3.3': 0,
           'MRI2.3': 0, # from my investigation
           'CCSM4-Utr': 3,
           'GISS2.1G': 0,
           'CESM1.2' : 3,
           'CESM2' : 3, 
           'CCSM4' : 3
           }

    allyears = np.zeros(len(MODELNAMES))
    for i, model in enumerate(MODELNAMES):
        allyears[i] = year.get(model)

    return allyears

def get_model_info():
    """
    gets information about the model
    returns modelyears
            cluster this will group models of the same family
    """
    years = get_model_years()
    resolution = get_resolution()
    cluster = get_cluster()

    return [years, resolution, cluster]

def read_means_file(filename):
    """
    read all the data from the file containing the means
    returns the mean
    """

    file1 = open(filename, "r")
    lines = list(file1)

    meanval, sdval = lines[2].split(",")

    meanint = np.float(meanval)

    return meanint

def get_anomaly():
    """
    gets the anomaly in the field for each of the models
    """

    climdiff = np.zeros(len(MODELNAMES))

    for i, model in enumerate(MODELNAMES):

        print(FIELDNAME)
        print(model)
        meanexpt = read_means_file(FILESTART + model + '/EOI400.' +
                                   FIELDNAME + '.data.txt')
        meancntl = read_means_file(FILESTART + model + '/E280.' +
                                   FIELDNAME + '.data.txt')

        climdiff[i] = meanexpt - meancntl


    return climdiff



def main():
    """
    1. get model information.  For example the year of the model
    2. second the anomaly in the field
    3. plot
    """
    modelyears, model_res, modelcluster = get_model_info()

    fieldanom = get_anomaly()

    # regress anomaly against model year
    plot_regress(modelyears, fieldanom, modelcluster, 'year',
                 FIELDNAME + ' anom',
                 FILESTART + '/allplots/' + FIELDNAME +
                 '/' + FIELDNAME + '_anom_vs_modelyear' + SUBSCRIPT,
                 1990., 2020.)

    # regress anomaly against model resolution
    plot_regress(model_res, fieldanom, modelcluster, 'horizontal gridboxes',
                 FIELDNAME + ' anom',
                 FILESTART + '/allplots/' + FIELDNAME +
                 '/' + FIELDNAME + '_anom_vs_nbox_atm' + SUBSCRIPT,
                 5000., 60000.)



##########################################################
# main program

LINUX_WIN = 'l'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/regridded/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
              ]
SUBSCRIPT = ''

#MODELNAMES = ['CCSM4-UoT',
#              'CCSM4',
#              'CESM1.2',
#              'IPSLCM6A',
#              'IPSLCM5A2',
#              'IPSLCM5A',
#              'NorESM-L',
#              'NorESM1-F',
#              'COSMOS',
#              'GISS',
#              'HadCM3',
#              'MIROC4m',
#              'MRI-CGCM2.3'
#            ]
#SUBSCRIPT = '_redu'

FIELDNAME = 'NearSurfaceTemperature'
#FIELDNAME = 'TotalPrecipitation'


main()
::::::::::::::
PLIOVAR_gradients.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the SST meridional gradient temperature change
vs global SSTtemperature change for each of the PlioMIP models

changed on October 17th to add the data to the figure

"""

import sys
import iris
import iris.quickplot as qplt
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


class GetProxy:
    """
    this class is to do with getting everything from Heathers excel files
    """
    def __init__(self, interval, datatype):
        """
        the interval is esentially which excel sheet we are getting data from
        t1 t2 or t3
        datatype = UK37 or MGCA
        """
        
        if datatype == 'UK37':
            self.filename = FILESTARTP + 'pliovar_uk37_ori_vs_bayspline.xlsx'
            self.bsloc = 8
        if datatype == 'MGCA':
            self.filename = FILESTARTP +  'pliovar_mgca_OrivsBaymag.xlsx'
            self.bsloc = 7
        self.metafile = FILESTARTP + 'pliovar_metadata_global_02102019.csv'
        self.pifile = FILESTARTP + 'modeloutput_pliovar.xls'
        self.interval = interval # this is the time range likely t1 t2 or t3
           
    def get_proxydata(self):
        """
        this will obtain in an array the latitude, longitude and SST of the 
        proxy data.  It will put them in an array
        
        returns for each latitude bound
        boundtemp : the average temperature in the latitude band
        boundtemp_bs : the average temperature in the latitude band using bayspline
        boundmin ; the minimum latitude of the band
        boundmax : the maximum latitude of the band
        nval: the number of points in the band (for weighting)
        """
        
        # reads into a dictionary
        dfs = pd.read_excel(self.filename, sheet_name=None)
        
        t1sheet = dfs.get(self.interval)
        

        self.sitenames = t1sheet.iloc[1:,0]
        self.nsites = len(self.sitenames)
        self.lon = np.zeros(self.nsites)
        self.lat = np.zeros(self.nsites)
        self.temppi = np.zeros(self.nsites)
        
        
        # get the temperatures
        self.sitetemp = t1sheet.iloc[1:,1]
        self.sitetemp_bs = t1sheet.iloc[1:,self.bsloc]
        
        
        # get the latitudes and longitudes
        self.get_lonlat() 
        
        # get the preindustrial temperatures
        self.get_piT() 
        
        # put the temperature anomalies into latitude bounds
        self.boundmin = -90. + (np.arange(0, 12) * 15.)
        self.boundmax = -75. + (np.arange(0, 12) * 15)
       
        boundtemp, nval = self.put_data_to_bounds(self.sitetemp.values - self.temppi)
        boundtemp_bs, nval_bs = self.put_data_to_bounds(self.sitetemp_bs.values - self.temppi)
       

        
        return boundtemp, boundtemp_bs, self.boundmin, self.boundmax, nval, nval_bs
    
    def get_lonlat(self):
        """
        will get the longitude and laitude from each site
        and add them to the self.lon and self.lat array
        """
        
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        df = pd.read_csv(self.metafile, encoding='latin-1')
        metadf = df[["name", "lon", "lat"]]
        lonlatdict = metadf.set_index('name').T.to_dict()
        
        #print(lonlatdict)
        #sys.exit(0)
        
        for i in range(0, self.nsites):
            sitedata = lonlatdict.get(self.sitenames.iloc[i],'lat')
            self.lat[i] = sitedata.get('lat')
            self.lon[i] = sitedata.get('lon')
            
        return
    
    def get_piT(self):
        """
        will get the pi temperature from each site from NOAASST
        and add to self.pitemp array
        """
        
        dfs = pd.read_excel(self.pifile, sheet_name='E280near')
        # gets the dictionary of longitudes and latitudes
        # from the metadatafile
        metadf = dfs[["site", "NOAAERSST5"]]
       
        pitempdict = metadf.set_index(['site']).T.to_dict()
        
        
        for i in range(0, self.nsites):
            noaadata = pitempdict.get((self.sitenames.iloc[i]))
            self.temppi[i] = noaadata.get('NOAAERSST5')
           
        return
    
    
    def put_data_to_bounds(self, tanom):
       """
       we now have the longitude, latitude and temperature of each datapoint
       we now put them into 15deg latitude bounded regions (defined by self.bounds)
       and find the average temperature in each region
       also return the number of points in each region
       """  
      
       boundtemp = np.zeros(12)
       count_boundtemp = np.zeros(12)
      
       for i in range(0, self.nsites):    
           # if temperature is a number add the temperature to the 
           # bound region
         
           if np.isfinite(tanom[i]):
               for bound in range(0,12):
              
                   if ((self.boundmin[bound] < self.lat[i]) &
                       (self.lat[i] <= self.boundmax[bound])):
            
                           boundtemp[bound] = (boundtemp[bound] + tanom[i])
                           count_boundtemp[bound] = count_boundtemp[bound] + 1
                           
                   
       # get average
      
       boundtemp = boundtemp / count_boundtemp
       
       print('bound temp',boundtemp)
       print('nbound',count_boundtemp)
       print(self.boundmin)
      
      
       return boundtemp, count_boundtemp
       
        
    
# end of class   
 
######################################################################
def combine_mgca_uk37(temp_uk37, temp_mgca, n_uk37, n_mgca,
                     boundmin_uk37, boundmax_uk37, boundmin_mgca, boundmax_mgca):
    """
    we have lots of arrays which show average temperature (temp_????) in a bounded
    box (boundmin_???? - boundmax_????).  We also have the number of points that make
    up the average temperature.
    
    We would like to average these depending on how many points are in the box
    
    returns: an array which contains the average combined temperature of uk37 and mg/ca
    """
    
    nvals = len(boundmin_uk37)
    temp_combined = np.zeros(nvals)
        
    for i in range(0, nvals):
        j = np.where(boundmin_mgca == boundmin_uk37[i])
        
        print(i, n_uk37[i], n_mgca[i])
        temp_combined[i] = (((np.nan_to_num(temp_uk37[i]) * n_uk37[i]) +
                            (np.nan_to_num(temp_mgca[j]) * n_mgca[j])) / 
                            (n_uk37[i] + n_mgca[j]))
    
    return temp_combined
 
#####################################################
def weightdata(tanom, lowerbounds, upperbounds):
        """
        we have some proxy data.  We would like to weight it to find the
        average value in each region.  
        Currently the proxy data is in latitudinal bands
        we want to find the global average and the (30S-30N) - (60N-75N)
        gradients
        
        input: temperature anomaly data, 
        lowervalue of the latituinal bound
        uppervalue of the latitudinal bound
        
        returns: merid_dy (30S-30N) - (60N-75N) - weighted by ocean area
                 glob_avg (average of (60S-75N) - weighted by ocean area)
        """
       
        # get land sea mask from pi model
        lsmcube = iris.load_cube(FILESTART + 'HadCM3/E280.SST.allmean.nc')
        lsmcube.data = lsmcube.data / lsmcube.data
        # get weights and multiply by lsm
        lsmcube.coord('latitude').guess_bounds()
        lsmcube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(lsmcube)
        lsmcube.data = lsmcube.data * grid_areas
       
        
        # find weights area between each lowerbound and upper bound
        weights = np.zeros(len(tanom))
        for i, lat in enumerate(lsmcube.coord('latitude').points):
            latcube = lsmcube.extract(iris.Constraint(latitude=lat))
            latdata = np.sum(latcube.data)
            for bound in range(0, len(lowerbounds)):
                if lowerbounds[bound] < lat <=upperbounds[bound]:
                   weights[bound] = weights[bound] + latdata
        
        # global average = temperautre in each bound * weights / total of weights

        
        globavg = np.nansum(weights * tanom) / np.nansum(weights)
        print('weights and tanom')
        print('weights',weights)
        print('tanom',tanom)
        print('upperbound',upperbounds)
        
        
        # get average in each bound
        
        tropavg_deep = bound_avg(-15., 15., lowerbounds, upperbounds, weights, tanom)
        tropavg = bound_avg(-30., 30., lowerbounds, upperbounds, weights, tanom)
        tropavg_nh = bound_avg(0., 30.,lowerbounds, upperbounds, weights, tanom)
        highlatavg = bound_avg(60., 75.,lowerbounds, upperbounds, weights, tanom)
        
        # get average polewards of 45degrees
        weightcount_midhighlat = 0.
        midhighlatavg = 0.
        for bound in range(0, len(lowerbounds)):
            # do poleward of 45 deg
            if ((lowerbounds[bound] >=45. or upperbounds[bound] <=-45.)
                 and (np.isfinite(tanom[bound]))):
                midhighlatavg = midhighlatavg + (weights[bound] * tanom[bound])
                weightcount_midhighlat = weightcount_midhighlat + weights[bound]
        
        midhighlatavg = midhighlatavg / weightcount_midhighlat
        
        # get gradients
        merid_dy = tropavg - highlatavg
        
        merid_dy_nh = tropavg_nh - highlatavg
        
        merid_dy_45 = tropavg_deep - midhighlatavg
        
    
        return merid_dy, merid_dy_nh, merid_dy_45, globavg
    
def bound_avg(minval, maxval, lowerbounds, upperbounds, weights, tanom):
    """
    averages the temperature over the range minval, maxval
    returns:  weighted averaged temperautre
    """
    
    avgval = 0.
    weightcount = 0.
      
    for bound in range(0, len(lowerbounds)):
        if lowerbounds[bound] >= minval and upperbounds[bound] <= maxval:
            avgval = avgval + (weights[bound] * tanom[bound]) 
            weightcount = weightcount + weights[bound]
    avgval = avgval / weightcount
    
    return avgval
            
############################################################
def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if field == 'SST' or field == 'NearSurfaceTemperature':
        if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'
                or modeluse == 'CESM1.0.5'):
            cube.units = 'Celsius'
        else:
            cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    if modeluse == 'EC-Earth3.1' and field == 'SST':
        cube.coord('latitude').bounds = None
        cube.coord('longitude').bounds = None

    cube.cell_methods = None

    return cube

def globmean(cube):
    """
    returns the global mean value of a SST cube (single value)
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    return tempcube.data

def getgradient(cube):
    """
    gets the gradient in the cube.  This is the average SST 60-75N
    minus the average SST equatorward of 30N
    input : cube
    output : gradient_et (30N-30S) - (60N-75N)
             gradient_na (30N-30S) - (60N-75N)(290E-5E)
             gradient_na_nh (30N-0S) - (60N-75N)(290E-5E)
    """

    grid_areas = iris.analysis.cartography.area_weights(cube)
    grid_areas_tropics = np.zeros(np.shape(grid_areas))
    grid_areas_deeptropics = np.zeros(np.shape(grid_areas))
    grid_areas_tropics_nh = np.zeros(np.shape(grid_areas))
    grid_areas_et = np.zeros(np.shape(grid_areas)) # 60N-75N
    grid_areas_na = np.zeros(np.shape(grid_areas)) # 60N - 75N 290E-360E
    grid_areas_45 = np.zeros(np.shape(grid_areas))

    nlat = len(cube.coord('latitude').points)
    nlon = len(cube.coord('longitude').points)
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points

    for j in range(0, nlat):
        if ((lats[j] >= 60.) and (lats[j] <= 75)):
            grid_areas_et[j, :] = grid_areas[j, :]
            for i in range(0, nlon):
                if (lons[i] >= 290 or lons[i]<=5.):
                    grid_areas_na[j, i] = grid_areas[j, i]
                else:
                    grid_areas_na[j, i] = 0.0
        else:
            grid_areas_et[j, :] = 0.0
            grid_areas_na[j, :] = 0.0

        if ((lats[j] <= 30.) and (lats[j] >= -30.)):
            grid_areas_tropics[j, :] = grid_areas[j, :]
        else:
            grid_areas_tropics[j, :] = 0.0
            
        if ((lats[j] <= 15.) and (lats[j] >= -15.)):
            grid_areas_deeptropics[j, :] = grid_areas[j, :]
        else:
            grid_areas_deeptropics[j, :] = 0.0
            
        if ((lats[j] >= 45.) or (lats[j] <= -45.)):
            grid_areas_45[j, :] = grid_areas[j, :]
        else:
            grid_areas_45[j, :] = 0.0
            
        if ((lats[j] <= 30.) and (lats[j] >= 0.)):
            grid_areas_tropics_nh[j, :] = grid_areas[j, :]
        else:
            grid_areas_tropics_nh[j, :] = 0.0

    temptrop = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_tropics)
    temptropdeep = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_deeptropics)
    temptrop_nh = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_tropics_nh)
    tempet = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_et)
    temp45 = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_45)
    tempna = cube.collapsed(['latitude', 'longitude'],
                            iris.analysis.MEAN, weights=grid_areas_na)

    gradient_et = temptrop.data - tempet.data 
    gradient_na = temptrop.data - tempna.data 
    gradient_nh = temptrop_nh.data - tempna.data
    gradient_deep_extra = temptropdeep.data - temp45.data
    
    print(np.sum(grid_areas))  
    print(np.sum(grid_areas_tropics)/np.sum(grid_areas))  
    print(np.sum(grid_areas_deeptropics)/np.sum(grid_areas))  
    print(np.sum(grid_areas_et)/np.sum(grid_areas))  
    print(np.sum(grid_areas_na)/np.sum(grid_areas)) 
    print(np.sum(grid_areas_45)/np.sum(grid_areas)) 
   
    
    return gradient_et, gradient_na, gradient_nh, gradient_deep_extra

def get_model_data(modelname):
    """
    1. gets the pliocene and the preindustrial SST data for each file
    2. calculates the global SSTA
    3. calculates the gradient as (SST polewards of 60deg) - (SST equatorward of 30deg)
    4. calculates the mPWP - PI gradient (gradient produced in 3.)

    input modelname
    output modTanom = the global mPWP-PI SSTA
           mod_gradanom = the mPWP (meridional SST gradient) minus the PI (meridional SST gradient)

    """

    #1. get data
    cubepi = get_data(FILESTART + modelname + '/E280.' + FIELDNAME + '.allmean.nc',
                      FIELDNAME, modelname)
    cubeplio = get_data(FILESTART + modelname + '/EOI400.' + FIELDNAME + '.allmean.nc',
                        FIELDNAME, modelname)
    #2 global mean anomaly
    meanpi = globmean(cubepi)
    meanplio = globmean(cubeplio)
    tempSSTA = meanplio - meanpi

    # calculate gradient
    gradpi_et, gradpi_na, gradpi_nh, gradpi_15NS_45NS = getgradient(cubepi)
    gradplio_et, gradplio_na, gradplio_nh, gradplio_15NS_45NS = getgradient(cubeplio)
    gradSSTA_et = gradplio_et - gradpi_et
    gradSSTA_na = gradplio_na - gradpi_na
    gradSSTA_nh = gradplio_nh - gradpi_nh
    gradSSTA_15NS_45NS = gradplio_15NS_45NS - gradpi_15NS_45NS

    return tempSSTA, gradSSTA_et, gradSSTA_na, gradSSTA_nh, gradSSTA_15NS_45NS

def plotdata(global_data, merid_grad_data, Tanom_model, gradanom_model, fileout):
    """
    this will plot the data and the model output
    we are plotting the global temerature anomaly vs the meridional gradient
    temperautre anomaly for both model and data
    """
    
    plt.scatter(global_data, merid_grad_data,
                label='proxy data', color='black',
                marker = 's')
    
    for i, model in enumerate(MODELNAMES):
         # add this to scatterplot
        if i < 6:
            plt.scatter(Tanom_model[i], gradanom_model[i], label=model)
        else:
            plt.scatter(Tanom_model[i], gradanom_model[i], label=model, marker='^')

    plt.xlabel('global mean SSTA')
    plt.ylabel('change in meridional gradient')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.tight_layout()

   
    plt.savefig(fileout + 'pdf')
    plt.savefig(fileout + 'png')
    plt.close()
    
    # write data to textfile
    txtout = open(fileout + 'txt', 'w+')

    txtout.write('model, global_mean, merid_gradient \n')
    txtout.write('proxy_data,' + 
                 np.str(np.around(global_data, 2)) + ',' + 
                 np.str(np.around(merid_grad_data,2)) + '\n')
    for i, model in enumerate(MODELNAMES):
        txtout.write(model + ',' + 
                     np.str(np.around(Tanom_model[i],2)) + ',' + 
                     np.str(np.around(gradanom_model[i],2)) + '\n')
    txtout.write('multimodelmean,' + 
                     np.str(np.around(np.mean(Tanom_model),2)) + ',' + 
                     np.str(np.around(np.mean(gradanom_model),2)) + '\n')
    txtout.close


def main():
    """
    1. Call a program that will get the data to plot
    2. Weight the proxy data by area of each latitude banc
    3. Get the output (global SST, meridional gradient SST anomaly)
       for each of the models
    4. Plot the data on a symplot
    """

    # 1. get data
    obj = GetProxy('t1', 'UK37') # get data for t1 timeslice
    (t1_temp, t1_temp_bs, 
     boundmin, boundmax,
     nval, nval_bs) = obj.get_proxydata()
    
    
    if MG_CA == 'y': # also get Mg/Ca data
        obj = GetProxy('t1', 'MGCA') # get data for t1 timeslice
        (t1_temp_mgca, t1_temp_bs_mgca, 
         boundmin_mgca, boundmax_mgca,
         nval_mgca, nval_bs_mgca)  = obj.get_proxydata()
       
        print('doing t1_comb')
        t1_comb = (combine_mgca_uk37(t1_temp, t1_temp_mgca, nval, nval_mgca,
                                     boundmin, boundmax, boundmin_mgca, boundmax_mgca))
        print('doing t1_comb_bs')
        t1_comb_bs = (combine_mgca_uk37(t1_temp_bs, t1_temp_bs_mgca, nval_bs, nval_bs_mgca,
                                     boundmin, boundmax, boundmin_mgca, boundmax_mgca))
        
        t1_temp = t1_comb
        t1_temp_bs = t1_comb_bs
        print('about to exit')
        
     
    
    # 2. weight proxydata (standard and bayspline)
    
    (merid_grad, merid_grad_nh, 
     merid_grad_15_45, global_ssta) = weightdata(t1_temp, boundmin, boundmax)
    
    (merid_grad_bs, merid_grad_nh_bs, 
     merid_grad_15_45_bs, global_ssta_bs) = weightdata(t1_temp_bs, boundmin, boundmax)
   
    
    # 3. get model results 
    Tanom = np.zeros(len(MODELNAMES))
    gradanom_et = np.zeros(len(MODELNAMES))
    gradanom_na = np.zeros(len(MODELNAMES))
    gradanom_na_nh = np.zeros(len(MODELNAMES))
    gradanom_15_45 = np.zeros(len(MODELNAMES))
    
    for i, model in enumerate(MODELNAMES):
        (Tanom[i], gradanom_et[i], 
         gradanom_na[i], 
         gradanom_na_nh[i],
         gradanom_15_45[i]) = get_model_data(model)
   
    #4. plot standard (gradient 30N-30S - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '.'
    plotdata(global_ssta, merid_grad, Tanom, gradanom_na, fileout)
    
    #plot standard (gradient 30N-0N - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_nh.'
    plotdata(global_ssta, merid_grad_nh, Tanom, gradanom_na_nh, fileout)
    
    #plot Bayspline (gradient 30N-30S - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_bayspline.'
    plotdata(global_ssta_bs, merid_grad_bs, Tanom, gradanom_na, fileout)
    
    
    #plot Bayspline (gradient 30N-0N - NH north atlantic)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_bayspline_nh.'
    plotdata(global_ssta_bs, merid_grad_nh_bs, Tanom, gradanom_na_nh, fileout)
    
    # plot original (gradient 15N-15S, 45N/S-90N/S)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_15NS_45NS-90NS.'
    plotdata(global_ssta, merid_grad_15_45, Tanom, gradanom_15_45, fileout)
    
    # plot Bayspline (gradient 15N-15S, 45N/S-90N/S)
    fileout = OUTSTART+'PlioVAR_gradients_uk37' + MGCASS + '_15NS_45NS-90NS_bayspline.'
    plotdata(global_ssta_bs, merid_grad_15_45_bs, Tanom, gradanom_15_45, fileout)



    return

    

# variable definition
LINUX_WIN = 'l'
FIELDNAME = 'SST'

if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    FILESTARTP = '/nfs/hera1/earjcti/PLIOMIP2/proxydata/' # where proxy data is
else:
    FILESTART = 'C:/Users/julia/OneDrive/WORK/DATA/regridded/'
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    FILESTARTP = 'C:/Users/julia/OneDrive/WORK/DATA/proxydata/'

UNITS = 'deg C'
TIMEPERIODS = ['pi', 'mPWP']
MODELNAMES = ['CCSM4', 'CCSM4-UoT',
              'CCSM4-Utr',  
              'CESM1.2','CESM2',
              'COSMOS', 'EC-Earth3.3', 
              'GISS2.1G', 'HadCM3',
              'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
              'MIROC4m', 'MRI2.3',
              'NorESM-L', 'NorESM1-F'
             ]


MG_CA = 'y'
if MG_CA == 'y':
    MGCASS = '_mgca'
else:
    MGCASS = ''

main()
::::::::::::::
plot_individual_models_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models 
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
from mpl_toolkits.basemap import Basemap, shiftgrid
#import Basemap
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys


def getmeanfield(fieldname, period):
    """
    get the mean values from the mean value file 
    
    inputs: fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
            period (probably mPWP PI or anomaly)

    returns the mean value from the multuimodelmean.nc file
    """
    
    meanfile = ('/nfs/hera1/earjcti/regridded/' + fieldname + 
                '_multimodelmean.nc')
    meanfield = fieldname + 'mean_' + period
    
    cube = iris.load_cube(meanfile, meanfield)

    return cube


def getmodelfield(modelname, fieldname, period):
     """
     get the mean values from the model data
     inputs: modelname (ie HadCM3)
             fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
             period (likely EOI400 or E280)
     returns:  a cube contatining the mean data from the model
     
     """
     
     modfile = ('/nfs/hera1/earjcti/regridded/' + modelname + '/' + 
                period + '.' + fieldname + '.allmean.nc')
    
     tempcube = iris.load(modfile)
     cube = tempcube[0]
     cube.units = UNITS
     
     return cube

class Plotalldata:   
    def __init(self):
        self.data = []
        
        
    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
              modelnames : the names of all the models
        """
        self.nmodels = len(modelnames)
        self.filestart = ('/nfs/hera1/earjcti/regridded/allplots/' +
                          fieldname + '/individual')
    
        for i in range(0, self.nmodels):
            
            cubedata = anom_cubes[i].data
            self.latitudes = anom_cubes[i].coord('latitude').points
            lon = anom_cubes[i].coord('longitude').points
            self.datatoplot, self.longitudes = (shiftgrid(180.,
                                                          cubedata,
                                                          lon,
                                                          start=False))
            self.model = modelnames[i]
            self.plotmap(i)
        

        return      
    
    def plotmap(self, i):
        """
        will plot the data in a map format
        
        """
        
        plotpos = np.mod(i, 8) + 1
        plt.subplot(3, 3, plotpos)
        lons, lats = np.meshgrid(self.longitudes, self.latitudes)
        
        map=Basemap(llcrnrlon=-180.0, urcrnrlon=180.0, 
                    llcrnrlat=-90.0, urcrnrlat=90.0, 
                    projection='cyl',resolution='l')
   
        map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines()
    
        V = np.arange(-5.0, 6.0, 1.0)    
        cs = map.contourf(x, y, self.datatoplot, V, cmap='RdBu_r',
                          extend='both')
        plt.title(self.model)
        
        print(i,self.nmodels)
        if plotpos == 8 or (i + 1) == self.nmodels:
            plt.subplot(3, 3, 9)
            plt.gca().set_visible(False)
            cbar = plt.colorbar(cs, orientation='horizontal')
            cbar.set_label(UNITS)
            
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                        + '.eps')
            plt.savefig(fileout, bbox_inches='tight')
            
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                    + '.png')
            
            plt.savefig(fileout, bbox_inches='tight')
        
        
##########################################################
# main program
# set up variable information

fieldname='NearSurfaceTemperature'
filename=' '
linux_win='l'


modelnames=['CESM1.0.5','COSMOS','EC-Earth3.1','GISS','HadCM3',
            'IPSLCM6A','IPSLCM5A2','IPSLCM5A',
            'MIROC4m','MRI-CGCM2.3',
            'NorESM-L','NorESM1-F',
            'UofT',
            ]

#modelnames=['GISS']
UNITS = 'Celsius'

#fieldnames=['TotalPrecipitation']
#units=['mm/day']

# set up cubelists to store data
mpwp_anom_cubes=iris.cube.CubeList([])
pi_anom_cubes=iris.cube.CubeList([])
anom_anom_cubes=iris.cube.CubeList([])


#################################################
# get mean data
mean_plio_cube = getmeanfield(fieldname, 'mPWP')
mean_pi_cube = getmeanfield(fieldname, 'pi')
mean_anom_cube = getmeanfield(fieldname, 'anomaly')


for model in range(0,len(modelnames)):
    model_plio_cube = getmodelfield(modelnames[model], fieldname, 'EOI400')
    model_pi_cube = getmodelfield(modelnames[model], fieldname, 'E280')
    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_anom_cubes.append(model_plio_cube - mean_plio_cube)
    pi_anom_cubes.append(model_pi_cube - mean_pi_cube)
    anom_anom_cubes.append(model_anom_cube - model_pi_cube)
    
##################################################
# plot the cubes for the model anomalies relative to the mean

obj = Plotalldata()
obj.plotdata(fieldname, mpwp_anom_cubes, modelnames)

#sys.exit(0)
::::::::::::::
plot_individual_models.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on 07.09.2019 by Julia

This program will plot a given field from the individual models
for either the Pliocene or the preindustrail or the difference between them

It will subtract the multimodel mean so that the differences are very clear
"""

import os
import sys
import numpy as np
#import matplotlib as mp
import matplotlib.pyplot as plt
#from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#import netCDF4
#from netCDF4 import Dataset, MFDataset
import iris
import iris.analysis.cartography
import iris.coord_categorisation

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def getmeanfield(period):
    """
    get the mean values from the mean value file

    inputs: fieldname (probably NearSurfaceTemperature or TotalPrecipitation)
            period (probably mPWP PI or anomaly)

    returns the mean value from the multuimodelmean.nc file
    """

    meanfile = (FILESTART + 'regridded/' + FIELDNAME +
                '_multimodelmean.nc')
    meanfield = FIELDNAME + 'mean_' + period

    cube = iris.load_cube(meanfile, meanfield)


    return cube


def getmodelfield(modelname, period):
    """
    get the mean values from the model data
    inputs: modelname (ie HadCM3)
            period (likely EOI400 or E280)
    returns:  a cube contatining the mean data from the model

    """

    modfile = (FILESTART + 'regridded/' + modelname + '/' +
               period + '.' + FIELDNAME + '.allmean.nc')

    tempcube = iris.load(modfile)
    cube = tempcube[0]
    cube.units = UNITS

    #this will make all the dimensions of all the cubes match.


    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube


class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, timeperiod, anom_cubes):
        self.nmodels = len(MODELNAMES)
        self.filestart = (FILESTART + '/regridded/allplots/' +
                          FIELDNAME + '/' + timeperiod + '_individual')
        self.timeperiod = timeperiod
        self.anom_cubes = anom_cubes

        if (FIELDNAME == 'NearSurfaceTemperature'
            or FIELDNAME == 'SST'):
                self.valmin = -5.
                self.valmax = 6.
                self.diff = 1.
                self.colormap = 'RdBu_r'

        if FIELDNAME == 'TotalPrecipitation':
            self.valmin = -2.
            self.valmax = 2.1
            self.diff = 0.2
            self.colormap = 'RdBu'


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        fig = plt.figure(figsize=(11.0, 8.5))
        for i in range(0, self.nmodels):

            cubedata = self.anom_cubes[i].data
            latitudes = self.anom_cubes[i].coord('latitude').points
            lon = self.anom_cubes[i].coord('longitude').points
            datatoplot, longitudes = (shiftgrid(180., cubedata,
                                                lon, start=False))
            #if (np.mod(i, 8) + 1) == 1:
            #    title_ = (MODELNAMES[i] + ':' +
            #              self.timeperiod + ' (model - MMM)')
            #else:
            #    title_ = (MODELNAMES[i])

            title_ = (MODELNAMES[i])
            self.plotmap(i, title_,
                         datatoplot, longitudes, latitudes, fig)


        return

    def plotmap(self, i, titlename, datatoplot, longitudes, latitudes, fig):
        """
        will plot the data in a map format

        """

        xplot = 4
        yplot = 4


        plotpos = np.mod(i, xplot * yplot) + 1
        plt.subplot(xplot, yplot, plotpos)
        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=-90.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        x, y = map(lons, lats)
        map.drawcoastlines(linewidth=0.5)

        V = np.arange(self.valmin, self.valmax, self.diff)
        cs = map.contourf(x, y, datatoplot, V, cmap=self.colormap,
                          extend='both')
        plt.title(titlename)


        if plotpos == (xplot * yplot) or (i + 1) == self.nmodels:
             # Shrink current axis by 20% and put a legend to the right
            plt.subplots_adjust(left=0.05, bottom=0.1, right=0.82, top=0.9,
                                wspace=0.1, hspace=0.0)

            cb_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])
           
            cbar = fig.colorbar(cs, cax=cb_ax, orientation='vertical')
            #cbar = plt.colorbar(fig, orientation='horizontal')
            #fig.colorbar(fix, ax=axs[:, col], shrink=0.6)
            cbar.ax.tick_params(labelsize=15)
            cbar.set_label(UNITS, fontsize=15)
            print('plotted colorbar')
            #plt.show()
            #plt.tight_layout()
            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.eps')
            plt.savefig(fileout, bbox_inches='tight')

            fileout = (self.filestart + np.str(np.int(np.ceil(i/8)))
                       + '.pdf')

            plt.savefig(fileout, bbox_inches='tight')
            plt.close()


##########################################################
# main program
# set up variable information

#FIELDNAME = 'NearSurfaceTemperature'
#UNITS = 'Celsius'
FIELDNAME = 'SST'
UNITS = 'Celsius'
#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'
#FIELDNAME = 'SST'
LINUX_WIN = 'l'

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'

MODELNAMES = ['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
             ]

#MODELNAMES = ['NorESM-L']



# set up cubelists to store data
mpwp_anom_cubes = iris.cube.CubeList([])
pi_anom_cubes = iris.cube.CubeList([])
anom_anom_cubes = iris.cube.CubeList([])


#################################################
# get mean data
mean_plio_cube = getmeanfield('mPWP')
mean_pi_cube = getmeanfield('pi')
mean_anom_cube = getmeanfield('anomaly')


for model, modelname in enumerate(MODELNAMES):
    model_plio_cube = getmodelfield(modelname, 'EOI400')
    model_pi_cube = getmodelfield(modelname, 'E280')
    print(modelname)
    if modelname == 'EC-Earth3.1' and FIELDNAME == 'SST':
       model_pi_cube.coord('latitude').bounds = None
       model_pi_cube.coord('longitude').bounds = None

    model_anom_cube = model_plio_cube - model_pi_cube

    mpwp_anom_cubes.append(model_plio_cube - mean_plio_cube)
    pi_anom_cubes.append(model_pi_cube - mean_pi_cube)
    anom_anom_cubes.append(model_anom_cube - mean_anom_cube)

##################################################
# plot the cubes for the model anomalies relative to the mean

obj = Plotalldata('mPWP', mpwp_anom_cubes)
obj.plotdata()

obj = Plotalldata('PI', pi_anom_cubes)
obj.plotdata()

obj = Plotalldata('mPWP-PI', anom_anom_cubes)
obj.plotdata()

#
::::::::::::::
precip_plot_for_IPCC_with_data.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on September 2020
# note this differs from DMC_for_IPCC in that it will overplot the
# data for the land as well as the ocean  (DMC_for_IPCC only overplots ocean 
# data)


#@author: earjcti
#
# This program plot a figure for IPCC.  This includes
# a) MPWP - PI SAT anomaly over land (MMM)
# b) MPWP - PI SST anomaly over ocean (MMM)
# c) data overplotted (land and ocean)
# d) Pliocene LSM


#import os
import numpy as np
import pandas as pd
#import scipy as sp
#import cf
import iris
#import iris.util
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
#from mpl_toolkits.axes_grid1 import make_axes_locatable
#import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#from netCDF4 import Dataset, MFDataset
#import iris.analysis.cartography
#import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
#import cf_units as unit
#from iris.experimental.equalise_cubes import equalise_attributes
import cartopy.crs as ccrs
#import matplotlib.ticker as mticker
#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
#from mpl_toolkits.basemap import Basemap

import sys

def make_cmap(colors, position=None, bit=False):
    '''
    I didn't write this I found it on the web.
    make_cmap takes a list of tuples which contain RGB values. The RGB
    values may either be in 8-bit [0 to 255] (in which bit must be set to
    True when called) or arithmetic [0 to 1] (default). make_cmap returns
    a cmap with equally spaced colors.
    Arrange your tuples so that the first color is the lowest value for the
    colorbar and the last is the highest.
    position contains values from 0 to 1 to dictate the location of each color.
    '''
    bit_rgb = np.linspace(0,1,256)
    if position == None:
        position = np.linspace(0,1,len(colors))
    else:
        if len(position) != len(colors):
            sys.exit("position length must be the same as colors")
        elif position[0] != 0 or position[-1] != 1:
            sys.exit("position must start with 0 and end with 1")
    if bit:
        for i in range(len(colors)):
            colors[i] = (bit_rgb[colors[i][0]],
                         bit_rgb[colors[i][1]],
                         bit_rgb[colors[i][2]])
    cdict = {'red':[], 'green':[], 'blue':[]}
    for pos, color in zip(position, colors):
        cdict['red'].append((pos, color[0], color[0]))
        cdict['green'].append((pos, color[1], color[1]))
        cdict['blue'].append((pos, color[2], color[2]))

    cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)
    return cmap


def customise_cmap():
    """
    customises colormap
    """
    colors = [(5, 48, 97),(6, 49, 98),(7, 51, 100),(8, 53, 102),
               (9, 55, 104),(11, 57, 106),(12, 59, 108),(13, 61, 110),
               (14, 63, 112),(15, 65, 114),(17, 67, 116),
               (18, 69, 118),(19, 71, 120),(20, 73, 121),(22, 75, 123),
               (23, 77, 125),(24, 79, 127),(25, 81, 129),(26, 82, 131),
               (28, 84, 133),(29, 86, 135),(30, 88, 137),(31, 90, 139),
               (32, 92, 141),(34, 94, 143),(35, 96, 145),
               (36, 98, 146),(37, 100, 148),(39, 102, 150),(40, 104, 152),
               (41, 106, 154),(42, 108, 156),(43, 110, 158),(45, 112, 160),
               (46, 113, 162),(47, 115, 164),(48, 117, 166),(49, 119, 168),
               (51, 121, 170),(52, 123, 171),(53, 125, 173),
               (54, 127, 175),(56, 129, 177),(57, 131, 179),(58, 133, 181),
               (59, 135, 183),(60, 137, 185),(62, 139, 187),(63, 141, 189),
               (64, 143, 191),(65, 145, 193),(67, 147, 195),(69, 148, 195),
               (71, 149, 196),(74, 150, 197),(76, 152, 197),
               (78, 153, 198),(81, 155, 199),(83, 156, 199),(86, 157, 200),
               (88, 159, 201),(90, 160, 202),(93, 161, 202),(95, 163, 203),
               (97, 164, 204),(100, 165, 204),(102, 166, 205),(105, 168, 206),
               (107, 169, 207),(109, 171, 207),(112, 172, 208),(114, 173, 209),
               (116, 175, 209),(119, 176, 210),(121, 177, 211),(124, 179, 211),
               (126, 180, 212),(128, 181, 213),(131, 183, 214),(133, 184, 214),
               (135, 185, 215),(138, 187, 216),(140, 188, 216),(143, 189, 217),
               (145, 191, 218),(147, 192, 219),(150, 193, 219),(152, 195, 220),
               (155, 196, 221),(157, 197, 221),(159, 198, 222),(162, 200, 223),
               (164, 201, 223),(166, 203, 224),(169, 204, 225),(171, 205, 226),
               (174, 207, 226),(176, 208, 227),(178, 209, 228),(181, 211, 228),
               (183, 212, 229),(185, 213, 230),(188, 214, 230),(190, 216, 231),
               (193, 217, 232),(195, 219, 233),(197, 220, 233),(200, 221, 234),
               (202, 223, 235),(204, 224, 235),(207, 225, 236),(209, 227, 237),
               (212, 228, 238),(214, 229, 238),(216, 230, 239),(219, 232, 240),
               (221, 233, 240),(224, 235, 241),(226, 236, 242),(228, 237, 243),
               (231, 239, 243),(233, 240, 244),
               (235, 241, 245),(238, 243, 245),(240, 244, 246),(243, 245, 247),
               (245, 246, 247),(247, 248, 248),(248, 248, 247),(248, 246, 245),
               (247, 243, 243),(247, 242, 241),(246, 240, 238),(246, 238, 236),
               (246, 235, 234),(245, 234, 232),(245, 232, 229),(244, 230, 227),
               (244, 227, 225),(243, 226, 223),(243, 224, 220),(242, 222, 218),
               (242, 220, 216),(241, 218, 214),(241, 216, 211),(240, 214, 209),
               (240, 211, 207),(240, 210, 205),(239, 208, 202),(239, 206, 200),
               (238, 203, 198),(238, 202, 196),(237, 200, 193),(237, 198, 191),
               (236, 195, 189),(236, 194, 187),(235, 192, 184),(235, 190, 182),
               (235, 187, 108),(234, 186, 178),(234, 184, 175),(233, 181, 173),
               (233, 179, 171),(232, 178, 169),(232, 176, 166),(231, 174, 164),
               (231, 172, 162),(230, 170, 160),(230, 168, 157),(230, 166, 155),
               (229, 163, 153),(229, 162, 151),(228, 160, 148),(228, 158, 146),
               (227, 156, 144),(227, 154, 142),(226, 152, 139),(226, 149, 137),
               (225, 147, 135),(225, 146, 133),(224, 144, 130),(224, 142, 128),
               (224, 140, 126),(223, 138, 124),(223, 135, 121),(222, 134, 119),
               (222, 132, 117),(221, 130, 115),(221, 128, 112),(220, 125, 110),
               (220, 124, 108),(219, 121, 106),(219, 120, 103),(219, 118, 101),
               (218, 115, 99),(218, 113, 97),(217, 112, 94),(217, 110, 92),
               (216, 108, 90),(216, 105, 88),(215, 104, 85),(215, 102, 83),
               (214, 100, 81),(214, 97, 79),(214, 96, 76),(211, 94, 76),
               (209, 92, 75),(207, 90, 74),(205, 88, 73),(203, 86, 72),
               (200, 84, 71),(198, 82, 70),(196, 80, 69),(194, 79, 68),
               (192, 77, 67),(190, 75, 67),(187, 73, 66),(185, 71, 65),
               (183, 69, 64),(181, 67, 63),(179, 65, 62),(177, 64, 61),
               (174, 62, 60),(172, 60, 59),(170, 58, 58),(168, 56, 58),
               (166, 54, 57),(163, 52, 56),(161, 50, 55),(159, 48, 54),
               (157, 47, 53),(155, 45, 52),(153, 43, 51),(150, 41, 50),
               (148, 39, 49),(146, 37, 49),(144, 35, 48),(142, 33, 47),
               (140, 32, 46),(137, 30, 45),(135, 28, 44),(133, 26, 43),
               (131, 24, 42),(129, 22, 41),(126, 20, 40),(124, 18, 40),
               (122, 16, 39),(120, 15, 38),(118, 13, 37),(116, 11, 36),
               (113, 9, 35),(111, 7, 34),(109, 5, 33),(107, 3, 32),
               (105, 1, 31),(103, 0, 31)]
    my_cmap = make_cmap(colors, bit=True)

    return my_cmap

def customise_cmap2():
    """
    as customise_cmap but 19 colors only + 2 white in middle added by Julia
    """
    colors = [(84, 48, 5), (113, 70, 16), (143, 93, 27), (173, 115, 38),
              (195, 137, 60), (206, 160, 97), (216, 182, 135),
              (227, 204, 173), (238, 226, 211), (248, 248, 247),
              (212, 230, 229), (176, 212, 209), (140, 194, 190),
              (103, 176, 170), (67, 158, 150), (44, 135, 127),
              (29, 110, 100), (14, 85, 74), (0, 60, 48)]
    my_cmap = make_cmap(colors, bit=True)
    return my_cmap

def get_lsm():
    """
    land sea mask is where the point is ocean in both pliocene and pi
    """
    lsm_pi_cube = iris.load_cube(LSM_PI_FILE)
    lsm_plio_cube = iris.load_cube(LSM_PLIO_FILE)
    lsm_cube_data = np.maximum(lsm_pi_cube.data, lsm_plio_cube.data)
    lsm_cube_ = lsm_pi_cube.copy(data=lsm_cube_data)
  
    return lsm_cube_, lsm_plio_cube


def get_model_data():
    """
    read in precipitation data and return
    """

    (lsm_cube, lsm_plio_cube) = get_lsm()

    precip_cube = iris.load_cube(PRECIP_MMM_FILE, 
                                 'TotalPrecipitationmean_anomaly')
  
   
    return precip_cube, lsm_plio_cube

 
def get_land_obs():
    """
    reads in the spredsheet from ulrich and returns precipitation
    """

    dfs = pd.read_excel(PRECIP_DATAFILE)
    sites = []
    lats = []
    lons = []
    precip = []
   
    row_locs = [2, 3, 4, 5, 6, 7, 8, 9, 11, 12]
    for rl in row_locs:
        precip_file = dfs.iloc[rl,11]
        if precip_file == '1000**':
            precip_file = 1000.
        print(rl, precip_file)
        if np.isfinite(precip_file):
           sites.append(dfs.iloc[rl, 0])
           lats.append(dfs.iloc[rl, 2])
           lons.append(dfs.iloc[rl, 3])
           precip.append(precip_file / 365.) # mm/year to mm/day

    return lats, lons, precip

def get_cru_precip(lats, lons):
    """
    get's the cru precip at the given latitude and longitude
    """
    
    crufile = ('/nfs/hera1/earjcti/regridded/CRUPRECIP/' + 
               'E280.TotalPrecipitation.allmean.nc')
    cube = iris.load_cube(crufile)
    print(cube.coord('latitude').points)
    
    cru_precip = np.zeros(len(lats))
    for i, lat in enumerate(lats):
        lat_ix = (np.abs(cube.coord('latitude').points - lat)).argmin()
        lon_ix = (np.abs(cube.coord('longitude').points - lons[i])).argmin()
        
        print(lat, cube.coord('latitude').points[lat_ix],
              lons[i], cube.coord('longitude').points[lon_ix] )


        cru_precip[i] = cube.data[lat_ix, lon_ix]
        if np.isfinite(cru_precip[i]):
            pass
        else:
            # get an average of surrounding ones
            surround = [cube.data[lat_ix + 1, lon_ix],
                        cube.data[lat_ix - 1, lon_ix],
                        cube.data[lat_ix, lon_ix + 1],
                        cube.data[lat_ix, lon_ix -1],
                        ]
            cru_precip[i] = np.nanmean(surround)

        # convert from mm/month to mm/day
        cru_precip[i] = cru_precip[i] * 12.0 / 365. 
           
    return cru_precip


   
def shift_lons(lons,lats,temp):
    """ 
    if two points are in the same location then shift longitude slightly so that both are 
    visible
    """

    new_lons =  np.zeros(np.shape(lons))
    new_lons[:] = lons[:]

    for i, lon in enumerate(lons):
        subscript_same = []
        for j in range(i+1, len(lons)):
            if (np.abs(lon - lons[j]) < 1.0 and np.abs(lats[i] - lats[j]) < 1.0):
                subscript_same.append(j)
                print(i,j)
        for s, subscript in enumerate(subscript_same):
            if lons[subscript] == new_lons[subscript]:
                new_lons[i] = lons[i] - 2.0
                print('here',i,new_lons[i],lons[i],lons[i]-0.5,s)
                new_lons[subscript] = lons[subscript] + 2.0 + (4.0 *s)
            
    print(lons[0],new_lons[0], new_lons[14], new_lons[15])
    return new_lons

def plot(model_cube, mask_cube, lats, lons, data):

    """
    plots the model anomaly with the data anomaly on top
    """

   
    # plot model
    vmin = -1.4
    vmax = 1.4
    incr = 0.1
    V = np.arange(vmin, vmax + incr, incr)
    mycmap = customise_cmap2()

    #brewer_cmap = cm.get_cmap('brewer_RdBu_11')
    ax = plt.axes(projection=ccrs.Robinson(central_longitude=0))
    cs = iplt.contourf(model_cube, levels=V,  extend='both',
                       cmap=mycmap)
    #cbar = plt.colorbar(cs,  orientation= 'horizontal',
    #                    ticks=[-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10])
    cbar = plt.colorbar(cs,  orientation= 'horizontal')
   
    cbar.set_label('mm/day')
    iplt.contour(mask_cube, colors='black', linewidths=0.1)
    plt.title('3.205Ma - PI precipitation anomaly')
    

    # overplot data 
  
    #norm = colors.Normalize(vmin = vmin, vmax = vmax)
    norm = colors.BoundaryNorm(boundaries=V, ncolors=mycmap.N)
 
    plt.scatter(lons, lats, c='black',  
                marker='o', s=90, transform=ccrs.Geodetic())

    plt.scatter(lons, lats, c=data,  marker='o', s=45,
                norm = norm , cmap=mycmap, transform=ccrs.Geodetic())
  
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Panom.png')
    plt.savefig('/nfs/hera1/earjcti/regridded/IPCC_Panom_.eps')


  
def main():
    """
    calling structure
    a) get's model data
    b) get's proxy data
    c) plots model data with proxy data on top
    """

    # get model data
    model_anom_cube, lsm_cube = get_model_data()

    # get land observations and cru precipitation at land points
    
    lats, lons, land_precip = get_land_obs()
    cru_land_precip = get_cru_precip(lats, lons)

    print('land precip obs',land_precip)
    print('cru precip',cru_land_precip)
   
    data_panom = land_precip - cru_land_precip
    print('precip anom',data_panom)
    
  
    plot(model_anom_cube, lsm_cube, lats, lons, data_panom)

##########################################################
# main program


LINUX_WIN = 'l'
FILESTART = '/nfs/hera1/earjcti/'

LSM_PLIO_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Plio_enh' 
            + '/Plio_enh/Plio_enh_LSM_v1.0.nc')
LSM_PI_FILE = (FILESTART + 'regridded/PlioMIP2_Boundary_conds/Modern_std' 
            + '/Modern_std/Modern_std_LSM_v1.0.nc')

PRECIP_MMM_FILE = FILESTART + 'regridded/TotalPrecipitation_multimodelmean.nc'  
PRECIP_DATAFILE = ('/nfs/hera1/earjcti/PLIOMIP2/proxydata/' + 
                 'PlioceneTerrestrial_IPCCAR6.xlsx')

main()

#sys.exit(0)
::::::::::::::
regrid_HadGEM3_clim.py
::::::::::::::
import iris 
import numpy as np
import numpy.ma as ma

def regrid_HadGEM(field, period):

    """
    creates the netcdf averaged files from HG3
    """

    periodalt = {'E280' : 'pi',
                 'Eoi400' : 'pliocene'}

    if field == 'NearSurfaceTemperature':
        fileend = ('/atmos/clims_hadgem3_' + periodalt.get(period) + 
                    '_airtemp_final.nc')
    if field == 'TotalPrecipitation':
        fileend = ('/atmos/clims_hadgem3_' + periodalt.get(period) + 
                    '_precip_final.nc')
  
    if field == 'SST':
       fileend = ('/ocean/clims_hadgem3_' + periodalt.get(period) + 
                    '_sst')
       if period == 'Eoi400':
           fileend = fileend + '_final.nc'
       else:
           fileend = fileend + '.nc'

    filein = ('/nfs/hera1/pliomip2/data/HadGEM3_new/climatologies/' 
          + period + fileend)

    cube = iris.load_cube(filein)
    cube.long_name = field
    if field == 'NearSurfaceTemperature':
        cube.data = cube.data - 273.15
        cube.units = 'Celsius'
    if field == 'TotalPrecipitation':
        cube.data = cube.data *60.*60.*24.
        cube.units = 'mm/day'
    print(cube)

    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())
    print('cubegrid',cubegrid)
    print('regridded_cube',regridded_cube)
   
    if field == 'SST':
        regridded_data = np.ma.asarray(regridded_cube.data) 
        
        for index, x in np.ndenumerate(regridded_data):
            if not np.isfinite(x):
                regridded_data.mask[index] = True
        
       
  

    fileout = ('/nfs/hera1/earjcti/regridded100/HadGEM3/' + period.upper() 
               + '.' + 
           field + '.mean_month.nc')

    iris.save(regridded_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    avg_cube =  regridded_cube.collapsed(['time'], iris.analysis.MEAN)
   
    fileout = ('/nfs/hera1/earjcti/regridded100/HadGEM3/' 
               + period.upper() + '.' + 
               field + '.allmean.nc')

    iris.save(avg_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    return (regridded_cube, avg_cube)


########################################
def avg_NorESM_SST(model, period):
    """
    averages NorESM that the NorESM group regridded
    """

    filename = ('/nfs/hera1/pliomip2/data/' + model + 
                 '/' + model + '_' + period + '.sst.climo.nc')
    cubeorig = iris.load_cube(filename)
    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    cube = cubeorig.regrid(cubegrid, iris.analysis.Linear())


    fileout = ('/nfs/hera1/earjcti/regridded100/'+ model +'/' 
               + period.upper() +  
               '.SST.mean_month.nc')

    iris.save(cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    avg_cube =  cube.collapsed(['time'], iris.analysis.MEAN)

    fileout = ('/nfs/hera1/earjcti/regridded100/' + model + '/' + period.upper() + 
               '.SST.allmean.nc')

    iris.save(avg_cube, fileout, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    return cube, avg_cube

################################################
def means_to_txt(modelname, annmeancube, avgcube, field, period):
    """ 
    creates the text file from HadGEM3
    """ 

    textout = ('/nfs/hera1/earjcti/regridded100/' + modelname 
               + '/' + period.upper() +
               '.' + field + '.data.txt')
    file1 =  open(textout, "w")

    # get mean field for cube

    avgcube.coord('latitude').guess_bounds()
    avgcube.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(avgcube)
    tempcube = avgcube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data 


    # get mean for each latitude
    tempcube = avgcube.collapsed(['longitude'], iris.analysis.MEAN)
    
    meanlat = tempcube.data 
    meanlat = np.squeeze(meanlat)
  
    
    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2))+', 0.0\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    annmeancube.coord('latitude').guess_bounds()
    annmeancube.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(annmeancube)
    tempcube = annmeancube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    if field == 'NearSurfaceTemperature':
        meanmon = tempcube.data -273.15
    else:
        meanmon = tempcube.data 

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', 0.0\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(avgcube.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', 0.0\n')

    file1.close()


#############################################

periods = ['Eoi400','E280']
fields = ['NearSurfaceTemperature','SST']
#fields = ['TotalPrecipitation']

for period in periods:
    for field in fields:
        (annmeancube, avgcube) = regrid_HadGEM(field, period)
        means_to_txt('HadGEM3',annmeancube,avgcube, field, period)


######
# also average noresm here because we need to do it somewhe
#models = ['NorESM1-F','NorESM-L']
#for period in periods:
#    for model in models:
#        annmeancube, avgcube = avg_NorESM_SST(model, period)
#        means_to_txt(model, annmeancube, avgcube, 'SST', period)
        
::::::::::::::
regrid_noresm.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019
"""
The NorESM group have now uploaded averaged data for SST.  We will need to 
get this in the required format for our paper.  
"""


import numpy as np
import iris
import iris.analysis.cartography
import iris.coord_categorisation
import sys


##############################################
def write_avg_to_file(moncube, anncube):
    """
    average the data and write to a file
    """


    textout = FILEDIR + PERIOD + '.SST.data.txt'

    file1 =  open(textout, "w")

    # 1, globalmean

    anncube.coord('latitude').guess_bounds()
    anncube.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(anncube)
    tempcube = anncube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data
    stdevann=-999.999


    # get mean for each latitude
    tempcube = anncube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)
    stdevlat = np.zeros(np.shape(meanlat)) - 1000.



    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    moncube.coord('latitude').guess_bounds()
    moncube.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(moncube)
    tempcube = moncube.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data
    stdevmon = np.zeros(12) - 1000.

    for i in range(0, 12):
        file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(anncube.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    
    
def main():
    """
    reprocesses the NorESM SST data.  
    We have a monthly averaged netcdf file.  
    We need an annual averaged netcdf file + a text file containing all the averages
    """

    filein = FILEDIR + PERIOD + '.SST.mean_month.nc'
    cube = iris.load_cube(filein, 'Ocean surface temperature')
    
    # write mean cube to a file
    meancube = cube.collapsed(['time'],  iris.analysis.MEAN)
    iris.save(meancube, FILEDIR + PERIOD + '.SST.allmean.nc', 
              netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)
    
    # get text data

    write_avg_to_file(cube, meancube)
    print('end of prog')
   
    
##########################################################
# main program


print('start')
LINUX_WIN = 'l'
MODELNAME = 'NorESM1-F'
PERIOD = 'E280' #EOI400 E280

if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART = '/nfs/hera1/earjcti/'
    
FILEDIR = (FILESTART + 'regridded/' + MODELNAME + '/')
main()
::::::::::::::
regrid_ocn_100yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
        if exptname  == 'E280':
            lsmfile = lsmstart+modelname+'/lsm.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        for name,  dimension in src.dimensions.iteritems():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.iteritems():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube


def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        print(cube.units)
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                       'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b.e12.B1850.f09_g16.preind.' + 
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

   
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear


        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if modelname  == 'EC-Earth3.1': # all fields in one file
        cube = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI-CGCM2.3'):
        cube = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube = get_miroc_tos()
    elif (modelname  == 'GISS'):
        cube = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        cube = get_cesm12(exptnamein)
    else:
        cube = iris.load_cube(filename)


    ndim = cube.ndim




    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid UofTdata or a field that was originally on a tripolar grid
    print('julia1')
    if ((modelname   == 'UofT-CCSM4')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname  == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'UofT' or modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' or modelname == 'CESM1.2'
        or modelname == 'CCSM4-1deg'):
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            cube.data = cube.data* 60. *60. *24. *1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'UofT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname  == 'UofT':
        # we need to add the missing time coordinate
        points = (np.arange(0, 1200)*30)+15. # go for middle of month
        u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

        regridded_cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    elif (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
          modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate



       # end of Uof T loop

    print('julia2')
    #print(regridded_cube.coord('time'))

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".0806.0905"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'UofT':
        if linux_win  == 'l':
            filename = filestart+modelname+'/'
            filename = filename+'UofT-CCSM4/'+exptnamein+'/'+atm_ocn_ind.get(fieldnamein)+'/'
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '_'+atm_ocn_ind.get(fieldnamein)+'_'+exptnamein+
                      '_'+modelname+'-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI-CGCM2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1':
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        print(fielduse)
        filename = (filestart+'EC-Earth3.1/'
                  +ECearth_EXPT.get(exptnamein)
                  +'.EC-Earth3.1.surface.nc')
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "IPSLCM5A2" # MIROC4m  COSMOS UofT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS
                   # new to this version CCSM4-2deg, CESM1.2
                   # CCSM4-1deg

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr']
#exptnamein = ['Eoi400']

#fieldnamein = ['pr']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['tos']
exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
regrid_ocn_50yr_avg_pre_HadGEM.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    
    if fieldnamein  == 'pr':
       cube.data = cube.data * 60. *60. *24. *1000.
       cube.name = 'Total precipitation'
       cube.long_name = 'Total precipitation'
       cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e21.B1850.f09_g17.' +
                           'CMIP6-piControl.001.cam.h0.'+
                           'LANDFRAC.1300.1399.nc')
            else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'
       or modelname == 'CESM2'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4-1deg'
          or modelname == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    else:
        cube100 = iris.load_cube(filename)


  

    ###########################################
    # reduce number of years to 50

    cube = reduce_years(cube100)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4-1deg'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }
    
    CESM2_TIME = {"E280" : ".1300.1399",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        if linux_win  == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        filename = (filename +  fielduse +
                      '_' + atm_ocn_ind.get(fieldnamein) +
                      '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CESM2" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['Eoi400','E280']

fieldnamein = ['tos']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
regrid_ocn_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname  == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 1
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        print(filenameuse)
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    cube = iris.util.squeeze(cube_temp)

    #if model  == 'MRI2.3':
    #    cube_temp.coord('pressure level').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'SST':
    #    cube_temp.coord('unspecified').rename('surface')

    #if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
    #    cube_temp.coord('ht').rename('surface')


    #cube_temp.coord('surface').points = 0.
    #cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_HadGEM3_atm(fielduse,fieldnamein):
    """
    """
    cube = iris.load_cube(filename)
    cube.coord('t').rename('time')
    print('julia',fielduse)
    if fielduse == 'Temperature T':
        cube.convert_units('Celsius')
    print(cube.data)
    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None

    #print(cube.name, fielduse, fieldname.get(fieldnamein))
  
    return cube

def get_HadGEM3_tos(exptin, fielduse, fieldnamein):
  
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    if exptin == 'Eoi400':
    # eoi400
        startyear = 2334
        endyear = 2434
        extra = 'v963'

    if exptin == 'E280':
    #e280
        startyear=1950
        endyear = 2050
        extra='q637'
       
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        if year >= 2394: 
            extra = 'x150'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    cube.var_name = fieldnamein
    cube.long_name = fieldname.get(fieldnamein)
    cube.standard_name = None


    return cube





def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_noresm_400(fieldname):
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename, fieldname)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
   # points = (np.arange(0, 1200)*30)+15. # go for middle of month
   # u  =  unit.Unit('days since 0800-01-01 00:00:00',
   #            calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
   # cube.add_dim_coord(iris.coords.DimCoord(points,
   #             standard_name = 'time',  long_name = 'time',
   #             var_name = 'time',
   #             units = u,
   #             bounds = None,
   #             coord_system = None,  circular = False), 0)
    
   # if fieldnamein  == 'pr':
   #    cube.data = cube.data * 60. *60. *24. *1000.
   #    cube.name = 'Total precipitation'
   #    cube.long_name = 'Total precipitation'
   #    cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    print(allcube)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400' :
            if modelname == 'CESM2':
               filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                          'b.e21.B1850.f09_g17.' + 
                          'PMIP4-midPliocene-eoi400.001.'+
                          'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280' or exptnamein == 'E400':
          #  if modelname == 'CESM2':
          #      filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
          #                 'b.e12.B1850.f09_g17.' +
          #                 'CMIP6-piControl.001.cam.h0.'+
          #                 'LANDFRAC.1100.1200.nc')
          #  else:
                 filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                           'b.e12.B1850.f09_g16.preind.' + 
                           'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr, fieldnamein):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    print(fieldnamein)
    if fieldnamein == 'clt':
        cube50yr = cube100yr
    else:
        cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4'
       or modelname == 'CESM2' or modelname == 'CCSM4-Utr'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

  
    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)

    if avg100yr == 'y':
        regridded = 'regridded100/'
    else:
        regridded = 'regridded/'


    # outfile
    if linux_win  == 'l':
        print(regridded, modelname, exptnameout, fieldnameout)
        outstart = ('/nfs/hera1/earjcti/'+ regridded +modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/' + regridded
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' + regridded
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (modelname  == 'EC-Earth3.1' or
       modelname == 'EC-Earth3.3'): # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'HadGEM3' and fieldnamein  == 'tos'):
        cube100 = get_HadGEM3_tos(exptnamein, fielduse, fieldnamein)
    elif (modelname  == 'HadGEM3' and fieldnamein  != 'tos'):
        cube100 = get_HadGEM3_atm(fielduse, fieldnamein)
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' 
          or modelname == 'CCSM4'
          or modelname == 'CESM2'):
        print('before',filename)
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'NorESM1-F' and exptnamein == 'E400'):
        cube100 = get_noresm_400(fielduse)
    else:
        cube100 = iris.load_cube(filename)

   
     

    ###########################################
    # reduce number of years to 50

    if avg100yr == 'y':
        cube = cube100
    else:
        cube = reduce_years(cube100, fielduse)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (modelname  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3'
             or modelname  == 'IPSLCM5A' or modelname == 'HadGEM3'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-Utr' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' 
        or modelname == 'CESM1.2'
        or modelname == 'CESM2'
        or modelname == 'CCSM4'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname == 'EC-Earth3.3'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or 
        modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    if mean_data.coord('latitude').has_bounds():
        mean_data.coord('latitude').bounds
    else:
        mean_data.coord('latitude').guess_bounds()

    if mean_data.coord('longitude').has_bounds():
        mean_data.coord('longitude').bounds
    else:
        mean_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    if mean_year_data.coord('latitude').has_bounds():
        mean_year_data.coord('latitude').bounds
    else:
        mean_year_data.coord('latitude').guess_bounds()

    if mean_year_data.coord('longitude').has_bounds():
        mean_year_data.coord('longitude').bounds
    else:
        mean_year_data.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')


    if mean_mon_data.coord('latitude').has_bounds():
        mean_mon_data.coord('latitude').bounds
    else:
        mean_mon_data.coord('latitude').guess_bounds()

    if mean_mon_data.coord('longitude').has_bounds():
        mean_mon_data.coord('longitude').bounds
    else:
        mean_mon_data.coord('longitude').guess_bounds()
    
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    #plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos",
        "clt" : "clt"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst",
        "tasE400" : "Reference height temperature",
        "prE400" : "Total (convective and large-scale) precipitation rate (liq + ice)",
        "tosE400" : "Ocean surface temperature"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp",
                     "clt" : "clt_Amon_CESM1.0.5_"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    HadGEM3_FILEFIELD = {"tas" : "airtemp",
                         "pr" : "precip",
                         "clt" : "totalcloud"}

    HadGEM3_LONGFIELD = {"tas" : "Temperature T", 
                         "pr" : "Total precipitation rate",
                         "clt" : "Total cloud"}

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                    "E400": "b.e21.B1850.f09_g17.CMIP6-piControl.400.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }

    HadGEM3_EXPT = {"Eoi400" : "pliocene",
                    "E280" : "pi"}
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1100001.120012"
                   }
    
    CESM2_TIME = {"E280" : ".110001-120012",
                  "E400" : ".0801.0900",
                  "Eoi400" : ".110001.120012"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon",
                 "clt":"Amon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        if linux_win  == 'l':
#            filename = filestart + 'UofT/'
#            filename = (filename + 'UofT-CCSM4/for_julia/' + 
#                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
            filename = (filestart + 'UofT/UofT-CCSM4/' + exptnamein + 
                        '/Amon/1x1_grid/' + fielduse + '_Amon_UofT-CCSM4_'
                        + CCSM4_UofT_TIME.get(exptnamein) + '.nc')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        
        
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
        if exptnamein == 'E400':
            filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_TREFHT_PRECT_month.nc')
            fielduse = NorESM_FIELDS.get(fieldnamein + 'E400')
            if fieldnamein == 'tos':
                filename = (filestart + modelname + '/CO2_400/' + 
                        'NorESM1-F_E400_SST_month.nc')
          
            

    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'Utrecht/CESM1.0.5/' + exptnamein + '/' +
                  CCSM42_FIELDS.get(fieldnamein) +
                  exptnamein + '_r1i1p1f1_gn_275001-285012.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            if exptnamein == 'Eoi400' or exptnamein == 'E400':
                filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
                filename = [filename1, filename2]
                fielduse = ['PRECC', 'PRECL']
            if exptnamein == 'E280':
                filename = (filestart + 'NCAR/b.e21.B1850.f09_g17.' + 
                            'CMIP6-piControl.001.cam.h0.PRECT.110001-120012.nc')
                fielduse = 'PRECT'
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            print(exptnamein)
            filename=(filestart + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
        if fieldnamein =='totcloud':
            filestart='/nfs/hera1/earjcti/PLIOMIP2/CESM2/clt_Amon_CESM2_'
            fielduse = 'clt'
            if exptnamein == 'Eoi400':
                filename = (filestart + 'midPliocene-eoi400_r1i1p1f1_'+
                            'gn_015101-020012.nc')
            if exptnamein == 'E280':
                filename = (filestart +'piControl_r1i1p1f1_gn_090001-099912.nc')
            
    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
                    
    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if fieldnamein == 'tos':
            fielduse = 'sea_surface_temperature'
            filename = (filestart + exptnamein + '/ocean/sst_sal_temp' 
                        + '/new_nemo_b')
        else:
            fielduse = HadGEM3_LONGFIELD.get(fieldnamein)
            filename = (filestart + exptnamein + '/atmos/times_hadgem3_' + 
                        HadGEM3_EXPT.get(exptnamein) + '_' +
                        HadGEM3_FILEFIELD.get(fieldnamein) + '_final.nc')
       
     
    print(fielduse, filename)
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "NorESM1-F" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                   # new to this version
                   # HadGEM3
                  

exptname  =  {
        "E280" : "E280",
        "Eoi280" : "EOI280",
        "Eoi350" : "EOI350",
        "Eoi400" : "EOI400",
        "Eoi450" : "EOI450",
        "Eoi560" : "EOI560",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi280" : "eoi280",
        "Eoi350" : "eoi350",
        "Eoi400" : "eoi400",
        "Eoi450" : "eoi450",
        "Eoi560" : "eoi560",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST",
        "clt" : "totcloud"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
#exptnamein = ['Eoi450']
avg100yr = 'n'

#fieldnamein = ['tas']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

fieldnamein = ['tos']
#exptnamein = ['Eoi400', 'E280','E560']
exptnamein = ['E400']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
    #filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])
#        sys.exit(0)
        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
regrid_ocn_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 3rd 2019


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 100 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#
# it can currently do MIROC4 and COSMOS


import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname,lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse  == "sst":
        if exptname  == 'Eoi400':
            lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
        if exptname  == 'E280':
            lsmfile = lsmstart+modelname+'/lsm.nc'
      
        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    return(cube)

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model  == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model  == 'HadCM3' and fielduse  == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))


    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)
    
    if fieldnamein  == 'pr':
       cube.data = cube.data * 60. *60. *24. *1000.
       cube.name = 'Total precipitation'
       cube.long_name = 'Total precipitation'
       cube.units = 'mm/day'
    
    
    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse_)
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]
            
    return singlecube
    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    
    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
       cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
       cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
       if fieldnameout == 'TotalPrecipitation':
           cube = cube1 + cube2
       if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
           cube1.convert_units('Celsius')
           cubedata = np.where((cube2.data > 0.01), 
                               -1.8, cube1.data)
           cube = cube1.copy(data=cubedata)
    else:
       cube = get_cesm12_singlecube(filename, fielduse)
    
    # put units as celcius if required
    if fielduse == 'TREFHT':
        print(cube.units)
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b40.B1850.f09_g16.PMIP4-pliomip2.' + 
                       'cam.h0.LANDFRAC.0851.0950.nc')
        if exptnamein == 'E280':
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' + 
                       'b.e12.B1850.f09_g16.preind.' + 
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')
   
        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01,  1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data  =  np.ma.array(cube.data,  mask = mymask)
    
    
    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
       or modelname == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = fieldnameout
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = fieldnameout

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=fieldnameout

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=fieldnameout
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=fieldnameout

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """


    print('moodelname is', modelname)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if linux_win  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+modelname+'/'+exptnameout+'.'+
        fieldnameout+'.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +modelname+'\\'+exptnameout+'.'+fieldnameout+'.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if modelname  == 'EC-Earth3.1': # all fields in one file
        cube100 = get_ecearth_cube(exptnamein,lsmstart)
    elif (modelname  == 'HadCM3' or modelname  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(modelname)
    elif ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (modelname  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (modelname  == 'MIROC4m' and fieldnamein  == 'tos'):
        cube100 = get_miroc_tos()
    elif (modelname  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (modelname  == 'CCSM4-2deg'):
        cube100 = get_ccsm4_2deg()
    elif (modelname  == 'CESM1.2' or modelname == 'CCSM4-1deg'):
        cube100 = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot(fieldnamein)
    else:
        cube100 = iris.load_cube(filename)


  

    ###########################################
    # reduce number of years to 50

    cube = reduce_years(cube100)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((modelname   == 'CCSM4-UoT')
        or (modelname  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (modelname  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname  == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if (modelname  == 'EC-Earth3.1' or modelname  == 'IPSLCM5A'
             or modelname  == 'IPSLCM5A2' or modelname  == 'IPSLCM6A'
             or modelname == 'CCSM4-2deg' or modelname =='GISS2.1G'):
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if (modelname  == 'NorESM1-F' 
        or modelname  == 'NorESM-L' or modelname == 'CESM1.2'
        or modelname == 'CCSM4-1deg'):
        print('regridded_cube.units',regridded_cube.units)
        print('j1',regridded_cube.data[:,0])
       
       # if precipitation is in m/s convert to mm/day
        if fieldnamein  == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24. *1000.
            print('j2',regridded_cube.data[:,0])
            cube.data = cube.data* 60. *60. *24. *1000.
            print('j3',regridded_cube.data[:,0])
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if (modelname  == 'CCSM4-UoT' or modelname  == 'NorESM1-F' or modelname  == 'NorESM-L'
        or modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'
        or modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
        or modelname  == 'HadCM3' or modelname == 'GISS2.1G'):
         # convert to celcius
        if fieldnamein  == 'tas' or fieldnamein  == 'tos':
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


 
        
    if (modelname  == 'COSMOS' or modelname  == 'MIROC4m' or
        modelname  == 'IPSLCM6A' or modelname  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    #print(regridded_cube.coord('time'))

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg '+fieldnamein)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS  = {"pr" : "pr",
        "tas" : "tas",
        "sic" : "SeaIceAreaFraction",
        "tos" : "tos"
        }

    COSMOS_FIELDS  = {"pr" : "TotalPrecip",
        "tas" : "NearSurfaceAirTemp",
        "sic" : "SeaIceAreaFraction",
        "tos" : "SeaSurfaceTemp"
        }

    ECearth_FIELDS  = {"pr" : "totp",
        "tas" : "tas",
        "tos" : "sst",
        "sic" : "SeaIceAreaFraction"
        }

    IPSLCM5A_FIELDS  = {"pr" : "TotalPrecip_pr",
        "tas" : "NearSurfaceTemp_tas",
        "sic" : "SeaIceAreaFraction",
        "tos": "SeasurfaceTemp_sst"
        }

    NorESM_FIELDS = {"pr" : "PRECT",
        "tas" : "TREFHT",
        "sic" : "SeaIceAreaFraction",
        "tos" : "sst"
        }
    
    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }
    
    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                     "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".0851.0950",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    atm_ocn_ind = {"tas": "Amon",
                 "pr": "Amon",
                 "tos":"Omon"}
    cosmos_version = {"tas": "",
                 "pr": "",
                 "tos":"_remapbil"}

    # get names for each model
    if modelname   ==  'MIROC4m':
        filename = filestart+modelname+'/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename+fielduse+
                      '/MIROC4m_'+exptnamein+'_'+atm_ocn_ind.get(fieldnamein)+'_'+fielduse+'.nc')
    if modelname   ==  'COSMOS':
        if linux_win  == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename+exptnamein+'.'+fielduse+
                      '_CMIP6_name_'+fieldnamein+
                      '_2650-2749_monthly_mean_time_series'+
                      cosmos_version.get(fieldnamein)+'.nc')
    if modelname   ==  'CCSM4-UoT':
        if linux_win  == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        
        filename = (filename +  fielduse +
                      '_' + atm_ocn_ind.get(fieldnamein) +
                      '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname  == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart+'LEEDS/HadCM3/'+exptuse+'/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                      +exptuse+'.'+fielduse+'.')
    if modelname  == 'EC-Earth3.1':
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        print(fielduse)
        filename = (filestart+'EC-Earth3.1/'
                  +ECearth_EXPT.get(exptnamein)
                  +'.EC-Earth3.1.surface.nc')
    if modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')

    if modelname  == 'NorESM1-F' or modelname  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart+modelname+'/'+modelname+'_'+
                 exptnamein+'_'+fielduse+'.nc')
    if modelname  == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein  == 'tos':
            filename = (filestart+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME_ALT.get(exptnamein)+'_rectilinear.nc')
        else:
            print(filestart, modelname, fielduse, IPSLCM6A_TIME.get(exptnamein), atm_ocn_ind.get(fieldnamein))
            filename = (filestart+modelname+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+'_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptnamein)+'.nc')
    if modelname  == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME1.get(exptnamein)+'.nc')
        filename.append(filestart+modelname+'/'+exptuse+'/'+fielduse+
                  '_'+atm_ocn_ind.get(fieldnamein)+
                  '_GISS-E2-1-G_'+GISS_TIME2.get(exptnamein)+'.nc')


    if modelname == 'CCSM4-2deg':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein
        
    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
    if modelname == 'CCSM4-1deg':
        if fieldnamein == 'pr':
            # this has been passed in two files. 
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'PRECL' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'TS' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  'ICEFRAC' +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(fieldnamein) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)
            
      
    retdata = [fielduse, filename]
    return(retdata)


##########################################################
# main program

filename  =  ' '
linux_win  =  'l'
modelname  = "CCSM4-2deg" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # new to this version CCSM4-2deg, CESM1.2
                   # CCSM4

exptname  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

fieldname  =  {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['Eoi400','E280']

#fieldnamein = ['pr']
fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win  == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if ((modelname  == 'IPSLCM5A' or modelname  == 'IPSLCM5A2'
             or modelname == 'CCSM4-2deg')
            and (fieldnamein[field]  == 'tos')):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if (modelname  == 'IPSLCM6A' or modelname  == 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]
     

        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
regrid_ocn_tripolar.py
::::::::::::::
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#Created on June2019 2019


#
# This very simple program will convert data that is on a tripolar grid onto a 
# rectilinear grid.  It will still need to be passed through regrid_ocn in order
# to calculate means etc.


import numpy as np
import cf
import iris
#import cfplot as cfp
#import matplotlib.pyplot as plt
from netCDF4 import Dataset, MFDataset
#import sys
#import os


def reformat_with_iris(exptnamein):
    """
    we are going to load the data as an iris cube and add some
    auxillary coordinates and then write out to a file called temporary.nc
    
    the data is currently in filename
    """
    
    print('reformatting ',filename)
    
    origcube = iris.load_cube(filename, 'Near-Surface Air Temperature')
    latcube = iris.load_cube(filename, 'array of t-grid latitudes')
    loncube = iris.load_cube(filename, 'array of t-grid longitudes')
    

    print(origcube)
   
    # promote the auxillary coordinates to dimension coordinates
    nt, ny, nx = origcube.shape
    origcube.coord('nlat').points=np.arange(0,ny,1)
    origcube.coord('nlat').rename('y')
    origcube.coord('y').var_name='y'
    origcube.coord('y').long_name=None
    origcube.coord('y').units=None
    origcube.coord('nlon').points=np.arange(0,nx,1)
    origcube.coord('nlon').rename('x')
    origcube.coord('x').var_name='x'
    origcube.coord('x').long_name=None
    origcube.coord('x').units=None
    print('j3',origcube)
    iris.util.promote_aux_coord_to_dim_coord(origcube, 'y')
    iris.util.promote_aux_coord_to_dim_coord(origcube, 'x')
    print(origcube.coord('x'))
    
 
    
    
    # add an auxillary coordinate for latitude and longitude these are 
    # 2d coordinates
    loncoord=iris.coords.AuxCoord(loncube.data,standard_name='longitude', 
                                  long_name='Longitude',var_name='nav_lon',
                                  units='degrees_east')
    latcoord=iris.coords.AuxCoord(latcube.data,standard_name='latitude', 
                                  long_name='Latitude',var_name='nav_lat',
                                  units='degrees_north')


    origcube.add_aux_coord(loncoord,[1,2])
    origcube.add_aux_coord(latcoord,[1,2])
    
    print('j6')
    print(origcube)
    print(origcube.coord('latitude'))
    #sys.exit(0)
    
    
    iris.save(origcube, exptnamein + '_temporary.nc', 
              fill_value=2.0E20)
    print('saved')

    


#####################################
def regrid_data(fieldnamein,fieldnameout,exptnamein,exptnameout,filename,modelname,linux_win,fielduse,filenameout):

   
    
    print('moodelname is',modelname)
    print('filename is',filename)
    print('fielduse is',fielduse)
      
    
    if ((modelname=='IPSLCM5A') and exptnamein=='Eoi400'):
       # there is a bit of an error in the file calendar so we will 
       # copy the data to a new file but without the error
       with Dataset(filename) as src, Dataset("temporary.nc", "w",format='NETCDF3_CLASSIC') as dst:
        # copy attributes
            for name in src.ncattrs():
                dst.setncattr(name, src.getncattr(name))
                #print('att',name)   
                # copy dimensions
            for name, dimension in src.dimensions.iteritems():
              
                if name != 'tbnds':   # don't copy across time counter bounds
                   # if fielduse=='SeasurfaceTemp_sst' and name=='y':
                   #     name='latitude'
                   # if fielduse=='SeasurfaceTemp_sst' and name=='x':
                   #     name='longitude'
                    dst.createDimension(name, (len(dimension)))
           
                     
            # copy all file data 
            for name, variable in src.variables.iteritems():
                print('name is',name,variable)
                if name !='time_counter_bnds' and name!='time_centered':
                    x = dst.createVariable(name, variable.datatype, 
                                               variable.dimensions)
                       
                    if name=='time_counter':
                    # convert from seconds to days and start at middle of month
                        dst.variables[name][:] = (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                    else:
                        dst.variables[name][:] = src.variables[name][:]
                    # copy attributes for this variable
                    for ncattr in src.variables[name].ncattrs():
                        attribute=src.variables[name].getncattr(ncattr)
                        #print('j2',name,ncattr,attribute)
                           
                        if ncattr=='calendar' and exptnamein=='Eoi400':
                            dst.variables[name].setncattr(ncattr,'360_day')
                        else:
                            if (ncattr=='units' and name=='time_counter'):
                            # change units from seconds to days
                                dst.variables[name].setncattr(ncattr,attribute.replace('seconds','days'))
                            else:
                                dst.variables[name].setncattr(ncattr,attribute) 
               
      
    
        
       origf=cf.read_field('temporary.nc')
       print('read copied dataset')
    else:
       if (modelname == 'IPSLCM6A'):
           print (filename)
           origf = cf.read_field(filename, select='sea_surface_temperature')
       elif (modelname == 'CESM1.0.5'):
           #origf = cf.read(filename)
           #print(origf)
           #sys.exit(0)
           reformat_with_iris(exptnamein) # the data is not in a very good format
                                # we will reformat and write to temporary.nc
           
           print('about to read temporaray.nc')
           
           origf=cf.read_field(exptnamein + '_temporary.nc', )
           # iterate over auxillary coordinates and see if they are
           # longitud eor latitude
           for aux in origf.auxs():
               print (origf.auxs(aux))
           a=origf.aux('latitude')
           a.units=('degrees_north')
           print(a)
           
           b=origf.aux('longitude')
           b.units=('degrees_east')
           print(b)
           print(origf)
           #sys.exit(0)  
               
           #for key, aux in self.auxs(ndim=2).iteritems():
           #     if aux.Units.islongitude:
           #sys.exit(0)
           print (origf)
           print (origf.data_axes())
           print (origf.coords())
           
         
       elif (modelname == 'HadGEM3'):
           
       else:
           origf=cf.read_field(filename)
        
        
    print (origf)
   
    
    gridf=cf.read_field('one_lev_one_deg.nc')
    #print  gridf
    
    print('about to regrid')
   
    # assume tripoplar grid
    if modelname == 'CESM1.0.5':
       regridf=origf.regrids(gridf,
                          method='bilinear',
                          src_axes={'X': 'ncdim%x','Y':'ncdim%y'},
                          src_cyclic=True)
    else:   
        regridf=origf.regrids(gridf,
                          method='bilinear',
                          src_axes={'X': 'ncdim%x','Y':'ncdim%y'},
                          src_cyclic=True)
    print('regridded')
    
    # see if we can remove auxillary coordinates
    if (modelname=='IPSLCM5A' or modelname=='IPSLCM5A2'):
        regridf.remove_item(description='T',role='a')
   
    
   
    # write to a temporary file so that we can read in as an iris cube and do all our iris analysis 
    # in exactly the same way as before
    
    print(filename)
    print(filenameout)
    cf.write(regridf,filenameout,fmt='NETCDF4_CLASSIC')
 
   
  
    
   
  

#############################################################################
def getnames(modelname,fieldname,exptname): 
    """
    parameters: 
        modelname - the model
        fieldname - the name of the field ie 'tos'
        exptname - the name of the experiment ie [Eoi400]
        
    returns:
        fielduse - the name of the field in the file
        filename - the input file
        filenameout = the output file
    
    this program will get the names of the files and the field for each
    of the models   
    
    """
    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                    "tas" : "NearSurfaceAirTemp",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "SeaSurfaceTemp"
                    }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }
    
    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }
    
    NorESM_FIELDS = {"pr" : "PRECT",
                     "ts" : "TREFHT",
                     "sic" : "SeaIceAreaFraction"
                     }
    
     
    CESM105_FIELDS = {"pr" : "TotalPrecip",
                      "tas" : "NearSurfaceAirTemp",
                      "sic" : "SeaIceAreaFraction",
                      "tos" : "SeaSurfaceTemp"
                      }

    ECearth_EXPT = {"Eoi400" : "mPlio",
                    "E280" : "PI"
                    }
    
    IPSLCM5A_EXPT = {"Eoi400" : "Eoi400",
                     "E280" : "PI"
                     }
    
    IPSLCM5A_TIME = {"Eoi400" : "3581_3680",
                     "E280" : "3600_3699"
                     }
    
    IPSLCM5A21_TIME = {"Eoi400" : "3381_3480",
                     "E280" : "6110_6209",
                     }
    
    IPSLCM6A_TIME = {"Eoi400" : "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                     "E280":"piControl_r1i1p1f1_gn_285001-304912",
                     }
    
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"
                   }

    # get names for each model
   
    if modelname == 'COSMOS':
        if linux_win=='l':
            filename=filestartin+'/AWI/COSMOS/'
            filename=filename+exptname+'/'
        else:
            filenameout=filestartout+'/COSMOS/'
        fielduse=COSMOS_FIELDS.get(fieldname)
        filename=(filename+exptname+'.'+fielduse+
                      '_CMIP6_name_'+fieldname+
                      '_2650-2749_monthly_mean_time_series.nc')
        filenameout=(filestartout+'COSMOS/'+exptname+'.'+fielduse+
                      '_CMIP6_name_'+fieldname+
                      '_2650-2749_monthly_mean_time_series_rectilinear.nc')
   
   
    if modelname=='IPSLCM5A' or modelname=='IPSLCM5A2':
        exptuse=exptname_l.get(exptname)
        if modelname=='IPSLCM5A':
            timeuse=IPSLCM5A_TIME.get(exptname)
        if modelname=='IPSLCM5A2':
            timeuse=IPSLCM5A21_TIME.get(exptname)
        fielduse=IPSLCM5A_FIELDS.get(fieldname)
        filename=(filestartin+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptname)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS.nc')
        filenameout=(filestartout+modelname+'/'
                  +IPSLCM5A_EXPT.get(exptname)+'.'
                  +fielduse+'_'+timeuse+'_monthly_TS_rectilinear.nc')
        print(filename)
        print(filenameout)
        
        
   
    if modelname=='IPSLCM6A':
        fielduse=MIROC_FIELDS.get(fieldname)
        filename=(filestartin+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptname)+'.nc')
        filenameout=(filestartout+modelname+'/'+fielduse+
                  '_Omon_IPSL-CM6A-LR_'+IPSLCM6A_TIME.get(exptname)+'_rectilinear.nc')
        
    if modelname == 'CESM1.0.5':
        print (filestartin)
        print (modelname)
        print (exptname)
        print (CESM105_FIELDS.get(fieldname))
        
        filename = (filestartin + modelname + '/' + exptname + '/' +
                  exptname + '_' + CESM105_FIELDS.get(fieldname) +
                  '.nc')
        fielduse = fieldname
        filenameout = (filestartout + modelname + '/' + exptname + '/' +
                  exptname + '_' + CESM105_FIELDS.get(fieldname) +
                  '.nc')
    
        print(filename)
        print(filenameout)
        #sys.exit(0)

    if modelname == 'HadGEM3':
        ending = 'midPliocene-eoi400_r1i1p1f1_gn_233401-239312.nc'
        filename = (filestartin + 'HadGEM3/tos_Omon_HadGEM3-GC31-LL_' + 
                   ending)
        fielduse = 'sea_surface_temperature'
        filenameout = filestartin + 'HadGEM3/regridded_' + ending
   
    
    return [fielduse,filename,filenameout]


##########################################################
# main program

filename=' '
linux_win='l'
modelname='HadGEM3' # IPSLCM5A, IPSLCM5A2
                   #                      IPSLCM6A
                   # 'CESM.1.0.5 HadGEM3
                   
#modelname = 'IPSLCM5A'

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}

# only need fields that are on a tripolar grid like orca
fieldname = {
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein=['ts','pr']
#exptnamein=['Eoi400','E280']

#fieldnamein=['pr']
fieldnamein=['tos'] # ocean tempeature or sst
#exptnamein=['E280','Eoi400']

exptnamein=['E280']
if linux_win=='l':
    filestart='/nfs/hera1/pliomip2/data/'
    if (modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2' 
        or modelname == 'COSMOS' or modelname =='CESM1.0.5'):
        filestartin='/nfs/hera1/pliomip2/data/'
    if modelname=='IPSLCM6A' or modelname == 'HadGEM3':
        filestartin='/nfs/hera1/earjcti/PLIOMIP2/'
    filestartout='/nfs/hera1/earjcti/PLIOMIP2/'
else:
    filestart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    
    


for expt in range(0,len(exptnamein)):
    for field in range(0,len(fieldnamein)):

        # call program to get model dependent names
        # fielduse, and  filename 
        fielduse, filename, filenameout = (getnames
                                           (modelname,fieldnamein[field],
                                            exptnamein[expt]))
        
        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])

        print ('filename',filename)
        
        regrid_data(fieldnamein[field],fieldnameout,exptnamein[expt],exptnameout,
                    filename,modelname,linux_win,fielduse,filenameout)

#sys.exit(0)
::::::::::::::
regrid_timeseries.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on July 21 2020

#
# This program will produce a regridded 1X1degree timeseries of a given field.  
# We will remove the annual cycle in order to look for interannual variability 
# etc.  


import numpy as np
from netCDF4 import Dataset
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################
def get_ecearth_cube(exptname, lsmstart):
    """
    get's the datacube from ecearth
    """
    allcube = iris.load(filename)
    print(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    if fielduse == "sst":
        if exptname == 'Eoi400':
            #lsmfile = lsmstart+modelname+'/lsm_plio_new.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_mPlio_LSM.nc'
        if exptname == 'E280':
            #lsmfile = lsmstart+modelname+'/lsm.nc'
            lsmfile = lsmstart+modelname+'/EC-Earth3.3_PI_LSM.nc'

        lsmcube = iris.load(lsmfile)
        #  qiong has said to take >0.5 as land and >0.5 as sea

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.5, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)


        cube.data = np.ma.array(cube.data, mask=mymask)
    return cube

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u = unit.Unit('days since 0800-01-01 00:00:00',
                      calendar=unit.CALENDAR_360_DAY)
        if model == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if model == 'MRI2.3':
        cube_temp.coord('pressure level').rename('surface')

    if model == 'HadCM3' and fielduse == 'SST':
        cube_temp.coord('unspecified').rename('surface')

    if model == 'HadCM3' and fielduse == 'NearSurfaceTemperature':
        cube_temp.coord('ht').rename('surface')


    cube_temp.coord('surface').points = 0.
    cube = cube_temp.extract(iris.Constraint(surface=0.))

    return cube

def get_noresm_ocn(exptnamein, fieldnamein):
    """
    get noresm ocean things one file per month

    """
    print(modelname, exptnamein)
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    fileend = '_remap.nc'
   
    
    if exptnamein == 'Eoi400' and modelname == 'NorESM1-F': 
        startyear = 2400
        endyear = 2500
    if exptnamein == 'E280' and modelname == 'NorESM1-F': 
        startyear = 1900
        endyear = 2000
    if exptnamein == 'E280' and modelname == 'NorESM-L': 
        startyear = 2100
        endyear = 2200
    if exptnamein == 'Eoi400' and modelname == 'NorESM-L': 
        startyear = 1100
        endyear = 1200
  
    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        for i, mon in enumerate(months):
            file = filename + np.str(year) + '-' + mon + fileend
            cubetemp = iris.load_cube(file, 'Ocean surface temperature')
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
    print(cube)
  
  
    return cube

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src, Dataset("temporary.nc", "w", format='NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name, src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name, dimension in src.dimensions.items():

            if name != 'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name, (len(dimension)))

        # copy all file data
        for name, variable in src.variables.items():
            print('name is', name, variable)
            if name not in ('time_counter_bnds', 'time_centered'):
                x = dst.createVariable(name, variable.datatype,
                                       variable.dimensions)
                if name == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:] = ((src.variables[name][:] / (60.*60.*24))
                                              -(src.variables[name][0] / (60.*60.*24)) + 15.)
                else:
                    dst.variables[name][:] = src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr == 'calendar' and exptname == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr == 'units' and name == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein in ('ts', 'tas'):
            cube.convert_units('Celsius')

        if exptname == 'Eoi400':
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_360_DAY)
        else:
            u = unit.Unit('days since 0800-01-01 00:00:00',
                          calendar=unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return cube

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i, t_slice in enumerate(cubeall.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return cube

def get_miroc_tos():
    """
    get miroc data
    """
    cube = iris.load_cube(filename)

    # MIROC didn't have the units set up correctly for 'tos'
    cube.coord('latitude').units = 'degrees'
    cube.coord('longitude').units = 'degrees'
    return cube

def get_HadGEM3():
    """
    here there is one file per month containing the data
    """
    months = ['01','02','03','04','05','06','07','08','09','10','11','12']
    filemid = 'o_1m_'
    fileend = '_grid-T.nc'
   
    # eoi400
    #startyear = 2334
    #endyear = 2434

    #e280
    startyear=1950
    endyear = 2050
    #endyear=2050

    allcubes = iris.cube.CubeList([])
   
    for year in range(startyear, endyear):
        # eoi400
        #if year < 2394: 
        #   extra = 'v963'
        #else:
        #    extra = 'x150'
        #e280
        extra='q637'
        for i, mon in enumerate(months):
            datestart = np.str(year) + mon + '01-'
            if i == 11:
                daterange = datestart + np.str(year+1) + months[0] + '01'
            else:
                daterange = datestart + np.str(year) + months[i+1] + '01'
            file = filename + extra + filemid + daterange + fileend
            print(file, fieldname)
            
            cubetemp = iris.load(file)
            cubetemp = iris.load_cube(file, fielduse)
            u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar
            cubetemp.coord('time').attributes = None
            cubetemp.coord('time').points = ((i+((year-startyear)*12))*30.)+15.
            cubetemp.coord('time').units = u
            allcubes.append(cubetemp)
       
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()
    print(cube.coord('time').points)

    return cube



def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """

    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse:
            cube = allcube[i]

    # put units as celcius if required
    if fielduse == 'tas':
        cube.units = 'Celsius'

    cube2 = iris.util.new_axis(cube, 'time')

    return cube

def get_ccsm4_uot(fieldnamein):
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """

    cube = iris.load_cube(filename)
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u = unit.Unit('days since 0800-01-01 00:00:00',
                  calendar=unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.

    cube.add_dim_coord(iris.coords.DimCoord(points,
                                            standard_name='time', long_name='time',
                                            var_name='time',
                                            units=u,
                                            bounds=None,
                                            coord_system=None, circular=False), 0)

    if fieldnamein == 'pr':
        cube.data = cube.data * 60. *60. *24. *1000.
        cube.name = 'Total precipitation'
        cube.long_name = 'Total precipitation'
        cube.units = 'mm/day'


    return cube

def get_cesm12_singlecube(filename_, fielduse_):
    """
    get a single cube from a single file
    this is needed because total precipitation is the sum of two cubes
    """
    allcube = iris.load(filename_)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        if allcube[i].var_name == fielduse_:
            singlecube = allcube[i]

    return singlecube

def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """

    if type(fielduse) is list: # perhaps a list containing large scale and convective precip
                               # that need to be added together for total precipitaiton
        cube1 = get_cesm12_singlecube(filename[0], fielduse[0])
        cube2 = get_cesm12_singlecube(filename[1], fielduse[1])
        if fieldnameout == 'TotalPrecipitation':
            cube = cube1 + cube2
        if fieldnameout == 'SST':
           # mask temperature (cube1) by ice fraction (cube2)
            cube1.convert_units('Celsius')
            cubedata = np.where((cube2.data > 0.01),
                                -1.8, cube1.data)
            cube = cube1.copy(data=cubedata)
    else:
        cube = get_cesm12_singlecube(filename, fielduse)

    # put units as celcius if required
    if fielduse == 'TREFHT':
        cube.convert_units('Celsius')

    if fieldnameout == 'SST':
        # we need to mask out the land
        if exptnamein == 'Eoi400':
            if modelname == 'CESM2':
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b.e21.B1850.f09_g17.' +
                           'PMIP4-midPliocene-eoi400.001.'+
                           'cam.h0.LANDFRAC.1101.1200.nc')
            else:
                filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                           'b40.B1850.f09_g16.PMIP4-pliomip2.' +
                           'LANDFRAC.1001.1100.nc')
        if exptnamein == 'E280':
            #if modelname == 'CESM2':
            #    filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
           #                'b.e21.B1850.f09_g17.' +
           #                'CMIP6-piControl.001.cam.h0.'+
           #                'LANDFRAC.1300.1399.nc')
           # else:
            filelsm = ('/nfs/hera1/pliomip2/data/NCAR/' +
                       'b.e12.B1850.f09_g16.preind.' +
                       'cam.h0.LANDFRAC.0701.0800.nc')
        lsmcube = get_cesm12_singlecube(filelsm, 'LANDFRAC')

        nt, nx, ny = np.shape(cube.data)
        mymask_2d = lsmcube[0].data
        mymask_2d = np.where(mymask_2d > 0.01, 1.0, 0.0)
        mymask = np.vstack([mymask_2d] * nt)

        cube.data = np.ma.array(cube.data, mask=mymask)


    return cube



######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube

    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years


    """

    if modelname in ('CCSM4', 'CESM1.2', 'CESM2'):
        print('CCSM',cube.coord('month').points)
        months = cube.coord('month').points
        months = months -1
        for i, month in enumerate(months):
            if month == 0: months[i] = 12
        print(months)
        cube.coord('month').points = months
 
    else:
        startyear = (cube.coord('year').points[0])
        endyear = (cube.coord('year').points[-1])
        # count the number of months that have the same year as the first index
        nstart = 0
        nend = 0
        for i in range(0, 12):
            if cube.coord('year').points[i] == startyear:
                nstart = nstart+1
        for i in range(-13, 0):
            if cube.coord('year').points[i] == endyear:
                nend = nend+1
        if nend != 12 or nstart != 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
            if nend + nstart == 12:
                for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                    cube.coord('year').points[i] = startyear


                else:

                    print('you have a partial year somewhere')
                    print('correct input data to provide full years')
                    print(nend, nstart)
                    sys.exit(0)


    return cube

######################################################
def cube_avg(cube):
    """
    Extract monthly averaged data from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months

    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)

    meanmonthcube.long_name = fieldnameout

    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')

    return meanmonthcube


def remove_ann_cycle(cube, mon_avg_cube):
    """
    removes the annual cycle stored in mon_avg_cube from cube
    """

    cubedata = cube.data
    for i, monthno in enumerate(cube.coord('month').points):
        mon_avg_data = (mon_avg_cube.extract(iris.Constraint(month=monthno))).data
        cubedata[i, :, :] = cubedata[i, :, :] - mon_avg_data


    timeseries_cube = cube.copy(data=cubedata)

    return timeseries_cube


##############################################
def regrid_data(fieldnamein, exptnamein):
    """
    regrid the data
    """

    # outfile
    if linux_win == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/' + modelname +
                    '/timeseries/' + exptnameout + '.' + fieldnameout + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
                    + modelname + '\\timeseries\\' + exptnameout 
                    + '.' + fieldnameout + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


    #####################################
    # get all data in a single cube
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3'): # all fields in one file
        cube = get_ecearth_cube(exptnamein, lsmstart)
    elif modelname in ('HadCM3', 'MRI2.3'):
        cube = get_hadcm3_cube(modelname)
    elif modelname in ('IPSLCM5A', 'IPSLCM5A2') and fieldnamein != 'tos':
        cube = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif modelname in ('NorESM1-F', 'NorESM-L') and fieldnamein == 'tos':
        cube = get_noresm_ocn(exptnamein, fieldnamein)
    elif modelname == 'IPSLCM6A':
        cube = get_ipslcm6()
    elif modelname in ('MIROC4m', 'tos'):
        cube = get_miroc_tos()
    elif modelname == 'GISS2.1G':
        cube = get_giss()
    elif modelname == 'CCSM4-Utr':
        cube = get_ccsm4_2deg()
    elif modelname in ('CESM1.2', 'CCSM4', 'CESM2'):
        cube = get_cesm12(exptnamein)
    elif (modelname == 'CCSM4-UoT'):
        cube = get_ccsm4_uot(fieldnamein)
    elif (modelname == 'HadGEM3'):
        cube = get_HadGEM3()  
    else:
        cube = iris.load_cube(filename)

  
    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid

    if ((modelname == 'CCSM4-UoT')
            or (modelname == 'IPSLCM5A' and fieldnamein == 'tos')
            or (modelname == 'IPSLCM5A2' and fieldnamein == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())


    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if modelname == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if modelname == 'EC-Earth3.1':
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'


    # regrid to mm/day from kg/m2/s if required
    if modelname in ('EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A',
                     'IPSLCM5A2', 'IPSLCM6A', 'CCSM4-Utr', 'GISS2.1G'):
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. *60. *24.
            cube.data = cube.data* 60. *60. *24.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'



    if modelname in ('NorESM1-F', 'NorESM-L', 'CESM1.2', 'CESM2', 'CCSM4'):

       # if precipitation is in m/s convert to mm/day
        if fieldnamein == 'pr':
            regridded_cube.data = regridded_cube.data * 60. * 60. * 24. * 1000.
            cube.data = cube.data * 60. * 60. * 24. * 1000.
            regridded_cube.name = 'Total precipitation'
            regridded_cube.long_name = 'Total precipitation'
            regridded_cube.units = 'mm/day'

    if modelname in ('CCSM4-UoT', 'NorESM1-F', 'NorESM-L', 'IPSLCM6A',
                     'EC-Earth3.1', 'EC-Earth3.3', 'IPSLCM5A', 'IPSLCM5A2',
                     'HadCM3', 'GISS2.1G'):
         # convert to celcius
        if fieldnamein in ('tas', 'tos'):
            regridded_cube.convert_units('Celsius')
            cube.convert_units('Celsius')


    if modelname in ('COSMOS', 'MIROC4m', 'IPSLCM6A', 'EC-Earth3.1'):
        regridded_cube.coord('time').units = refdate


    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube, 'time', name='month')
    iris.coord_categorisation.add_year(regridded_cube, 'time', name='year')


     # correct the start month if required
    regridded_cube = correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_cube = cube_avg(regridded_cube)



    # remove annual cyble
    anom_cube = remove_ann_cycle(regridded_cube, mean_mon_cube)

    # to check we have removed the average properly get the monthly
    # average of the anomaly cube it should be zero

    new_mean_mon_cube = cube_avg(anom_cube)
    qplt.contourf(new_mean_mon_cube[2, :, :], levels=np.arange(-0.01, 0.011, 0.001), extend='both')
    plt.show()



    # write the cubes out to a file

    outfile = outstart+'timeseries_no_ann_cycle.nc'
    iris.save(anom_cube, outfile, netcdf_format='NETCDF3_CLASSIC', fill_value=2.0E20)




#############################################################################
def getnames(modelname, filestart, fieldnamein, exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    MIROC_FIELDS = {"pr" : "pr",
                    "tas" : "tas",
                    "sic" : "SeaIceAreaFraction",
                    "tos" : "tos"
                    }

    COSMOS_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                     }

    ECearth_FIELDS = {"pr" : "totp",
                      "tas" : "tas",
                      "tos" : "sst",
                      "sic" : "SeaIceAreaFraction"
                      }

    IPSLCM5A_FIELDS = {"pr" : "TotalPrecip_pr",
                       "tas" : "NearSurfaceTemp_tas",
                       "sic" : "SeaIceAreaFraction",
                       "tos": "SeasurfaceTemp_sst"
                       }

    NorESM_FIELDS = {"pr" : "PRECT",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "sst"
                    }

    CCSM42_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "NearSurfaceAirTemp",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "SeaSurfaceTemp"
                      }

    CESM12_FIELDS = {"pr" : "TotalPrecip",
                     "tas" : "TREFHT",
                     "sic" : "SeaIceAreaFraction",
                     "tos" : "TS"
                     }

    CESM12_EXTRA = {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.cam.h0.",
                    "E280": "b.e12.B1850.f09_g16.preind.cam.h0.",
                    }

    CESM2_EXTRA = {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                   "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.cam.h0.",
                   }

    CCSM4_EXTRA = {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                   "E280": "b40.B1850.f09_g16.preind.cam.h0.",
                   }

    ECearth_EXPT = {"Eoi400": "mPlio",
                    "E280":"PI"
                   }

    CESM12_EXPT = {"Eoi400": "PlioMIP2",
                   "E280":"PI"
                   }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
                     "E280":"PI"
                    }

    CESM12_TIME = {"E280" : ".0701.0800",
                   "Eoi400" : ".1101.1200"
                   }

    CESM2_TIME = {"E280" : ".110001-120012",
                  "Eoi400" : ".1101.1200"
                 }

    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
                     "E280":"3600_3699"
                    }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
                       "E280":"6110_6209",
                      }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
                     "E280":"piControl_r1i1p1f1_gr_285001-304912",
                     }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
                         "E280":"piControl_r1i1p1f1_gn_285001-304912",
                        }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
                  "E280":"piControl_r1i1p1f1_gn_490101-495012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
                 }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
                       "E280":"piControl_r1i1p1f1_gr1_150101-160012",
                       "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                      }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
                  "E280":"piControl_r1i1p1f1_gn_495101-500012",
                  "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
                  }
    atm_ocn_ind = {"tas": "Amon",
                   "pr": "Amon",
                   "tos":"Omon"}
    cosmos_version = {"tas": "",
                      "pr": "",
                      "tos":"_remapbil"}

    # get names for each model
    if modelname == 'MIROC4m':
        filename = filestart + modelname + '/'
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filename + fielduse + '/MIROC4m_'+exptnamein
                    + '_' + atm_ocn_ind.get(fieldnamein) + '_' + fielduse + '.nc')
    if modelname == 'COSMOS':
        if linux_win == 'l':
            filename = filestart+'/AWI/COSMOS/'
            filename = filename+exptnamein+'/'
        else:
            filename = filestart+'/COSMOS/'
        fielduse = COSMOS_FIELDS.get(fieldnamein)
        filename = (filename + exptnamein + '.' + fielduse +
                    '_CMIP6_name_' + fieldnamein +
                    '_2650-2749_monthly_mean_time_series' +
                    cosmos_version.get(fieldnamein) + '.nc')
    if modelname == 'CCSM4-UoT':
        if linux_win == 'l':
            filename = filestart + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' +
                        exptnamein + '/' + atm_ocn_ind.get(fieldnamein) + '/')
        else:
            filename = filestart + 'UofT-CCSM4\\' + exptnamein + '\\'
        fielduse = MIROC_FIELDS.get(fieldnamein)

        filename = (filename + fielduse +
                    '_' + atm_ocn_ind.get(fieldnamein) +
                    '_' + exptnamein + '_UofT-CCSM4_gr.nc')
    if modelname == 'HadCM3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = fieldname.get(fieldnamein)
        filename = (filestart + 'LEEDS/HadCM3/' + exptuse + '/' + fielduse + '/'
                    + exptuse + '.' + fielduse + '.')
    if modelname == 'MRI2.3':
        exptuse = exptname_l.get(exptnamein)
        fielduse = MIROC_FIELDS.get(fieldnamein)
        filename = (filestart+'MRI-CGCM2.3/'+fielduse+'/'
                    +exptuse+'.'+fielduse+'.')
    if modelname == 'EC-Earth3.1' or modelname == 'EC-Earth3.3':
        fileend = '_surface.nc'
        if fieldnamein == 'tos':
            fileend = '_ci-sst.nc'
        exptuse = exptname_l.get(exptnamein)
        fielduse = ECearth_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/'
                    + modelname
                    + '_'
                    + ECearth_EXPT.get(exptnamein)
                    + fileend)
    if modelname == 'IPSLCM5A' or modelname == 'IPSLCM5A2':
        exptuse = exptname_l.get(exptnamein)
        if modelname == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if modelname == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse 
                        + '_monthly_TS_rectilinear.nc')
        else:
            filename = (filestart+modelname + '/'
                        + IPSLCM5A_EXPT.get(exptnamein) + '.'
                        + fielduse + '_' + timeuse + '_monthly_TS.nc')

    if modelname == 'NorESM1-F' or modelname == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(fieldnamein)
        filename = (filestart + modelname + '/sst_regrid_1degree/'
                    + modelname + '_' + exptnamein + '.' + fielduse + '.')
    if modelname == 'IPSLCM6A':
        fielduse = MIROC_FIELDS.get(fieldnamein)
        if fieldnamein == 'tos':
            filename = (filestart + modelname + '/' + fielduse +
                        '_Omon_IPSL-CM6A-LR_' + IPSLCM6A_TIME_ALT.get(exptnamein) 
                        + '_rectilinear.nc')
        else:
            filename = (filestart + modelname + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) + '_IPSL-CM6A-LR_' 
                        + IPSLCM6A_TIME.get(exptnamein) + '.nc')
    if modelname == 'GISS2.1G':
        fielduse = fieldnamein
        exptuse = exptname_l.get(exptnamein)
        filename = []
        filename.append(filestart + modelname + '/' + exptuse + '/' + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME1.get(exptnamein)
                        + '.nc')
        filename.append(filestart + modelname + '/' + exptuse + '/' 
                        + fielduse +
                        '_' + atm_ocn_ind.get(fieldnamein) +
                        '_GISS-E2-1-G_' + GISS_TIME2.get(exptnamein) + '.nc')

    if modelname == 'CCSM4-Utr':
        filename=(filestart + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(fieldnamein) +
                  '.nc')
        fielduse = fieldnamein

    if modelname == 'CESM1.2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM12_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                  CESM12_EXTRA.get(exptnamein) +
                  CESM12_FIELDS.get(fieldnamein) +
                  CESM12_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CESM2':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'PRECL' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']
        if fieldnamein == 'tos':
            # if we are passing SST we also need to pass ice fraciton
            filename1 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'TS' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                  CESM2_EXTRA.get(exptnamein) +
                  'ICEFRAC' +
                  CESM2_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']
        if fieldnamein =='tas':
            filename=(filestart + 'NCAR/' +
                      CESM2_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CESM2_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)

    if modelname == 'CCSM4':
        if fieldnamein == 'pr':
            # this has been passed in two files.
            # for convective and large scale precipitaiton put filename in a list
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'PRECL' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['PRECC', 'PRECL']

        if fieldnamein == 'tos':
            # if we are using SST we also need to pass ice fraction
            filename1 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'TS' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename2 = (filestart + 'NCAR/' +
                         CCSM4_EXTRA.get(exptnamein) +
                         'ICEFRAC' +
                         CCSM4_TIME.get(exptnamein) + '.nc')
            filename = [filename1, filename2]
            fielduse = ['TS', 'ICEFRAC']

        if fieldnamein == 'tas':
            filename=(filestart + 'NCAR/' +
                      CCSM4_EXTRA.get(exptnamein) +
                      CESM12_FIELDS.get(fieldnamein) +
                      CCSM4_TIME.get(exptnamein) + '.nc')
            fielduse = CESM12_FIELDS.get(fieldnamein)


    if modelname == 'HadGEM3':
        filename = []
        filestart = '/nfs/hera1/pliomip2/data/HadGEM3_new/timeseries/' 
        if exptnamein == 'Eoi400':
            filename = filestart + 'Eoi400/ocean/sst_sal_temp/new_nemo_b'
        if exptnamein == 'E280':
            filename = filestart + 'E280/ocean/sst_sal_temp/new_nemo_b'
       
        fielduse = 'sea_surface_temperature'

    print(fielduse, filename)
    retdata = [fielduse, filename]
    return retdata


##########################################################
# main program

filename =  ' '
linux_win =  'l'
modelname = "HadGEM3" # MIROC4m  COSMOS CCSM4-UoT EC-Earth3.1
                   # HadCM3 MRI2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

exptname = {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"
            }

exptname_l = {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"
            }

fieldname = {
        "pr" : "TotalPrecipitation",
        "tas" : "NearSurfaceTemperature",
        "sic" : "SeaIceConcentration",
        "tos": "SST"
        }


# this is regridding where all results are in a single file
#fieldnamein = ['pr','tas','tos']
exptnamein = ['E280']

fieldnamein = ['tos']
#fieldnamein = ['tos'] # ocean tempeature or sst
#exptnamein = ['Eoi400']

#fieldnamein = ['tos','pr','tas']
#exptnamein = ['Eoi400', 'E280']
#exptnamein = ['E560']
if linux_win == 'l':
    filestart = '/nfs/hera1/pliomip2/data/'
else:
    filestart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


for expt in range(0, len(exptnamein)):
    for field in range(0, len(fieldnamein)):

        if (modelname in ('IPSLCM5A', 'IPSLCM5A2', 'CCSM4-Utr')
            and fieldnamein[field] == 'tos'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'
        if modelname in ('IPSLCM6A', 'GISS2.1G'):
            filestart = '/nfs/hera1/earjcti/PLIOMIP2/'


        # call program to get model dependent names
        # fielduse,  and  filename
        retdata = getnames(modelname, filestart, fieldnamein[field], exptnamein[expt])

        fielduse = retdata[0]
        filename = retdata[1]


        fieldnameout = fieldname.get(fieldnamein[field])
        exptnameout = exptname.get(exptnamein[expt])




        print('filename is', filename)




        regrid_data(fieldnamein[field], exptnamein[expt])

#sys.exit(0)
::::::::::::::
regrid_winds_50yr_avg.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on june 24 2020; copied from regrid_ocn_50yr_avg.py


#
# This program will regrid some of the data that is needed for PLIOMIP2.
# We will put 50 year average fields onto a 1deg X 1deg standard grid
# it can be used where experiments have been uploaded with 100 years in
# one file
#



import numpy as np
import iris
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os

###################################################
# get all data from files as a single cube
##################################################

def get_hadcm3_cube(model):
    """
    get's the datacube from hadcm3 or mri-cgcm2.3
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if model  == 'MRI2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filename+yearuse+'.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if model  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)

    print(allcubes)
    equalise_attributes(allcubes)
    cube = allcubes.concatenate_cube()

 
    return(cube)

def get_ipslcm5_atm(exptname, fieldnamein):
    """
      get data from the atmospheric files in ipslcm5

      there is a bit of an error in the file calendar so we will
    """
    # copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        print(src.dimensions)
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            print('name is', name, variable)
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    print(ncattr, exptname)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        else:
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = fieldnamein
        if fieldnamein  == 'pr':
            fieldreq = 'Precip Totale liq+sol'
        if fieldnamein  == 'tas':
            fieldreq = 'Temperature 2m'


        cube = iris.load_cube('temporary.nc', fieldreq)

        if fieldnamein  == 'ts' or fieldnamein  == 'tas':
            cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm6():
    """
    get ipslcm6 data
    here 200 years have been supplied.  We only want the last 100 years
    """
    cubeall = iris.load_cube(filename)

    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cubeall.slices(['latitude', 
                                                 'longitude',
                                                 'air_pressure'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube = cubelist.concatenate_cube()
    return(cube)

def get_giss():
    """
    get giss data
    here are multiple files containing the data
    """
    allcubes = iris.cube.CubeList([])
    for file in range(0, len(filename)):
        cubetemp = iris.load_cube(filename[file])
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()

    return(cube)
    
    
def get_ccsm4_2deg():
    """
    get ccsm4_2deg utrecht data
    """
    
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
    
    # put units as celcius if required
    if fielduse == 'tas':
        print(cube.units)
        cube.units='Celsius'
    
    cube2 = iris.util.new_axis(cube, 'time')
    print('julia check')
    
    return cube

def get_ccsm4_uot():
    """
    get Uof T cube (need to add a dimension)
    if precip convert to mm/day
    """
    
    print(filename)
    cubes = iris.load(filename)
    cube = cubes[0]
    pressures = cubes[1].data
    
    print(cube)
    print(pressures)
    
    
    points = (np.arange(0, 1200)*30)+15. # go for middle of month
    u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of the way
                                               # the data was sent.
    
    cube.add_dim_coord(iris.coords.DimCoord(points,
                standard_name = 'time',  long_name = 'time',
                var_name = 'time',
                units = u,
                bounds = None,
                coord_system = None,  circular = False), 0)

    cube.add_dim_coord(iris.coords.DimCoord(pressures,
                standard_name = 'air_pressure',  long_name = 'pressure',
                var_name = 'pressure',
                units = 'Pa',
                bounds = None,
                coord_system = None,  circular = False), 1)


    return cube

    
def get_cesm12(exptnamein):
    """
    get cesm1.2 data
    """
    allcube = iris.load(filename)
    ncubes = len(allcube)
    for i in range(0, ncubes):
        print(allcube[i].var_name,fielduse)
        if allcube[i].var_name == fielduse:
            cube = allcube[i]
        if allcube[i].var_name == "lev":
            cubelev = allcube[i]

    if exptnamein == 'E280' or MODELNAME == 'CCSM4' or MODELNAME == 'CESM1.2':
        pressures = cubelev.data * 100.
        print(pressures)
        cube.add_dim_coord(iris.coords.DimCoord(pressures,
                standard_name = 'air_pressure',  long_name = 'pressure',
                var_name = 'pressure',
                units = 'Pa',
                bounds = None,
                coord_system = None,  circular = False), 1)

    if exptname == 'E280' and MODELNAME == 'CESM1.2':
        points = (np.arange(0, 1200)*30)+15. # go for middle of month
        u  =  unit.Unit('days since 0800-01-01 00:00:00',
               calendar = unit.CALENDAR_360_DAY) # put as 360 day calendar because of
        cube.coord('time').points = points
        cube.coord('time').units = u
        iris.util.promote_aux_coord_to_dim_coord(cube, 'time')
       
         #cube.add_dim_coord(iris.coords.DimCoord(points,
         #       standard_name = 'time',  long_name = 'time',
         #       var_name = 'time',
         #       units = u,
         #       bounds = None,
         #       coord_system = None,  circular = False), 0)

    return cube


def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

######################################################
def correct_start_month(cube):
    """
    parameters: cube
    returns: the same cube
    
    if month doesn't start on january we will have to change some of the
    years from the end to match those at the start to give 100 full years

    
    """
    print('julia3')

    print(cube.coord('time').points)
    print(cube.coord('time').units)
    print(cube.coord('year').points)
    print(cube.coord('month').points)

    
    startyear = (cube.coord('year').points[0])
    endyear = (cube.coord('year').points[-1])
    # count the number of months that have the same year as the first index
    nstart = 0
    nend = 0
    for i in range(0, 12):
        if cube.coord('year').points[i]  == startyear:
            nstart = nstart+1
    for i in range(-13, 0):
        if cube.coord('year').points[i]  == endyear:
            nend = nend+1
    if nend!= 12 or nstart!= 12:
        # oops we don't have a full year at the start and the end
        # check to see whether we can aggregate into one year
        if nend+nstart  == 12:
            for i in range(-1 * nend, 0):
                # move the last few to the start of the cube
                cube.coord('year').points[i] = startyear
        

        else:

            print('you have a partial year somewhere')
            print('correct input data to provide full years')
            print(nend, nstart)
            sys.exit(0)

    
    return cube

######################################################    
def cube_avg(cube, exptnamein):
    """
    Extract averaged data and standard deviation values from a cube

    Parameters:
    cube (iris cube): A cube with montly data that we average

    Returns:
    meanmonthcube (iris cube): the input cube averaged over each of the 12 calendar months
    sdmonthcube (iris cube): the standard deviation on the values in meanmonthcube
    meanyearcube (iris cube): annual average values for each calendar year
    meancube (iris cube) : the long term mean value at each gridpoint
    sdcube (iris cube): the interannual standard deviation on the mean at each gridpoint
    """

    meanmonthcube = cube.aggregated_by('month', iris.analysis.MEAN)
    
    # NORESM and CESM1.2 does not start at month = 1,
    #it starts at month = 2. but should be 1
    # we are doing dome roundabout way of reordering the data
    if (MODELNAME  == 'NorESM1-F' or MODELNAME  == 'NorESM-L'
       or MODELNAME == 'CESM1.2' or MODELNAME == 'CCSM4'
       or (MODELNAME == 'CESM2' and exptnamein == 'E280')):
        allcubes = iris.cube.CubeList([])
        for mon in range(2, 13):
            slice  =  meanmonthcube.extract(iris.Constraint(month = mon))
            # attempt to reorder time coordinate
            slice.coord('time').points = mon-1
            slice.coord('month').points = mon-1
            slice.coord('time').bounds = None
            allcubes.append(slice)
        # do december (month 1)
        slice  =  meanmonthcube.extract(iris.Constraint(month = 1))
        slice.coord('time').points = 12
        slice.coord('month').points = 12
        slice.coord('time').bounds = None
        allcubes.append(slice)
        #process
        meanmonthcube = (allcubes.merge_cube())
        print(meanmonthcube.coord('time').points)
        print(meanmonthcube.coord('month').points)

    meanmonthcube.long_name = FIELDNAMEOUT
    
    iris.util.promote_aux_coord_to_dim_coord(meanmonthcube, 'month')
    sdmonthcube = cube.aggregated_by('month', iris.analysis.STD_DEV)
    sdmonthcube.long_name = FIELDNAMEOUT

    # mean and standard deviation for each year
    meanyearcube = cube.aggregated_by('year', iris.analysis.MEAN)
    meanyearcube.long_name=FIELDNAMEOUT

    # mean and interannual standard deviation over dataset
    meancube = meanmonthcube.collapsed('time', iris.analysis.MEAN)
    meancube.long_name=FIELDNAMEOUT
    sdcube = meanyearcube.collapsed('time', iris.analysis.STD_DEV)
    sdcube.long_name=FIELDNAMEOUT

    return [meanmonthcube, sdmonthcube, meanyearcube, sdcube, meancube]


def mon_avg(cube):
    """
    get the monthly average data from a cube

    Parameters:
    cube (iris cube) contains monthly data (ie t=12)

    Returns:
    meanmon (np array) with mean global values for each of the 12 months
    """

    cube.coord('latitude').guess_bounds()
    cube.coord('longitude').guess_bounds()
    grid_areas2 = iris.analysis.cartography.area_weights(cube)
    tempcube = cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas2)
    meanmon = tempcube.data

    return meanmon

def get_monthly_sd(cube):
    """
    get monthly values of the standard deviation on the
    globally averaged mean from a cube

    Parameters:
        cube (iris cube) : a cube with monthly data from the whole dataset
                           (tdim=nyears*nmonths)
    Returns:
        stdevmon (numpy array) : an array with the monthly standard deviation
                                 on the monthly global mean

    """


    stdevmon = np.zeros(12)
    for mon in range(1, 13):
        # mon_slice is the slice from this month.  It should have a time
        # dimension of t where t is the number of years
        mon_slice = cube.extract(iris.Constraint(month=mon))
        mon_slice.coord('latitude').guess_bounds()
        mon_slice.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(mon_slice)
        mon_avg = mon_slice.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
        stdevcube = mon_avg.collapsed(['time'], iris.analysis.STD_DEV)
        stdevmon[mon-1] = stdevcube.data

    return stdevmon



def area_means(allmeancube, yearmeancube):
    """
    get the globally averaged mean and standard deviation
    and latitudinal means and standard deviation

    Parameters:
    allmeancube (iris cube) lat/lon cube of temporal mean tdim=1
    yearmeancube (iris cube)  lat/lon/time cube of yearly averages tdim=nyears

    Returns
    meanann (float) global long term mean
    stdevann (float)  global long term standard deviation of the mean
    mean lat (np.array(nlats) latitudinally averaged long term mean
    stdevlat (np.array(nlats)standard deviation of the latitudinal mean

    """

    allmeancube.coord('latitude').guess_bounds()
    allmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(allmeancube)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                   iris.analysis.MEAN, weights=grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = allmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    yearmeancube.coord('latitude').guess_bounds()
    yearmeancube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(yearmeancube)
    tempcube = yearmeancube.collapsed(['latitude', 'longitude'],
                                      iris.analysis.MEAN, weights=grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data


    # get standard deviation for each latitude
    tempcube = yearmeancube.collapsed(['longitude'], iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann=stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)

    return [meanann, stdevann, meanlat, stdevlat]

##############################################
def regrid_data(exptnamein, filename, fielduse, constraint):
    """
    regrid the data
    """


    print('moodelname is', MODELNAME)
    print('filename is', filename)
    print('fielduse is', fielduse)
    print('exptnamein is', exptnamein)



    # outfile
    if LINUX_WIN  == 'l':
        outstart = ('/nfs/hera1/earjcti/regridded/'+MODELNAME+'/'+ EXPTNAMEOUT +'.'+
        FIELDNAMEOUT + '.')
        lsmstart = '/nfs/hera1/earjcti/regridded/'
    else:
        outstart = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
              +MODELNAME+'\\' + EXPTNAMEOUT + '.' + FIELDNAMEOUT + '.')
        lsmstart = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


   
    #####################################
    # get all data in a single cube
    if (MODELNAME  == 'EC-Earth3.1' or
       MODELNAME == 'EC-Earth3.3'): # all fields in one file
        cube100 = iris.load_cube(filename, fielduse)
    elif (MODELNAME  == 'HadCM3' or MODELNAME  == 'MRI2.3'):
        cube100 = get_hadcm3_cube(MODELNAME)
    elif ((MODELNAME  == 'IPSLCM5A' or MODELNAME  == 'IPSLCM5A2') and
        (fieldnamein != 'tos')):
        cube100 = get_ipslcm5_atm(exptnamein, fieldnamein)
    elif (MODELNAME  == 'IPSLCM6A'):
        cube100 = get_ipslcm6()
    elif (MODELNAME  == 'GISS2.1G'):
        cube100 = get_giss()
    elif (MODELNAME  == 'CCSM4-Utr'):
        cube100 = get_ccsm4_2deg()
    elif (MODELNAME  == 'CESM1.2' 
          or MODELNAME == 'CCSM4'
          or MODELNAME == 'CESM2'):
        cube100 = get_cesm12(exptnamein)
    elif (MODELNAME == 'CCSM4-UoT'):
        cube100 = get_ccsm4_uot()
    else:
        cube100 = iris.load_cube(filename)

    print(cube100)

    cube_level = cube100.extract(constraint)
   
    ###########################################
    # reduce number of years to 50

    print(cube_level)
    #sys.exit(0)
    cube = reduce_years(cube_level)
    cube.data = cube.data.astype('float32')
    ndim = cube.ndim


    # now regrid the cube onto a 1X1 grid (we will first try regridding the raw data)
    # we have stored the grid we want in a file 'one_lev_one_deg.nc'

    # do not need to regrid CCSM4_UoTdata or a field that was originally on a tripolar grid
   
    if ((MODELNAME   == 'CCSM4-UoT')
        or (MODELNAME  == 'IPSLCM5A' and fieldnamein  == 'tos')
        or (MODELNAME  == 'IPSLCM5A2' and fieldnamein  == 'tos')):
        regridded_cube = cube
    else:
        cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
        regridded_cube = cube.regrid(cubegrid, iris.analysis.Linear())



    refdate = 'days since 0800-01-01 00:00:00'

    # for cosmos
    if MODELNAME  == 'COSMOS':
        # cosmos data is in a strange time coordinate line yyyymmdd
        # we need to convert it to days since reference time
        origpoints = regridded_cube.coord('time').points
        npoints = len(origpoints)
        print(npoints)

        print(origpoints)
        yeararr = np.zeros(npoints)
        montharr = np.zeros(npoints)
        dayarr = np.zeros(npoints)
        daydecimal = np.zeros(npoints)
        dayssinceref = np.zeros(npoints)
        for i in range(0, npoints):
            origstr = str(origpoints[i])
            yeararr[i] = origstr[:][0:4]
            montharr[i] = origstr[:][4:6]
            dayarr[i] = origstr[:][6:8]
            daydecimal[i] = origstr[:][8:]
            dayssinceref[i] = dayssinceref[i-1]+dayarr[i]+daydecimal[i]-daydecimal[i-1]
        # subtract 1 from days since reference date (as reference date will be 1st Jan)
        dayssinceref = dayssinceref-1


        regridded_cube.coord('time').points = dayssinceref
        #  end of COSMOS loop

    # for EC-Earth3.1
    if (MODELNAME  == 'EC-Earth3.1'):
    # convert from hours to days
        origpoints = regridded_cube.coord('time').points
        newpoints = origpoints/24.
        regridded_cube.coord('time').points = newpoints
        refdate = 'days since 2390-01-01 00:00:00'
   
        
    if (MODELNAME  == 'COSMOS' or MODELNAME  == 'MIROC4m' or
        MODELNAME  == 'IPSLCM6A' or 
        MODELNAME  == 'EC-Earth3.1'):
          regridded_cube.coord('time').units = refdate


       
    print(regridded_cube.coord('time'))
    print('refdate is',refdate)
  

    # add auxillary coordinates month and year
    iris.coord_categorisation.add_month_number(regridded_cube,  'time',  name = 'month')
    iris.coord_categorisation.add_year(regridded_cube,  'time',  name = 'year')
    
    # correct the start month if required
    regridded_cube=correct_start_month(regridded_cube)
    
    # calculate averages
    mean_mon_data, sd_mon_data, mean_year_data, sd_data, mean_data = cube_avg(regridded_cube, exptnamein)


   
    print('j4 all mon', np.shape(mean_data))
    print('j5 all year', np.shape(sd_data))

    # extract monthly data from cubes

    jan_slice  =  regridded_cube.extract(iris.Constraint(month = 1))
    feb_slice  =  regridded_cube.extract(iris.Constraint(month = 2))
    mar_slice  =  regridded_cube.extract(iris.Constraint(month = 3))
    apr_slice  =  regridded_cube.extract(iris.Constraint(month = 4))
    may_slice  =  regridded_cube.extract(iris.Constraint(month = 5))
    jun_slice  =  regridded_cube.extract(iris.Constraint(month = 6))
    jul_slice  =  regridded_cube.extract(iris.Constraint(month = 7))
    aug_slice  =  regridded_cube.extract(iris.Constraint(month = 8))
    sep_slice  =  regridded_cube.extract(iris.Constraint(month = 9))
    oct_slice  =  regridded_cube.extract(iris.Constraint(month = 10))
    nov_slice  =  regridded_cube.extract(iris.Constraint(month = 11))
    dec_slice  =  regridded_cube.extract(iris.Constraint(month = 12))




    # write the cubes out to a file

    outfile = outstart+'mean_month.nc'
    iris.save(mean_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'sd_month.nc'
    iris.save(sd_mon_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allmean.nc'
    iris.save(mean_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    outfile = outstart+'allstdev.nc'
    iris.save(sd_data, outfile, netcdf_format = 'NETCDF3_CLASSIC', fill_value = 2.0E20)

    ##########################################################################
    # get the global mean and standard deviation and write them all out to a file
    #
    textout = outstart+'data.txt'

    file1 =  open(textout, "w")

    # get mean field for cube

    mean_data.coord('latitude').guess_bounds()
    mean_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_data)
    tempcube = mean_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    meanann = tempcube.data


    # get mean for each latitude
    tempcube = mean_data.collapsed(['longitude'], iris.analysis.MEAN)
    meanlat = tempcube.data
    meanlat = np.squeeze(meanlat)


    # get standard deviation
    # 1. mean for each year

    mean_year_data.coord('latitude').guess_bounds()
    mean_year_data.coord('longitude').guess_bounds()
    grid_areas  =  iris.analysis.cartography.area_weights(mean_year_data)
    tempcube = mean_year_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    stdevann = stdevcube.data

    plt.plot(tempcube.data)
    plt.plot([0, 100], [meanann, meanann])
    plt.plot([0, 100], [meanann+stdevann+stdevann, meanann+stdevann+stdevann])
    plt.plot([0, 100], [meanann-stdevann-stdevann, meanann-stdevann-stdevann])
    plt.title('data and data+/-2sd')

    # get standard deviation for each latitude
    tempcube = mean_year_data.collapsed(['longitude'],  iris.analysis.MEAN)
    stdevcube = tempcube.collapsed(['time'], iris.analysis.STD_DEV)
    #stdevann = stdevcube.data
    stdevlat = stdevcube.data
    stdevlat = np.squeeze(stdevlat)






    # write out to a file
    file1.write('global annual mean and standard deviation\n')
    file1.write('------------------------------------------\n')
    if ndim>= 4:
        file1.write(np.str(np.round(meanann[0], 2))+', '+np.str(np.round(stdevann[0], 3))+'\n')
    else:
        file1.write(np.str(np.round(meanann, 2))+', '+np.str(np.round(stdevann, 3))+'\n')

    # get monthly means and standard deviation
    file1.write('monthly means and standard deviations \n')
    file1.write('----------------------------------------')
    file1.write('month    mean    sd  \n')

    mean_mon_data.coord('latitude').guess_bounds()
    mean_mon_data.coord('longitude').guess_bounds()
    grid_areas2  =  iris.analysis.cartography.area_weights(mean_mon_data)
    tempcube = mean_mon_data.collapsed(['latitude', 'longitude'],
                                iris.analysis.MEAN, weights = grid_areas2)
    meanmon = tempcube.data

    # get monthly average using grid areas from year average
    # to calculate standard deviation
    #print(np.shape(jan_slice))
    #print(np.shape(grid_areas))
    #sys.exit(0)

    jan_avg = jan_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    feb_avg = feb_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    mar_avg = mar_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    apr_avg = apr_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    may_avg = may_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jun_avg = jun_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    jul_avg = jul_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    aug_avg = aug_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    sep_avg = sep_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    oct_avg = oct_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    nov_avg = nov_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)
    dec_avg = dec_slice.collapsed(['latitude', 'longitude'], iris.analysis.MEAN, weights = grid_areas)

    stdevmon = np.zeros(12)

    stdevcube = jan_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[0] = stdevcube.data
    stdevcube = feb_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[1] = stdevcube.data
    stdevcube = mar_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[2] = stdevcube.data
    stdevcube = apr_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[3] = stdevcube.data
    stdevcube = may_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[4] = stdevcube.data
    stdevcube = jun_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[5] = stdevcube.data
    stdevcube = jul_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[6] = stdevcube.data
    stdevcube = aug_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[7] = stdevcube.data
    stdevcube = sep_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[8] = stdevcube.data
    stdevcube = oct_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[9] = stdevcube.data
    stdevcube = nov_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[10] = stdevcube.data
    stdevcube = dec_avg.collapsed(['time'], iris.analysis.STD_DEV)
    stdevmon[11] = stdevcube.data

    for i in range(0, 12):
        if ndim>= 4:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i].data[0], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')
        else:
            file1.write(np.str(i+1)+', '+np.str(np.round(meanmon[i], 2))+', '+np.str(np.round(stdevmon[i], 3))+'\n')

    # get latitudinal means and standard deviation
    file1.write('zonal means and standard deviations \n')
    file1.write('----------------------------------------\n')
    file1.write('latitude    mean    sd  \n')
    for i in range(0, len(meanlat)):
        file1.write(np.str(mean_data.coord('latitude').points[i])+', '+np.str(np.round(meanlat[i], 2))+', '+np.str(np.round(stdevlat[i], 3))+'\n')

    file1.close()


    ########################################################
    # check that we have averaged properly.  To do this we are
    # going to plot the annual cycle of the global mean field
    # for the regridded dataset and also for each year



    #global mean
    plt.subplot(2, 2, 1) # global mean from each year
    #subcube = subcube_mean_mon.copy(data = )  # set up structure of subcube

    if cube.coord('latitude').has_bounds():
        cube.coord('latitude').bounds
    else:
        cube.coord('latitude').guess_bounds()

    if cube.coord('longitude').has_bounds():
        cube.coord('longitude').bounds
    else:
        cube.coord('longitude').guess_bounds()

    grid_areas  =  iris.analysis.cartography.area_weights(cube)
    newcube = cube.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)
    nt = len(newcube.data)
    nyears = np.int(nt/12)
    print('nyears is', nyears)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = newcube.data[tstart:tend]
        plt.plot(plotdata, color = 'r')



    # global mean from average

    grid_areas  =  iris.analysis.cartography.area_weights(mean_mon_data)
    temporal_mean  =  mean_mon_data.collapsed(['latitude', 'longitude'],  iris.analysis.MEAN, weights = grid_areas)

    plt.plot(temporal_mean.data, color = 'b', label = 'avg')
    plt.title('globavg ' + FIELDNAMEIN)
    plt.legend()




    # check at 30N
    plt.subplot(2, 2, 2)

    bounds = cube.coord('latitude').bounds
    nbounds = cube.coord('latitude').nbounds
    nbounds, dummy = np.shape(bounds)
    for i in range(0, nbounds):
        if (bounds[i, 0]>= 32. >= bounds[i, 1] or bounds[i, 0]<= 32. <bounds[i, 1]):
            index = i
    #print(cube)
    if ndim>= 4:
        subcube = cube[:, :, index, :]
    else:
        subcube = cube[:, index, :]
    cube_avg_30N = subcube.collapsed(['longitude'], iris.analysis.MEAN)

    for i in range(0, nyears):
        tstart = i*12
        tend = (i+1)*12
        plotdata = cube_avg_30N.data[tstart:tend]
        plt.plot(plotdata, color = 'r')

    #mean at 30N
    slice_30N =  mean_mon_data.extract(iris.Constraint(latitude = 32))
    mean_30N = slice_30N.collapsed(['longitude'],  iris.analysis.MEAN)


    plt.plot(mean_30N.data, color = 'b', label = 'avg')
    plt.title('average at 30N by month')
    plt.legend()
    plt.show()
    plt.close()


#############################################################################
def getnames(exptnamein):

# this program will get the names of the files and the field for each
# of the model

    # set up model specific dictionaries
    
    COSMOS_FIELDS  = {"ua" : "u-velocity", "va" : "v-velocity"}

    ECearth_FIELDS  = {"ua" : "U component of wind", 
                       "va" : "V component of wind" }

    HadCM3_FIELDS  = {"ua" : "U COMPNT OF WIND ON PRESSURE LEVELS", 
                      "va" : "V COMPNT OF WIND ON PRESSURE LEVELS" }

    IPSLCM5A_FIELDS  = { }

    NorESM_FIELDS = {"ua" : "U", "va" : "V"}
    
    CCSM42_FIELDS = {"ua" : "U", "va": "V" }
    
    CESM12_FIELDS = {"ua": "U", "va": "V"}
    
    CESM12_EXTRA =  {"Eoi400": "b.e12.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b.e12.B1850.f09_g16.preind.",
                    }
    
    CESM2_EXTRA =  {"Eoi400": "b.e21.B1850.f09_g17.PMIP4-midPliocene-eoi400.001.cam.h0.",
                     "E280": "b.e21.B1850.f09_g17.CMIP6-piControl.001.",
                    }
    
    CCSM4_EXTRA =  {"Eoi400": "b40.B1850.f09_g16.PMIP4-pliomip2.",
                     "E280": "b40.B1850.f09_g16.preind.",
                    }

    ECearth_EXPT = {"Eoi400": "mPlio",
              "E280":"PI"
              }
    
    CESM12_EXPT = {"Eoi400": "PlioMIP2",
              "E280":"PI"
              }

    IPSLCM5A_EXPT = {"Eoi400": "Eoi400",
              "E280":"PI"
              }

    CESM12_TIME = {"E280" : ".0707.0806",
                   "Eoi400" : ".1101.1200"
                   }
    
    CESM2_TIME = {"E280" : ".1201.1300",
                   "Eoi400" : ".1101.1200"
                   }
    
    CCSM4_TIME = {"Eoi400" : ".1001.1100",
                  "E280" : ".0081.0180"
                  }

    IPSLCM5A_TIME = {"Eoi400": "3581_3680",
              "E280":"3600_3699"
              }

    IPSLCM5A21_TIME = {"Eoi400": "3381_3480",
              "E280":"6110_6209",
              }
    IPSLCM6A_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr_185001-204912",
              "E280":"piControl_r1i1p1f1_gr_285001-304912",
              }
    IPSLCM6A_TIME_ALT = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_185001-204912",
              "E280":"piControl_r1i1p1f1_gn_285001-304912",
              }
    GISS_TIME1 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_305101-310012",
              "E280":"piControl_r1i1p1f1_gn_490101-495012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_190101-195012"
              }
    CCSM4_UofT_TIME = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gr1_160101-170012",
              "E280":"piControl_r1i1p1f1_gr1_150101-160012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    GISS_TIME2 = {"Eoi400": "midPliocene-eoi400_r1i1p1f1_gn_310101-315012",
              "E280":"piControl_r1i1p1f1_gn_495101-500012",
              "E560": "abrupt-2xCO2_r1i1p1f1_gn_195101-200012"
              }
    
    # get names for each model
    if MODELNAME   ==  'MIROC4m':
        filename = FILESTART + MODELNAME + '/'
        fielduse = FIELDNAMEIN
        filename = (filename + fielduse + '/MIROC4m_' + 
                    exptnamein + '_Amon_' + fielduse + '.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
   

    if MODELNAME   ==  'COSMOS':
        if LINUX_WIN  == 'l':
            filename = FILESTART + 'AWI/COSMOS/'
            filename = filename + exptnamein + '/'
        else:
            filename = FILESTART + '/COSMOS/'
        fielduse = COSMOS_FIELDS.get(FIELDNAMEIN)
        filename = (filename + exptnamein + '.' + FIELDNAMEIN +
                    '_2650-2749_monthly_mean_time_series.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
   

    if MODELNAME   ==  'CCSM4-UoT':
        if LINUX_WIN  == 'l':
            filename = FILESTART + 'UofT/'
            filename = (filename + 'UofT-CCSM4/for_julia/' + 
                        exptnamein + '/Amon/')
        else:
            filename = FILESTART+'UofT-CCSM4\\'+exptnamein+'\\'
        fielduse = FIELDNAMEIN
        
        filename = (filename +  fielduse +
                      '_Amon_' + exptnamein + '_UofT-CCSM4_gr.nc')
        constraint = iris.Constraint(air_pressure = LEVEL)
        

    if MODELNAME  == 'HadCM3':
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = HadCM3_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + 'LEEDS/HadCM3/' + exptuse
                    + '/' + FIELDNAMEIN + '/'
                    + exptuse + '.' + FIELDNAMEIN + '.')
        constraint = iris.Constraint(p = LEVEL)

    if MODELNAME  == 'MRI2.3':
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = FIELDNAMEIN + str(np.int(LEVEL))
        
        filename = (FILESTART + 'MRI-CGCM2.3/' + fielduse + 
                    '/' + exptuse + '.' + fielduse + '.')
        constraint = None
      
    if MODELNAME  == 'EC-Earth3.1' or MODELNAME == 'EC-Earth3.3':
        fileend = '_uv.nc'
        exptuse = EXPTNAME_L.get(exptnamein)
        fielduse = ECearth_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + MODELNAME + '/'
                    + MODELNAME 
                    + '_' 
                    + ECearth_EXPT.get(exptnamein) 
                    + fileend)
        constraint = iris.Constraint(air_pressure = LEVEL * 100)
   

    if MODELNAME  == 'IPSLCM5A' or MODELNAME  == 'IPSLCM5A2':
        exptuse = EXPTNAME_L.get(exptnamein)
        if MODELNAME  == 'IPSLCM5A':
            timeuse = IPSLCM5A_TIME.get(exptnamein)
        if MODELNAME  == 'IPSLCM5A2':
            timeuse = IPSLCM5A21_TIME.get(exptnamein)
        fielduse = IPSLCM5A_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART+MODELNAME+'/'
                  +IPSLCM5A_EXPT.get(exptnamein)+'.'
                  +IPSLCM5A_FIELD.get(fielduse)+'_'+timeuse+'_monthly_TS.nc')
        constraint = None

    if MODELNAME  == 'NorESM1-F' or MODELNAME  == 'NorESM-L':
        fielduse = NorESM_FIELDS.get(FIELDNAMEIN)
        filename = (FILESTART + MODELNAME + '/' + MODELNAME + '_' 
                    + exptnamein + '_' + fielduse + '.nc')
        constraint = iris.Constraint(pressure = LEVEL)

    if MODELNAME  == 'IPSLCM6A':
        fielduse = FIELDNAMEIN
        filename = ('/nfs/hera1/earjcti/PLIOMIP2/' + MODELNAME 
                    + '/' + fielduse + '_' + 'Amon_IPSL-CM6A-LR_'
                   + IPSLCM6A_TIME.get(exptnamein)+'.nc')
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
    if MODELNAME  == 'GISS2.1G':
        fielduse = FIELDNAMEIN
        exptuse = EXPTNAME_L.get(exptnamein)
        filename = []
        filename.append('/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
                        + exptuse + '/' + fielduse + '_Amon'
                        + '_GISS-E2-1-G_' 
                        + GISS_TIME1.get(exptnamein) + '.nc')
        filename.append('/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
                        + exptuse + '/' + fielduse + '_Amon'
                        + '_GISS-E2-1-G_' 
                        + GISS_TIME2.get(exptnamein) + '.nc')
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
       

    if MODELNAME == 'CCSM4-Utr':
        filename=(FILESTART + 'CESM1.0.5/' + exptnamein + '/' +
                  exptnamein + '_' + CCSM42_FIELDS.get(FIELDNAMEIN) +
                  '.nc')
        fielduse = FIELDNAMEIN
        
    if MODELNAME == 'CESM1.2':
        filename=(FILESTART + 'NCAR/' + 
                  CESM12_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CESM12_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
        

    
    if MODELNAME == 'CESM2':
        print(exptnamein)
        print(CESM2_TIME.get(exptnamein))
        print(CESM2_EXTRA.get(exptnamein))
        print(CESM12_FIELDS.get(FIELDNAMEIN))

        filename=(FILESTART + 'NCAR/' + 
                  CESM2_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CESM2_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
               
            
    if MODELNAME == 'CCSM4':
        filename = (FILESTART + 'NCAR/' + 
                  CCSM4_EXTRA.get(exptnamein) + 
                  CESM12_FIELDS.get(FIELDNAMEIN) +
                  CCSM4_TIME.get(exptnamein) + '.nc')
        fielduse = CESM12_FIELDS.get(FIELDNAMEIN)
        constraint = iris.Constraint(air_pressure = LEVEL * 100.)
      
        
    print(fielduse,filename,constraint)
    retdata = [fielduse, filename, constraint]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "IPSLCM5A" # MIROC4m  COSMOS CCSM4UoT -EC-Earth3.3
                   # HadCM3 MRI
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-Utr, CESM1.2
                   # CCSM4
                   # new to this version
                   # EC-Earth3.3 CESM2 (b.e21)

EXPTNAME  =  {
        "E280" : "E280",
        "Eoi400" : "EOI400",
        "E400":"E400",
        "E560": "E560"}

EXPTNAME_L  =  {
        "E280" : "e280",
        "Eoi400" : "eoi400",
        "E400":"e400",
        "E560": "e560"}



# this is regridding where all results are in a single file
FIELDNAMEIN = 'va'
LEVEL = 850.
EXPTNAMEIN = ['Eoi400', 'E280', 'E400']
#EXPTNAMEIN = ['E400']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'




for exptname in EXPTNAMEIN:


    # call program to get model dependent names
    # fielduse,  and  filename
    retdata = getnames(exptname)

    fielduse = retdata[0]
    filename = retdata[1]
    lev_constraint = retdata[2]
    print(fielduse, filename, lev_constraint)
    #sys.exit(0)

    FIELDNAMEOUT = FIELDNAMEIN + '_' + np.str(LEVEL)
    EXPTNAMEOUT = EXPTNAME.get(exptname)

    regrid_data(exptname, filename, fielduse, lev_constraint)

::::::::::::::
sea_SAT_relationships.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
#Created on 05/03/2020


#@author: earjcti

This program will plot the land temperature relationships
requested by IPCC in particular

- Land vs sea: I suggest reporting both (1) all sea vs GMAT, and 
(2) sea from 60S - 60N vs GMST. 
These metrics are also needed to help inform GMAT reconstructions from the proxy data, 
which are primarily marine-based.


"""

import warnings
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.analysis.cartography
import iris.coord_categorisation
from scipy import stats

warnings.filterwarnings("ignore")

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match.
    """

    for coord in cube.coords():
        name = coord.standard_name
        if name not in ['latitude', 'longitude']:
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points = coord.points.astype('float32')
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name

    return cube

def get_lsm(filein):
    """
    returns a  numpy array of land points and one of sea points
    """
    tempcube = iris.load_cube(filein)
    cubegrid = iris.load_cube('one_lev_one_deg.nc')
    lsmcube = tempcube.regrid(cubegrid, iris.analysis.Linear())
    landpoints = lsmcube.data
    seapoints = (lsmcube.data - 1.0) * (-1.0)

    return landpoints, seapoints

def get_mean_data(model, expt, field):
    """
    gets the cube of mean data for a single model

    Parameters
    ----------
    model : the name of the model we are interested in
    expt : whether it is the experiment or the control

    Returns
    -------
    a cube with the mean data from this file
    grid_areas = the size of the grid for averaging
    """

    filename = (FILESTART + 'regridded/' + model
                + '/' + expt + '.' + field + '.allmean.nc')

    print(field, filename)
   
    cube = iris.load_cube(filename)
    cube2 = resort_coords(cube)

    cube2.coord('latitude').guess_bounds()
    cube2.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube2)

    return cube2, grid_areas


def get_region(latmin, latmax, cube, mask, grid_areas):
    """
    Gets the average temeprature over land within the
    bounded range

    Parameters
    ----------
    latmin, latmax : the range of latitudes we are extracting data from
    cube : the cube containing average temperatures t
    mask : numpy array containing the mask we want.  Could be a land mask
           a sea mask or ones everywhere (ie all points)
    grid_areas : the size of each gridpoint for weighting

    Returns
    -------
    region_avg : scalar containing the average temperature over the
                     required region

    """

    grid_areas_mask = grid_areas * mask
    grid_areas_band = np.zeros(grid_areas.shape)
    lats = cube.coord('latitude').points

    for j, lat in enumerate(lats):
        if latmin <= lat <= latmax:
            grid_areas_band[j, :] = grid_areas_mask[j, :]

    region_avg = cube.collapsed(['longitude', 'latitude'],
                                iris.analysis.MEAN,
                                weights=grid_areas_band)

    return region_avg.data

def scatter_sea_vs_global(allanom, seaanom, plottype, txtfile):
    """
    plot the global temperature anomaly vs the 
    ocean temperature anomaly for theregion on one plot
    also outputs the regression equation

    Parameters
    ----------
    allanom : temperature anomaly from the globe
    seaanom : temperature anomaly from the sea
    plot type: '' - global or latitude range

    Returns
    -------
    None.

    """

    titlename = 'MPWP - PI: global vs ocean ' + plottype + ' temperature '
    ax = plt.subplot(1, 1, 1)

    for i, model in enumerate(MODELNAMES):
        if i % 4 == 0: # i divides 4 with no remainder
            ax.scatter(seaanom[i], allanom[i], label = model) 
        elif i % 4 == 1 :
            ax.scatter(seaanom[i], allanom[i], label = model, marker='^') 
        elif i % 4 == 2 :
            ax.scatter(seaanom[i], allanom[i], label = model, marker='<') 
        else:
            ax.scatter(seaanom[i], allanom[i], label = model, marker='v') 
    #plt.title(titlename)
    plt.xlabel('ocean temperature ' + plottype + ' anomaly (' + UNITS + ')')
    plt.ylabel('global SAT ' + ' anomaly (' + UNITS + ')')


    # Shrink current axis by 20% and put a legend to the right
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    fileout = (FILEOUTSTART + '/sea_SAT'  + plottype + '_anomaly.eps')
    plt.savefig(fileout)
    fileout = (FILEOUTSTART + '/sea_SAT'  + plottype + '_anomaly.pdf')
    #plt.show()
    plt.savefig(fileout)
    plt.close()


    # print out the regression equation and the data to a file
    print(plottype)
    print('=======')
    (slope, intercept, r_value, 
            p_value, std_err) = stats.linregress(seaanom, allanom)
    print('GMSAT = ' + np.str(np.around(slope, 3)) + 
          'x OSAT + ' + np.str(np.around(intercept, 3)))
    print('pvalue = ' + np.str(np.around(p_value, 3)) + 
          ' rvalue = ' + np.str(np.round(r_value, 3)) + 
          ' rsq = ' + np.str(np.round(r_value * r_value, 3)))
    print('   ')

    # print out the values to a file
    
    txtfile.write("modelname, global mean SAT, oceanSAT " + plottype + '\n')
    for i, model in enumerate(MODELNAMES):
        txtfile.write((model + ',' + np.str(np.around(allanom[i],3)) + 
                       ',' + np.str(np.around(seaanom[i],3)) + '\n'))
    
      

#####################################
def main():
    """
    Tha main control of the program to plot the
    polar amplification by temperature by latitude band

    """

    if LINUX_WIN == 'w': 
        exptlsm = (FILESTART + 'regridded/PlioMIP2_Boundary_conds' +
                   '/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'regridded/PlioMIP2_Boundary_conds' +
                   '/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc')
    else:
        exptlsm = (FILESTART + 'PlioMIP2_Boundary_conds/Plio_enh' +
                   '/Plio_enh/Plio_enh_LSM_v1.0.nc')
        cntllsm = (FILESTART+'PlioMIP2_Boundary_conds/Modern_std/' +
                   'Modern_std/Modern_std_LSM_v1.0.nc')


    ########################################################
    # setup: get the lsm for the land sea contrast plot

    exptland, exptsea = get_lsm(exptlsm)
    cntlland, cntlsea = get_lsm(cntllsm)
    fullmask = np.ones(np.shape(exptsea))
    

    #########################################################
    # need to get data from annual mean plot

    sea_anomaly = np.zeros((len(MODELNAMES)))
    sea_6060_anomaly = np.zeros((len(MODELNAMES))) # from 60N-60S
    all_anomaly = np.zeros((len(MODELNAMES)))
    
    for modelno, modeluse in enumerate(MODELNAMES):

        # get mean data
        (exptcube_SAT, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME, FIELD_SAT)
        (cntlcube_SAT, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME, FIELD_SAT)
        (exptcube_SST, grid_areas_expt) = get_mean_data(modeluse, EXPTNAME, FIELD_SST)
        (cntlcube_SST, grid_areas_cntl) = get_mean_data(modeluse, CNTLNAME, FIELD_SST)
        
        # get all data
        expt_data = get_region(-90.0, 90.0, exptcube_SAT, 
                               fullmask, grid_areas_expt)
        cntl_data = get_region(-90.0, 90.0, cntlcube_SAT, 
                               fullmask, grid_areas_cntl)
        print(expt_data, cntl_data)
        all_anomaly[modelno] = expt_data - cntl_data
        

        # get sea anomaly
        expt_data = get_region(-90.0, 90.0, exptcube_SST, 
                               exptsea, grid_areas_expt)
        cntl_data = get_region(-90.0, 90.0, cntlcube_SST, 
                               cntlsea, grid_areas_cntl)
        sea_anomaly[modelno] = expt_data - cntl_data

        # get sea anomaly from 60N-60S
        expt_data = get_region(-60.0, 60.0, exptcube_SST, 
                               exptsea, grid_areas_expt)
        cntl_data = get_region(-60.0, 60.0, cntlcube_SST, 
                               cntlsea, grid_areas_cntl)
        sea_6060_anomaly[modelno] = expt_data - cntl_data

   # plot everything on one plot and write results to a text file
    filetext = open((DATAOUTSTART + '/data_for_sea_SAT_anomaly.txt'), "w+")
   
    scatter_sea_vs_global(all_anomaly, sea_anomaly, '', filetext)
    scatter_sea_vs_global(all_anomaly, sea_6060_anomaly, '_60N-60S_', filetext)
    
    filetext.close  

##########################################################
# DEFINITIONS

LINUX_WIN = 'l'
FIELD_SAT = 'NearSurfaceTemperature'
FIELD_SST = 'SST'
UNITS = 'degC'
EXPTNAME = 'EOI400'
CNTLNAME = 'E280'

MODELNAMES=['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]


if LINUX_WIN == 'w':
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
    FILEOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD_SAT + '\\')
    DATAOUTSTART = ('C:\\Users\\julia\\OneDrive\\WORK\\DATA\\' +
                        'regridded\\allplots\\' + FIELD_SAT + '\\')
else:
    FILESTART = '/nfs/hera1/earjcti/'
    FILEOUTSTART = '/nfs/hera1/earjcti/regridded/allplots/' + FIELD_SAT + '/'
    DATAOUTSTART = '/nfs/hera1/earjcti/regridded/alldata/'

FILEOUT = FILESTART + 'regridded/alldata/data_for_sea_SAT_relationships.txt'


main()
::::::::::::::
statistical_significance_of_changes.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sep 21 2019

@author: earjcti

This will plot the MMM precipitation and temperature anomalies.  It will hatch the
statistically robust changes based on the followin method (Mba et al 2018)

1. Roughly 80% of models must agree on the direction of the change
   (if we have 13 models then 10 must agree)
2. The ((ensemble mean change) / (the ensemble standard deviation))>1.
   (note I am not sure whether to get the ensemble standard deviation from
    pi or the mPWP maybe I will try both)

"""

import sys
import iris
import iris.quickplot as qplt
import iris.plot as iplt
#from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import matplotlib.ticker as mticker
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
import netCDF4


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if field == 'SST' or field == 'NearSurfaceTemperature':
        if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
            cube.units = 'Celsius'
        else:
            cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    if modeluse == 'EC-Earth3.1' and field == 'SST':
        cube.coord('latitude').bounds = None
        cube.coord('longitude').bounds = None

    cube.cell_methods = None

    return cube



def get_ind_model_anomaly():
    """
    gets the anomaly data for all the models puts them into a list of cubes for
    returning to the calling program
    """

    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        cubepi = get_data(FILESTART + model + '/E280.' + FIELDNAME + '.allmean.nc',
                          FIELDNAME, model)
        cubeplio = get_data(FILESTART + model + '/EOI400.' + FIELDNAME + '.allmean.nc',
                            FIELDNAME, model)
        cubediff = cubeplio - cubepi
        cubelist.append(cubediff)


    return cubelist

def check_sign(cubelist):
    """
    we are checking that a number (NSIGN) of cubes has the same sign in a field
    for example if the field is temperature we may want to see that 10/13 models all show the
    same sign of temperature change

    input a list of cubes from models
    output a numpy array which is 1 where at least 'NSIGN' models have the same sign
           and 0 when they don't
    """

    cube = cubelist[0]
    ydim, xdim = np.shape(cube.data)
    posarr = np.zeros((ydim, xdim))
    negarr = np.zeros((ydim, xdim))

    for cubeno, cube in enumerate(cubelist):
        cubedata = cube.data
        temparr = np.ma.where(cubedata > 0, 1, 0) # temporary array
        posarr = posarr + temparr
        temparr = np.ma.where(cubedata < 0, 1, 0)
        negarr = negarr + temparr

    # if there are more than NSIGN elements in posarr or negarr than the same sign arr is set
    sign_arr = np.ma.where(posarr >= NSIGN, 1, 0)
    temparr = np.ma.where(negarr >= NSIGN, 1, 0)
    sign_arr = sign_arr + temparr
    newcube = cube.copy(data=sign_arr)

    return newcube

def check_large_anomaly(ratiocube):
    """
    if ratio is greater than 1 set to 1, otherwise set to zero
    """
    temparr = np.ma.where(ratiocube.data > 1, 1, 0)
    newcube = ratiocube.copy(data=temparr)

    return newcube


def main():
    """
    1. get the change in the field from all the models
    2. find out where > 70% of the models agree in the sign of the change (region1)
    3. get mean change in the field
    4. get the standard deviation for the control climate
    5. find out where mean change / standard deviation > 1 (region2)
    6. plot, hatching where region 1 and region2 are satisfied
    """

    namefield = {"NearSurfaceTemperature" : "SAT",
                 "TotalPrecipitation" : "Precipitation",
                 "SST" : "SST"
                 }

    modelcubelist = get_ind_model_anomaly()
    cube_sign = check_sign(modelcubelist) # checks where a given number of them are the correct sign
    meancube = iris.load_cube(FILESTART + FIELDNAME + '_multimodelmean.nc',
                              FIELDNAME + 'mean_anomaly')  # get mean
    stdevcube = iris.load_cube(FILESTART + FIELDNAME + '_multimodelmean.nc',
                               FIELDNAME + 'std_pi') # get standard deviation
    cube_mean_std_sign = check_large_anomaly(meancube / stdevcube)

    #qplt.contourf(cube_sign)
    ax = plt.axes(projection = ccrs.PlateCarree())
    if FIELDNAME == 'TotalPrecipitation':
       
        qplt.contourf(meancube, np.arange(-1.4, 1.6, 0.2), cmap='RdBu', extend='both')
        plt.figtext(0.02, 0.97,'d)',
                   horizontalalignment='left',
                   verticalalignment='top',
                   fontsize=20)
    else:
        qplt.contourf(meancube, np.arange(0, 5.5, 0.5), cmap='Reds', extend='both')
    iplt.contourf(cube_sign, 1, hatches=[None, '///'], colors='none')
    iplt.contourf(cube_mean_std_sign, 1, hatches=[None, 3 * '\\\''], colors='none')
    plt.gca().coastlines()
    gl = ax.gridlines(crs=ccrs.PlateCarree(), linestyle='-', draw_labels='true')
    gl.xlabels_top = False
    gl.ylabels_left = False
    gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])
    gl.xformatter = LONGITUDE_FORMATTER
    gl.yformatter = LATITUDE_FORMATTER
    plt.title(namefield.get(FIELDNAME) +' anomaly: multimodel mean')

    plt.savefig(OUTSTART + FIELDNAME + 'robust_anomalies.eps')
    plt.savefig(OUTSTART + FIELDNAME + 'robust_anomalies.pdf')
    plt.close()

    OUTNC = FILESTART + 'dummy.nc'
    if FIELDNAME == 'NearSurfaceTemperature':
        OUTNC = FILESTART + 'alldata/data_for_fig2.nc'
    if FIELDNAME == 'TotalPrecipitation':
        OUTNC = FILESTART + 'alldata/data_for_fig5d.nc'
        
    print(OUTNC)
    
    cubelist = iris.cube.CubeList([meancube, cube_sign, cube_mean_std_sign])
    iris.save(cubelist, OUTNC)        
    


    return



# variable definition
LINUX_WIN = 'l'

#FIELDNAME = 'NearSurfaceTemperature'
FIELDNAME = 'SST'
UNITS = 'deg C'

#FIELDNAME = 'TotalPrecipitation'
#UNITS = 'mm/day'


if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    OUTSTART = ' '


MODELNAMES = ['CCSM4-Utr', 'COSMOS', 'CESM1.2', 'CESM2','CCSM4',
              'EC-Earth3.3', 'GISS2.1G', 'HadCM3',
              'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
              'MIROC4m', 'MRI2.3',
              'NorESM-L', 'NorESM1-F',
              'CCSM4-UoT'
             ]
#ODELNAMES = ['EC-Earth3.1']
NSIGN = np.floor(len(MODELNAMES) * 0.8) # *0.8 is 80%

main()
::::::::::::::
temperature_diagnostics.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

#Created on April 29th


#@author: earjcti
#
# This program will read all the means from the regridded files and plot them


import os
import numpy as np
import scipy as sp
import iris
import matplotlib as mp
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import netCDF4
#from mpl_toolkits.basemap import Basemap, shiftgrid
#import Basemap
from netCDF4 import Dataset, MFDataset
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
import iris.analysis.cartography
import iris.coord_categorisation
import iris.quickplot as qplt
import iris.plot as iplt
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys

def resort_coords(cube):
    """
    this will make all the dimensions of the cube match. 
    """
    
    for coord in cube.coords():        
        name=coord.standard_name
        if name !='latitude' and name!='longitude':
            if name==None:
                if coord.long_name==None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)
                
    for coord in cube.coords():   # now this will be longitude or latitude
        coord.points=coord.points.astype('float32') 
        coord.var_name = coord.standard_name
        coord.long_name = coord.standard_name
       
    return cube  

def get_pliomip1_data(fieldreq):
    """
    we will get the data from pliomip1
    returns an array of the mean data and the min, max mean, of the seasonal cycle
    """
    
    PLIOMIP1_FILE = (FILESTART + 'PLIOMIP1/means_for_' 
                     + fieldreq + '.txt')
    
    f1 = open(PLIOMIP1_FILE)
    
    lines = f1.readlines()
    lines[:] = [line.rstrip('\n') for line in lines]
            
    # find line which contains 'modelnameglobal[mean_ocean_eoi400'
    # which is the start of the land sea contrast
    string = 'modelnameglobal[mean_ocean_eoi400'
    land_amplification = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, mean_ocean_eoi400, meanocean_e280, meanocean_anom, 
        mean_land_eoi400, mean_land_e280, mean_land_anom) = line.split(',')
        amp = np.float(mean_land_anom) / np.float(meanocean_anom)
        if model != 'MEAN':
            land_amplification.append(amp)
            
    land_amp_arr = np.asarray(land_amplification, dtype=float)
    
    # find line which contains 'modelname20N-20S[mean_ocean_eoi400'
    # which is the start of the land sea contrast over the tropics
    string = 'modelname20N-20S[mean_ocean_eoi400'
    index=0
    land_amplification_20 = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
     
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, mean_ocean_eoi400, meanocean_e280, meanocean_anom, 
        mean_land_eoi400, mean_land_e280, mean_land_anom) = line.split(',')
        amp = np.float(mean_land_anom) / np.float(meanocean_anom)
        if model != 'MEAN':
            land_amplification_20.append(amp)
            
    land_amp20_arr = np.asarray(land_amplification_20, dtype=float)
    
    # find line which contains '45N-90N_anom'
    # which is the start of the fields averaged over certain regions
    string = '45N-90N_anom'
    index=0
    NH_SH_ratio45 = []
    PA_NH_60 = []
    PA_SH_60 = []
    for i, line in enumerate(lines):
        if string in line:
            index = i
     
            
    for i in range(index + 1, len(lines)):
        line = lines[i]
        if 'modelname' in line:
            break   
        (model, anom_45_90N, anom_45_90S, 
         anom_60_90N, anom_60_90S) = line.split(',')
        NH_SH_ratio = np.float(anom_45_90N) / np.float(anom_45_90S)
      
        if model != 'MEAN':
            NH_SH_ratio45.append(NH_SH_ratio)
            PA_NH_60.append(anom_60_90N)
            PA_SH_60.append(anom_60_90S)
   
    NH_SH_ratio45_arr = np.asarray(NH_SH_ratio45, dtype=float)
    PA_NH_60_arr = np.asarray(PA_NH_60, dtype = float)
    PA_SH_60_arr = np.asarray(PA_SH_60, dtype = float)
    
        
    return (land_amp_arr, land_amp20_arr, NH_SH_ratio45_arr, 
            PA_NH_60_arr, PA_SH_60_arr)

#####################################
def plotmean(modelnames,field,exptname,cntlname,linux_win,units):
   
    if linux_win=='w':
        fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\'+field+'\\'
        exptlsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
        cntllsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
    else:
        fileoutstart='/nfs/hera1/earjcti/regridded/allplots/'+field+'/'
        dataoutstart='/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm=FILESTART+'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
        cntllsm=FILESTART+'PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
     
 
    ########################################################
    # setup: get the lsm for the land sea contrast plot
    
    tempcube=iris.load_cube(exptlsm)
    cubegrid=iris.load_cube('one_lev_one_deg.nc')
    exptlsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
    exptsea=(exptlsmcube.data - 1.0)*(-1.0)
    
   
    tempcube=iris.load_cube(cntllsm)
    cntllsmcube=tempcube.regrid(cubegrid,iris.analysis.Linear())
    cntlsea=(cntllsmcube.data - 1.0)*(-1.0)
    
 
    #########################################################
    # need to get data from annual mean plot
    
    nh_anomaly=np.zeros(len(modelnames))
    sh_anomaly=np.zeros(len(modelnames))
    nh_anomaly_extratropics=np.zeros(len(modelnames))
    sh_anomaly_extratropics=np.zeros(len(modelnames))
    nh_anomaly_polar=np.zeros(len(modelnames))
    sh_anomaly_polar=np.zeros(len(modelnames))
    all_anomaly=np.zeros(len(modelnames))
    land_anomaly=np.zeros(len(modelnames))
    sea_anomaly=np.zeros(len(modelnames))
    land_anomaly_tropics=np.zeros(len(modelnames))
    sea_anomaly_tropics=np.zeros(len(modelnames))
    
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        exptfile=FILESTART+'regridded/'+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+'regridded/'+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        cube=iris.load_cube(exptfile)
        exptcube = resort_coords(cube)
        cube=iris.load_cube(cntlfile)
        cntlcube = resort_coords(cube)
    
        #######################################
        # compare NH vs SH temperature
        exptcube.coord('latitude').guess_bounds()
        exptcube.coord('longitude').guess_bounds()
        cntlcube.coord('latitude').guess_bounds()
        cntlcube.coord('longitude').guess_bounds()
        grid_areas_expt = iris.analysis.cartography.area_weights(exptcube)
        grid_areas_cntl = iris.analysis.cartography.area_weights(cntlcube)
        
        # exptcube
        nlat=len(exptcube.coord('latitude').points)
        lats=exptcube.coord('latitude').points
        
        grid_areas_nh=np.zeros(grid_areas_expt.shape)
        grid_areas_sh=np.zeros(grid_areas_expt.shape)
        grid_areas_nh_extratropics=np.zeros(grid_areas_expt.shape)
        grid_areas_sh_extratropics=np.zeros(grid_areas_expt.shape)
        grid_areas_nh_polar=np.zeros(grid_areas_expt.shape)
        grid_areas_sh_polar=np.zeros(grid_areas_expt.shape)
        polarval=60.0
        for j in range(0,nlat):
            if lats[j] <0:
                grid_areas_sh[j,:]=grid_areas_expt[j,:]
            else:
                grid_areas_nh[j,:]=grid_areas_expt[j,:]
            if lats[j] < -45.0:
                grid_areas_sh_extratropics[j,:]=grid_areas_expt[j,:]
            if lats[j] > 45.0:
                grid_areas_nh_extratropics[j,:]=grid_areas_expt[j,:]
            if lats[j] <= -1.0*polarval:
                grid_areas_sh_polar[j,:]=grid_areas_expt[j,:]
            if lats[j] >= polarval:
                grid_areas_nh_polar[j,:]=grid_areas_expt[j,:]
           
        expt_nh = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh)
        expt_sh = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh)
        expt_anom = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN,weights=grid_areas_expt)
        expt_nh_extratropics = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_extratropics)
        expt_sh_extratropics = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_extratropics)
        expt_nh_polar = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_polar)
        expt_sh_polar = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_polar)
        
        
        
        #cntlcube
        nlat=len(cntlcube.coord('latitude').points)
        lats=cntlcube.coord('latitude').points
        grid_areas_nh=np.zeros(grid_areas_cntl.shape)
        grid_areas_sh=np.zeros(grid_areas_cntl.shape)
        for j in range(0,nlat):
            if lats[j] <0:
                grid_areas_sh[j,:]=grid_areas_cntl[j,:]
            else:
                grid_areas_nh[j,:]=grid_areas_cntl[j,:]
           
        cntl_nh = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh)
        cntl_sh = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh)
        cntl_anom = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN,weights=grid_areas_cntl)
        cntl_nh_extratropics = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_extratropics)
        cntl_sh_extratropics = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_extratropics)
        cntl_nh_polar = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_nh_polar)
        cntl_sh_polar = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sh_polar)
        
        
        nh_anomaly[modelno]=expt_nh.data-cntl_nh.data
        sh_anomaly[modelno]=expt_sh.data-cntl_sh.data
        nh_anomaly_extratropics[modelno]=expt_nh_extratropics.data-cntl_nh_extratropics.data
        sh_anomaly_extratropics[modelno]=expt_sh_extratropics.data-cntl_sh_extratropics.data
        nh_anomaly_polar[modelno]=expt_nh_polar.data-cntl_nh_polar.data
        sh_anomaly_polar[modelno]=expt_sh_polar.data-cntl_sh_polar.data
        all_anomaly[modelno]=expt_anom.data-cntl_anom.data
      
        
        #######################################
        # compare land with sea (globally and for tropics)
        
        
        # expt
        # first check grid
        for i in range(0,len(exptcube.coord('latitude').points)):
            if exptcube.coord('latitude').points[i] !=exptlsmcube.coord('latitude').points[i]:
                print('differences in lsm and gridded data',i)
                sys.exit(0)
       
    
        for i in range(0,len(exptcube.coord('longitude').points)):    
            if exptcube.coord('longitude').points[i] != exptlsmcube.coord('longitude').points[i]:
                print('differences in lsm and gridded data',i,exptcube.coord('longitude').points[i],
                  exptlsmcube.coord('longitude').points[i])
                sys.exit(0)
            
        grid_areas_land=grid_areas_expt * exptlsmcube.data  
        grid_areas_sea=grid_areas_expt * exptsea
        
        expt_land = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_land)
        expt_sea = exptcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sea)
        
        # get grid areas and experiment means for tropics
        nlat=len(exptcube.coord('latitude').points)
        lats=exptcube.coord('latitude').points
        grid_areas_land_tropics=np.zeros(grid_areas_land.shape)
        grid_areas_sea_tropics=np.zeros(grid_areas_sea.shape)
        for j in range(0,nlat):
            if ((lats[j] < 20.) and (lats[j] > -20.)):
                grid_areas_land_tropics[j,:]=grid_areas_land[j,:]
                grid_areas_sea_tropics[j,:]=grid_areas_sea[j,:]
    
        expt_land_tropics = exptcube.collapsed(['longitude', 'latitude'],
                                               iris.analysis.MEAN, weights=grid_areas_land_tropics)
        expt_sea_tropics = exptcube.collapsed(['longitude', 'latitude'], 
                                              iris.analysis.MEAN, weights=grid_areas_sea_tropics)
    
        # cntl
        # first check grid
        for i in range(0,len(cntlcube.coord('latitude').points)):
            if cntlcube.coord('latitude').points[i] !=cntllsmcube.coord('latitude').points[i]:
                print('differences in lsm and gridded data',i)
                sys.exit(0)
       
    
        for i in range(0,len(cntlcube.coord('longitude').points)):    
            if cntlcube.coord('longitude').points[i] != cntllsmcube.coord('longitude').points[i]:
                print('differences in lsm and gridded data',i,cntlcube.coord('longitude').points[i],
                  cntllsmcube.coord('longitude').points[i])
                sys.exit(0)
            
        grid_areas_land=grid_areas_cntl * cntllsmcube.data  
        grid_areas_sea=grid_areas_cntl * cntlsea
    
        cntl_land = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_land)
        cntl_sea = cntlcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_sea)
    
        # get grid areas and experiment means for tropics
        nlat=len(cntlcube.coord('latitude').points)
        lats=cntlcube.coord('latitude').points
        grid_areas_land_tropics=np.zeros(grid_areas_land.shape)
        grid_areas_sea_tropics=np.zeros(grid_areas_sea.shape)
        for j in range(0,nlat):
            if ((lats[j] < 20.) and (lats[j] > -20.)):
                grid_areas_land_tropics[j,:]=grid_areas_land[j,:]
                grid_areas_sea_tropics[j,:]=grid_areas_sea[j,:]
    
        cntl_land_tropics = cntlcube.collapsed(['longitude', 'latitude'],
                                               iris.analysis.MEAN, weights=grid_areas_land_tropics)
        cntl_sea_tropics = cntlcube.collapsed(['longitude', 'latitude'], 
                                              iris.analysis.MEAN, weights=grid_areas_sea_tropics)
    
    
        land_anomaly[modelno]=expt_land.data-cntl_land.data
        sea_anomaly[modelno]=expt_sea.data-cntl_sea.data
    
        land_anomaly_tropics[modelno]=expt_land_tropics.data-cntl_land_tropics.data
        sea_anomaly_tropics[modelno]=expt_sea_tropics.data-cntl_sea_tropics.data
    
    # get data from PlioMIP1 if required
    if PLIOMIP1 == 'y':
        (pliomip1_landsea_amp, 
         pliomip1_tropics_landsea_amp,
         pliomip1_NH_SH_ratio45,
         pliomip1_PA_NH,
         pliomip1_PA_SH) = get_pliomip1_data(field)
   
    
    # plot NH SH contrast
    ax=plt.subplot(1,1,1)
    ax.plot(modelnames,nh_anomaly,'x',label='NH anomaly')
    ax.plot(modelnames,sh_anomaly,'x',label='SH anomaly')
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    plt.xticks(rotation='90')
    plt.title('mPWP - PI anomaly for each hemisphere')
    fileout=fileoutstart+'/hemisphere_difference.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/hemisphere_difference.pdf'
    plt.savefig(fileout)
    plt.close()
    
    
    # plot NH SH contrast for extratropics
    ax=plt.subplot(2,1,1)
    ax.plot(modelnames,nh_anomaly_extratropics,'x',label='>45N anom')
    ax.plot(modelnames,sh_anomaly_extratropics,'x',label='<45S anom')
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    #plt.xticks(rotation='45')
    ax.tick_params(axis='x',labelbottom='False')
    plt.title('Plio_core - PI_Ctl; extratropical NH/SH anomaly')
    plt.figtext(0.02, 0.97,'c)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    ax=plt.subplot(2,1,2)
    ax.plot(modelnames,nh_anomaly_extratropics/sh_anomaly_extratropics
            ,'x',label='>45N/<45S')
    ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
    
    if PLIOMIP1 == 'y':
       for mod_p1 in pliomip1_NH_SH_ratio45:
            ax.axhline(y=mod_p1, color='grey', alpha=0.4)
    
    
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.4*box.height), box.width * 0.8, (0.9*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel('factor')
    plt.xticks(rotation='90')
    #plt.title('mPWP - PI anomaly')
    
    fileout=fileoutstart+'/hemisphere_difference_extratropics.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/hemisphere_difference_extratropics.pdf'
    plt.savefig(fileout)
    plt.close()
    
    # write out hemisphere difference extratropics
    
    txtout = open(FILEOUT, "w+") 
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3c \n')
        
        writedata = ("model_name, nh_anom_et, sh_anom_et \n")
    
        txtout.write(writedata)
        for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(nh_anomaly_extratropics[i],2)) + ',' + 
                         np.str(np.around(sh_anomaly_extratropics[i],2)) + '\n')
            txtout.write(writedata)
   
     # plot polar amplification 
    ax=plt.subplot(1,1,1)
    labelname='>'+np.str(np.int(polarval))+'N amplification'
    ax.plot(modelnames,nh_anomaly_polar/all_anomaly,'x',label=labelname)
    labelname='<'+np.str(np.int(polarval))+'S amplification'
    ax.plot(modelnames,sh_anomaly_polar/all_anomaly,'x',label=labelname)
    box = ax.get_position()
    ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.7, (0.7*box.height)])
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel('factor')
    plt.xticks(rotation='90')
    ax.axhline(y=1.0, xmin=0.0, xmax=len(modelnames), color='r')
    #ax.tick_params(axis='x',labelbottom='False')
    plt.title('Plio_Core - PI_Ctl; polar amplification factor')
    plt.figtext(0.02, 0.97,'d)',
     horizontalalignment='left',
     verticalalignment='top',
     fontsize=20)
    
    print('NH polar amplification', np.mean(nh_anomaly_polar) / np.mean(all_anomaly))
    print('SH polar amplification', np.mean(sh_anomaly_polar) / np.mean(all_anomaly))
    print('total polar amplification', np.mean(nh_anomaly_polar + sh_anomaly_polar) / (2.0 * np.mean(all_anomaly)))
    print('all polar amplification', (nh_anomaly_polar + sh_anomaly_polar) / (2.0 * all_anomaly))
    nh_amps = [x1 / x2 for (x1, x2) in zip(nh_anomaly_polar, all_anomaly)] 
    sh_amps = [x1 / x2 for (x1, x2) in zip(sh_anomaly_polar, all_anomaly)] 
    print('nh amps unsorted', nh_amps)
    print('nh amps',np.sort(nh_amps))
    print('nh median', np.median(nh_amps))
    print('sh median', np.median(sh_amps))
    print('nh percentiles 10/50/90',np.percentile(nh_amps, 10),
          np.percentile(nh_amps,50), np.percentile(nh_amps,90))
    print('sh percentiles 10/50/90',np.percentile(sh_amps, 10),
          np.percentile(sh_amps,50), np.percentile(sh_amps,90))
   
   
    
    fileout=fileoutstart+'/polar_amplification_'+np.str(np.int(polarval))+'.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/polar_amplification_'+np.str(np.int(polarval))+'.pdf'
    plt.savefig(fileout)
    plt.close()
    
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3d \n')
        
        writedata = ("model_name, nh_anom_polar, sh_anom_polar, global_anom \n")
    
        txtout.write(writedata)
        for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(nh_anomaly_polar[i],2)) + ',' + 
                         np.str(np.around(sh_anomaly_polar[i],2)) + ',' + 
                         np.str(np.around(all_anomaly[i],2)) + 
                         '\n')
            txtout.write(writedata)
    
    
    # plot land sea contrast
    if field != 'TotalPrecipitation':
        ax=plt.subplot(2, 1, 1)
    else:
        ax = plt.subplot(1, 1, 1)
    ax.plot(modelnames,land_anomaly,'x',label='Land anomaly')
    ax.plot(modelnames,sea_anomaly,'x',label='Sea anomaly')
    box = ax.get_position()
    if field != 'TotalPrecipitation':
        ax.set_position([box.x0, box.y0+(0.3*box.height), box.width * 0.8, (0.8*box.height)])
        ax.tick_params(axis='x',labelbottom='False')
        plt.figtext(0.02, 0.97,'b)',
                       horizontalalignment='left',
                       verticalalignment='top',
                       fontsize=20)
    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    
    plt.ylabel(units)
   
    #plt.xticks(rotation='45')
    plt.title('Plio_Core - PI_Ctrl global land-sea anomaly')
    
    
    # plot land sea contrast tropics]
    
    ax2=plt.subplot(2,1,2)
    ax2.plot(modelnames,land_anomaly_tropics,'x',label='Land anomaly')
    ax2.plot(modelnames,sea_anomaly_tropics,'x',label='Sea anomaly')
    box = ax2.get_position()
    ax2.set_position([box.x0, box.y0+(0.4*box.height), box.width * 0.8, (0.9*box.height)])
    ax2.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.ylabel(units)
    plt.xticks(rotation='90')
    plt.title('Plio_Core - PI_Ctrl 20N-20S land-sea anomaly')
    
    print('sea anomaly tropics',np.mean(sea_anomaly_tropics))

    fileout=fileoutstart+'/land_sea_contrast.pdf'
    plt.savefig(fileout)
    fileout=fileoutstart+'/land_sea_contrast.eps'
    plt.savefig(fileout)
    plt.close()
    
    # write out
    
    if field == 'NearSurfaceTemperature':
        txtout.write('data for 3b \n')
    else:
        txtout.write('data for 6b \n' )
        
    writedata = ("model_name, land_anomaly, sea_anomaly, " + 
                 "tropical_land_anomaly, tropical_sea_anomaly\n")
    
    
    txtout.write(writedata)
    for i, mod in enumerate(modelnames):
            writedata = (mod + ',' + 
                         np.str(np.around(land_anomaly[i],2)) + ',' + 
                         np.str(np.around(sea_anomaly[i],2)) + ',' + 
                         np.str(np.around(land_anomaly_tropics[i],2)) + ',' + 
                         np.str(np.around(sea_anomaly_tropics[i],2)) + '\n')
            print(writedata)
            txtout.write(writedata)
        
    txtout.close
    print('mean land anomaly', np.mean(land_anomaly))
    print('mean sea anomaly', np.mean(sea_anomaly))
    
    ######################################
    # plot land sea contrast as a factor
    factor_land=land_anomaly / sea_anomaly
    factor_land_tropics=land_anomaly_tropics / sea_anomaly_tropics
   
    if field != 'NearSurfaceTemperature':
        ax=plt.subplot(2,1,1)
    else:
        ax = plt.subplot(1, 1, 1)
    ax.plot(modelnames,factor_land,'x')
    ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
    if PLIOMIP1 == 'y':
        for mod_amp in pliomip1_landsea_amp:
            ax.axhline(y=mod_amp, color='grey', alpha=0.4)
           
    box = ax.get_position()
    if field != 'NearSurfaceTemperature':
        ax.set_position([box.x0 + (0.1 * box.width),
                         box.y0+(0.3*box.height), 
                         box.width * 0.8, (0.8*box.height)])
        ax.tick_params(axis='x',labelbottom='False')
    else:
        ax.set_position([box.x0, box.y0+(0.3*box.height), 
                         box.width, (0.7*box.height)])
    plt.figtext(0.02, 0.97,'b)',
                horizontalalignment='left',
                verticalalignment='top',
                fontsize=20)
    plt.xticks(rotation='90')
    plt.ylabel('land amplification')
   
    #plt.xticks(rotation='45')
    plt.title('mPWP - PI; land_anomaly / sea anomaly')
    
    if field != 'NearSurfaceTemperature':
        ax=plt.subplot(2,1,2)
        ax.plot(modelnames,factor_land_tropics,'x')
        ax.plot(modelnames,(np.zeros(len(modelnames))+1.0))
        if PLIOMIP1 == 'y':
            for mod_amp in pliomip1_tropics_landsea_amp:
                ax.axhline(y=mod_amp, color='grey', alpha=0.4)
        
        box = ax.get_position()
        ax.set_position([box.x0 + (0.1 * box.width), 
                         box.y0+(0.3*box.height), 
                        box.width * 0.8, (0.9*box.height)])
        plt.ylabel('land amplification')
        plt.xticks(rotation='90')
        plt.title('mPWP - PI (20N-20S); land_anomaly / sea anomaly')
        plt.figtext(0.02, 0.97,'b)',
                   horizontalalignment='left',
                   verticalalignment='top',
                   fontsize=20)
    fileout=fileoutstart+'/land_sea_amplification.eps'
    plt.savefig(fileout)
    fileout=fileoutstart+'/land_sea_amplification.pdf'
    plt.savefig(fileout)
    plt.close()
   
    
   
   
    
   
    
    
    
        
################################################################
def plotmap(modelnames,field,exptname,cntlname,linux_win,units):  
    # this subprogram will 
    # 1. read in the data from each map.  
    # 2. It will normalise the data by subtracting the mean and dividing by the spatial standard deviation
    # 3. it will then plot the field change by number of standard deviations above and below the m2an

    normalized_cubes=iris.cube.CubeList([])
    standard_dev=[]
    if linux_win=='w':
        FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
        fileoutstart='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\allplots\\'+field+'\\'
        exptlsm=FILESTART+'regridded/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    else:
        FILESTART='/nfs/hera1/earjcti/'
        fileoutstart='/nfs/hera1/earjcti/regridded/allplots/'+field+'/'
        dataoutstart='/nfs/hera1/earjcti/regridded/alldata/'
        exptlsm=FILESTART+'PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
      
    
    lsmcube=iris.load_cube(exptlsm)
        
    for modelno in range(0,len(modelnames)):
        modeluse=modelnames[modelno]
        print(modeluse)
        exptfile=FILESTART+'regridded/'+modeluse+'/'+exptname+'.'+field+'.allmean.nc'
        cntlfile=FILESTART+'regridded/'+modeluse+'/'+cntlname+'.'+field+'.allmean.nc'
        cube=iris.load_cube(exptfile)
        exptcube = resort_coords(cube)
        exptcube.data=exptcube.data.astype('float32') # change to float32 for concatentation later
       
       
        for coord in exptcube.coords():
            if coord.standard_name !='longitude' and coord.standard_name !='latitude':
                exptcube.remove_coord(coord)
                
        cube=iris.load_cube(cntlfile)
        cntlcube = resort_coords(cube)
        cntlcube.data=cntlcube.data.astype('float32')
        
        for coord in cntlcube.coords():
            if coord.standard_name !='longitude' and coord.standard_name !='latitude':
                cntlcube.remove_coord(coord)
    
        
        diffcube=exptcube-cntlcube
       
        #print(exptcube.coord('surface'))
      
        #######################################
        # get mean and spatial standard deviation
        diffcube.coord('latitude').guess_bounds()
        diffcube.coord('longitude').guess_bounds()
       
        grid_areas_diff = iris.analysis.cartography.area_weights(diffcube)
       
        diffmean = diffcube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas_diff)
        diffsd = diffcube.collapsed(['longitude', 'latitude'], iris.analysis.STD_DEV)
        
        standard_dev.append(diffsd.data)
       
        ########################################
        # normalise the cube by dividing by the mean and std dev
        # and plot
        
        normcube=(diffcube-diffmean)/diffsd
        print(normcube)
      
        
        V=np.arange(-4,4.5,0.5)
        mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
        newcolors=mycmap(np.linspace(0,1,len(V+2)))
        white=([1,1,1,1])
        print((len(V)/2)-1,(len(V)/2)+2)
        print((np.ceil(len(V)/2)-1),np.floor((len(V)/2)+2))
        
        newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
        
        mycmap=ListedColormap(newcolors)
           
        #Draw the contour w
       
        qplt.contourf(normcube, V,extend='both',cmap=mycmap)
        #qplt.contourf(normcube, V,extend='both',cmap='RdBu_r')
        
       
        #sys.exit()
        qplt.contour(lsmcube,1,colors='black')
        plt.title(modeluse+': '+field+'\n No of stddev from mean'
                  +'('+(np.str(np.round(diffsd.data,1)))+units+')')

        if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/'+modeluse+'.eps'
            plt.savefig(fileout)
            
        fileout=fileoutstart+'Map_normalised//'+modeluse+'.pdf'
        plt.savefig(fileout)
        plt.close()
        
        #normcube.Coord(1, standard_name='model', long_name='model', var_name='model', units='1', 
       #                       bounds=None, attributes=None, coord_system=None)
        tempcube=iris.util.new_axis(normcube)
        tempcube.add_dim_coord(iris.coords.DimCoord(modelno, 
                standard_name='model_level_number', long_name='model', 
                var_name='model', 
                units=None,
                bounds=None,
                coord_system=None, circular=False),0)
        # tempcube needs to be dtype=float64
        
        
      
        normalized_cubes.append(tempcube)
        
    
    ################################################
    # END OF MODEL LOOP
    equalise_attributes(normalized_cubes)
   
    print(normalized_cubes)
    print(normalized_cubes[0])
    print(normalized_cubes[1])
    print(normalized_cubes[0].dtype)
    print(normalized_cubes[1].dtype)
    allnormcube=normalized_cubes.concatenate_cube()
    
    meancube=allnormcube.collapsed(['model_level_number'], iris.analysis.MEAN)
    maxcube=allnormcube.collapsed(['model_level_number'], iris.analysis.MAX)
    mincube=allnormcube.collapsed(['model_level_number'], iris.analysis.MIN)
    mediancube=allnormcube.collapsed(['model_level_number'], iris.analysis.MEDIAN)
    
    ###########################
    # plot the mean value
    
    V=np.arange(-2.5,2.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(meancube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    print(standard_dev)
    print(np.amin(standard_dev))
    print(np.amax(standard_dev))
    print(minval,maxval)
    #sys.exit(0)
    plt.title(field+'\n Mean No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/meandiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//meandiff.pdf'
    plt.savefig(fileout)
    plt.close()
    plt.show()
        
    
    ###########################
    # plot the median value
    
        
    qplt.contourf(mediancube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Median No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/mediandiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//mediandiff.pdf'
    plt.savefig(fileout)
    plt.close()
    plt.show()
        
    
    ###########################
    # plot the maximum value
    
    V=np.arange(-4.5,4.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(maxcube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Maximum No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/maxdiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//maxdiff.pdf'
    plt.savefig(fileout)
    plt.close()
   
    
    ###########################
    # plot the minimum value
    
    V=np.arange(-2.5,2.75,0.25)
    mycmap = plt.cm.get_cmap('RdBu_r',len(V+2))
    newcolors=mycmap(np.linspace(0,1,len(V+2)))
    white=([1,1,1,1])
    newcolors[np.int(np.ceil((len(V)/2)-1)):np.int(np.floor((len(V)/2)+2))
                  ,:]=white
    mycmap=ListedColormap(newcolors)
        
    qplt.contourf(mincube, V,extend='both',cmap=mycmap)
    qplt.contour(lsmcube,1,colors='black') 
    minval=np.str(np.round(np.amin(standard_dev),1))
    maxval=np.str(np.round(np.amax(standard_dev),1))
    plt.title(field+'\n Minimum No of stddev from mean'
                  +'('+minval+'-'+maxval+units+')')

    if linux_win=='l':
            fileout=fileoutstart+'Map_normalised/mindiff.eps'
            plt.savefig(fileout)
            
    fileout=fileoutstart+'Map_normalised//mindiff.pdf'
    plt.savefig(fileout)
    plt.close()
   

        
         


##########################################################
# main program

filename=' '
linux_win='l'
if linux_win=='w':
    FILESTART='C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'
else:
    FILESTART='/nfs/hera1/earjcti/'
        

modelnames=['CESM2', 'IPSLCM6A', 'COSMOS', 
            'EC-Earth3.3', 'CESM1.2', 'IPSLCM5A',
            'MIROC4m', 'IPSLCM5A2', 'HadCM3',
            'GISS2.1G', 'CCSM4', 
            'CCSM4-Utr', 'CCSM4-UoT', 
            'NorESM-L', 'MRI2.3', 'NorESM1-F'
            ]


#modelnames=['HadCM3','NorESM-L']
fieldnames=['NearSurfaceTemperature']
#units=['degC']
#
#fieldnames=['TotalPrecipitation']
units=['mm/day']
exptname='EOI400'
cntlname='E280'
PLIOMIP1 = 'n'

for field in range(0,len(fieldnames)):
    if fieldnames[field] == 'TotalPrecipitation':
        FILEOUT = FILESTART + 'regridded/alldata/data_for_6b.txt'
        
    if fieldnames[field] == 'NearSurfaceTemperature':
        FILEOUT = FILESTART + 'regridded/alldata/data_for_3b_3c_3d.txt'
       
    
    # will plot NH vs SHPlio_enh_LSM_v1.0Plio_enh_LSM_v1.0
    plotmean(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])
     
    # plotmap will show where the field is larger or smaller than the mean  
    plotmap(modelnames,fieldnames[field],exptname,cntlname,linux_win,units[field])

#sys.exit(0)
::::::::::::::
temperature_gradients.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the SST zonal and meridional gradients acropss the atlantic and 
the pacific

"""

import sys
import iris
import iris.quickplot as qplt
#from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    print(modeluse)
    if modeluse == 'MMM':
        print(filereq,field)
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def get_within_region(cube, region):
    """
    This routine will mask out all the regions that are not 
    input: a temperature cube, a region name
    output: a new cube with all data not within the region masked out.
    """


    if region == 'TROPATL':
        latmin = -20.
        latmax = 20.
        lonmin = 300.
        lonmax = 360.
        
    if region == 'TROPPAC':
        latmin = -20.
        latmax = 20.
        lonmin = 140.
        lonmax = 260.
        
    if region == 'MERIDATL':
        latmin = -70.
        latmax = 70.
        lonmin = 290.
        lonmax = 360.
        
    if region == 'MERIDPAC':
        latmin = -70.
        latmax = 60.
        lonmin = 150.
        lonmax = 260.
        
        
    cubedata = cube.data
    lats = cube.coord('latitude').points
    lons = cube.coord('longitude').points
    

    for j in range(0, len(lats)):
        cubedata[j, :] = np.ma.masked_where(lons < lonmin, 
                                            cubedata[j, :])
        cubedata[j, :] = np.ma.masked_where(lons > lonmax, 
                                            cubedata[j, :])
  
        
    for i in range(0, len(lons)):
        cubedata[:, i] = np.ma.masked_where(lats < latmin, 
                                            cubedata[:, i])
        cubedata[:, i] = np.ma.masked_where(lats > latmax, 
                                            cubedata[:, i])
        
    newcube = cube.copy(data=cubedata)
    
    #if region == 'MERIDPAC':
    #plot
    #    contour = qplt.contourf(newcube)
    #    plt.gca().coastlines()
    #    plt.show()
    #    sys.exit(0)

    return [newcube, lonmin, lonmax, latmin, latmax]



class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    it can either plot the zonal mean or the meridional mean
    """
    def __init__(self, region, timeperiod, lonmin, lonmax, latmin, latmax):

        """
        inputs are:
                    
        """

        fullregname  =  {
                         "TROPATL" : "Atlantic ",
                         "TROPPAC" : "Pacific ",
                         "MERIDATL" : "Atlantic ",
                         "MERIDPAC": "Pacific "}
    
        colorreq = {
                    "mPWP" : "red",
                    "pi" : "blue",
                    "mPWP-pi" : "green"}
        
        hatchreq = {
                    "mPWP" : "",
                    "pi" : "",
                    "mPWP-pi" : ""}
        
        self.region = region
        self.timeperiod = timeperiod
        self.regiontitle = fullregname.get(region)
        self.colorreq = colorreq.get(timeperiod)
        self.hatchreq = hatchreq.get(timeperiod)


        if region == 'TROPATL':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(latmin)) + 
            #                    '-' + np.str(np.int(latmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 10.
            else:
                self.valmin = 20.
                self.valmax = 30.
                
        if region == 'TROPPAC':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(latmin)) + '-' 
            #                    + np.str(np.int(latmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 10.
            else:
                self.valmin = 20.
                self.valmax = 32.
                
        if region == 'MERIDATL':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(lonmin)) + 
            #                    '-' + np.str(np.int(lonmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = -0.
                self.valmax = 20.
            else:
                self.valmin = -10.
                self.valmax = 30.
                
        if region == 'MERIDPAC':
            #self.regiontitle = (self.regiontitle + 
            #                    np.str(np.int(lonmin)) + '-' 
            #                    + np.str(np.int(lonmax)))
            if timeperiod == 'mPWP-pi':
                self.valmin = 0.
                self.valmax = 20.
            else:
                self.valmin = -5.
                self.valmax = 35.
            



    def plotzm(self, cube, cubemin, cubemax, ax, colorname):
        """
        plot the zonal mean
        """
    

        cube_zm = cube.collapsed('latitude', iris.analysis.MEAN)
        cube_zm_min = cubemin.collapsed('latitude', iris.analysis.MEAN)
        cube_zm_max = cubemax.collapsed('latitude', iris.analysis.MEAN)
        lons = cube_zm.coord('longitude').points
        
        #cube_zm_var = cubevar.collapsed('latitude', iris.analysis.MEAN)
        #zm_std = np.sqrt(cube_zm_var.data)
        #cube_zm_2sigma = cube_zm_var.copy(data=zm_std * 2)
        ax.plot(lons, cube_zm.data, label=self.timeperiod, color=self.colorreq)
        ax.fill_between(lons, cube_zm_min.data, cube_zm_max.data, alpha=0.2, 
                        color=self.colorreq,
                        hatch=self.hatchreq)
        plt.title('20N-20S Mean SST:' +  self.regiontitle)
        ax.set_xlabel('longitude')
        ax.set_ylabel('deg C', color=colorname)
        ax.set_ylim(self.valmin, self.valmax)
        ax.tick_params(axis='y', labelcolor=colorname)
        
    
        return

    def plotmm(self, cube, cubemin, cubemax, ax, colorname, fig):
        """
        plot the meridional mean from the cube
        """

        cube_mm = cube.collapsed('longitude', iris.analysis.MEAN)
        cube_mm_min = cubemin.collapsed('longitude', iris.analysis.MEAN)
        cube_mm_max = cubemax.collapsed('longitude', iris.analysis.MEAN)
        lats = cube_mm.coord('latitude').points
        
        for i, lat in enumerate(lats):
            print(lat, cube_mm.data[i], cube_mm_min.data[i], cube_mm_max.data[i])
        ax.plot(cube_mm.data, lats, label=self.timeperiod, color=self.colorreq)
        ax.fill_betweenx(lats, cube_mm_min.data, cube_mm_max.data, alpha=0.2, 
                        color=self.colorreq,
                        hatch=self.hatchreq)
        fig.suptitle('Zonal Mean SST: ' +  self.regiontitle)
        ax.set_xlabel('deg C', color=colorname)
        ax.set_ylabel('latitude')
        ax.set_xlim(self.valmin, self.valmax)
        ax.tick_params(axis='x', labelcolor=colorname)
   
    
        return


# emd of class

def main_time(timeperiod, region):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = (FILESTART + FIELDNAME + '_multimodelmean.nc')

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')
    
    max_cube = get_data(filename, FIELDNAME + 'max_' + timeperiod, 
                         'MMM')
    
    min_cube = get_data(filename, FIELDNAME + 'min_' + timeperiod, 
                         'MMM')
    
    if timeperiod == 'anomaly':
        sd_cube = get_data(filename, 'SSTanomaly_multimodel_stddev', 
                         'MMM')
    else:
        sd_cube = get_data(filename, FIELDNAME + 'std_' + timeperiod, 
                         'MMM')
    variance_cube = np.square(sd_cube)

    # get the mean value within the region and the standard deviation within the 
    # region
    regioncube, lonmin, lonmax, latmin, latmax = get_within_region(mean_cube, region)
    regioncubemax, lonmin, lonmax, latmin, latmax = get_within_region(max_cube, region)
    regioncubemin, lonmin, lonmax, latmin, latmax = get_within_region(min_cube, region)
    regionvariance, lonmin, lonmax, latmin, latmax = get_within_region(variance_cube, region)


    return regioncube, regionvariance, regioncubemin, regioncubemax, lonmin, lonmax, latmin, latmax

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    cubelist = iris.cube.CubeList([])
    
    figno =  {"MERIDATL": "a)",
            "MERIDPAC": "b)",
            "TROPATL": "c)",
            "TROPPAC": "d)"
    }
    
    for j, region in enumerate(REGIONNAMES):
        
        
        
        # get cubes for each timeperiod and plot
        fig, ax1 = plt.subplots() 
        for i in range(0, len(TIMEPERIODS)):
            (cube_time_region, cube_region_variance, 
             cube_region_min, cube_region_max,
             lonmin, lonmax, latmin, latmax)  = main_time(TIMEPERIODS[i], region)
            
            cubelist.append(cube_time_region)
            cubelist.append(cube_region_min)
            cubelist.append(cube_region_max)
       
            plobj = Plotalldata(region, TIMEPERIODS[i], lonmin, lonmax, latmin, latmax)
            if region[0:3] == 'TRO': 
                plobj.plotzm(cube_time_region, cube_region_min,
                             cube_region_max,
                             ax1, 'black')
                outname = OUTSTART + 'zonalmean' + region
                
    
            
                
            if region[0:3] == 'MER':             
                plobj.plotmm(cube_time_region, cube_region_min,
                             cube_region_max,
                             ax1,'black', fig)
                outname = OUTSTART + 'mm_' + region
        
       
        box = ax1.get_position()
        #print(box)
        #sys.exit(0)
        #ax1.set_position([box.x0+(box.height*0.2), box.y0, 1.0, 1.0])
        # plot zonal or meridional mean for the difference
        
        (cubediff_region, cubediff_region_variance, 
         cubediff_region_min, cubediff_region_max,
             lonmin, lonmax, latmin, latmax)  = main_time('anomaly', region)
        print(np.mean(cubediff_region_max.data))
       
        
        plobjdiff = Plotalldata(region, TIMEPERIODS[1] + '-' + TIMEPERIODS[0],
                                lonmin, lonmax, latmin, latmax)
       
        if region[0:3] == 'TRO':             
           ax2 = ax1.twinx() 
           
           plobjdiff.plotzm(cubediff_region, cubediff_region_min,
                            cubediff_region_max, ax2, 'green') 
           
           print('julia',np.mean(cubediff_region.data), 
                 np.mean(cubediff_region_min.data),
                 np.mean(cubediff_region_max.data)
                 )
           #sys.exit(0)
           
           
        if region[0:3] == 'MER':             
            ax2 = ax1.twiny() 
            ax2.xaxis.set_ticks_position("bottom")
            ax2.xaxis.set_label_position("bottom")
            ax2.spines["bottom"].set_position(("axes", -0.2))
            plobjdiff.plotmm(cubediff_region, cubediff_region_min,
                             cubediff_region_max, ax2, 'green', fig) 
           
         # plot the legend and close the plot 
        
        #fig.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
        #fig.legend()
        plt.figtext(0.02, 0.97,figno.get(region),
                    horizontalalignment='left',
                    verticalalignment='top',
                    fontsize=20)
        #plt.subplots_adjust(top=0.85)
        fig.tight_layout(rect=[0, 0.03, 1, 0.95])
        #plt.show()
        #sys.exit(0)
        plt.savefig(outname + '.eps')
        plt.savefig(outname + '.pdf')
       
        iris.save(cubelist, OUTWRITE, netcdf_format='NETCDF3_CLASSIC', fill_value=1.0E20)        
    
       
    return    
        
        

# variable definition
LINUX_WIN = 'l'
FIELDNAME = 'SST'

if LINUX_WIN == 'l':
    FILESTART = ('/nfs/hera1/earjcti/regridded/')
    OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
    OUTWRITE = FILESTART + 'alldata/data_for_fig4.nc'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    OUTSTART = FILESTART + 'allplots\\' + FIELDNAME + '\\'
    OUTWRITE = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\alldata\\data_for_fig4.nc'
    

REGIONNAMES = ['TROPATL', 'TROPPAC', 'MERIDATL', 'MERIDPAC']
#REGIONNAMES = ['TROPATL']

UNITS = 'deg C'
#TIMEPERIODS = ['pi']
TIMEPERIODS = ['pi', 'mPWP']


main()
::::::::::::::
temperature_isotherms_old.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the multimodel mean near surface air temperature with
+/-2 degree isotherms marked out.  It is to understand which regions
are likely to support thermofrost

"""

import sys
import iris
from mpl_toolkits.basemap import Basemap, shiftgrid
import numpy as np
import matplotlib.pyplot as plt


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def plotmean_newaxis(cube, modelno_):
    """
    this will add a new axis to the cube which contains the model number
    this is needed for concatenation
    """

    tempcube_ = iris.util.new_axis(cube)
    tempcube_.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                 standard_name='model_level_number',
                                                 long_name='model',
                                                 var_name='model',
                                                 units=None,
                                                 bounds=None,
                                                 coord_system=None,
                                                 circular=False), 0)
    return tempcube_

def get_within_pm2deg(cube):
    """
    This routine will mask out all the regions that are not within
    + or - 2degrees
    input: a temperature cube
    output: a new cube with all data not within +/-deg masked out.
    """

    cubedata = cube.data

    newdata = np.ma.masked_outside(cubedata, -2.0, 2.0, copy=True)

    newcube = cube.copy(data=newdata)

    return newcube

class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, cube, timeperiod_, fieldname, modelname,
                 outname):

        """
        inputs are:
                    modelname - like 'HadCM3' or 'MMM' multimodelmean
                    cube - a single cube containing the data
        """


        self.titlename = timeperiod_ + ' ' + modelname
        self.cube = cube
        self.outname = outname

        if fieldname == 'NearSurfaceTemperature':
            self.valmin = -30.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'RdBu_r'
            self.use_cbar = 'y'
            
        if fieldname == 'MMM':
            self.valmin = 0.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'gist_ncar_r'
            self.use_cbar = 'n'

        if fieldname == 'TEMP +/- 2DEG':
            self.valmin = -10.
            self.valmax = -5.0
            self.diff = 1.0
            self.colormap = 'jet'
            self.use_cbar = 'n'



    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """


        cubedata = self.cube.data
        latitudes = self.cube.coord('latitude').points
        lon = self.cube.coord('longitude').points
        datatoplot, longitudes = (shiftgrid(180., cubedata,
                                            lon, start=False))


        self.plotmap(datatoplot, longitudes, latitudes)


        return

    def plotmap(self, datatoplot, longitudes, latitudes):
        """
        will plot the data in a map format

        """

        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=-180.0, urcrnrlon=180.0,
                      llcrnrlat=0.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        lonmap, latmap = map(lons, lats)
        map.drawcoastlines()

        values = np.arange(self.valmin, self.valmax, self.diff)
        contourplot = map.contourf(lonmap, latmap, datatoplot, values,
                                   cmap=self.colormap,
                                   extend='both')
        plt.title(self.titlename)

        if self.use_cbar == 'y':
            cbar = plt.colorbar(contourplot, orientation='horizontal')
            cbar.set_label(UNITS, size=10)
            cbar.ax.tick_params(labelsize=8, labelrotation=60)


        fileout = (self.outname + '.eps')
        plt.savefig(fileout, bbox_inches='tight')

        fileout = (self.outname + '.png')#

        plt.savefig(fileout, bbox_inches='tight')
        plt.close()

def main_model_ind():
    """
    get the data from the individual models
    plot the multimodel mean and the individual models on the same 
    figure
    """
    
    # get a mean value of all the cubes which are within +/- 2deg
    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + model + '/' + exptname.get(timeperiod) +
                    '.' + FIELDNAME + '.allmean.nc')

        mod_cube = get_data(filename, FIELDNAME, model)
        mod_cube_2deg = get_within_pm2deg(mod_cube)

        if i == 0:
            tempcube_orig = mod_cube_2deg.copy(data=mod_cube_2deg.data)
        else:
            tempcube_orig.data = mod_cube_2deg.data
        tempcube = plotmean_newaxis(tempcube_orig, i)
        cubelist.append(tempcube)


    twodeg_cube = cubelist.concatenate_cube()
    any_in_range_cube = twodeg_cube.collapsed(['model_level_number'],
                                              iris.analysis.MEAN)


# now we want to plot in blue if multimodel mean is in the range and
# in red if only some of the models are in range

    data_mmm = permafrost_mean_cube.data
    data_any = any_in_range_cube.data
    ysize, xsize = np.shape(data_mmm)
    data_both = np.zeros(np.shape(data_mmm))
    for i in range(0, xsize):
        for j in range(0, ysize):
            if np.ma.is_masked(data_mmm[j, i]):
                data_both[j, i] = data_any[j, i]
            else:
                data_both[j, i] = -100.

    all_mean_cube = any_in_range_cube.copy(data=data_both)

    plioobj = Plotalldata(all_mean_cube, timeperiod, 'TEMP +/- 2DEG',
                          '+/- 2deg: any (red) MMM (blue)',
                          OUTSTART + timeperiod +'_within_2deg')
    plioobj.plotdata()
    
    
    
def main_time(timeperiod):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    toplot: y if we want to plot, n if we don't
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = ('/nfs/hera1/earjcti/regridded/'
                + FIELDNAME + '_multimodelmean.nc')

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')

    permafrost_mean_cube = get_within_pm2deg(mean_cube)

# now we need to get all the models and see if any of the values
# are within +/-degC
    if len(MODELNAMES) > 1:
        main_model_ind()


    return permafrost_mean_cube

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    if len(TIMEPERIODS) ==1 : # just do main_timeperiod
        dummy = main_time(TIMEPERIODS[0])
        
    if len(TIMEPERIODS) == 2:  # plotting two timeperiods on same figure
        cube1 = main_time(TIMEPERIODS[0])
        cube2 = main_time(TIMEPERIODS[1])
        
        data1 = cube1.data
        data2 = cube2.data
        ysize, xsize = np.shape(data1)
        databoth = np.zeros((ysize, xsize))
      
        for i in range(0, xsize):
            for j in range(0, ysize):
                if np.ma.is_masked(data1[j, i]):
                    pass
                else:
                    data1[j, i] = 10
                    databoth[j, i] = databoth[j, i] + data1[j, i]
                if np.ma.is_masked(data2[j, i]):
                    pass
                else:
                    data2[j, i] = 20
                    databoth[j, i] = databoth[j, i] + data2[j, i]
       
        databoth = np.where(databoth == 0, np.nan, databoth) 
        print(databoth[:,0])

        cubeboth = cube1.copy(data=databoth)
        
        plotobj = Plotalldata(cubeboth, '', 'MMM',
                              '+/- 2deg: mPWP (green), PI (orange), both (blue)',
                              OUTSTART + 'MMM_within_2deg')
        plotobj.plotdata()
        plt.show()
        
        

# variable definition
FILESTART = ('/nfs/hera1/earjcti/regridded/')
FIELDNAME = 'NearSurfaceTemperature'
OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
UNITS = 'deg C'
#TIMEPERIODS = ['pi']
#TIMEPERIODS = ['mPWP']
TIMEPERIODS = ['pi','mPWP']

if len(TIMEPERIODS) > 1:
    MODELNAMES = []
else:
    MODELNAMES = ['CESM1.0.5', 'COSMOS', 'EC-Earth3.1', 'GISS', 'HadCM3',
                  'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
                  'MIROC4m', 'MRI-CGCM2.3',
                  'NorESM-L', 'NorESM1-F',
                  'UofT'
                  ]

main()
::::::::::::::
temperature_isotherms.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 12 16:43:50 2019

@author: earjcti

This will plot the multimodel mean near surface air temperature with
isotherms (at a given level) marked out.  It is to understand which regions
are likely to support thermofrost

"""

import sys
import os
import iris
import numpy as np
import matplotlib.pyplot as plt

#os.environ['PROJ_LIB'] = 'C:/Users/julia/Miniconda3/envs/py3/Library/share'
os.environ['PROJ_LIB'] = '/nfs/see-fs-02_users/earjcti/anaconda2/envs/py3/share/proj'
from mpl_toolkits.basemap import Basemap, shiftgrid


def get_data(filereq, field, modeluse):
    """
    gets the field (field) from the file (filereq) and loads it
    into an iris cube (the model name is in modeluse)
    outputs a cube of the data that is as simple as possible
    """

    if modeluse == 'MMM':
        cube = iris.load_cube(filereq, field)
    else:
        cubes = iris.load(filereq)
        cube = cubes[0]
    cube.data = cube.data.astype('float32')

    if (modeluse == 'MIROC4m' or modeluse == 'COSMOS'):
        cube.units = 'Celsius'
    else:
        cube.convert_units('Celsius')

    for coord in cube.coords():
        name = coord.standard_name
        if name != 'latitude' and name != 'longitude':
            if name is None:
                if coord.long_name is None:
                    cube.remove_coord(coord.var_name)
                else:
                    cube.remove_coord(coord.long_name)
            else:
                cube.remove_coord(name)

    cube.cell_methods = None

    return cube

def plotmean_newaxis(cube, modelno_):
    """
    this will add a new axis to the cube which contains the model number
    this is needed for concatenation
    """

    tempcube_ = iris.util.new_axis(cube)
    tempcube_.add_dim_coord(iris.coords.DimCoord(modelno_,
                                                 standard_name='model_level_number',
                                                 long_name='model',
                                                 var_name='model',
                                                 units=None,
                                                 bounds=None,
                                                 coord_system=None,
                                                 circular=False), 0)
    return tempcube_

class Plotalldata:
    """
    This will plot the data from the timeperiod (ie mpwp or pi)
    """
    def __init__(self, cubelist, timeperiod_, fieldname, modelname,
                 outname):

        """
        inputs are:
                    modelname - like 'HadCM3' or 'MMM' multimodelmean
                    cube - a single cube containing the data
        """


        self.titlename = timeperiod_ + ' ' + modelname
        self.cubelist = cubelist
        self.outname = outname

        if fieldname == 'NearSurfaceTemperature':
            self.valmin = -30.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'RdBu_r'
            self.use_cbar = 'y'
            
        if fieldname == 'MMM':
            self.valmin = 0.
            self.valmax = 30.
            self.diff = 5.
            self.colormap = 'gist_ncar_r'
            self.use_cbar = 'n'

     


    def plotdata(self):
        """
        this will plot all the cubes to a .eps or .png file
        input anom_cubes : a list of cubes containing the anomalies from the mean
        """

        cube = self.cubelist[0]
        latitudes = cube.coord('latitude').points
        lon = cube.coord('longitude').points

        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 30., 180.)
        plt.savefig(self.outname + '_Asia.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_Asia.png', bbox_inches='tight')
        plt.close()
        
        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 180., 360.)
        plt.savefig(self.outname + '_America.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_America.png', bbox_inches='tight')
        plt.close()
        
        plt.subplot(1,1,1)
        self.plotmap(lon, latitudes, 0., 360.)
        plt.savefig(self.outname + '_Globe.eps', bbox_inches='tight')
        plt.savefig(self.outname + '_Globe.png', bbox_inches='tight')
        plt.close()


        return

    def plotmap(self, longitudes, latitudes, left, right):
        """
        will plot the data in a map format

        """

        lons, lats = np.meshgrid(longitudes, latitudes)

        map = Basemap(llcrnrlon=left, urcrnrlon=right,
                      llcrnrlat=0.0, urcrnrlat=90.0,
                      projection='cyl', resolution='l')

        #map.drawmapboundary
        lonmap, latmap = map(lons, lats)
        map.drawcoastlines()

        for i, cube in enumerate(self.cubelist):
            cubedata = cube.data
            contourplot = map.contour(lonmap, latmap, cube.data, 
                                      ISOTHERM_NEEDED,
                                   colors=COLORUSE[i], linewidths=1.5,
                                   linestyles='solid')

        plt.title(self.titlename)
    



def main_model_ind():
    """
    get the data from the individual models
    plot the multimodel mean and the individual models on the same 
    figure
    """
    
    # get a mean value of all the cubes which are within +/- 2deg
    cubelist = iris.cube.CubeList([])
    for i, model in enumerate(MODELNAMES):
        filename = (FILESTART + model + '/' + exptname.get(timeperiod) +
                    '.' + FIELDNAME + '.allmean.nc')

        mod_cube = get_data(filename, FIELDNAME, model)
        mod_cube_2deg = get_within_pm2deg(mod_cube)

        if i == 0:
            tempcube_orig = mod_cube_2deg.copy(data=mod_cube_2deg.data)
        else:
            tempcube_orig.data = mod_cube_2deg.data
        tempcube = plotmean_newaxis(tempcube_orig, i)
        cubelist.append(tempcube)


    twodeg_cube = cubelist.concatenate_cube()
    any_in_range_cube = twodeg_cube.collapsed(['model_level_number'],
                                              iris.analysis.MEAN)


# now we want to plot in blue if multimodel mean is in the range and
# in red if only some of the models are in range

    data_mmm = permafrost_mean_cube.data
    data_any = any_in_range_cube.data
    ysize, xsize = np.shape(data_mmm)
    data_both = np.zeros(np.shape(data_mmm))
    for i in range(0, xsize):
        for j in range(0, ysize):
            if np.ma.is_masked(data_mmm[j, i]):
                data_both[j, i] = data_any[j, i]
            else:
                data_both[j, i] = -100.

    all_mean_cube = any_in_range_cube.copy(data=data_both)

    plioobj = Plotalldata(all_mean_cube, timeperiod, 'TEMP +/- 2DEG',
                          '+/- 2deg: any (red) MMM (blue)',
                          OUTSTART + timeperiod +'_within_2deg')
    plioobj.plotdata()
    
    
    
def main_time(timeperiod):
    """
    the main routine for a single timeperiod (likely mPWP or PI)
    toplot: y if we want to plot, n if we don't
    """

    exptname = {"pi" : "E280",
                "mPWP" : "EOI400"}

    filename = (FILESTART  + FIELDNAME + '_multimodelmean.nc')
    print(filename)

    mean_cube = get_data(filename, FIELDNAME + 'mean_' + timeperiod, 
                         'MMM')

# now we need to get all the models and see if any of the values
# are within +/-degC
    #if len(MODELNAMES) > 1:
    #    main_model_ind()


    return mean_cube

def main():
    """
    the main routine that will split the timeperiod up if appropriate
    if there are two timeperiods it will plot them on the same figure
    """
    
    cubelist_times = iris.cube.CubeList([])
    if len(TIMEPERIODS) ==1 : # just do main_timeperiod
        cube = main_time(TIMEPERIODS[0])
        cubelist_times.append(cube)
        
    if len(TIMEPERIODS) == 2:  # plotting two timeperiods on same figure
    
        cube1 = main_time(TIMEPERIODS[0])
        cubelist_times.append(cube1)
        cube2 = main_time(TIMEPERIODS[1])
        cubelist_times.append(cube2)
        
        data1 = cube1.data
        data2 = cube2.data
        
        
       
    # plot isotherm contour
        title = np.str(ISOTHERM_NEEDED) + 'deg: mPWP (red), PI (blue)'
        plotobj = Plotalldata(cubelist_times, '', 'MMM',
                          title,
                          OUTSTART + 'MMM_' + np.str(ISOTHERM_NEEDED) + 'deg')
        plotobj.plotdata()
       
        

# variable definition
LINUX_WIN = 'l'
ISOTHERM_NEEDED = 0.0

if LINUX_WIN == 'l':
    FILESTART = '/nfs/hera1/earjcti/regridded/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\regridded\\'
    
FIELDNAME = 'NearSurfaceTemperature'
OUTSTART = FILESTART + 'allplots/' + FIELDNAME + '/'
UNITS = 'deg C'
#TIMEPERIODS = ['pi']
#TIMEPERIODS = ['mPWP']
TIMEPERIODS = ['pi','mPWP']
COLORUSE = ['blue','red']

#if len(TIMEPERIODS) > 1:
#    MODELNAMES = []
#else:
#    MODELNAMES = ['CESM1.0.5', 'COSMOS', 'EC-Earth3.1', 'GISS', 'HadCM3',
#                  'IPSLCM6A', 'IPSLCM5A2', 'IPSLCM5A',
#                  'MIROC4m', 'MRI-CGCM2.3',
#                  'NorESM-L', 'NorESM1-F',
#                  'UofT'
#                  ]

main()
::::::::::::::
temporary_assess_land_sea.py
::::::::::::::
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

"""
Created on Fri Sep 18 10:42:28 2020

IPCC were not happy with the way that we had done the land sea contrast in the paper.
This program will calculate it based on the individual models land sea mask.
This superceeds extract_ipcc_data.py

@author: julia
"""


import numpy as np
import pandas as pd
import iris
import iris.quickplot as qplt
import matplotlib.pyplot as plt
from netCDF4 import Dataset
import iris.analysis.cartography
import iris.coord_categorisation
import cf_units as unit
from iris.experimental.equalise_cubes import equalise_attributes
import sys
#import os
def reduce_years(cube100yr):
    """
    this will supply a cube with 100 years of data
    we reduce this to 50 years
    """
    
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube100yr.slices(['latitude', 'longitude'])):
        if i >= 600:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube50yr = cubelist.concatenate_cube()
    
    return cube50yr

###########################
def get_land_sea_mask():
    """
    the land mask is where the land_frac = 100% in both pliocene & pi
    the sea mask is where the sea_frac = 100% in both pliocene & pi
    returns land_mask and sea_mask as a cube
    """

    def get_ipsl_lsm(file, fieldnames):
        # get's the ipsl lsm which is sum of terrestrial and land ice
        cubes = iris.load(file, fieldnames)
        cube = cubes[0] + cubes[1]
        lsm_cube = cube.collapsed('time_counter', iris.analysis.MEAN)
        return lsm_cube

    def change_to_2d(cube):
        # if cube is 3d then extract the first time dimension only
        if cube.ndim == 2:
            cube_2d = cube
        else:
            cube_2d = cube[0, :, :]
       
        return cube_2d

    ############################################
    if MODELNAME == 'IPSLCM5A' or MODELNAME == 'IPSLCM5A2':
        plio_lsm_cube = get_ipsl_lsm(LSM_PLIO, FIELDLSM)
        pi_lsm_cube = get_ipsl_lsm(LSM_PI, FIELDLSM)
    else:
        plio_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PI, FIELDLSM))
        pi_lsm_cube = iris.util.squeeze(iris.load_cube(LSM_PLIO, FIELDLSM))
   
    plio_lsm_cube2 = change_to_2d(plio_lsm_cube)
    pi_lsm_cube2 = change_to_2d(pi_lsm_cube)
   
    plio_lsm_data = plio_lsm_cube2.data
    pi_lsm_data = pi_lsm_cube2.data

    if MODELNAME == 'IPSLCM6A':
        plio_lsm_data = plio_lsm_data / 100.0
        pi_lsm_data = pi_lsm_data / 100.0

    if MODELNAME == 'EC-Earth3.3':
        plio_lsm_data = np.where(plio_lsm_data > 0.5, 1.0, 0.0)
        pi_lsm_data = np.where(pi_lsm_data > 0.5, 1.0, 0.0)
  
    land_mask = np.zeros(np.shape(plio_lsm_data))
    sea_mask = np.zeros(np.shape(plio_lsm_data))

    for ix, plio_mask in np.ndenumerate(plio_lsm_data):
        if plio_mask == 1.0 and pi_lsm_data[ix] == 1.0:
            land_mask[ix] = 1.0
        #if pi_lsm_data[ix] > 0:
        #    land_mask[ix] = 1.0
        if plio_mask == 0.0 and pi_lsm_data[ix] == 0.0:
            sea_mask[ix] = 1.0

    land_cube = plio_lsm_cube2.copy(data=land_mask)
    land_cube.var_name = 'land_mask'
    land_cube.long_name = 'land_mask'
    print('getting sea mask')
    sea_cube = pi_lsm_cube2.copy(data=sea_mask)
    sea_cube.var_name = 'sea_mask'
    sea_cube.long_name = 'sea_mask'
    print('got sea mask')

    return land_cube, sea_cube

def get_hadcm3_data(filestart):
    """
    gets the nsat data from HadCM3 and MRI
    called by get_nsat_data
    """
    allcubes = iris.cube.CubeList([])
    startyear = 0
    endyear = 100
    if MODELNAME  == 'MRI-CGCM2.3':
        startyear = startyear+1
        endyear = endyear+1

    for i in range(startyear, endyear):
        yearuse = str(i).zfill(3)
        filenameuse = (filestart + yearuse + '.nc')
        cubetemp = iris.load_cube(filenameuse)

        u  =  unit.Unit('days since 0800-01-01 00:00:00',
                                  calendar = unit.CALENDAR_360_DAY)
        if MODELNAME  == 'HadCM3':
            cubetemp.coord('t').rename('time')
        cubetemp.coord('time').points = (np.arange(0, 12)+((i-startyear)*12))*30.

        cubetemp.coord('time').units = u

        allcubes.append(cubetemp)


    equalise_attributes(allcubes)
    cube_temp = allcubes.concatenate_cube()

    if MODELNAME  == 'MRI-CGCM2.3':
        cube_temp.coord('pressure level').rename('surface')
  
    if MODELNAME  == 'HadCM3':
        cube_temp.coord('ht').rename('surface')

    cube_temp.coord('surface').points = 0.
    cube  =  cube_temp.extract(iris.Constraint(surface = 0.))

    return cube

def get_ipslcm6a_data(file):
    """
    for ipslcm6a we have 200 years in the file.  but we only need 100 years
    """
    cube = iris.load_cube(file, FIELDNAME)
    # reduce number of years
    cubelist = iris.cube.CubeList([])
    for i,  t_slice in enumerate(cube.slices(['latitude', 'longitude'])):
        if i >= 1200:
            t_slice.coord('time').bounds = None
            t_slice2 = iris.util.new_axis(t_slice, 'time')
            cubelist.append(t_slice2)

    cube100yr = cubelist.concatenate_cube()
  
    return cube100yr

def get_ipsl5_data(filename, exptname):
    """
    gets nsat data from ipsl
    there is a bit of an error in the file calendar so we will
    """
# copy the data to a new file but without the error
    with Dataset(filename) as src,  Dataset("temporary.nc",  "w", format = 'NETCDF3_CLASSIC') as dst:
        # copy attributes
        for name in src.ncattrs():
            dst.setncattr(name,  src.getncattr(name))
        # copy dimensions
        #for name,  dimension in src.dimensions.iteritems():
        for name,  dimension in src.dimensions.items():

            if name !=  'tbnds':   # don't copy across time counter bounds
                dst.createDimension(name,  (len(dimension)))

        # copy all file data
        for name,  variable in src.variables.items():
            if name != 'time_counter_bnds' and name!= 'time_centered':
                x  =  dst.createVariable(name,  variable.datatype,
                                       variable.dimensions)
                if name  == 'time_counter':
                    # convert from seconds to days and start at middle of month
                    dst.variables[name][:]  =  (src.variables[name][:] / (60.*60.*24))-(src.variables[name][0] / (60.*60.*24))+15.
                else:
                    dst.variables[name][:]  =  src.variables[name][:]
                # copy attributes for this variable
                for ncattr in src.variables[name].ncattrs():
                    attribute = src.variables[name].getncattr(ncattr)
                    if ncattr  == 'calendar' and exptname  == 'Eoi400':
                        dst.variables[name].setncattr(ncattr, '360_day')
                    else:
                        if (ncattr  == 'units' and name  == 'time_counter'):
                    # change units from seconds to days
                            dst.variables[name].setncattr(ncattr, attribute.replace('seconds', 'days'))
                        elif ncattr !='_FillValue':
                            dst.variables[name].setncattr(ncattr, attribute)

        fieldreq = 'Temperature 2m'

        cube = iris.load_cube('temporary.nc', fieldreq)

        cube.convert_units('Celsius')

        if exptname  == 'Eoi400':
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                              calendar = unit.CALENDAR_360_DAY)
        else:
            u  =  unit.Unit('days since 0800-01-01 00:00:00',
                          calendar = unit.CALENDAR_365_DAY)
        cube.coord('time').units = u

        return(cube)

def get_ipslcm5a2_data(filename):
    """
    gets the data for ipslcm5a2
    and removes all auxillary coordinates
    """
    cubelist = iris.cube.CubeList([])
    cube = iris.load_cube(filename, FIELDNAME)
    for coord in cube.aux_coords:
        coord.rename('toremove')
        cube.remove_coord('toremove')
    return cube

def get_giss_data(filenames):
    """
    gets giss data: this is in two files
    """ 
    allcubes = iris.cube.CubeList([])
    for file in filenames:
        cubetemp = iris.load_cube(file, FIELDNAME)
        allcubes.append(cubetemp)

    equalise_attributes(allcubes)

    cube = allcubes.concatenate_cube()
  
    return(cube)
    
def get_nsat_data():
    """
    get the average temperature from the pliocene and the preindustrial
    """
        

    if MODELNAME == 'HadCM3' or MODELNAME == 'MRI-CGCM2.3':
        cube_plio = get_hadcm3_data(FILENAME_PLIO)
        cube_pi = get_hadcm3_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM6A':
        cube_plio = get_ipslcm6a_data(FILENAME_PLIO)
        cube_pi = get_ipslcm6a_data(FILENAME_PI)
    elif MODELNAME == 'IPSLCM5A':
        cube_pi = iris.load(FILENAME_PI)[0]
        cube_plio = get_ipsl5_data(FILENAME_PLIO,'Eoi400')
    elif MODELNAME == 'IPSLCM5A2':
        cube_plio = get_ipslcm5a2_data(FILENAME_PLIO)
        cube_pi = get_ipslcm5a2_data(FILENAME_PI)
    elif MODELNAME == 'GISS2.1G':
        cube_plio = get_giss_data(FILENAME_PLIO)
        cube_pi = get_giss_data(FILENAME_PI)
    else:
        cube_plio = iris.load_cube(FILENAME_PLIO, FIELDNAME)
        cube_pi = iris.load_cube(FILENAME_PI, FIELDNAME)
    
   
    cube_plio50 = reduce_years(cube_plio)
    cube_pi50 = reduce_years(cube_pi)

    cube_plio_avg = cube_plio50.collapsed('time', iris.analysis.MEAN)
    cube_pi_avg = cube_pi50.collapsed('time', iris.analysis.MEAN)

    return cube_plio_avg, cube_pi_avg


def get_global_avg(land_cube, sea_cube, cube_nsat):
    """
    get's global average temperature, and also global avg for
    the land and the ocean
    """


    if cube_nsat.coord('latitude').has_bounds():
        cube_nsat.coord('latitude').bounds
    else:
        cube_nsat.coord('latitude').guess_bounds()

    if cube_nsat.coord('longitude').has_bounds():
        cube_nsat.coord('longitude').bounds
    else:
        cube_nsat.coord('longitude').guess_bounds()

  
    grid_areas = iris.analysis.cartography.area_weights(cube_nsat)
    grid_areas_land = grid_areas * land_cube.data
    grid_areas_sea = grid_areas * sea_cube.data

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas)

    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)

    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15
   
    return avg_temp_data, avg_temp_land, avg_temp_sea, grid_areas


def get_regional_landsea(rmax, rmin, land_cube, sea_cube, cube_nsat,
                         grid_areas_region):
    """
    gets the mean temperature for latitude bands for average and for
    land and sea  
    """

    lats = cube_nsat.coord('latitude').points
    grid_areas_use = grid_areas_region * 1.0
    for j, lat in enumerate(lats):
        if rmin > lat or rmax < lat:
            grid_areas_use[j, :] = 0.0
    grid_areas_land = grid_areas_use * land_cube.data
    grid_areas_sea = grid_areas_use * sea_cube.data
    

    avg_cube = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_use)
   
    avg_cube_land = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_land)
   
    avg_cube_sea = cube_nsat.collapsed(['latitude', 'longitude'],
                              iris.analysis.MEAN, weights=grid_areas_sea)

    avg_temp_data = avg_cube.data
    avg_temp_land = avg_cube_land.data
    avg_temp_sea = avg_cube_sea.data

    if avg_temp_data > 200.:
        avg_temp_data = avg_temp_data - 273.15
        avg_temp_land = avg_temp_land -273.15
        avg_temp_sea = avg_temp_sea -273.15

    return [avg_temp_data, avg_temp_land, avg_temp_sea]
           
                                                       


##############################################
def get_land_sea_contrast():
    """
    get the land sea contrast for this model
    """

    print('moodelname is', MODELNAME)
    print('filename is', FILENAME_PLIO)
    print('lsm is', LSM_PLIO)

    # get land and sea mask
    land_mask_cube, sea_mask_cube = get_land_sea_mask()

    # get temporally averaged nsat data
    print('getting temporally averaged nsat data')
    cube_nsat_plio, cube_nsat_pi = get_nsat_data()
    

    # get land and sea temperatures
    print('getting land sea temperatures')
    (avg_T_plio, 
     avg_land_T_plio, 
     avg_sea_T_plio,
     gridareas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                      cube_nsat_plio)

 

    (avg_T_pi, 
     avg_land_T_pi,
     avg_sea_T_pi,
     grid_areas) = get_global_avg(land_mask_cube, sea_mask_cube,
                                     cube_nsat_pi)

    avg_T_anom = avg_T_plio - avg_T_pi
    avg_T_landanom = avg_land_T_plio - avg_land_T_pi
    avg_T_seaanom = avg_sea_T_plio - avg_sea_T_pi



    regionmax = [90.0, 60.0, 30.0, 0.0, -30.0, -60.0]
    regionmin = [60.0, 30.0, 0.0, -30.0, -60.0, -90.0]
    land_sea_region_pi = np.zeros((3, len(regionmax)))
    land_sea_region_plio = np.zeros((3, len(regionmax)))

    #plt.subplot(2,1,1)
    #V = [0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2]
    #qplt.contourf(cube_nsat_plio - cube_nsat_pi, levels=V)
    #plt.subplot(2,1,2)
    #qplt.contourf((sea_mask_cube + land_mask_cube),  levels=V)
    #plt.show()
    #sys.exit(0)
   


    for i, rmax in enumerate(regionmax):
        land_sea_region_pi[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_pi,
                                                        grid_areas)
        land_sea_region_plio[:, i] = get_regional_landsea(rmax, regionmin[i],
                                                        land_mask_cube,
                                                        sea_mask_cube,
                                                        cube_nsat_plio,
                                                        grid_areas)


    
    
    # regrid the pliocene and preindustrial data onto 1deg grid

    cubegrid = iris.load_cube('/nfs/see-fs-02_users/earjcti/PYTHON/PROGRAMS/CEMAC/PLIOMIP2/one_lev_one_deg.nc')
    regridded_pi = cube_nsat_pi.regrid(cubegrid, iris.analysis.Linear())
   
    regridded_plio = cube_nsat_plio.regrid(cubegrid, iris.analysis.Linear())
    
    
    # get land data
    exptlsm='/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Plio_enh/Plio_enh/Plio_enh_LSM_v1.0.nc'
    cntllsm='/nfs/hera1/earjcti/PlioMIP2_Boundary_conds/Modern_std/Modern_std/Modern_std_LSM_v1.0.nc'
    lsmctl = iris.load_cube(cntllsm)
    lsmexp = iris.load_cube(exptlsm)

    regridded_plio.coord('latitude').guess_bounds()
    regridded_plio.coord('longitude').guess_bounds()

    grid_areas2 = iris.analysis.cartography.area_weights(regridded_plio)
   
    plio_all, plio_land, plio_sea = get_regional_landsea(-30, -60, lsmexp, lsmexp,
                                    regridded_plio, grid_areas2)
   
    pi_all, pi_land, pi_sea = get_regional_landsea(-30, -60, lsmctl, lsmctl,
                                    regridded_pi, grid_areas2)


    print('land 90N-60N using correct_lsm', 
          land_sea_region_plio[1, 5],land_sea_region_pi[1,5],
          land_sea_region_plio[1, 5]-land_sea_region_pi[1,5])

    print('ls2 vals', plio_land, pi_land, plio_land - pi_land)
    
 
    print('lsmexpshape',lsmexp.shape) 
          

    print('land sea mask cube', land_mask_cube.shape)

    land_area2 = lsmexp.collapsed(['latitude', 'longitude'],
                       iris.analysis.SUM, weights=grid_areas2).data

    land_area =  land_mask_cube.collapsed(['latitude', 'longitude'],
                              iris.analysis.SUM, weights=grid_areas).data


    print('ratio is', land_area2 / land_area)

    qplt.contourf(lsmexp)
    plt.show()
    qplt.contourf(land_mask_cube)
    plt.show()

#############################################################################
def getnames():

# this program will get the names of the files and the field for each
# of the model

  
    # get names for each model

    if MODELNAME == 'CESM2':
        file_e280 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                         'f09_g17.CMIP6-piControl.' + 
                         '001.cam.h0.TREFHT.110001-120012.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                           'f09_g17.PMIP4-midPliocene-eoi400.' + 
                           '001.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e21.B1850.' + 
                      'f09_g17.PMIP4-midPliocene-eoi400.001.' + 
                      'cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'COSMOS':
        file_e280 = (FILESTART + 'AWI/COSMOS/E280/E280.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        file_eoi400 = (FILESTART + 'AWI/COSMOS/Eoi400/Eoi400.tas'
                      '_2650-2749_monthly_mean_time_series.nc')
        fielduse =  "2m temperature"
        lsm_e280 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                    "E280_et_al/E280.slf.atm.nc")
        lsm_eoi400 = ("/nfs/hera1/pliomip2/data/AWI/COSMOS/land_sea_masks/" + 
                      "Eoi400_et_al/Eoi400.slf.atm.nc")
        fieldlsm = "SLF"

    if MODELNAME == 'EC-Earth3.3':
        file_e280 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_surface.nc'
        file_eoi400 = FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_surface.nc'
        fielduse = 'Air temperature at 2m'
        lsm_e280 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_PI_LSM.nc'
        lsm_eoi400 =  FILESTART + 'EC-Earth3.3/EC-Earth3.3_mPlio_LSM.nc'
        fieldlsm = 'Land/sea mask'

    if MODELNAME == 'CESM1.2':
        file_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0701.0800.nc')
        file_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.cam.h0.TREFHT.1101.1200.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0701.0800.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b.e12.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.cam.h0.LANDFRAC.1101.1200.nc')
        fieldlsm = 'Fraction of sfc area covered by land'       
  
    if MODELNAME   ==  'MIROC4m':
        file_e280 = (FILESTART + 'MIROC4m/tas/MIROC4m_E280_Amon_tas.nc')
        file_eoi400 = (FILESTART + 'MIROC4m/tas/MIROC4m_Eoi400_Amon_tas.nc')
        fielduse = "tas"
        lsm_e280 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Exxx_fx_sftlf.nc')
        lsm_eoi400 = (FILESTART + 'MIROC4m/sftlf/MIROC4m_Eoixxx_fx_sftlf.nc')
        fieldlsm = "sftlf"

    if MODELNAME  == 'HadCM3':
        file_e280 = (FILESTART+'LEEDS/HadCM3/e280/NearSurfaceTemperature/' + 
                     'e280.NearSurfaceTemperature.')
        file_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/NearSurfaceTemperature/' + 
                     'eoi400.NearSurfaceTemperature.')
        fielduse = "TEMPERATURE AT 1.5M"
        lsm_e280 = (FILESTART+'LEEDS/HadCM3/e280/qrparm.mask.nc')
        lsm_eoi400 = (FILESTART+'LEEDS/HadCM3/eoi400/P4_enh_qrparm.mask.nc')
        fieldlsm = 'LAND MASK (LOGICAL: LAND=TRUE)'

    if MODELNAME == 'CCSM4':
        file_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                         'f09_g16.preind.cam.h0.TREFHT.0081.0180.nc')
        file_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                       'f09_g16.PMIP4-pliomip2.TREFHT.1001.1100.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'NCAR/b40.B1850.' + 
                    'f09_g16.preind.cam.h0.LANDFRAC.0081.0180.nc')
        lsm_eoi400 = (FILESTART + 'NCAR/b40.B1850.' + 
                      'f09_g16.PMIP4-pliomip2.LANDFRAC.1001.1100.nc')
        fieldlsm = 'Fraction of sfc area covered by land'

    if MODELNAME == 'CCSM4_Utr':
        file_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                     'tas_Amon_CESM1.0.5_E280_r1i1p1f1_gn_275001-285012.nc')  
        file_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                       'tas_Amon_CESM1.0.5_Eoi400_r1i1p1f1_gn_190001-200012.nc')
        fielduse = 'Reference height temperature'
        lsm_e280 = (FILESTART + 'Utrecht/CESM1.0.5/E280/' + 
                    'land_sea_mask_Amon_CESM1.0.5_b.PI_1pic_f19g16_NESSC' + 
                    '_control_r1i1p1f1_gn.nc')
        lsm_eoi400 = (FILESTART + 'Utrecht/CESM1.0.5/Eoi400/' +
                      'land_sea_mask_Amon_CESM1.0.5_b.PLIO_5Ma_Eoi400_' + 
                      'f19g16_NESSC_control_r1i1p1f1_gn.nc')
        fieldlsm = 'LANDMASK[D=1]'
  
    if MODELNAME == 'CCSM4_UoT':
        start = FILESTART + 'UofT/UofT-CCSM4/'
        file_e280 = (start + '/E280/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_piControl_r1i1p1f1_gn_150101-160012.nc')  
        file_eoi400 = (start + '/Eoi400/Amon/native_grid/tas_Amon_' + 
                     'UofT-CCSM4_midPliocene-eoi400_r1i1p1f1_gn_' + 
                       '160101-170012.nc') 
        fielduse = 'air_temperature'
        lsm_e280 = start + 'for_julia/E_mask.nc'
        lsm_eoi400 = start + 'for_julia/Eoi_mask.nc'
        fieldlsm = 'gridbox land fraction'
      
    if MODELNAME == 'NorESM-L':
       file_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_TREFHT.nc')
       file_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_TREFHT.nc')
       fielduse = 'Reference height temperature'
       lsm_e280 = (FILESTART + 'NorESM-L/NorESM-L_E280_land_sea_mask.nc')
       lsm_eoi400 = (FILESTART + 'NorESM-L/NorESM-L_Eoi400_land_sea_mask.nc')
       fieldlsm = 'Fraction of sfc area covered by land'


    if MODELNAME  == 'MRI-CGCM2.3':
        file_e280 = (FILESTART + 'MRI-CGCM2.3/tas/e280.tas.')
        file_eoi400 = (FILESTART + 'MRI-CGCM2.3/tas/eoi400.tas.')
        fielduse = 'near surface air temperature [degC]'
        lsm_e280 = (FILESTART + 'MRI-CGCM2.3/sftlf.nc')
        lsm_eoi400 = lsm_e280
        fieldlsm = 'landsea mask [0 - 1]'


    if MODELNAME  == 'GISS2.1G':
        start = '/nfs/hera1/earjcti/PLIOMIP2/GISS2.1G/'
        mid = 'e280/tas_Amon_GISS-E2-1-G_piControl_r1i1p1f1_gn_'
        file_e280 = ([start + mid + '490101-495012.nc', 
                      start + mid + '495101-500012.nc'])
        mid = 'eoi400/tas_Amon_GISS-E2-1-G_midPliocene-eoi400_r1i1p1f1_gn_'
        file_eoi400 = ([start + mid + '305101-310012.nc',
                        start + mid + '310101-315012.nc'])
        fielduse = 'air_temperature'
        lsm_e280 = start + 'e280/NASA-GISS_PIctrl_all_fland.nc'
        lsm_eoi400 = start + 'eoi400/NASA-GISS_PlioMIP2_all_fland.nc'
        fieldlsm = 'fland'

    if MODELNAME == 'NorESM1-F':
        file_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_TREFHT.nc'
        file_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_TREFHT.nc'
        lsm_e280 = FILESTART + 'NorESM1-F/NorESM1-F_E280_land_sea_mask.nc'
        lsm_eoi400 = FILESTART + 'NorESM1-F/NorESM1-F_Eoi400_land_sea_mask.nc'
        fielduse = 'Reference height temperature'
        fieldlsm =  'Fraction of sfc area covered by land'

        
    if MODELNAME == 'IPSLCM6A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM6A/'
        file_e280 = start + 'tas_Amon_IPSL-CM6A-LR_piControl_r1i1p1f1_gr_285001-304912.nc'
        file_eoi400 = (start + 'tas_Amon_IPSL-CM6A-LR_midPliocene-eoi400_' + 
                       'r1i1p1f1_gr_185001-204912.nc')
        lsm_e280 = start + 'sftlf_fx_IPSL-CM6A-LR_piControl_r1i1p1f1_gr.nc'
        lsm_eoi400 = start + 'sftlf_fx_IPSL-CM6A-LR_midPliocene-eoi400_r1i1p1f1_gr.nc'
        fielduse = 'air_temperature'
        fieldlsm = 'land_area_fraction'

    if MODELNAME == 'IPSLCM5A':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A/PI.NearSurfaceTemp_tas_3600_3699_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A/Eoi400.NearSurfaceTemp_tas_3581_3680_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Tas'
        fieldlsm = ['Fraction ter', 'Fraction lic']
   

    if MODELNAME == 'IPSLCM5A2':
        start = '/nfs/hera1/earjcti/PLIOMIP2/IPSLCM5A/'
        file_e280 = FILESTART + 'IPSLCM5A2/PI.NearSurfaceTemp_tas_6110_6209_monthly_TS.nc'
        file_eoi400 = (FILESTART + 'IPSLCM5A2/Eoi400.NearSurfaceTemp_tas_3381_3480_monthly_TS.nc')
        lsm_e280 = start + 'E280_LSM_IPSLCM5A.nc'
        lsm_eoi400 = start + 'Eoi400_LSM_IPSLCM5A.nc'
        fielduse = 'Temperature 2m'
        fieldlsm = ['Fraction ter', 'Fraction lic']
            
            
      
    retdata = [fielduse, file_e280, file_eoi400,
               fieldlsm, lsm_e280, lsm_eoi400]
    return(retdata)


##########################################################
# main program

FILENAME  =  ' '
LINUX_WIN  =  'l'
MODELNAME  = "IPSLCM5A" # MIROC4m  COSMOS CCSM4_UoT EC-Earth3.1
                   # HadCM3 MRI-CGCM2.3
                   # IPSLCM5A,  IPSLCM5A2
                   # NorESM1-F NorESM-L
                   # IPSLCM6A GISS2.1G
                   # CCSM4-2deg, CESM1.2
                   # CCSM4
                   # EC-Earth3.3 CESM2 (b.e21)
                  
FIELDNAMEIN = ['tas']

if LINUX_WIN  == 'l':
    FILESTART = '/nfs/hera1/pliomip2/data/'
else:
    FILESTART = 'C:\\Users\\julia\\OneDrive\\WORK\\DATA\\'


# call program to get model dependent names
# fielduse,  and  filename
retdata = getnames()

FIELDNAME = retdata[0]
FILENAME_PI = retdata[1]
FILENAME_PLIO = retdata[2]
FIELDLSM = retdata[3]
LSM_PI = retdata[4]
LSM_PLIO = retdata[5]

print('fieldname is',FIELDNAME)

get_land_sea_contrast()

#sys.exit(0)
